



<!DOCTYPE html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="PyTorch 0.3.0 中文文档">
      
      
        <link rel="canonical" href="http://pytorch.apachecn.org/53/">
      
      
        <meta name="author" content="ApacheCN Team">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.3">
    
    
      
        <title>torch - PyTorch 0.3.0 中文文档</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#ff7043">
      
    
    
      <script src="../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    

    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        }
    });
    </script>
    
    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>
    
    <!-- google webmaster -->
    <meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo" />
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="deep-orange" data-md-color-accent="deep-orange">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="../#torch" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="http://pytorch.apachecn.org" title="PyTorch 0.3.0 中文文档" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                PyTorch 0.3.0 中文文档
              </span>
              <span class="md-header-nav__topic">
                torch
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/apachecn/pytorch-doc-zh/" title="前往 Github 仓库" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      pytorch-doc-zh
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href=".." title="主页" class="md-tabs__link">
          主页
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../0/" title="中文教程" class="md-tabs__link">
          中文教程
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../45/" title="中文文档" class="md-tabs__link md-tabs__link--active">
          中文文档
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="http://pytorch.apachecn.org" title="PyTorch 0.3.0 中文文档" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    PyTorch 0.3.0 中文文档
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/apachecn/pytorch-doc-zh/" title="前往 Github 仓库" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      pytorch-doc-zh
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      主页
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        主页
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href=".." title="主页" class="md-nav__link">
      主页
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      中文教程
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        中文教程
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../0/" title="初学者教程" class="md-nav__link">
      初学者教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../1/" title="PyTorch 深度学习: 60 分钟极速入门教程" class="md-nav__link">
      PyTorch 深度学习: 60 分钟极速入门教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../2/" title="PyTorch 是什么？" class="md-nav__link">
      PyTorch 是什么？
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../3/" title="自动求导: 自动微分" class="md-nav__link">
      自动求导: 自动微分
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../4/" title="神经网络" class="md-nav__link">
      神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../5/" title="训练一个分类器" class="md-nav__link">
      训练一个分类器
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../6/" title="可选: 数据并行" class="md-nav__link">
      可选: 数据并行
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../7/" title="PyTorch for former Torch users" class="md-nav__link">
      PyTorch for former Torch users
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../8/" title="Tensors" class="md-nav__link">
      Tensors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../9/" title="Autograd (自动求导)" class="md-nav__link">
      Autograd (自动求导)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../10/" title="nn package" class="md-nav__link">
      nn package
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../11/" title="Multi-GPU examples" class="md-nav__link">
      Multi-GPU examples
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../12/" title="跟着例子学习 PyTorch" class="md-nav__link">
      跟着例子学习 PyTorch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../13/" title="Warm-up: numpy" class="md-nav__link">
      Warm-up: numpy
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../14/" title="PyTorch: Tensors" class="md-nav__link">
      PyTorch: Tensors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../15/" title="PyTorch: 变量和autograd" class="md-nav__link">
      PyTorch: 变量和autograd
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../16/" title="PyTorch: 定义新的autograd函数" class="md-nav__link">
      PyTorch: 定义新的autograd函数
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../17/" title="TensorFlow: 静态图" class="md-nav__link">
      TensorFlow: 静态图
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../18/" title="PyTorch: nn包" class="md-nav__link">
      PyTorch: nn包
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../19/" title="PyTorch: optim包" class="md-nav__link">
      PyTorch: optim包
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../20/" title="PyTorch: 定制化nn模块" class="md-nav__link">
      PyTorch: 定制化nn模块
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../21/" title="PyTorch: 动态控制流程 + 权重共享" class="md-nav__link">
      PyTorch: 动态控制流程 + 权重共享
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../22/" title="迁移学习教程" class="md-nav__link">
      迁移学习教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../23/" title="数据加载和处理教程" class="md-nav__link">
      数据加载和处理教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../24/" title="针对NLP的Pytorch深度学习" class="md-nav__link">
      针对NLP的Pytorch深度学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../25/" title="PyTorch介绍" class="md-nav__link">
      PyTorch介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../26/" title="PyTorch深度学习" class="md-nav__link">
      PyTorch深度学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../27/" title="词汇嵌入:编码词汇语义" class="md-nav__link">
      词汇嵌入:编码词汇语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../28/" title="序列模型和 LSTM 网络（长短记忆网络）" class="md-nav__link">
      序列模型和 LSTM 网络（长短记忆网络）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../29/" title="高级教程: 作出动态决策和 Bi-LSTM CRF" class="md-nav__link">
      高级教程: 作出动态决策和 Bi-LSTM CRF
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../30/" title="中级教程" class="md-nav__link">
      中级教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../31/" title="用字符级RNN分类名称" class="md-nav__link">
      用字符级RNN分类名称
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../32/" title="基与字符级RNN（Char-RNN）的人名生成" class="md-nav__link">
      基与字符级RNN（Char-RNN）的人名生成
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../33/" title="用基于注意力机制的seq2seq神经网络进行翻译" class="md-nav__link">
      用基于注意力机制的seq2seq神经网络进行翻译
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../34/" title="强化学习（DQN）教程" class="md-nav__link">
      强化学习（DQN）教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../35/" title="Writing Distributed Applications with PyTorch" class="md-nav__link">
      Writing Distributed Applications with PyTorch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../36/" title="空间转换网络 (Spatial Transformer Networks) 教程" class="md-nav__link">
      空间转换网络 (Spatial Transformer Networks) 教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../37/" title="高级教程" class="md-nav__link">
      高级教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../38/" title="用 PyTorch 做 神经转换 (Neural Transfer)" class="md-nav__link">
      用 PyTorch 做 神经转换 (Neural Transfer)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../39/" title="使用 numpy 和 scipy 创建扩展" class="md-nav__link">
      使用 numpy 和 scipy 创建扩展
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../40/" title="使用 ONNX 将模型从 PyTorch 迁移到 Caffe2 和 Mobile" class="md-nav__link">
      使用 ONNX 将模型从 PyTorch 迁移到 Caffe2 和 Mobile
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../41/" title="为 pytorch 自定义 C 扩展" class="md-nav__link">
      为 pytorch 自定义 C 扩展
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../42/" title="项目相关" class="md-nav__link">
      项目相关
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../43/" title="项目贡献者" class="md-nav__link">
      项目贡献者
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../44/" title="组织学习交流群" class="md-nav__link">
      组织学习交流群
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" checked>
    
    <label class="md-nav__link" for="nav-3">
      中文文档
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        中文文档
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../45/" title="介绍" class="md-nav__link">
      介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../46/" title="自动求导机制" class="md-nav__link">
      自动求导机制
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../47/" title="广播语义" class="md-nav__link">
      广播语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../48/" title="CUDA 语义" class="md-nav__link">
      CUDA 语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../49/" title="扩展 PyTorch" class="md-nav__link">
      扩展 PyTorch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../50/" title="多进程的最佳实践" class="md-nav__link">
      多进程的最佳实践
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../51/" title="序列化语义" class="md-nav__link">
      序列化语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../52/" title="Package 参考" class="md-nav__link">
      Package 参考
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        torch
      </label>
    
    <a href="./" title="torch" class="md-nav__link md-nav__link--active">
      torch
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tensors" title="Tensors (张量)" class="md-nav__link">
    Tensors (张量)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#creation-ops" title="Creation Ops (创建操作)" class="md-nav__link">
    Creation Ops (创建操作)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#indexing-slicing-joining-mutating-ops" title="Indexing, Slicing, Joining, Mutating Ops (索引, 切片, 连接, 换位) 操作" class="md-nav__link">
    Indexing, Slicing, Joining, Mutating Ops (索引, 切片, 连接, 换位) 操作
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-sampling" title="Random sampling (随机采样)" class="md-nav__link">
    Random sampling (随机采样)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#in-place-random-sampling" title="In-place random sampling (直接随机采样)" class="md-nav__link">
    In-place random sampling (直接随机采样)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#serialization" title="Serialization (序列化)" class="md-nav__link">
    Serialization (序列化)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#parallelism" title="Parallelism (并行化)" class="md-nav__link">
    Parallelism (并行化)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#math-operations" title="Math operations (数学操作)" class="md-nav__link">
    Math operations (数学操作)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pointwise-ops" title="Pointwise Ops (逐点操作)" class="md-nav__link">
    Pointwise Ops (逐点操作)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reduction-ops" title="Reduction Ops (归约操作)" class="md-nav__link">
    Reduction Ops (归约操作)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison-ops" title="Comparison Ops (比较操作)" class="md-nav__link">
    Comparison Ops (比较操作)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-operations" title="Other Operations (其它操作)" class="md-nav__link">
    Other Operations (其它操作)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#blas-and-lapack-operations-blaslapack" title="BLAS and LAPACK Operations (BLAS和LAPACK操作)" class="md-nav__link">
    BLAS and LAPACK Operations (BLAS和LAPACK操作)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../54/" title="torch.Tensor" class="md-nav__link">
      torch.Tensor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../55/" title="torch.sparse" class="md-nav__link">
      torch.sparse
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../56/" title="torch.Storage" class="md-nav__link">
      torch.Storage
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../57/" title="torch.nn" class="md-nav__link">
      torch.nn
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../58/" title="torch.optim" class="md-nav__link">
      torch.optim
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../59/" title="Automatic differentiation package - torch.autograd" class="md-nav__link">
      Automatic differentiation package - torch.autograd
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../60/" title="Probability distributions - torch.distributions" class="md-nav__link">
      Probability distributions - torch.distributions
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../61/" title="Multiprocessing package - torch.multiprocessing" class="md-nav__link">
      Multiprocessing package - torch.multiprocessing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../62/" title="Distributed communication package - torch.distributed" class="md-nav__link">
      Distributed communication package - torch.distributed
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../63/" title="Legacy package - torch.legacy" class="md-nav__link">
      Legacy package - torch.legacy
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../64/" title="torch.cuda" class="md-nav__link">
      torch.cuda
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../65/" title="torch.utils.ffi" class="md-nav__link">
      torch.utils.ffi
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../66/" title="torch.utils.data" class="md-nav__link">
      torch.utils.data
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../67/" title="torch.utils.model_zoo" class="md-nav__link">
      torch.utils.model_zoo
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../68/" title="torch.onnx" class="md-nav__link">
      torch.onnx
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../69/" title="torchvision 参考" class="md-nav__link">
      torchvision 参考
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../70/" title="torchvision" class="md-nav__link">
      torchvision
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../71/" title="torchvision.datasets" class="md-nav__link">
      torchvision.datasets
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../72/" title="torchvision.models" class="md-nav__link">
      torchvision.models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../73/" title="torchvision.transforms" class="md-nav__link">
      torchvision.transforms
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../74/" title="torchvision.utils" class="md-nav__link">
      torchvision.utils
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../75/" title="项目相关" class="md-nav__link">
      项目相关
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../76/" title="项目贡献者" class="md-nav__link">
      项目贡献者
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../77/" title="组织学习交流群" class="md-nav__link">
      组织学习交流群
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tensors" title="Tensors (张量)" class="md-nav__link">
    Tensors (张量)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#creation-ops" title="Creation Ops (创建操作)" class="md-nav__link">
    Creation Ops (创建操作)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#indexing-slicing-joining-mutating-ops" title="Indexing, Slicing, Joining, Mutating Ops (索引, 切片, 连接, 换位) 操作" class="md-nav__link">
    Indexing, Slicing, Joining, Mutating Ops (索引, 切片, 连接, 换位) 操作
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-sampling" title="Random sampling (随机采样)" class="md-nav__link">
    Random sampling (随机采样)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#in-place-random-sampling" title="In-place random sampling (直接随机采样)" class="md-nav__link">
    In-place random sampling (直接随机采样)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#serialization" title="Serialization (序列化)" class="md-nav__link">
    Serialization (序列化)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#parallelism" title="Parallelism (并行化)" class="md-nav__link">
    Parallelism (并行化)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#math-operations" title="Math operations (数学操作)" class="md-nav__link">
    Math operations (数学操作)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pointwise-ops" title="Pointwise Ops (逐点操作)" class="md-nav__link">
    Pointwise Ops (逐点操作)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reduction-ops" title="Reduction Ops (归约操作)" class="md-nav__link">
    Reduction Ops (归约操作)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison-ops" title="Comparison Ops (比较操作)" class="md-nav__link">
    Comparison Ops (比较操作)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-operations" title="Other Operations (其它操作)" class="md-nav__link">
    Other Operations (其它操作)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#blas-and-lapack-operations-blaslapack" title="BLAS and LAPACK Operations (BLAS和LAPACK操作)" class="md-nav__link">
    BLAS and LAPACK Operations (BLAS和LAPACK操作)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/apachecn/pytorch-doc-zh/edit/master/docs/53.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="torch">torch</h1>
<p>torch package 包含了多维张量的数据结构, 以及基于其上的多种数学操作. 此外, 它还提供了许多用于高效序列化 Tensor 和任意类型的实用工具包, 以及一起其它有用的实用工具包.</p>
<p>它有一个 CUDA 的对应实现, 它使您能够在计算能力 &gt;=0.3 的 NVIDIA GPU 上进行张量运算.</p>
<h2 id="tensors">Tensors (张量)</h2>
<pre class="codehilite"><code class="language-py">torch.is_tensor(obj)</code></pre>


<p>如果 &lt;cite&gt;obj&lt;/cite&gt; 是一个 pytorch tensor, 则返回True.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>obj</strong> (<em>Object</em>) – 用于测试的对象</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">torch.is_storage(obj)</code></pre>


<p>如果 &lt;cite&gt;obj&lt;/cite&gt; 是一个 pytorch storage object, 则返回True.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>obj</strong> (<em>Object</em>) – 用于测试的对象</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">torch.set_default_tensor_type(t)</code></pre>


<pre class="codehilite"><code class="language-py">torch.numel(input) → int</code></pre>


<p>返回 <code>input</code> Tensor 中的元素总数.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入的 &lt;cite&gt;Tensor&lt;/cite&gt;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(1,2,3,4,5)
&gt;&gt;&gt; torch.numel(a)
120
&gt;&gt;&gt; a = torch.zeros(4,4)
&gt;&gt;&gt; torch.numel(a)
16</code></pre>


<pre class="codehilite"><code class="language-py">torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None)</code></pre>


<p>设置打印选项. 从 Numpy 中采集数据</p>
<p>| Parameters: | </p>
<ul>
<li><strong>precision</strong> – 浮点输出精度的位数 (默认值为 8).</li>
<li><strong>threshold</strong> – 触发汇总显示而不是完全显示(repr)的数组元素的总数 (默认值为 1000).</li>
<li><strong>edgeitems</strong> – 每个维度开始和结束时总结的数组项数 (默认值为 3).</li>
<li><strong>linewidth</strong> – 插入换行符的每行字符数 (默认值为 80). Thresholded matricies(阈值矩阵) 将忽略这个参数.</li>
<li><strong>profile</strong> – 用于漂亮格式的打印. 可以用以下任何选项来进行覆盖 (default, short, full)</li>
</ul>
<p>|
| --- | --- |</p>
<h3 id="creation-ops">Creation Ops (创建操作)</h3>
<pre class="codehilite"><code class="language-py">torch.eye(n, m=None, out=None)</code></pre>


<p>返回对角线位置全为1, 其它位置全为0的二维 tensor.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>n</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 行数</li>
<li><strong>m</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>,</em> <em>optional</em>) – 列数. 如果为 None,则默认为 &lt;cite&gt;n&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出 tensor</li>
</ul>
<p>|
| --- | --- |
| Returns: | 一个对角线位置全为1, 其它位置全为0的二维 tensor. |
| --- | --- |
| Return type: | <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> |
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.eye(3)
 1  0  0
 0  1  0
 0  0  1
[torch.FloatTensor of size 3x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.from_numpy(ndarray) → Tensor</code></pre>


<p>从 <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.14)"><code>numpy.ndarray</code></a> 类 创建一个 <a href="tensors.html#torch.Tensor" title="torch.Tensor"><code>Tensor</code></a> 类.</p>
<p>返回 tensor 和 &lt;cite&gt;ndarray&lt;/cite&gt; 共享相同的内存. 对 tensor 的修改将反映在 &lt;cite&gt;ndarray&lt;/cite&gt; 中, 反之亦然. 返回 tensor 不可调整大小.</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = numpy.array([1, 2, 3])
&gt;&gt;&gt; t = torch.from_numpy(a)
&gt;&gt;&gt; t
torch.LongTensor([1, 2, 3])
&gt;&gt;&gt; t[0] = -1
&gt;&gt;&gt; a
array([-1,  2,  3])</code></pre>


<pre class="codehilite"><code class="language-py">torch.linspace(start, end, steps=100, out=None) → Tensor</code></pre>


<p>返回 <code>start</code> 和 <code>end</code> 之间等间隔 <code>steps</code> 点的一维 Tensor.</p>
<p>输出 是尺寸 <code>steps</code> 为一维 tensor</p>
<p>| Parameters: | </p>
<ul>
<li><strong>start</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 点集合的起始值</li>
<li><strong>end</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 点集合的结束值</li>
<li><strong>steps</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 在 <code>start</code> 和 <code>end</code> 之间的样本数</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出结果的 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.linspace(3, 10, steps=5)

 3.0000
 4.7500
 6.5000
 8.2500
 10.0000
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.linspace(-10, 10, steps=5)

-10
 -5
 0
 5
 10
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5)

-10
 -5
 0
 5
 10
[torch.FloatTensor of size 5]</code></pre>


<pre class="codehilite"><code class="language-py">torch.logspace(start, end, steps=100, out=None) → Tensor</code></pre>


<p>返回一个在 <code>\(10^{start}\)</code> 和 <code>\(10^{end}\)</code> 之间的对数间隔 <code>steps</code> 点的一维 Tensor</p>
<p>输出是长度为 <code>steps</code> 的一维 tensor</p>
<p>| Parameters: | </p>
<ul>
<li><strong>start</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 点集合的起始值</li>
<li><strong>end</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 点集合的结束值</li>
<li><strong>steps</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 在 <code>start</code> 和 <code>end</code> 之间的样本数</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出结果<code>Tensor</code></li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5)

 1.0000e-10
 1.0000e-05
 1.0000e+00
 1.0000e+05
 1.0000e+10
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5)

 1.2589
 2.1135
 3.5481
 5.9566
 10.0000
[torch.FloatTensor of size 5]</code></pre>


<pre class="codehilite"><code class="language-py">torch.ones(*sizes, out=None) → Tensor</code></pre>


<p>返回填充了标量值 &lt;cite&gt;1&lt;/cite&gt; 的 Tensor, 其形状由可变参数 <code>sizes</code> 定义.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>sizes</strong> (<em>int...</em>) – 一组定义输出 Tensor 形状的整数</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出结果 Tensor</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.ones(2, 3)

 1  1  1
 1  1  1
[torch.FloatTensor of size 2x3]

&gt;&gt;&gt; torch.ones(5)

 1
 1
 1
 1
 1
[torch.FloatTensor of size 5]</code></pre>


<pre class="codehilite"><code class="language-py">torch.ones_like(input, out=None) → Tensor</code></pre>


<p>返回一个用标量值 &lt;cite&gt;1&lt;/cite&gt; 填充的张量, 大小与 <code>input</code> 相同.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入的大小将决定输出的大小.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出结果 Tensor</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; input = torch.FloatTensor(2, 3)
&gt;&gt;&gt; torch.ones_like(input)

 1  1  1
 1  1  1
[torch.FloatTensor of size 2x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.arange(start=0, end, step=1, out=None) → Tensor</code></pre>


<p>从 &lt;cite&gt;start&lt;/cite&gt; 用步长为 <code>step</code> 开始, 间隔在 <code>[start, end)</code> 中的值返回大小层次为 <code>\(floor((end - start) / step)\)</code> 的一维 Tensor.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>start</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 点集合的起始值</li>
<li><strong>end</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 点集合的结束值</li>
<li><strong>step</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 每对相邻点之间的间隔</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.arange(5)

 0
 1
 2
 3
 4
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.arange(1, 4)

 1
 2
 3
[torch.FloatTensor of size 3]

&gt;&gt;&gt; torch.arange(1, 2.5, 0.5)

 1.0000
 1.5000
 2.0000
[torch.FloatTensor of size 3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.range(start, end, step=1, out=None) → Tensor</code></pre>


<p>返回一个在 <code>start</code> 到 <code>end</code> 并且步长为 <code>step</code> 的区间内, 大小为 <code>\(floor((end - start) / step) + 1\)</code> 为一维 Tensor. <code>step</code> 是 tensor 中两个值之间的差距. <code>\(x_{i+1} = x_i + step\)</code></p>
<p>Warning</p>
<p>此功能已被弃用, 以支持 <a href="#torch.arange" title="torch.arange"><code>torch.arange()</code></a>.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>start</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 点集合的起始值</li>
<li><strong>end</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 点集合的结束值</li>
<li><strong>step</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 每对相邻点之间的间隔</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.range(1, 4)

 1
 2
 3
 4
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.range(1, 4, 0.5)

 1.0000
 1.5000
 2.0000
 2.5000
 3.0000
 3.5000
 4.0000
[torch.FloatTensor of size 7]</code></pre>


<pre class="codehilite"><code class="language-py">torch.zeros(*sizes, out=None) → Tensor</code></pre>


<p>返回填充了标量值为 &lt;cite&gt;0&lt;/cite&gt; 的 Tensor, 其形状由可变参量 <code>sizes</code> 定义.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>sizes</strong> (<em>int...</em>) – 定义输出 Tensor 形状的一组整数.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出结果 Tensor</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.zeros(2, 3)

 0  0  0
 0  0  0
[torch.FloatTensor of size 2x3]

&gt;&gt;&gt; torch.zeros(5)

 0
 0
 0
 0
 0
[torch.FloatTensor of size 5]</code></pre>


<pre class="codehilite"><code class="language-py">torch.zeros_like(input, out=None) → Tensor</code></pre>


<p>返回一个用标量值 &lt;cite&gt;0&lt;/cite&gt; 填充的 Tensor, 其大小与 <code>input</code> 相同.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入的大小将决定输出的大小.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出结果 Tensor</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; input = torch.FloatTensor(2, 3)
&gt;&gt;&gt; torch.zeros_like(input)

 0  0  0
 0  0  0
[torch.FloatTensor of size 2x3]</code></pre>


<h3 id="indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops (索引, 切片, 连接, 换位) 操作</h3>
<pre class="codehilite"><code class="language-py">torch.cat(seq, dim=0, out=None) → Tensor</code></pre>


<p>在给定维度上对输入的张量序列 <code>seq</code> 进行连接操作. 所有张量必须具有相同的形状(在 <a href="#torch.cat" title="torch.cat"><code>cat</code></a> 维度中除外) 或为空.</p>
<p><a href="#torch.cat" title="torch.cat"><code>torch.cat()</code></a> 可以看做是 <a href="#torch.split" title="torch.split"><code>torch.split()</code></a> 和 <a href="#torch.chunk" title="torch.chunk"><code>torch.chunk()</code></a> 的逆操作.</p>
<p><a href="#torch.cat" title="torch.cat"><code>cat()</code></a> 可以通过下面的例子更好地理解.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>seq</strong> (<em>sequence of Tensors</em>) – 可以是任何相同类型的 &lt;cite&gt;Tensor&lt;/cite&gt; 的 Python 序列.</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>,</em> <em>optional</em>) – tensors 级联的维数</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出参数</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x

 0.5983 -0.0341  2.4918
 1.5981 -0.5265 -0.8735
[torch.FloatTensor of size 2x3]

&gt;&gt;&gt; torch.cat((x, x, x), 0)

 0.5983 -0.0341  2.4918
 1.5981 -0.5265 -0.8735
 0.5983 -0.0341  2.4918
 1.5981 -0.5265 -0.8735
 0.5983 -0.0341  2.4918
 1.5981 -0.5265 -0.8735
[torch.FloatTensor of size 6x3]

&gt;&gt;&gt; torch.cat((x, x, x), 1)

 0.5983 -0.0341  2.4918  0.5983 -0.0341  2.4918  0.5983 -0.0341  2.4918
 1.5981 -0.5265 -0.8735  1.5981 -0.5265 -0.8735  1.5981 -0.5265 -0.8735
[torch.FloatTensor of size 2x9]</code></pre>


<pre class="codehilite"><code class="language-py">torch.chunk(tensor, chunks, dim=0)</code></pre>


<p>在给定维度(轴)上将输入张量进行分块处理.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 待分块的输入张量.</li>
<li><strong>chunks</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 要返回的分块的个数.</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 切分张量所需要沿着的维度.</li>
</ul>
<p>|
| --- | --- |</p>
<pre class="codehilite"><code class="language-py">torch.gather(input, dim, index, out=None) → Tensor</code></pre>


<p>沿给定轴 &lt;cite&gt;dim&lt;/cite&gt; ,将输入索引张量 &lt;cite&gt;index&lt;/cite&gt; 指定位置的值进行聚合.</p>
<p>对一个 3 维张量,输出可以定义为:</p>
<pre class="codehilite"><code class="language-py">out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2</code></pre>


<p>如果 <code>input</code> 是 size 为 <code>\((x_0, x_1..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})\)</code> 且 <code>dim</code> = i 的 n 维张量,则 <code>index</code> 必须是具有 size 为 <code>\((x_0, x_1, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})\)</code> 的 n 维张量,其中 y &gt;= 1 ,并且 <code>out</code> 将与 <code>index</code> 的 size 相同.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 源张量</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 索引的轴</li>
<li><strong>index</strong> (<em>LongTensor</em>) – 聚合元素的下标</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 目标张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; t = torch.Tensor([[1,2],[3,4]])
&gt;&gt;&gt; torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]]))
 1  1
 4  3
[torch.FloatTensor of size 2x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.index_select(input, dim, index, out=None) → Tensor</code></pre>


<p>沿着指定维度 <code>dim</code> 对输入进行切片,取 <code>index</code> 中指定的相应项 ( <code>index</code> 为一个 &lt;cite&gt;LongTensor&lt;/cite&gt; ),然后返回到一个新的张量.</p>
<blockquote>
<p>返回的张量与原始张量 &lt;cite&gt;Tensor&lt;/cite&gt; 有相同的维度(在指定轴上).</p>
</blockquote>
<p>Note</p>
<p>返回的张量不与原始张量共享内存空间.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 索引的轴</li>
<li><strong>index</strong> (<em>LongTensor</em>) – 包含索引下标的一维张量</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出参数/目标张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; x

 1.2045  2.4084  0.4001  1.1372
 0.5596  1.5677  0.6219 -0.7954
 1.3635 -1.2313 -0.5414 -1.8478
[torch.FloatTensor of size 3x4]

&gt;&gt;&gt; indices = torch.LongTensor([0, 2])
&gt;&gt;&gt; torch.index_select(x, 0, indices)

 1.2045  2.4084  0.4001  1.1372
 1.3635 -1.2313 -0.5414 -1.8478
[torch.FloatTensor of size 2x4]

&gt;&gt;&gt; torch.index_select(x, 1, indices)

 1.2045  0.4001
 0.5596  0.6219
 1.3635 -0.5414
[torch.FloatTensor of size 3x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.masked_select(input, mask, out=None) → Tensor</code></pre>


<p>根据掩码张量 <code>mask</code> 中的二元值,取输入张量中的指定项 ( <code>mask</code> 为一个 &lt;cite&gt;ByteTensor&lt;/cite&gt; ),将取值返回到一个新的一维张量.</p>
<p>张量 <code>mask</code> 与 <code>input</code> 的 shape 或维度不需要相同,但是他们必须是 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> .</p>
<p>Note</p>
<p>返回的张量不与原始张量共享内存空间.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量</li>
<li><strong>mask</strong> (<a href="tensors.html#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) – 掩码张量,包含了二元索引值</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出参数/目标张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; x

 1.2045  2.4084  0.4001  1.1372
 0.5596  1.5677  0.6219 -0.7954
 1.3635 -1.2313 -0.5414 -1.8478
[torch.FloatTensor of size 3x4]

&gt;&gt;&gt; mask = x.ge(0.5)
&gt;&gt;&gt; mask

 1  1  0  1
 1  1  1  0
 1  0  0  0
[torch.ByteTensor of size 3x4]

&gt;&gt;&gt; torch.masked_select(x, mask)

 1.2045
 2.4084
 1.1372
 0.5596
 1.5677
 0.6219
 1.3635
[torch.FloatTensor of size 7]</code></pre>


<pre class="codehilite"><code class="language-py">torch.nonzero(input, out=None) → LongTensor</code></pre>


<p>返回一个包含输入 <code>input</code> 中非零元素索引的张量. 输出张量中的每行包含 <code>input</code> 中非零元素的索引.</p>
<p>如果输入张量 <code>input</code> 有 &lt;cite&gt;n&lt;/cite&gt; 维,则输出的索引张量 <code>out</code> 的 size 为 &lt;cite&gt;z x n&lt;/cite&gt; , 这里 &lt;cite&gt;z&lt;/cite&gt; 是输入张量 <code>input</code> 中所有非零元素的个数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量/源张量</li>
<li><strong>out</strong> (<em>LongTensor__,</em> <em>optional</em>) – 包含索引值的输出张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.nonzero(torch.Tensor([1, 1, 1, 0, 1]))

 0
 1
 2
 4
[torch.LongTensor of size 4x1]

&gt;&gt;&gt; torch.nonzero(torch.Tensor([[0.6, 0.0, 0.0, 0.0],
...                             [0.0, 0.4, 0.0, 0.0],
...                             [0.0, 0.0, 1.2, 0.0],
...                             [0.0, 0.0, 0.0,-0.4]]))

 0  0
 1  1
 2  2
 3  3
[torch.LongTensor of size 4x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.split(tensor, split_size, dim=0)</code></pre>


<p>将输入张量分割成相等 size 的 chunks (如果可分).</p>
<p>如果沿指定维的张量形状大小不能被 <code>split_size</code> 整分, 则最后一个分块会小于其它分块.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 待分割张量.</li>
<li><strong>split_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 单个分块的 size 大小.</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 沿着此维进行分割.</li>
</ul>
<p>|
| --- | --- |</p>
<pre class="codehilite"><code class="language-py">torch.squeeze(input, dim=None, out=None)</code></pre>


<p>将 <code>input</code> 张量 size 中的 &lt;cite&gt;1&lt;/cite&gt; 去除并返回.</p>
<p>如果 &lt;cite&gt;input&lt;/cite&gt; 的 shape 如 <code>\((A x 1 x B x C x 1 x D)\)</code> ,那么输出 shape 就为: <code>\((A x B x C x D)\)</code></p>
<p>当给定 <code>dim</code> 时,那么挤压操作只在给定维度上.例如, &lt;cite&gt;input&lt;/cite&gt; 的 shape 为: <code>\((A x 1 x B)\)</code> , &lt;cite&gt;squeeze(input, 0)&lt;/cite&gt; 将会保持张量不变,只有用 &lt;cite&gt;squeeze(input, 1)&lt;/cite&gt; , shape 会变成 <code>\((A x B)\)</code> .</p>
<p>Note</p>
<p>作为上述的一个例外,size 为 1 的一维张量不会改变维度.</p>
<p>Note</p>
<p>返回张量与输入张量共享内存,所以改变其中一个的内容会改变另一个.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>,</em> <em>optional</em>) – 如果给定 <code>dim</code> 时,则 &lt;cite&gt;input&lt;/cite&gt; 只会在给定维度执行挤压</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.zeros(2,1,2,1,2)
&gt;&gt;&gt; x.size()
(2L, 1L, 2L, 1L, 2L)
&gt;&gt;&gt; y = torch.squeeze(x)
&gt;&gt;&gt; y.size()
(2L, 2L, 2L)
&gt;&gt;&gt; y = torch.squeeze(x, 0)
&gt;&gt;&gt; y.size()
(2L, 1L, 2L, 1L, 2L)
&gt;&gt;&gt; y = torch.squeeze(x, 1)
&gt;&gt;&gt; y.size()
(2L, 2L, 1L, 2L)</code></pre>


<pre class="codehilite"><code class="language-py">torch.stack(sequence, dim=0, out=None)</code></pre>


<p>沿着一个新维度对输入张量序列进行连接.</p>
<p>序列中所有的张量都应该为相同 size .</p>
<p>| Parameters: | </p>
<ul>
<li><strong>sequence</strong> (<em>Sequence</em>) – 待连接的张量序列.</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 插入的维度.必须介于 0 与待连接的张量序列数（包含）之间.</li>
</ul>
<p>|
| --- | --- |</p>
<pre class="codehilite"><code class="language-py">torch.t(input, out=None) → Tensor</code></pre>


<p>预期 <code>input</code> 为一个矩阵 (2 维张量), 并转置 0, 1 维.</p>
<p>可以被视为函数 &lt;cite&gt;transpose(input, 0, 1)&lt;/cite&gt; 的简写函数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x

 0.4834  0.6907  1.3417
-0.1300  0.5295  0.2321
[torch.FloatTensor of size 2x3]

&gt;&gt;&gt; torch.t(x)

 0.4834 -0.1300
 0.6907  0.5295
 1.3417  0.2321
[torch.FloatTensor of size 3x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.take(input, indices) → Tensor</code></pre>


<p>在给定的索引处返回一个新的 &lt;cite&gt;Tensor&lt;/cite&gt; ,其元素为 <code>input</code> . 输入张量被看作是一维张量.结果与索引具有相同的 shape .</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量</li>
<li><strong>indices</strong> (<em>LongTensor</em>) – 进入 &lt;cite&gt;Tensor&lt;/cite&gt; 的索引</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; src = torch.Tensor([[4, 3, 5],
...                     [6, 7, 8]])
&gt;&gt;&gt; torch.take(src, torch.LongTensor([0, 2, 5]))
 4
 5
 8
[torch.FloatTensor of size 3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.transpose(input, dim0, dim1, out=None) → Tensor</code></pre>


<p>返回输入矩阵 <code>input</code> 的转置.交换给定维度 <code>dim0</code> 和 <code>dim1</code> .</p>
<p><code>out</code> 张量与 <code>input</code> 张量共享内存,所以改变其中一个会导致另外一个也被修改.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量</li>
<li><strong>dim0</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 转置的第一个维度</li>
<li><strong>dim1</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 转置的第二个维度</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x

 0.5983 -0.0341  2.4918
 1.5981 -0.5265 -0.8735
[torch.FloatTensor of size 2x3]

&gt;&gt;&gt; torch.transpose(x, 0, 1)

 0.5983  1.5981
-0.0341 -0.5265
 2.4918 -0.8735
[torch.FloatTensor of size 3x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.unbind(tensor, dim=0)</code></pre>


<p>移除一个张量的维度.</p>
<p>移除指定维后,返回一个元组,包含了沿着指定维切片后的各个切片 (已经没有了移除的维度).</p>
<p>| Parameters: | </p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要执行 unbind 的张量/输入张量.</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 要移除的维度.</li>
</ul>
<p>|
| --- | --- |</p>
<pre class="codehilite"><code class="language-py">torch.unsqueeze(input, dim, out=None)</code></pre>


<p>返回在指定位置插入维度 size 为 1 的新张量.</p>
<p>返回张量与输入张量共享内存,所以改变其中一个的内容会改变另一个.</p>
<p>如果 &lt;cite&gt;dim&lt;/cite&gt; 为负,则将会被转化 <code>\(dim + input.dim() + 1\)</code> .</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 插入维度的索引</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.Tensor([1, 2, 3, 4])
&gt;&gt;&gt; torch.unsqueeze(x, 0)
 1  2  3  4
[torch.FloatTensor of size 1x4]
&gt;&gt;&gt; torch.unsqueeze(x, 1)
 1
 2
 3
 4
[torch.FloatTensor of size 4x1]</code></pre>


<h2 id="random-sampling">Random sampling (随机采样)</h2>
<pre class="codehilite"><code class="language-py">torch.manual_seed(seed)</code></pre>


<p>设置生成随机数的种子,并返回一个 &lt;cite&gt;torch._C.Generator&lt;/cite&gt; 对象.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>seed</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a> <em>or</em> <em>long</em>) – 种子.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">torch.initial_seed()</code></pre>


<p>返回用于生成随机数字的初始种子 (python &lt;cite&gt;long&lt;/cite&gt;) .</p>
<pre class="codehilite"><code class="language-py">torch.get_rng_state()</code></pre>


<p>以ByteTensor的形式返回随机数发生器的状态.</p>
<pre class="codehilite"><code class="language-py">torch.set_rng_state(new_state)</code></pre>


<p>设置随机数发生器的参数.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>new_state</strong> (<a href="tensors.html#torch.ByteTensor" title="torch.ByteTensor"><em>torch.ByteTensor</em></a>) – 理想状态</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><code>torch.``default_generator</code> <em>= <torch._C.Generator object at 0x28bcc10></em></p>
<pre class="codehilite"><code class="language-py">torch.bernoulli(input, out=None) → Tensor</code></pre>


<p>从伯努利分布中抽取二进制随机数 (0 或 1).</p>
<p>The <code>input</code> 张量包含用于抽取二进制随机数的概率. 因此, <code>input</code> 中的所有值必须在这个范围内: <code>\(0 &amp;lt;= input_i &amp;lt;= 1\)</code></p>
<p>根据 <code>input</code> 张量第 &lt;cite&gt;i&lt;/cite&gt; 个概率值, 输出张量的第 &lt;cite&gt;i&lt;/cite&gt; 个元素将取值为1.</p>
<p>返回的 <code>out</code> 张量的值只有 0 或者 1 并且大小与 <code>input</code> 张量相同.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 伯努利分布的概率值</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.Tensor(3, 3).uniform_(0, 1) # generate a uniform random matrix with range [0, 1]
&gt;&gt;&gt; a

 0.7544  0.8140  0.9842
 0.5282  0.0595  0.6445
 0.1925  0.9553  0.9732
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.bernoulli(a)

 1  1  1
 0  0  1
 0  1  1
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1
&gt;&gt;&gt; torch.bernoulli(a)

 1  1  1
 1  1  1
 1  1  1
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0
&gt;&gt;&gt; torch.bernoulli(a)

 0  0  0
 0  0  0
 0  0  0
[torch.FloatTensor of size 3x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.multinomial(input, num_samples, replacement=False, out=None) → LongTensor</code></pre>


<p>返回一个张量, 其中每一行包含在 <code>input</code> 张量对应行中多项式分布取样的 <code>num_samples</code> 索引.</p>
<p>Note</p>
<p><code>input</code> 的每行值不需要总和为 1 (我们只使用这些值作为权重), 但必须是非负且非零和的.</p>
<p>取样时从左向右排列(第一个样本在第一列).</p>
<p>如果 <code>input</code> 是一个向量, 则 <code>out</code> 是一个大小为 &lt;cite&gt;num_samples&lt;/cite&gt; 的向量.</p>
<p>如果 <code>input</code> 是一个 &lt;cite&gt;m&lt;/cite&gt; 行的矩阵, 则 <code>out</code> 是一个 &lt;cite&gt;m × n&lt;/cite&gt; 的矩阵.</p>
<p>如果参数 &lt;cite&gt;replacement&lt;/cite&gt; 是 <code>True</code>, 则可重复取样. 否则, 样本在每行不能被重复取样.</p>
<p>参数 <code>num_samples</code> 必须小于 <code>input</code> 长度 (如果是一个矩阵, 则是 <code>input</code> 的列数).</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 包含概率值的张量</li>
<li><strong>num_samples</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 抽取的样本数</li>
<li><strong>replacement</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>,</em> <em>optional</em>) – 是否重复抽取样本</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; weights = torch.Tensor([0, 10, 3, 0]) # create a Tensor of weights
&gt;&gt;&gt; torch.multinomial(weights, 4)

 1
 2
 0
 0
[torch.LongTensor of size 4]

&gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True)

 1
 2
 1
 2
[torch.LongTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.normal()</code></pre>


<pre class="codehilite"><code class="language-py">torch.normal(means, std, out=None)</code></pre>


<p>返回一个随机数张量, 随机数从给定平均值和标准差的离散正态分布中抽取.</p>
<p>参数 <code>means</code> 是一个包含每个输出元素的正态分布均值的张量.</p>
<p>参数 <a href="#torch.std" title="torch.std"><code>std</code></a> 是一个包含每个输出元素的正态分布标准差的张量.</p>
<p>其中 <code>means</code> 和 <a href="#torch.std" title="torch.std"><code>std</code></a> 的形状不需要匹配, 但是每个张量中的元素总数需要相同.</p>
<p>Note</p>
<p>当形状不匹配时, <code>means</code> 的形状将作为返回输出张量的形状.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>means</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 均值</li>
<li><strong>std</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 标准差</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))

 1.5104
 1.6955
 2.4895
 4.9185
 4.9895
 6.9155
 7.3683
 8.1836
 8.7164
 9.8916
[torch.FloatTensor of size 10]</code></pre>


<pre class="codehilite"><code class="language-py">torch.normal(mean=0.0, std, out=None)</code></pre>


<p>功能与上面函数类似, 但所有被抽取的元素共享均值.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>means</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>,</em> <em>optional</em>) – 所有分布的均值</li>
<li><strong>std</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 每个元素标准差的张量</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1, 6))

 0.5723
 0.0871
 -0.3783
 -2.5689
 10.7893
[torch.FloatTensor of size 5]</code></pre>


<pre class="codehilite"><code class="language-py">torch.normal(means, std=1.0, out=None)</code></pre>


<p>功能与上面函数类似, 但所有被抽取的元素共享标准差.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>means</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 每个元素均值的张量</li>
<li><strong>std</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>,</em> <em>optional</em>) – 所有分布的标准差</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.normal(means=torch.arange(1, 6))

 1.1681
 2.8884
 3.7718
 2.5616
 4.2500
[torch.FloatTensor of size 5]</code></pre>


<pre class="codehilite"><code class="language-py">torch.rand(*sizes, out=None) → Tensor</code></pre>


<p>在区间 <code>\([0, 1)\)</code> 中, 返回一个填充了均匀分布的随机数的张量.</p>
<p>这个张量的形状由可变参数 <code>sizes</code> 来定义.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>sizes</strong> (<em>int...</em>) – 定义输出张量形状的整数集.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.rand(4)

 0.9193
 0.3347
 0.3232
 0.7715
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.rand(2, 3)

 0.5010  0.5140  0.0719
 0.1435  0.5636  0.0538
[torch.FloatTensor of size 2x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.randn(*sizes, out=None) → Tensor</code></pre>


<p>返回一个从正态分布中填充随机数的张量, 其均值为 0 , 方差为 1 .</p>
<p>这个张量的形状被可变参数 <code>sizes</code> 定义.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>sizes</strong> (<em>int...</em>) – 定义输出张量形状的整数集.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.randn(4)

-0.1145
 0.0094
-1.1717
 0.9846
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.randn(2, 3)

 1.4339  0.3351 -1.0999
 1.5458 -0.9643 -0.3558
[torch.FloatTensor of size 2x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.randperm(n, out=None) → LongTensor</code></pre>


<p>返回一个从 <code>0</code> to <code>n - 1</code> 的整数的随机排列.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>n</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 上限 (唯一的)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.randperm(4)

 2
 1
 3
 0
[torch.LongTensor of size 4]</code></pre>


<h3 id="in-place-random-sampling">In-place random sampling (直接随机采样)</h3>
<p>在Tensors模块上还定义了许多 in-place 随机采样函数,可以点击参考它们的文档:</p>
<ul>
<li><a href="tensors.html#torch.Tensor.bernoulli_" title="torch.Tensor.bernoulli_"><code>torch.Tensor.bernoulli_()</code></a> - 是 <a href="#torch.bernoulli" title="torch.bernoulli"><code>torch.bernoulli()</code></a> 的 in-place 版本</li>
<li><a href="tensors.html#torch.Tensor.cauchy_" title="torch.Tensor.cauchy_"><code>torch.Tensor.cauchy_()</code></a> - 从柯西分布中抽取数字</li>
<li><a href="tensors.html#torch.Tensor.exponential_" title="torch.Tensor.exponential_"><code>torch.Tensor.exponential_()</code></a> - 从指数分布中抽取数字</li>
<li><a href="tensors.html#torch.Tensor.geometric_" title="torch.Tensor.geometric_"><code>torch.Tensor.geometric_()</code></a> - 从几何分布中抽取元素</li>
<li><a href="tensors.html#torch.Tensor.log_normal_" title="torch.Tensor.log_normal_"><code>torch.Tensor.log_normal_()</code></a> - 对数正态分布中的样本</li>
<li><a href="tensors.html#torch.Tensor.normal_" title="torch.Tensor.normal_"><code>torch.Tensor.normal_()</code></a> - 是 <a href="#torch.normal" title="torch.normal"><code>torch.normal()</code></a> 的 in-place 版本</li>
<li><a href="tensors.html#torch.Tensor.random_" title="torch.Tensor.random_"><code>torch.Tensor.random_()</code></a> - 离散均匀分布中采样的数字</li>
<li><a href="tensors.html#torch.Tensor.uniform_" title="torch.Tensor.uniform_"><code>torch.Tensor.uniform_()</code></a> - 正态分布中采样的数字</li>
</ul>
<h2 id="serialization">Serialization (序列化)</h2>
<pre class="codehilite"><code class="language-py">torch.save(obj, f, pickle_module=&lt;module 'cPickle' from '/usr/lib64/python2.7/lib-dynload/cPickle.so'&gt;, pickle_protocol=2)</code></pre>


<p>将一个对象保存到一个磁盘文件中.</p>
<p>另见: <a href="notes/serialization.html#recommend-saving-models">保存模型的推荐方法</a></p>
<p>参数: obj: 要保存的对象 f: 类文件对象 (必须实现返回文件描述符的 fileno 方法) 或包含文件名的字符串 pickle_module: 用于 pickling 元数据和对象的模块 pickle_protocol: 可以指定来覆盖默认协议</p>
<pre class="codehilite"><code class="language-py">torch.load(f, map_location=None, pickle_module=&lt;module 'cPickle' from '/usr/lib64/python2.7/lib-dynload/cPickle.so'&gt;)</code></pre>


<p>从磁盘文件中加载一个用 <a href="#torch.save" title="torch.save"><code>torch.save()</code></a> 保存的对象.</p>
<table>
<thead>
<tr>
<th>Func:</th>
<th>&lt;cite&gt;torch.load&lt;/cite&gt; 使用 Python 的解封 (unpickling) 设施, 但特殊对待张量下的存储 (storages).</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>它们首先在 CPU 上反序列化, 然后移动到所保存的设备上. 如果这个过程失败了 (例如, 因为运行时的系统没有确定的设备), 将会抛出异常. 然而, 使用 map_location 参数, 存储可以被动态地重新映射到另一组设备上.</p>
<p>如果 map_location 是可调用对象, 则对于每个序列化存储, 它都将以两个参数调用一次: storage 和 location. 参数 storage 是驻留在 CPU 上的存储的初始反序列化. 每个序列化后的存储都有一个与之关联的位置标签, 它标识了保存它的设备, 而此标签是传递给 map_location 的第二个参数. 对于 CPU 张量, 内建的位置标签是 ‘cpu’, 对于 CUDA 张量, 内建的位置标签是 ‘cuda:device_id’ (例如 ‘cuda:2’). map_location 要么返回 None , 要么返回一个存储. 如果 map_location 返回存储, 它将用作已移动到正确设备上的, 最终反序列化的对象. 否则, 如果没有指明 map_location, 即返回 None, &lt;cite&gt;torch.load&lt;/cite&gt; 会回落到默认的行为.</p>
<p>如果 map_location 是一个字典, 它用于将出现在文件 (键) 中的位置标签, 重新映射到另一个位置标签, 它出现在值中并指明在哪里存放存储.</p>
<p>用户扩展可以使用 register_package 来注册他们自己的位置标签, 以及标记和反序列化方法.</p>
<p>参数: f: 一个类文件对象 (必须实现返回文件描述符的 fileno, 以及 seek 方法), 或者包含文件名的字符串. map_location: 一个函数或者一个指明如何重新映射存储位置的字典 pickle_module: 用于解封 (unpickling) 元数据和对象的模块 (必须匹配用于序列化文件的 pickle_module) 示例:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.load('tensors.pt')
# Load all tensors onto the CPU
&gt;&gt;&gt; torch.load('tensors.pt', map_location=lambda storage, loc: storage)
# Load all tensors onto GPU 1
&gt;&gt;&gt; torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))
# Map tensors from GPU 1 to GPU 0
&gt;&gt;&gt; torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})</code></pre>


<h2 id="parallelism">Parallelism (并行化)</h2>
<pre class="codehilite"><code class="language-py">torch.get_num_threads() → int</code></pre>


<p>获得 OpenMP 并行化操作的线程数目</p>
<pre class="codehilite"><code class="language-py">torch.set_num_threads(int)</code></pre>


<p>设置 OpenMP 并行化操作的线程数目</p>
<h2 id="math-operations">Math operations (数学操作)</h2>
<h3 id="pointwise-ops">Pointwise Ops (逐点操作)</h3>
<pre class="codehilite"><code class="language-py">torch.abs(input, out=None) → Tensor</code></pre>


<p>计算给定 <code>input</code> 张量的元素的绝对值.</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.abs(torch.FloatTensor([-1, -2, 3]))
FloatTensor([1, 2, 3])</code></pre>


<pre class="codehilite"><code class="language-py">torch.acos(input, out=None) → Tensor</code></pre>


<p>用 <code>input</code> 元素的反余弦返回一个新的张量.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – The result &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

-0.6366
 0.2718
 0.4469
 1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.acos(a)
 2.2608
 1.2956
 1.1075
 nan
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.add()</code></pre>


<pre class="codehilite"><code class="language-py">torch.add(input, value, out=None)</code></pre>


<p>将标量值 <code>value</code> 添加到输入张量 attr:&lt;cite&gt;input&lt;/cite&gt; 的每个元素并返回一个新的结果张量.</p>
<p><code>\(out = tensor + value\)</code></p>
<p>如果输入张量 <code>input</code> 是 FloatTensor 或者 DoubleTensor 类型, 则 <code>value</code> 必须为实数, 否则为整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>value</strong> (<em>Number</em>) – 要添加到 <code>input</code> 每个元素的数</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

 0.4050
-1.2227
 1.8688
-0.4185
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.add(a, 20)

 20.4050
 18.7773
 21.8688
 19.5815
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.add(input, value=1, other, out=None)</code></pre>


<p>张量 <code>other</code> 的每个元素乘以标量值 <code>value</code> 并加到张量 <code>input</code> 上, 返回生成的张量 <code>out</code> .</p>
<p>张量 <code>input</code> 的形状与张量 <code>other</code> 的形状必须 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p><code>\(out = input + (other * value)\)</code></p>
<p>如果张量 <code>other</code> 是 FloatTensor 或者 DoubleTensor 类型, 则 <code>value</code> 必须为实数, 否则为整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 第一个输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>value</strong> (<em>Number</em>) – 张量 <code>other</code> 的标量乘数</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 第二个输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; import torch
&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

-0.9310
 2.0330
 0.0852
-0.2941
[torch.FloatTensor of size 4]

&gt;&gt;&gt; b = torch.randn(2, 2)
&gt;&gt;&gt; b

 1.0663  0.2544
-0.1513  0.0749
[torch.FloatTensor of size 2x2]

&gt;&gt;&gt; torch.add(a, 10, b)
 9.7322
 4.5770
-1.4279
 0.4552
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) → Tensor</code></pre>


<p>将张量 <code>tensor1</code> 逐元素除以张量 <code>tensor2</code>, 然后乘以标量值 <code>value</code> 并加到张量 <code>tensor</code> 上.</p>
<p>张量 <code>tensor</code>, 张量 <code>tensor1</code>, 张量 <code>tensor2</code> 的形状必须 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>对于类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或者 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 的张量输入, <code>value</code> 必须为实数, 否则为整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 张量, 对 &lt;cite&gt;tensor1 ./ tensor2&lt;/cite&gt; 进行相加</li>
<li><strong>value</strong> (<em>Number__,</em> <em>optional</em>) – 标量, 对 &lt;cite&gt;tensor1 ./ tensor2&lt;/cite&gt; 进行相乘</li>
<li><strong>tensor1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分子张量, 即作为被除数</li>
<li><strong>tensor2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分母张量, 即作为除数</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; t = torch.randn(2, 3)
&gt;&gt;&gt; t1 = torch.randn(1, 6)
&gt;&gt;&gt; t2 = torch.randn(6, 1)
&gt;&gt;&gt; torch.addcdiv(t, 0.1, t1, t2)

 0.0122 -0.0188 -0.2354
 0.7396 -1.5721  1.2878
[torch.FloatTensor of size 2x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) → Tensor</code></pre>


<p>将张量 <code>tensor1</code> 逐元素与张量 <code>tensor2</code> 相乘, 然后乘以标量值 <code>value</code> 并加到张量 <code>tensor</code> 上.</p>
<p>张量 <code>tensor</code>, 张量 <code>tensor1</code>, 张量 <code>tensor2</code> 的形状必须 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>对于类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或者 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 的张量输入, <code>value</code> 必须为实数, 否则为整数. :param tensor: 张量, 对 &lt;cite&gt;tensor1 .<em> tensor2&lt;/cite&gt; 进行相加 :type tensor: Tensor :param value: 标量, 对 &lt;cite&gt;tensor1 .</em> tensor2&lt;/cite&gt; 进行相乘 :type value: Number, optional :param tensor1: 张量, 作为乘子1 :type tensor1: Tensor :param tensor2: 张量, 作为乘子2 :type tensor2: Tensor :param out: 输出张量 :type out: Tensor, optional</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; t = torch.randn(2, 3)
&gt;&gt;&gt; t1 = torch.randn(1, 6)
&gt;&gt;&gt; t2 = torch.randn(6, 1)
&gt;&gt;&gt; torch.addcmul(t, 0.1, t1, t2)

 0.0122 -0.0188 -0.2354
 0.7396 -1.5721  1.2878
[torch.FloatTensor of size 2x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.asin(input, out=None) → Tensor</code></pre>


<p>返回一个新的 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素为张量 <code>input</code> 的每个元素的反正弦.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
 0.2718
 0.4469
 1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.asin(a)
-0.6900
 0.2752
 0.4633
 nan
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.atan(input, out=None) → Tensor</code></pre>


<p>返回一个新的 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素为张量 <code>input</code> 的每个元素的反正切.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
 0.2718
 0.4469
 1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.atan(a)
-0.5669
 0.2653
 0.4203
 0.9196
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.atan2(input1, input2, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是输入张量 <code>input1</code> 和输入张量 <code>input2</code> 元素的反正切.</p>
<p>输入张量 <code>input1</code> 的形状和输入张量 <code>input2</code> 的形状必须可 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 第一个输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>input2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 第二个输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
 0.2718
 0.4469
 1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.atan2(a, torch.randn(4))
-2.4167
 2.9755
 0.9363
 1.6613
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.ceil(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 的元素向上取整(取不小于每个元素的最小整数).</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

 1.3869
 0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.ceil(a)

 2
 1
-0
-0
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.clamp(input, min, max, out=None) → Tensor</code></pre>


<p>将输入张量 <code>input</code> 所有元素限制在区间 &lt;cite&gt;[min, max]&lt;/cite&gt; 中并返回一个结果张量.</p>
<pre class="codehilite"><code class="language-py">      | min, if x_i &lt; min
y_i = | x_i, if min &lt;= x_i &lt;= max
      | max, if x_i &gt; max</code></pre>


<p>如果输入张量 <code>input</code> 的类型 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或者 &lt;cite&gt;DoubleTensor&lt;/cite&gt;, 那么参数 <a href="#torch.min" title="torch.min"><code>min</code></a> 和 <a href="#torch.max" title="torch.max"><code>max</code></a> 必须为实数, 否则为整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>min</strong> (<em>Number</em>) – 限制范围下限</li>
<li><strong>max</strong> (<em>Number</em>) – 限制范围上限</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

 1.3869
 0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5)

 0.5000
 0.3912
-0.5000
-0.5000
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.clamp(input, *, min, out=None) → Tensor</code></pre>


<p>张量 <code>input</code> 的所有元素值大于或者等于 <a href="#torch.min" title="torch.min"><code>min</code></a>.</p>
<p>如果张量 <code>input</code> 的类型是 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或者 &lt;cite&gt;DoubleTensor&lt;/cite&gt;, 则 <code>value</code> 必须是实数, 否则应该是整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>value</strong> (<em>Number</em>) – 输出中每个元素的最小值</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

 1.3869
 0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.clamp(a, min=0.5)

 1.3869
 0.5000
 0.5000
 0.5000
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.clamp(input, *, max, out=None) → Tensor</code></pre>


<p>张量 <code>input</code> 的所有元素值小于或者等于 <a href="#torch.max" title="torch.max"><code>max</code></a>.</p>
<p>如果张量 <code>input</code> 的类型是 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或者 &lt;cite&gt;DoubleTensor&lt;/cite&gt;, 则 <code>value</code> 必须是实数, 否则应该是整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>value</strong> (<em>Number</em>) – 输出中每个元素的最大值</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

 1.3869
 0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.clamp(a, max=0.5)

 0.5000
 0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.cos(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 每个元素的余弦.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
 0.2718
 0.4469
 1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.cos(a)
 0.8041
 0.9633
 0.9018
 0.2557
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.cosh(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 每个元素的双曲余弦.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
 0.2718
 0.4469
 1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.cosh(a)
 1.2095
 1.0372
 1.1015
 1.9917
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.div()</code></pre>


<pre class="codehilite"><code class="language-py">torch.div(input, value, out=None)</code></pre>


<p>将张量 <code>input</code> 的元素逐一除以标量值 <code>value</code> , 其结果作为一个新的张量返回.</p>
<p><code>\(out = tensor / value\)</code></p>
<p>如果张量 <code>input</code> 的类型是 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或者 &lt;cite&gt;DoubleTensor&lt;/cite&gt;, 则标量值 <code>value</code> 必须是实数, 否则应该是整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>value</strong> (<em>Number</em>) – 除数, 被张量 <code>input</code> 的元素除</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a

-0.6147
-1.1237
-0.1604
-0.6853
 0.1063
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.div(a, 0.5)

-1.2294
-2.2474
-0.3208
-1.3706
 0.2126
[torch.FloatTensor of size 5]</code></pre>


<pre class="codehilite"><code class="language-py">torch.div(input, other, out=None)</code></pre>


<p>张量 <code>input</code> 的元素与张量 <code>other</code> 的元素逐一相除. 返回一个新的结果张量 <code>out</code> . 张量 <code>input</code> 与张量 <code>other</code> 的形状必须可 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p><code>\(out_i = input_i / other_i\)</code></p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分子 &lt;cite&gt;Tensor&lt;/cite&gt; (被除数)</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分母 &lt;cite&gt;Tensor&lt;/cite&gt; (除数)</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4,4)
&gt;&gt;&gt; a

-0.1810  0.4017  0.2863 -0.1013
 0.6183  2.0696  0.9012 -1.5933
 0.5679  0.4743 -0.0117 -0.1266
-0.1213  0.9629  0.2682  1.5968
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; b = torch.randn(8, 2)
&gt;&gt;&gt; b

 0.8774  0.7650
 0.8866  1.4805
-0.6490  1.1172
 1.4259 -0.8146
 1.4633 -0.1228
 0.4643 -0.6029
 0.3492  1.5270
 1.6103 -0.6291
[torch.FloatTensor of size 8x2]

&gt;&gt;&gt; torch.div(a, b)

-0.2062  0.5251  0.3229 -0.0684
-0.9528  1.8525  0.6320  1.9559
 0.3881 -3.8625 -0.0253  0.2099
-0.3473  0.6306  0.1666 -2.5381
[torch.FloatTensor of size 4x4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.erf(tensor, out=None) → Tensor</code></pre>


<p>计算每个元素的误差函数.</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.erf(torch.Tensor([0, -1., 10.]))
torch.FloatTensor([0., -0.8427, 1.])</code></pre>


<pre class="codehilite"><code class="language-py">torch.erfinv(tensor, out=None) → Tensor</code></pre>


<p>计算每个元素的反向误差函数.</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.erfinv(torch.Tensor([0, 0.5., -1.]))
torch.FloatTensor([0., 0.4769, -inf])</code></pre>


<pre class="codehilite"><code class="language-py">torch.exp(tensor, out=None) → Tensor</code></pre>


<p>计算每个元素的指数.</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.exp(torch.Tensor([0, math.log(2)]))
torch.FloatTensor([1, 2])</code></pre>


<pre class="codehilite"><code class="language-py">torch.floor(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 的元素向下取整(取不大于每个元素的最大整数).</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

 1.3869
 0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.floor(a)

 1
 0
-1
-1
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.fmod(input, divisor, out=None) → Tensor</code></pre>


<p>计算除法余数.</p>
<p>被除数和除数可能同时含有整数和浮点数. 这时余数的正负与被除数 &lt;cite&gt;tensor&lt;/cite&gt; 相同.</p>
<p>当除数 <code>divisor</code> 是一个张量时r, 张量 <code>input</code> 和张量 <code>divisor</code> 的形状必须可 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 被除数</li>
<li><strong>divisor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 除数. 可能是一个数或者是一个与被除数相同形状的张量.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.fmod(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2)
torch.FloatTensor([-1, -0, -1, 1, 0, 1])
&gt;&gt;&gt; torch.fmod(torch.Tensor([1, 2, 3, 4, 5]), 1.5)
torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5])</code></pre>


<p>See also</p>
<p><a href="#torch.remainder" title="torch.remainder"><code>torch.remainder()</code></a>, 其计算等价于 Python’s &lt;cite&gt;%&lt;/cite&gt; 操作符的元素余数</p>
<pre class="codehilite"><code class="language-py">torch.frac(tensor, out=None) → Tensor</code></pre>


<p>计算张量 &lt;cite&gt;tensor&lt;/cite&gt; 每个元素的分数部分.</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.frac(torch.Tensor([1, 2.5, -3.2])
torch.FloatTensor([0, 0.5, -0.2])</code></pre>


<pre class="codehilite"><code class="language-py">torch.lerp(start, end, weight, out=None)</code></pre>


<p>基于标量值 <code>weight</code>: , 在张量 <code>start</code> 与张量 <code>end</code> 之间做线性插值 并返回结果张量 <code>out</code> .</p>
<p><code>\(out_i = start_i + weight * (end_i - start_i)\)</code></p>
<p>张量 <code>start</code> 和张量 <code>end</code> 的形状必须可 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>start</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 起始点 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>end</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 终点 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>weight</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 插值公式的权重</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; start = torch.arange(1, 5)
&gt;&gt;&gt; end = torch.Tensor(4).fill_(10)
&gt;&gt;&gt; start

 1
 2
 3
 4
[torch.FloatTensor of size 4]

&gt;&gt;&gt; end

 10
 10
 10
 10
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.lerp(start, end, 0.5)

 5.5000
 6.0000
 6.5000
 7.0000
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.log(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 所有元素的自然对数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a

-0.4183
 0.3722
-0.3091
 0.4149
 0.5857
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.log(a)

 nan
-0.9883
 nan
-0.8797
-0.5349
[torch.FloatTensor of size 5]</code></pre>


<pre class="codehilite"><code class="language-py">torch.log1p(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是(1 + <code>input</code>) 的自然对数.</p>
<p><code>\(y_i = log(x_i + 1)\)</code></p>
<p>Note</p>
<p>对于较小的张量 <code>input</code> 的值, 此函数比 <a href="#torch.log" title="torch.log"><code>torch.log()</code></a> 更精确.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a

-0.4183
 0.3722
-0.3091
 0.4149
 0.5857
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.log1p(a)

-0.5418
 0.3164
-0.3697
 0.3471
 0.4611
[torch.FloatTensor of size 5]</code></pre>


<pre class="codehilite"><code class="language-py">torch.mul()</code></pre>


<pre class="codehilite"><code class="language-py">torch.mul(input, value, out=None)</code></pre>


<p>将输入张量 <code>input</code> 的每个元素与标量值 <code>value</code> 相乘并返回一个新的结果张量.</p>
<p><code>\(out = tensor * value\)</code></p>
<p>如果张量 <code>input</code> 的类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, 则 <code>value</code> 应该是实数, 否则为整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>value</strong> (<em>Number</em>) – 与张量 <code>input</code> 每个元素相乘的数</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a

-0.9374
-0.5254
-0.6069
[torch.FloatTensor of size 3]

&gt;&gt;&gt; torch.mul(a, 100)

-93.7411
-52.5374
-60.6908
[torch.FloatTensor of size 3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.mul(input, other, out=None)</code></pre>


<p>张量 <code>input</code> 的元素与张量 <code>other</code> 的元素逐一相乘. 其结果作为一个新的张量返回.</p>
<p>张量 <code>input</code> 和张量 <code>other</code> 的形状必须可 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p><code>\(out_i = input_i * other_i\)</code></p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 第一个乘数 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 第二个乘数 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4,4)
&gt;&gt;&gt; a

-0.7280  0.0598 -1.4327 -0.5825
-0.1427 -0.0690  0.0821 -0.3270
-0.9241  0.5110  0.4070 -1.1188
-0.8308  0.7426 -0.6240 -1.1582
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; b = torch.randn(2, 8)
&gt;&gt;&gt; b

 0.0430 -1.0775  0.6015  1.1647 -0.6549  0.0308 -0.1670  1.0742
-1.2593  0.0292 -0.0849  0.4530  1.2404 -0.4659 -0.1840  0.5974
[torch.FloatTensor of size 2x8]

&gt;&gt;&gt; torch.mul(a, b)

-0.0313 -0.0645 -0.8618 -0.6784
 0.0934 -0.0021 -0.0137 -0.3513
 1.1638  0.0149 -0.0346 -0.5068
-1.0304 -0.3460  0.1148 -0.6919
[torch.FloatTensor of size 4x4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.neg(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 的元素的负值.</p>
<p><code>\(out = -1 * input\)</code></p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a

-0.4430
 1.1690
-0.8836
-0.4565
 0.2968
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.neg(a)

 0.4430
-1.1690
 0.8836
 0.4565
-0.2968
[torch.FloatTensor of size 5]</code></pre>


<pre class="codehilite"><code class="language-py">torch.pow()</code></pre>


<pre class="codehilite"><code class="language-py">torch.pow(input, exponent, out=None)</code></pre>


<p>对输入张量 <code>input</code> 按元素求 <code>exponent</code> 次幂值并返回结果张量(其值作为结果张量的元素).</p>
<p>幂值 <code>exponent</code> 可以是一个单一的浮点数 <code>float</code> 或者是一个与张量 <code>input</code> 有相同元素数的张量 <code>Tensor</code> .</p>
<p>当指数 <code>exponent</code> 是一个标量时, 执行操作:</p>
<p><code>\(out_i = x_i ^ {exponent}\)</code></p>
<p>当指数 <code>exponent</code> 是一个张量, 执行操作:</p>
<p><code>\(out_i = x_i ^ {exponent_i}\)</code></p>
<p>当幂值 <code>exponent</code> 是一个张量, 张量 <code>input</code> 和张量 <code>exponent</code> 的形状必须可 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>exponent</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 指数</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

-0.5274
-0.8232
-2.1128
 1.7558
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.pow(a, 2)

 0.2781
 0.6776
 4.4640
 3.0829
[torch.FloatTensor of size 4]

&gt;&gt;&gt; exp = torch.arange(1, 5)
&gt;&gt;&gt; a = torch.arange(1, 5)
&gt;&gt;&gt; a

 1
 2
 3
 4
[torch.FloatTensor of size 4]

&gt;&gt;&gt; exp

 1
 2
 3
 4
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.pow(a, exp)

 1
 4
 27
 256
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.pow(base, input, out=None)</code></pre>


<p><code>base</code> 是一个标量浮点值, <code>input</code> 是一个张量. 返回的张量 <code>out</code> 的形状与张量 <code>input</code> 的形状相同.</p>
<p>执行操作:</p>
<p><code>\(out_i = base ^ {input_i}\)</code></p>
<p>| Parameters: | </p>
<ul>
<li><strong>base</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 幂运算的底数</li>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 指数</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; exp = torch.arange(1, 5)
&gt;&gt;&gt; base = 2
&gt;&gt;&gt; torch.pow(base, exp)

 2
 4
 8
 16
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.reciprocal(input, out=None) → Tensor</code></pre>


<p>返回一个新的 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 元素的倒数, i.e. <code>\(1.0 / x\)</code></p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

 1.3869
 0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.reciprocal(a)

 0.7210
 2.5565
-1.1583
-1.8289
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.remainder(input, divisor, out=None) → Tensor</code></pre>


<p>计算元素的除法的余数.</p>
<p>除数与被除数可能同时包含整数或浮点数. 余数与除数有相同的符号.</p>
<p>当除数 <code>divisor</code> 是一个张量, 张量 <code>input</code> 的形状和张量 <code>divisor</code> 得形状必须可 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 被除数</li>
<li><strong>divisor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 除数. 可能是一个数或者可能是一个与被除数大小相同的张量</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.remainder(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2)
torch.FloatTensor([1, 0, 1, 1, 0, 1])
&gt;&gt;&gt; torch.remainder(torch.Tensor([1, 2, 3, 4, 5]), 1.5)
torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5])</code></pre>


<p>See also</p>
<p><a href="#torch.fmod" title="torch.fmod"><code>torch.fmod()</code></a> 同样计算除法余数, 等效于C库函数中的 <code>fmod()</code></p>
<pre class="codehilite"><code class="language-py">torch.round(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是输入张量的元素四舍五入到最近的整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

 1.2290
 1.3409
-0.5662
-0.0899
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.round(a)

 1
 1
-1
-0
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.rsqrt(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 元素的平方根的倒数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

 1.2290
 1.3409
-0.5662
-0.0899
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.rsqrt(a)

 0.9020
 0.8636
 nan
 nan
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.sigmoid(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 元素的sigmoid值.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

-0.4972
 1.3512
 0.1056
-0.2650
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.sigmoid(a)

 0.3782
 0.7943
 0.5264
 0.4341
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.sign(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 元素的符号.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
 0.2718
 0.4469
 1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.sign(a)

-1
 1
 1
 1
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.sin(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 元素的正弦.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
 0.2718
 0.4469
 1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.sin(a)
-0.5944
 0.2684
 0.4322
 0.9667
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.sinh(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 元素的双曲正弦.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
 0.2718
 0.4469
 1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.sinh(a)
-0.6804
 0.2751
 0.4619
 1.7225
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.sqrt(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 元素的平方根.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

 1.2290
 1.3409
-0.5662
-0.0899
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.sqrt(a)

 1.1086
 1.1580
 nan
 nan
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.tan(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 元素的正切.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
 0.2718
 0.4469
 1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.tan(a)
-0.7392
 0.2786
 0.4792
 3.7801
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.tanh(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 元素的双曲正切.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
 0.2718
 0.4469
 1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.tanh(a)
-0.5625
 0.2653
 0.4193
 0.8648
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.trunc(input, out=None) → Tensor</code></pre>


<p>返回一个新的张量 &lt;cite&gt;Tensor&lt;/cite&gt; , 其元素是张量 <code>input</code> 元素的截断整数值 (直接去除小数部分) .</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

-0.4972
 1.3512
 0.1056
-0.2650
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.trunc(a)

-0
 1
 0
-0
[torch.FloatTensor of size 4]</code></pre>


<h3 id="reduction-ops">Reduction Ops (归约操作)</h3>
<pre class="codehilite"><code class="language-py">torch.cumprod(input, dim, out=None) → Tensor</code></pre>


<p>返回元素 <code>input</code> 在给定维度 <code>dim</code> 下的累积积.</p>
<p>例如, 如果 <code>input</code> 是一个N元张量, 结果也是一个N元张量, 元素为: <code>\(y_i = x_1 * x_2 * x_3 * ... * x_i\)</code></p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 进行操作的维度</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(10)
&gt;&gt;&gt; a

 1.1148
 1.8423
 1.4143
-0.4403
 1.2859
-1.2514
-0.4748
 1.1735
-1.6332
-0.4272
[torch.FloatTensor of size 10]

&gt;&gt;&gt; torch.cumprod(a, dim=0)

 1.1148
 2.0537
 2.9045
-1.2788
-1.6444
 2.0578
-0.9770
-1.1466
 1.8726
-0.8000
[torch.FloatTensor of size 10]

&gt;&gt;&gt; a[5] = 0.0
&gt;&gt;&gt; torch.cumprod(a, dim=0)

 1.1148
 2.0537
 2.9045
-1.2788
-1.6444
-0.0000
 0.0000
 0.0000
-0.0000
 0.0000
[torch.FloatTensor of size 10]</code></pre>


<pre class="codehilite"><code class="language-py">torch.cumsum(input, dim, out=None) → Tensor</code></pre>


<p>返回元素 <code>input</code> 在给定维度 <code>dim</code> 下的累积和.</p>
<p>例如, 如果 <code>input</code> 是一个N元张量, 结果将也是一个N元张量, 元素为: <code>\(y_i = x_1 + x_2 + x_3 + ... + x_i\)</code></p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 进行操作的维度</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(10)
&gt;&gt;&gt; a

-0.6039
-0.2214
-0.3705
-0.0169
 1.3415
-0.1230
 0.9719
 0.6081
-0.1286
 1.0947
[torch.FloatTensor of size 10]

&gt;&gt;&gt; torch.cumsum(a, dim=0)

-0.6039
-0.8253
-1.1958
-1.2127
 0.1288
 0.0058
 0.9777
 1.5858
 1.4572
 2.5519
[torch.FloatTensor of size 10]</code></pre>


<pre class="codehilite"><code class="language-py">torch.dist(input, other, p=2) → float</code></pre>


<p>返回(<code>input</code> - <code>other</code>)的p-范数 <code>input</code> 和 <code>other</code> 的形状必须满足 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 右侧输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>p</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>,</em> <em>optional</em>) – 所计算的范数.</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.randn(4)
&gt;&gt;&gt; x

 0.2505
-0.4571
-0.3733
 0.7807
[torch.FloatTensor of size 4]

&gt;&gt;&gt; y = torch.randn(4)
&gt;&gt;&gt; y

 0.7782
-0.5185
 1.4106
-2.4063
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.dist(x, y, 3.5)
3.302832063224223
&gt;&gt;&gt; torch.dist(x, y, 3)
3.3677282206393286
&gt;&gt;&gt; torch.dist(x, y, 0)
inf
&gt;&gt;&gt; torch.dist(x, y, 1)
5.560028076171875</code></pre>


<pre class="codehilite"><code class="language-py">torch.mean()</code></pre>


<pre class="codehilite"><code class="language-py">torch.mean(input) → float</code></pre>


<p>返回张量 <code>input</code> 所有元素的均值.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

-0.2946 -0.9143  2.1809
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.mean(a)
0.32398951053619385</code></pre>


<pre class="codehilite"><code class="language-py">torch.mean(input, dim, keepdim=False, out=None) → Tensor</code></pre>


<p>返回张量 <code>input</code> 在给定维度 <code>dim</code> 上每行的均值.</p>
<p>如果 <code>keepdim</code> 是 <code>True</code>, 输出张量的大小与输入张量 <code>input</code> 相同, 除了维度 <code>dim</code> 是1. 另外, <code>dim</code> 被挤压 (参看 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a> ), 导致输出张量减少一维.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 要减少的维度</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>,</em> <em>optional</em>) – 输出张量的维度 <code>dim</code> 保持与否</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输出张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a

-1.2738 -0.3058  0.1230 -1.9615
 0.8771 -0.5430 -0.9233  0.9879
 1.4107  0.0317 -0.6823  0.2255
-1.3854  0.4953 -0.2160  0.2435
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; torch.mean(a, 1)

-0.8545
 0.0997
 0.2464
-0.2157
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.mean(a, 1, True)

-0.8545
 0.0997
 0.2464
-0.2157
[torch.FloatTensor of size 4x1]</code></pre>


<pre class="codehilite"><code class="language-py">torch.median()</code></pre>


<pre class="codehilite"><code class="language-py">torch.median(input) → float</code></pre>


<p>返回输出张量 <code>input</code> 所有元素的中位数.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input &lt;cite&gt;Tensor&lt;/cite&gt;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

 0.4729 -0.2266 -0.2085
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.median(a)
-0.2085</code></pre>


<pre class="codehilite"><code class="language-py">torch.median(input, dim=-1, keepdim=False, values=None, indices=None) -&gt; (Tensor, LongTensor)</code></pre>


<p>返回输出张量 <code>input</code> 在给定维度 <code>dim</code> 下每行的中位数. 同时返回一个包含中位数的索引 &lt;cite&gt;LongTensor&lt;/cite&gt;.</p>
<p><code>dim</code> 的缺省值为输入张量 <code>input</code> 的最后一维.</p>
<p>如果 <code>keepdim</code> 是 <code>True</code>, 输出张量与输入张量 <code>input</code> 形状相同, 除了维数 <code>dim</code> 是1. 另外, <code>dim</code> 被挤压 (参看 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a> ), 导致输出张量比输入张量 <code>input</code> 少一维.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 要减少的维度</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 输出张量的维度 <code>dim</code> 保留与否</li>
<li><strong>values</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
<li><strong>indices</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量索引</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a

 -0.6891 -0.6662
 0.2697  0.7412
 0.5254 -0.7402
 0.5528 -0.2399
[torch.FloatTensor of size 4x2]

&gt;&gt;&gt; a = torch.randn(4, 5)
&gt;&gt;&gt; a

 0.4056 -0.3372  1.0973 -2.4884  0.4334
 2.1336  0.3841  0.1404 -0.1821 -0.7646
-0.2403  1.3975 -2.0068  0.1298  0.0212
-1.5371 -0.7257 -0.4871 -0.2359 -1.1724
[torch.FloatTensor of size 4x5]

&gt;&gt;&gt; torch.median(a, 1)
(
 0.4056
 0.1404
 0.0212
-0.7257
[torch.FloatTensor of size 4]
,
 0
 2
 4
 1
[torch.LongTensor of size 4]
)</code></pre>


<pre class="codehilite"><code class="language-py">torch.mode(input, dim=-1, keepdim=False, values=None, indices=None) -&gt; (Tensor, LongTensor)</code></pre>


<p>返回输入张量 <code>input</code> 在给定维数 <code>dim</code> 下每行元素的众数值. 同时也返回众数值的索引 &lt;cite&gt;LongTensor&lt;/cite&gt;.</p>
<p>维度 <code>dim</code> 的缺省值是输入张量 <code>input</code> 的最后一维. .</p>
<p>如果 <code>keepdim</code> 是 <code>True</code>, 输出张量的大小与输入张量 <code>input</code> 相同, 除了维度 <code>dim</code> 是1. 另外, <code>dim</code> 被挤压 (参看 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a> ), 导致输出张量减少一维.</p>
<p>Note</p>
<p>这个函数至今没有为 <code>torch.cuda.Tensor</code> 定义.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 要减少的维度</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 输出张量的维度 <code>dim</code> 保持与否</li>
<li><strong>values</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
<li><strong>indices</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果索引张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a

 -0.6891 -0.6662
 0.2697  0.7412
 0.5254 -0.7402
 0.5528 -0.2399
[torch.FloatTensor of size 4x2]

&gt;&gt;&gt; a = torch.randn(4, 5)
&gt;&gt;&gt; a

 0.4056 -0.3372  1.0973 -2.4884  0.4334
 2.1336  0.3841  0.1404 -0.1821 -0.7646
-0.2403  1.3975 -2.0068  0.1298  0.0212
-1.5371 -0.7257 -0.4871 -0.2359 -1.1724
[torch.FloatTensor of size 4x5]

&gt;&gt;&gt; torch.mode(a, 1)
(
-2.4884
-0.7646
-2.0068
-1.5371
[torch.FloatTensor of size 4]
,
 3
 4
 2
 0
[torch.LongTensor of size 4]
)</code></pre>


<pre class="codehilite"><code class="language-py">torch.norm()</code></pre>


<pre class="codehilite"><code class="language-py">torch.norm(input, p=2) → float</code></pre>


<p>返回输入张量 <code>input</code> 的p-范数</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>p</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>,</em> <em>optional</em>) – 范数计算中的幂指数值</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

-0.4376 -0.5328  0.9547
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.norm(a, 3)
1.0338925067372466</code></pre>


<pre class="codehilite"><code class="language-py">torch.norm(input, p, dim, keepdim=False, out=None) → Tensor</code></pre>


<p>返回输入张量 <code>input</code> 在给定维度 <code>dim</code> 下每行元素的p-范数.</p>
<p>如果 <code>keepdim</code> 是 <code>True</code>, 输出张量的大小与输入张量 <code>input</code> 相同, 除非维度 <code>dim</code> 是1. 另外, <code>dim</code> 被挤压 (参看 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a> ), 导致输出张量减少一维.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>p</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 范数计算中的幂指数值</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 要减少的维度</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 输出张量的维度 <code>dim</code> 保持与否</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4, 2)
&gt;&gt;&gt; a

-0.6891 -0.6662
 0.2697  0.7412
 0.5254 -0.7402
 0.5528 -0.2399
[torch.FloatTensor of size 4x2]

&gt;&gt;&gt; torch.norm(a, 2, 1)

 0.9585
 0.7888
 0.9077
 0.6026
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.norm(a, 0, 1, True)

 2
 2
 2
 2
[torch.FloatTensor of size 4x1]</code></pre>


<pre class="codehilite"><code class="language-py">torch.prod()</code></pre>


<pre class="codehilite"><code class="language-py">torch.prod(input) → float</code></pre>


<p>返回输入张量 <code>input</code> 所有元素的乘积.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量 &lt;cite&gt;Tensor&lt;/cite&gt;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

 0.6170  0.3546  0.0253
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.prod(a)
0.005537458061418483</code></pre>


<pre class="codehilite"><code class="language-py">torch.prod(input, dim, keepdim=False, out=None) → Tensor</code></pre>


<p>返回输入张量 <code>input</code> 在给定维度 <code>dim</code> 下每行元素的积.</p>
<p>如果 <code>keepdim</code> 是 <code>True</code>, 输出张量的大小与输入张量 <code>input</code> 相同, 除了维度 <code>dim</code> 是1. 另外, <code>dim</code> 被挤压 (参看 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a> ), 导致输出张量减少一维.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 要减少的维度</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 输出张量的维度 <code>dim</code> 保持与否</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4, 2)
&gt;&gt;&gt; a

 0.1598 -0.6884
-0.1831 -0.4412
-0.9925 -0.6244
-0.2416 -0.8080
[torch.FloatTensor of size 4x2]

&gt;&gt;&gt; torch.prod(a, 1)

-0.1100
 0.0808
 0.6197
 0.1952
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.std()</code></pre>


<pre class="codehilite"><code class="language-py">torch.std(input, unbiased=True) → float</code></pre>


<p>返回输入张量 <code>input</code> 所有元素的标准差.</p>
<p>如果 <code>unbiased</code> 是 <code>False</code> , 那么标准差将通过有偏估计计算.否则, Bessel’s correction 将被使用.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>unbiased</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 是否使用无偏估计</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

-1.3063  1.4182 -0.3061
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.std(a)
1.3782334731508061</code></pre>


<pre class="codehilite"><code class="language-py">torch.std(input, dim, keepdim=False, unbiased=True, out=None) → Tensor</code></pre>


<p>返回输入张量 <code>input</code> 在给定维度 <code>dim</code> 下每行元素的标准差.</p>
<p>如果 <code>keepdim</code> 是 <code>True</code>, 输出张量的大小与输入张量 <code>input</code> 相同, 除了维度 <code>dim</code> 是 1. 另外, <code>dim</code> 被挤压 (参看 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a> ), 导致输出张量减少一维.</p>
<p>如果 <code>unbiased</code> 是 <code>False</code> , 那么标准差将通过有偏估计来计算. 否则, Bessel’s correction 将被使用.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 要减少的维度</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 输出张量的维度 <code>dim</code> 保持与否</li>
<li><strong>unbiased</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 是否使用无偏估计</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a

 0.1889 -2.4856  0.0043  1.8169
-0.7701 -0.4682 -2.2410  0.4098
 0.1919 -1.1856 -1.0361  0.9085
 0.0173  1.0662  0.2143 -0.5576
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; torch.std(a, dim=1)

 1.7756
 1.1025
 1.0045
 0.6725
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.sum()</code></pre>


<pre class="codehilite"><code class="language-py">torch.sum(input) → float</code></pre>


<p>返回输入张量 <code>input</code> 所有元素的和.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量 &lt;cite&gt;Tensor&lt;/cite&gt;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

 0.6170  0.3546  0.0253
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.sum(a)
0.9969287421554327</code></pre>


<pre class="codehilite"><code class="language-py">torch.sum(input, dim, keepdim=False, out=None) → Tensor</code></pre>


<p>返回输入张量 <code>input</code> 在给定维度 <code>dim</code> 下每行元素的和.</p>
<p>如果 <code>keepdim</code> 是 <code>True</code>, 输出张量的大小与输入张量 <code>input</code> 相同, 除了维度 <code>dim</code> 是 1. 另外, <code>dim</code> 被挤压 (参看 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a> ), 导致输出张量减少一维.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 要减少的维度</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 输出张量的维度 <code>dim</code> 保持与否</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a

-0.4640  0.0609  0.1122  0.4784
-1.3063  1.6443  0.4714 -0.7396
-1.3561 -0.1959  1.0609 -1.9855
 2.6833  0.5746 -0.5709 -0.4430
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; torch.sum(a, 1)

 0.1874
 0.0698
-2.4767
 2.2440
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.var()</code></pre>


<pre class="codehilite"><code class="language-py">torch.var(input, unbiased=True) → float</code></pre>


<p>返回输入张量 <code>input</code> 的方差.</p>
<p>如果 <code>unbiased</code> 是 <code>False</code> , 方差的计算将通过有偏估计计算. 否则, Bessel’s correction 将会被使用.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>unbiased</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 是否使用无偏估计</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

-1.3063  1.4182 -0.3061
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.var(a)
1.899527506513334</code></pre>


<pre class="codehilite"><code class="language-py">torch.var(input, dim, keepdim=False, unbiased=True, out=None) → Tensor</code></pre>


<p>返回输入张量 <code>input</code> 在给定维度 <code>dim</code> 下每行的方差.</p>
<p>如果 <code>keepdim</code> 是 <code>True</code>, 输出张量的大小与输入张量 <code>input</code> 相同, 除了维度 <code>dim</code> 是 1. 另外, <code>dim</code> 被挤压 (参看 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), 导致输出张量减少一维.</p>
<p>如果 <code>unbiased</code> 是<code>False</code>, 方差的计算将通过有偏估计计算. 否则, Bessel’s correction 将会被使用.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 要减少的维度</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 输出张量的维度 <code>dim</code> 保留与否</li>
<li><strong>unbiased</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 是否使用无偏估计</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a

-1.2738 -0.3058  0.1230 -1.9615
 0.8771 -0.5430 -0.9233  0.9879
 1.4107  0.0317 -0.6823  0.2255
-1.3854  0.4953 -0.2160  0.2435
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; torch.var(a, 1)

 0.8859
 0.9509
 0.7548
 0.6949
[torch.FloatTensor of size 4]</code></pre>


<h3 id="comparison-ops">Comparison Ops (比较操作)</h3>
<pre class="codehilite"><code class="language-py">torch.eq(input, other, out=None) → Tensor</code></pre>


<p>比较元素是否相等</p>
<p>第二个元素可以是一个数字或 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> 为与第一个参数形状相同的张量.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 待比较张量</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 比较张量或数</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量, 须为 ByteTensor 类型或与 input (Tensor) 同类型</li>
</ul>
<p>|
| --- | --- |
| Returns: | 一个 torch.ByteTensor 张量, 待比较和要比较张量逐位置比较, 相等为 1 , 不等为 0 |
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.eq(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))
1  0
0  1
[torch.ByteTensor of size 2x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.equal(tensor1, tensor2) → bool</code></pre>


<p>如果两个张量有相同的形状和元素值, 则返回 <code>True</code> , 否则 <code>False</code> .</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.equal(torch.Tensor([1, 2]), torch.Tensor([1, 2]))
True</code></pre>


<pre class="codehilite"><code class="language-py">torch.ge(input, other, out=None) → Tensor</code></pre>


<p>逐元素比较 <code>input</code> 和 <code>other</code> , 即是否 <strong>input&gt;=other</strong> .</p>
<p>第二个参数可以为一个数或形状可 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> 为和第一个参数相同类型的张量.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 待对比的张量</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 对比的张量或 <code>float</code> 值</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量. 必须为 <code>ByteTensor</code> 或者与第一个参数 <code>tensor</code> 相同类型.</li>
</ul>
<p>|
| --- | --- |
| Returns: | 一个 <code>torch.ByteTensor</code> 张量, 包含了每个位置的比较结果(是否 input &gt;= other ). |
| --- | --- |
| Return type: | 张量 |
| --- | --- |</p>
<p>Return type: 张量</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.ge(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))
 1  1
 0  1
[torch.ByteTensor of size 2x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.gt(input, other, out=None) → Tensor</code></pre>


<p>逐元素比较 <code>input</code> 和 <code>other</code> , 即是否 <strong>input&gt;other</strong> 如果两个张量有相同的形状和元素值, 则返回 <code>True</code> ,否则 <code>False</code>.</p>
<p>第二个参数可以为一个数或形状可 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> 为和第一个参数相同类型的张量.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 待对比的张量</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 对比的张量或 <code>float</code> 值</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量. 必须为 <code>ByteTensor</code> 或者与第一个参数 <code>tensor</code> 相同类型.</li>
</ul>
<p>|
| --- | --- |
| Returns: | 一个 <code>torch.ByteTensor</code> 张量, 包含了每个位置的比较结果(是否 input &gt; other ). |
| --- | --- |
| Return type: | 张量 |
| --- | --- |</p>
<p>Return type: 张量</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.gt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))
 0  1
 0  0
[torch.ByteTensor of size 2x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.kthvalue(input, k, dim=None, keepdim=False, out=None) -&gt; (Tensor, LongTensor)</code></pre>


<p>取输入张量 <code>input</code> 指定维上第 <code>k</code> 个最小值. 如果不指定 <code>dim</code> , 则默认为 <code>input</code> 的最后一维.</p>
<p>返回一个元组 &lt;cite&gt;(values,indices)&lt;/cite&gt; ,其中 <code>indices</code> 是原始输入张量 <code>input</code> 中沿 <code>dim</code> 维的第 <code>k</code> 个最小值下标.</p>
<p>如果 <code>keepdim</code> 为 <code>True</code> , <code>values</code> 和 <code>indices</code> 张量都和 <code>input</code> 大小相同, 除了在所有值都为1的 <code>dim</code> 维度上. 如果 <code>keepdim</code> 为 <code>False</code> , <code>dim</code> 被压缩. (参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a> ), 使 <code>values</code> 和 <code>indices</code> 两个张量比 <code>input</code> 张量小一个的维度.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>k</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 第 <code>k</code> 个最小值</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>,</em> <em>optional</em>) – 沿着此维进行排序</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 输出张量是否保持维度 <code>dim</code> 不变</li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>,</em> <em>optional</em>) – 输出元组 ( Tensor, LongTensor ) 可选参数(作为输出 buffers )</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.arange(1, 6)
&gt;&gt;&gt; x

 1
 2
 3
 4
 5
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.kthvalue(x, 4)
(
 4
[torch.FloatTensor of size 1]
,
 3
[torch.LongTensor of size 1]
)

&gt;&gt;&gt; x=torch.arange(1,7).resize_(2,3)
&gt;&gt;&gt; x

1  2  3
4  5  6
[torch.FloatTensor of size 2x3]

&gt;&gt;&gt; torch.kthvalue(x,2,0,True)
(
4  5  6
[torch.FloatTensor of size 1x3]
 ,
1  1  1
[torch.LongTensor of size 1x3]
)</code></pre>


<pre class="codehilite"><code class="language-py">torch.le(input, other, out=None) → Tensor</code></pre>


<p>逐元素比较 <code>input</code> 和 <code>other</code> , 即是否 <strong>input&lt;=other</strong> 如果两个张量有相同的形状和元素值, 则返回 <code>True</code> ,否则 <code>False</code> .</p>
<p>第二个参数可以为一个数或形状可 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> 为和第一个参数相同类型的张量.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 待对比的张量</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 对比的张量或 <code>float</code> 值</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量. 必须为 <code>ByteTensor</code> 或者与第一个参数 <code>tensor</code> 相同类型.</li>
</ul>
<p>|
| --- | --- |
| Returns: | 一个 <code>torch.ByteTensor</code> 张量, 包含了每个位置的比较结果(是否 input &lt;= other ). |
| --- | --- |
| Return type: | 张量 |
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.le(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))
 1  0
 1  1
[torch.ByteTensor of size 2x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.lt(input, other, out=None) → Tensor</code></pre>


<p>逐元素比较 <code>input</code> 和 <code>other</code> , 即是否 <strong>input&lt;other</strong> 如果两个张量有相同的形状和元素值, 则返回 <code>True</code> ,否则 <code>False</code> .</p>
<p>第二个参数可以为一个数或形状可 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> 为和第一个参数相同类型的张量.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 待对比的张量</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 对比的张量或 <code>float</code> 值</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量. 必须为 <code>ByteTensor</code> 或者与第一个参数 <code>tensor</code> 相同类型.</li>
</ul>
<p>|
| --- | --- |
| Returns: | 一个 <code>torch.ByteTensor</code> 张量, 包含了每个位置的比较结果(是否 input &lt; other ). |
| --- | --- |
| Return type: | 张量 |
| --- | --- |</p>
<p>Return type: 张量</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.lt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))
 0  0
 1  0
[torch.ByteTensor of size 2x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.max()</code></pre>


<pre class="codehilite"><code class="language-py">torch.max(input) → float</code></pre>


<p>返回输入 <code>input</code> 张量所有元素的最大值.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

 0.4729 -0.2266 -0.2085
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.max(a)
0.4729</code></pre>


<pre class="codehilite"><code class="language-py">torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)</code></pre>


<p>返回输入张量 <code>input</code> 在给定维度 <code>dim</code> 上每行的最大值, 并同时返回每个最大值的位置索引.</p>
<p>如果 <code>keepdim</code> 为 <code>True</code> , <code>values</code> 和 <code>indices</code> 张量都和 <code>input</code> 尺寸相同, 除了在所有值都为 1 的 <code>dim</code> 维度上. 如果 <code>keepdim</code> 为 <code>False</code> , <code>dim</code> 被压缩. (参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a> ), 使 <code>values</code> 和 <code>indices</code> 两个张量比 <code>input</code> 张量小一个的维度.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>k</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 第 <code>k</code> 个最小值</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>,</em> <em>optional</em>) – 沿着此维进行排序</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 输出张量是否保持维度 <code>dim</code> 不变</li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>,</em> <em>optional</em>) – 输出元组 (max, max_indices)</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt; a = torch.randn(4, 4)
&gt;&gt; a

0.0692  0.3142  1.2513 -0.5428
0.9288  0.8552 -0.2073  0.6409
1.0695 -0.0101 -2.4507 -1.2230
0.7426 -0.7666  0.4862 -0.6628
torch.FloatTensor of size 4x4]

&gt;&gt;&gt; torch.max(a, 1)
(
 1.2513
 0.9288
 1.0695
 0.7426
[torch.FloatTensor of size 4]
,
 2
 0
 0
 0
[torch.LongTensor of size 4]
)</code></pre>


<pre class="codehilite"><code class="language-py">torch.max(input, other, out=None) → Tensor</code></pre>


<p>输入 <code>input</code> 每一个元素和对应的比较张量 <code>other</code> 进行比较, 留下较大的元素 &lt;cite&gt;max&lt;/cite&gt;.</p>
<p>要比较的张量 <code>input</code> 与比较张量 <code>other</code> 不必大小一致, 但它们一定要能 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> .</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要比较张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 比较张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

 1.3869
 0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b

 1.0067
-0.8010
 0.6258
 0.3627
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.max(a, b)

 1.3869
 0.3912
 0.6258
 0.3627
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.min()</code></pre>


<pre class="codehilite"><code class="language-py">torch.min(input) → float</code></pre>


<p>返回输入张量 <code>input</code> 所有元素的最小值.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

 0.4729 -0.2266 -0.2085
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.min(a)
-0.22663167119026184</code></pre>


<pre class="codehilite"><code class="language-py">torch.min(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)</code></pre>


<p>返回输入张量 <code>input</code> 在给定维度 <code>dim</code> 下每行元素的最小值. 其中第二个返回值是每个被找出的最小值的索引位置 ( argmin ) .</p>
<p>如果 <code>keepdim</code> 是 <code>True</code>, 输出张量的大小与输入张量 <code>input</code> 相同, 除了维数 <code>dim</code> 是 1 . 另外, <code>dim</code> 被挤压 (参看 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a> ), 导致输出张量比输入张量 <code>input</code> 少一维.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 要减少的维度</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 输出张量的维度 <code>dim</code> 保持与否</li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>,</em> <em>optional</em>) – 两个输出张量的结果元组 (min, min_indices)</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt; a = torch.randn(4, 4)
&gt;&gt; a

0.0692  0.3142  1.2513 -0.5428
0.9288  0.8552 -0.2073  0.6409
1.0695 -0.0101 -2.4507 -1.2230
0.7426 -0.7666  0.4862 -0.6628
torch.FloatTensor of size 4x4]

&gt;&gt; torch.min(a, 1)

0.5428
0.2073
2.4507
0.7666
torch.FloatTensor of size 4]

3
2
2
1
torch.LongTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.min(input, other, out=None) → Tensor</code></pre>


<p>输入 <code>input</code> 每一个元素和对应的比较张量 <code>other</code> 进行比较, 留下较小的元素 &lt;cite&gt;min&lt;/cite&gt; .</p>
<p>要比较的张量 <code>input</code> 与比较张量 <code>other</code> 不必尺寸一致, 但它们一定要能广播 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> .</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 第一个张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 第二个张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出的张量 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

 1.3869
 0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b

 1.0067
-0.8010
 0.6258
 0.3627
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.min(a, b)

 1.0067
-0.8010
-0.8634
-0.5468
[torch.FloatTensor of size 4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.ne(input, other, out=None) → Tensor</code></pre>


<p>逐元素比较 <code>input</code> 和 <code>other</code> , 即是否 <strong>tensor != other</strong> 如果两个张量有相同的形状和元素值, 则返回 <code>True</code> , 否则 <code>False</code> .</p>
<p>第二个参数可以为一个数或形状广播 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> 为和第一个参数相同类型的张量.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 待对比的张量</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 对比的张量或 <code>float</code> 值</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出张量. 必须为 <code>ByteTensor</code> 或者与第一个参数 <code>tensor</code> 相同类型.</li>
</ul>
<p>|
| --- | --- |
| Returns: | 一个 <code>torch.ByteTensor</code> 张量, 包含了每个位置的比较结果 (是否 input != other ) . |
| --- | --- |
| Return type: | 张量 |
| --- | --- |</p>
<p>返回类型: <code>Tensor</code></p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.ne(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))
 0  1
 1  0
[torch.ByteTensor of size 2x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.sort(input, dim=None, descending=False, out=None) -&gt; (Tensor, LongTensor)</code></pre>


<p>对输入张量 <code>input</code> 沿着指定维按升序排序.</p>
<p>如果不给定 <code>dim</code> ,则默认为输入的最后一维.</p>
<p>如果指定参数 <code>descending</code> 为 <code>True</code> , 则按降序排序.</p>
<p>返回元组 (sorted_tensor, sorted_indices) , sorted_indices 为原始输入中的下标.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要对比的张量</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>,</em> <em>optional</em>) – 沿着此维排序</li>
<li><strong>descending</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>,</em> <em>optional</em>) – 布尔值, 控制升降排序</li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>,</em> <em>optional</em>) – 输出张量. 必须为 ByteTensor 或者与第一个参数 tensor 相同类型.</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; sorted, indices = torch.sort(x)
&gt;&gt;&gt; sorted

-1.6747  0.0610  0.1190  1.4137
-1.4782  0.7159  1.0341  1.3678
-0.3324 -0.0782  0.3518  0.4763
[torch.FloatTensor of size 3x4]

&gt;&gt;&gt; indices

 0  1  3  2
 2  1  0  3
 3  1  0  2
[torch.LongTensor of size 3x4]

&gt;&gt;&gt; sorted, indices = torch.sort(x, 0)
&gt;&gt;&gt; sorted

-1.6747 -0.0782 -1.4782 -0.3324
 0.3518  0.0610  0.4763  0.1190
 1.0341  0.7159  1.4137  1.3678
[torch.FloatTensor of size 3x4]

&gt;&gt;&gt; indices

 0  2  1  2
 2  0  2  0
 1  1  0  1
[torch.LongTensor of size 3x4]</code></pre>


<pre class="codehilite"><code class="language-py">torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)</code></pre>


<p>沿给定 dim 维度返回输入张量 <code>input</code> 中 <code>k</code> 个最大值. 如果不指定 <code>dim</code> , 则默认为 &lt;cite&gt;input&lt;/cite&gt; 的最后一维. 如果为 <code>largest</code> 为 <code>False</code> ,则返回最小的 &lt;cite&gt;k&lt;/cite&gt; 个值. 返回一个元组 &lt;cite&gt;(values, indices)&lt;/cite&gt; , 其中 indices 是原始输入张量 input 中测元素下标. 如果设定布尔值 <code>sorted</code> 为 <code>True</code> , 将会确保返回的 &lt;cite&gt;k&lt;/cite&gt; 个值被排序.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量</li>
<li><strong>k</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – “top-k” 中的 k</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>,</em> <em>optional</em>) – 排序的维</li>
<li><strong>largest</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>,</em> <em>optional</em>) – 布尔值, 控制返回最大或最小值</li>
<li><strong>sorted</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>,</em> <em>optional</em>) – 布尔值, 控制返回值是否排序</li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>,</em> <em>optional</em>) – 可选输出张量 (Tensor, LongTensor) output buffers</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.arange(1, 6)
&gt;&gt;&gt; x

 1
 2
 3
 4
 5
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.topk(x, 3)
(
 5
 4
 3
[torch.FloatTensor of size 3]
,
 4
 3
 2
[torch.LongTensor of size 3]
)
&gt;&gt;&gt; torch.topk(x, 3, 0, largest=False)
(
 1
 2
 3
[torch.FloatTensor of size 3]
,
 0
 1
 2
[torch.LongTensor of size 3]
)</code></pre>


<h3 id="other-operations">Other Operations (其它操作)</h3>
<pre class="codehilite"><code class="language-py">torch.cross(input, other, dim=-1, out=None) → Tensor</code></pre>


<p>返回沿着维度 &lt;cite&gt;dim&lt;/cite&gt; 上, 两个张量 &lt;cite&gt;input&lt;/cite&gt; 和 &lt;cite&gt;other&lt;/cite&gt; 的向量积 (叉积), &lt;cite&gt;input&lt;/cite&gt; 和 &lt;cite&gt;other&lt;/cite&gt; 必须有相同的形状, 且指定的 &lt;cite&gt;dim&lt;/cite&gt; 维上 &lt;cite&gt;size&lt;/cite&gt; 必须为 3.</p>
<p>如果不指定 &lt;cite&gt;dim&lt;/cite&gt;, 则默认为第一个尺度为 3 的维.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 第二个输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>,</em> <em>optional</em>) – 沿着此维进行叉积操作.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(4, 3)
&gt;&gt;&gt; a

-0.6652 -1.0116 -0.6857
 0.2286  0.4446 -0.5272
 0.0476  0.2321  1.9991
 0.6199  1.1924 -0.9397
[torch.FloatTensor of size 4x3]

&gt;&gt;&gt; b = torch.randn(4, 3)
&gt;&gt;&gt; b

-0.1042 -1.1156  0.1947
 0.9947  0.1149  0.4701
-1.0108  0.8319 -0.0750
 0.9045 -1.3754  1.0976
[torch.FloatTensor of size 4x3]

&gt;&gt;&gt; torch.cross(a, b, dim=1)

-0.9619  0.2009  0.6367
 0.2696 -0.6318 -0.4160
-1.6805 -2.0171  0.2741
 0.0163 -1.5304 -1.9311
[torch.FloatTensor of size 4x3]

&gt;&gt;&gt; torch.cross(a, b)

-0.9619  0.2009  0.6367
 0.2696 -0.6318 -0.4160
-1.6805 -2.0171  0.2741
 0.0163 -1.5304 -1.9311
[torch.FloatTensor of size 4x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.diag(input, diagonal=0, out=None) → Tensor</code></pre>


<ul>
<li>如果输入是一个向量( &lt;cite&gt;1D&lt;/cite&gt; 张量), 则返回一个以 &lt;cite&gt;input&lt;/cite&gt; 为对角线元素的 &lt;cite&gt;2D&lt;/cite&gt; 方阵.</li>
<li>如果输入是一个矩阵( &lt;cite&gt;2D&lt;/cite&gt; 张量), 则返回一个包含 &lt;cite&gt;input&lt;/cite&gt; 对角线元素的1D张量.</li>
</ul>
<p>参数 &lt;cite&gt;diagonal&lt;/cite&gt; 指定对角线:</p>
<ul>
<li><code>diagonal</code> = 0, 主对角线.</li>
<li><code>diagonal</code> &gt; 0, 主对角线之上.</li>
<li><code>diagonal</code> &lt; 0, 主对角线之下.</li>
</ul>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>diagonal</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>,</em> <em>optional</em>) – 指定对角线</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<p>获得以 &lt;cite&gt;input&lt;/cite&gt; 为对角线的方阵:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a

 1.0480
-2.3405
-1.1138
[torch.FloatTensor of size 3]

&gt;&gt;&gt; torch.diag(a)

 1.0480  0.0000  0.0000
 0.0000 -2.3405  0.0000
 0.0000  0.0000 -1.1138
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.diag(a, 1)

 0.0000  1.0480  0.0000  0.0000
 0.0000  0.0000 -2.3405  0.0000
 0.0000  0.0000  0.0000 -1.1138
 0.0000  0.0000  0.0000  0.0000
[torch.FloatTensor of size 4x4]</code></pre>


<p>获得给定矩阵的第k条对角线:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a

-1.5328 -1.3210 -1.5204
 0.8596  0.0471 -0.2239
-0.6617  0.0146 -1.0817
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.diag(a, 0)

-1.5328
 0.0471
-1.0817
[torch.FloatTensor of size 3]

&gt;&gt;&gt; torch.diag(a, 1)

-1.3210
-0.2239
[torch.FloatTensor of size 2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor</code></pre>


<p>计算输入张量的直方图.</p>
<p>以 &lt;cite&gt;min&lt;/cite&gt; 和 &lt;cite&gt;max&lt;/cite&gt; 为 &lt;cite&gt;range&lt;/cite&gt; 边界, 将其均分成 &lt;cite&gt;bins&lt;/cite&gt; 个直条, 然后将排序好的数据划分到各个直条 &lt;cite&gt;(bins)&lt;/cite&gt; 中. 如果 &lt;cite&gt;min&lt;/cite&gt; 和 &lt;cite&gt;max&lt;/cite&gt; 都为 0, 则利用数据中的最大最小值作为边界.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量</li>
<li><strong>bins</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 直方图 &lt;cite&gt;bins&lt;/cite&gt; (直条)的个数(默认100个)</li>
<li><strong>min</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – &lt;cite&gt;range&lt;/cite&gt; 的下边界(包含)</li>
<li><strong>max</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – &lt;cite&gt;range&lt;/cite&gt; 的上边界(包含)</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
</ul>
<p>|
| --- | --- |
| Returns: | 直方图 |
| --- | --- |
| Return type: | <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> |
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.histc(torch.FloatTensor([1, 2, 1]), bins=4, min=0, max=3)
FloatTensor([0, 2, 1, 0])</code></pre>


<pre class="codehilite"><code class="language-py">torch.renorm(input, p, dim, maxnorm, out=None) → Tensor</code></pre>


<p>返回一个张量, 包含规范化后的各个子张量, 使得沿着 &lt;cite&gt;dim&lt;/cite&gt; 维划分的各子张量的 &lt;cite&gt;p&lt;/cite&gt; 范数小于 &lt;cite&gt;maxnorm&lt;/cite&gt;</p>
<p>Note</p>
<p>如果 p 范数的值小于 &lt;cite&gt;maxnorm&lt;/cite&gt;, 则当前子张量不需要修改.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>p</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 范数的 &lt;cite&gt;p&lt;/cite&gt;</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 沿着此维切片, 得到张量子集</li>
<li><strong>maxnorm</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 每个子张量的范数的最大值</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 结果张量</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.ones(3, 3)
&gt;&gt;&gt; x[1].fill_(2)
&gt;&gt;&gt; x[2].fill_(3)
&gt;&gt;&gt; x

 1  1  1
 2  2  2
 3  3  3
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.renorm(x, 1, 0, 5)

 1.0000  1.0000  1.0000
 1.6667  1.6667  1.6667
 1.6667  1.6667  1.6667
[torch.FloatTensor of size 3x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.trace(input) → float</code></pre>


<p>返回输入 2 维矩阵对角线元素的和(迹).</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.arange(1, 10).view(3, 3)
&gt;&gt;&gt; x

 1  2  3
 4  5  6
 7  8  9
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.trace(x)
15.0</code></pre>


<pre class="codehilite"><code class="language-py">torch.tril(input, diagonal=0, out=None) → Tensor</code></pre>


<p>返回一个张量, 包含输入矩阵 ( &lt;cite&gt;2D&lt;/cite&gt; 张量)的下三角部分, 其余部分被设为 0.</p>
<p>这里所说的下三角部分为矩阵指定对角线 &lt;cite&gt;diagonal&lt;/cite&gt; 在线里的和下面的元素.</p>
<p>参数 &lt;cite&gt;diagonal&lt;/cite&gt; 控制对角线.</p>
<ul>
<li><code>diagonal</code> = 0, 主对角线.</li>
<li><code>diagonal</code> &gt; 0, 主对角线之上.</li>
<li><code>diagonal</code> &lt; 0, 主对角线之下.</li>
</ul>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>diagonal</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>,</em> <em>optional</em>) – 指定对角线</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(3,3)
&gt;&gt;&gt; a

 1.3225  1.7304  1.4573
-0.3052 -0.3111 -0.1809
 1.2469  0.0064 -1.6250
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.tril(a)

 1.3225  0.0000  0.0000
-0.3052 -0.3111  0.0000
 1.2469  0.0064 -1.6250
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.tril(a, diagonal=1)

 1.3225  1.7304  0.0000
-0.3052 -0.3111 -0.1809
 1.2469  0.0064 -1.6250
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.tril(a, diagonal=-1)

 0.0000  0.0000  0.0000
-0.3052  0.0000  0.0000
 1.2469  0.0064  0.0000
[torch.FloatTensor of size 3x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.triu(input, diagonal=0, out=None) → Tensor</code></pre>


<p>返回一个张量, 包含输入矩阵 ( &lt;cite&gt;2D&lt;/cite&gt; 张量)的上三角部分, 其余部分被设为 0.</p>
<p>这里所说的下三角部分为矩阵指定对角线 &lt;cite&gt;diagonal&lt;/cite&gt; 在线里的和上面的元素.</p>
<p>参数 &lt;cite&gt;diagonal&lt;/cite&gt; 控制对角线.</p>
<ul>
<li><code>diagonal</code> = 0, 主对角线.</li>
<li><code>diagonal</code> &gt; 0, 主对角线之上.</li>
<li><code>diagonal</code> &lt; 0, 主对角线之下.</li>
</ul>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>diagonal</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>,</em> <em>optional</em>) – 指定对角线</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出 &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(3,3)
&gt;&gt;&gt; a

 1.3225  1.7304  1.4573
-0.3052 -0.3111 -0.1809
 1.2469  0.0064 -1.6250
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.triu(a)

 1.3225  1.7304  1.4573
 0.0000 -0.3111 -0.1809
 0.0000  0.0000 -1.6250
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.triu(a, diagonal=1)

 0.0000  1.7304  1.4573
 0.0000  0.0000 -0.1809
 0.0000  0.0000  0.0000
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.triu(a, diagonal=-1)

 1.3225  1.7304  1.4573
-0.3052 -0.3111 -0.1809
 0.0000  0.0064 -1.6250
[torch.FloatTensor of size 3x3]</code></pre>


<h3 id="blas-and-lapack-operations-blaslapack">BLAS and LAPACK Operations (BLAS和LAPACK操作)</h3>
<pre class="codehilite"><code class="language-py">torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor</code></pre>


<p>执行保存在 <code>batch1</code> 和 <code>batch2</code> 中的矩阵的批量点乘, 伴随着一个减少的相加步骤 (所有的矩阵乘法沿第一维累加). <code>mat</code> 被相加到最终的结果中.</p>
<p><code>batch1</code> 和 <code>batch2</code> 必须是三维的张量, 且每个包含相同数量的矩阵.</p>
<p>如果 <code>batch1</code> 是一个 &lt;cite&gt;b x n x m&lt;/cite&gt; 的张量, <code>batch2</code> 是一个 &lt;cite&gt;b x m x p<code>的张量, 那么 ::attr:</code>mat&lt;/cite&gt; 必须是 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> 且是一个 &lt;cite&gt;n x p&lt;/cite&gt; 的张量, 同时 attr:&lt;cite&gt;out&lt;/cite&gt; 将是一个 &lt;cite&gt;n x p&lt;/cite&gt; 的张量.</p>
<p>换句话说, <code>\(res = (beta * M) + (alpha * sum(batch1_i @ batch2_i, i = 0, b))\)</code></p>
<p>对于 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或者 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 类型的输入, 参数 &lt;cite&gt;beta&lt;/cite&gt; 和 &lt;cite&gt;alpha&lt;/cite&gt; 必须是实数, 否则他们应该是整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – 作用于 <code>mat</code> 的乘子 (系数)</li>
<li><strong>mat</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要被相加的矩阵</li>
<li><strong>alpha</strong> (<em>Number__,</em> <em>optional</em>) – 作用于 &lt;cite&gt;batch1 @ batch2&lt;/cite&gt; 的乘子</li>
<li><strong>batch1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要相乘的第一批矩阵</li>
<li><strong>batch2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要相乘的第二批矩阵</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出的张量结果</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; M = torch.randn(3, 5)
&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.addbmm(M, batch1, batch2)

 -3.1162  11.0071   7.3102   0.1824  -7.6892
 1.8265   6.0739   0.4589  -0.5641  -5.4283
 -9.3387  -0.1794  -1.2318  -6.8841  -4.7239
[torch.FloatTensor of size 3x5]</code></pre>


<pre class="codehilite"><code class="language-py">torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None) → Tensor</code></pre>


<p>执行矩阵 <code>mat1</code> 和 <code>mat2</code> 的相乘. 矩阵 <code>mat</code> 将与相乘的最终计算结果相加.</p>
<p>如果 <code>mat1</code> 是一个 &lt;cite&gt;n x m&lt;/cite&gt; 的张量, <code>mat2</code> 是一个 &lt;cite&gt;m x p<code>的张量, 那么 ::attr:</code>mat&lt;/cite&gt; 必须是 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> 且是一个 &lt;cite&gt;n x p&lt;/cite&gt; 的张量, 同时 attr:&lt;cite&gt;out&lt;/cite&gt; 将是一个 &lt;cite&gt;n x p&lt;/cite&gt; 的张量.</p>
<p>换句话说, <code>\(out = (beta * M) + (alpha * mat1 @ mat2)\)</code></p>
<p>对于 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或者 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 类型的输入, 参数 &lt;cite&gt;beta&lt;/cite&gt; 和 &lt;cite&gt;alpha&lt;/cite&gt; 必须是实数, 否则他们应该是整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – 作用于:attr:[<code>](#id1)mat</code>的乘子</li>
<li><strong>mat</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要被相加的矩阵</li>
<li><strong>alpha</strong> (<em>Number__,</em> <em>optional</em>) – 作用于<code>mat1 @ mat2</code>的乘子</li>
<li><strong>mat1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要相乘的第一个矩阵</li>
<li><strong>mat2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要相乘的第二个矩阵</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出结果</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; M = torch.randn(2, 3)
&gt;&gt;&gt; mat1 = torch.randn(2, 3)
&gt;&gt;&gt; mat2 = torch.randn(3, 3)
&gt;&gt;&gt; torch.addmm(M, mat1, mat2)

-0.4095 -1.9703  1.3561
 5.7674 -4.9760  2.7378
[torch.FloatTensor of size 2x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None) → Tensor</code></pre>


<p>执行矩阵 <code>mat</code> 和向量 <code>vec</code> 的相乘. 矩阵 <code>tensor</code> 将与相乘的最终计算结果相加.</p>
<p>如果 <code>mat</code> 是一个 &lt;cite&gt;n x m&lt;/cite&gt; 的张量, <code>vec</code> 是一个长度为 &lt;cite&gt;m&lt;/cite&gt; 的一维张量, 那么 :<code>tensor</code> 必须是 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> 且是一个长度为 &lt;cite&gt;n&lt;/cite&gt; 的一维张量, 同时 attr:&lt;cite&gt;out&lt;/cite&gt; 将是一个长度为 &lt;cite&gt;n&lt;/cite&gt; 的一维张量.</p>
<p>&lt;cite&gt;alpha&lt;/cite&gt; 和 &lt;cite&gt;beta&lt;/cite&gt; 分别是 &lt;cite&gt;mat * vec&lt;/cite&gt; 和 &lt;cite&gt;tensor&lt;/cite&gt; 的缩放因子.</p>
<p>换句话说, <code>\(out = (beta * tensor) + (alpha * (mat @ vec2))\)</code></p>
<p>对于 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或者 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 类型的输入, 参数 &lt;cite&gt;beta&lt;/cite&gt; 和 &lt;cite&gt;alpha&lt;/cite&gt; 必须是实数, 否则他们应该是整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – 作用于 <code>tensor</code> 的乘子</li>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要被相加的向量</li>
<li><strong>alpha</strong> (<em>Number__,</em> <em>optional</em>) – 作用于 &lt;cite&gt;mat @ vec&lt;/cite&gt; 的乘子</li>
<li><strong>mat</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要被相乘的矩阵</li>
<li><strong>vec</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要被要乘的向量</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出结果</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; M = torch.randn(2)
&gt;&gt;&gt; mat = torch.randn(2, 3)
&gt;&gt;&gt; vec = torch.randn(3)
&gt;&gt;&gt; torch.addmv(M, mat, vec)

-2.0939
-2.2950
[torch.FloatTensor of size 2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None) → Tensor</code></pre>


<p>执行向量:attr:&lt;cite&gt;vec1&lt;/cite&gt; 和 <code>vec2</code> 的外积, 并把外积计算结果与矩阵 <code>mat</code> 相加.</p>
<p>可选值 <code>beta</code> 和 <code>alpha</code> 是标量, 分别与 <code>mat</code> 和 <code>\((vec1 \otimes vec2)\)</code> 相乘.</p>
<p>换句话说, <code>\(out = (beta * mat) + (alpha * vec1 \otimes vec2)\)</code></p>
<p>如果 <code>vec1</code> 是一个长度为 &lt;cite&gt;n&lt;/cite&gt; 的向量, <code>vec2</code> 是一个长度为 &lt;cite&gt;m&lt;/cite&gt; 的向量, 那么 <code>mat</code> 必须是 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> 且是一个大小为 &lt;cite&gt;n x m&lt;/cite&gt; 的矩阵, 同时 <code>out</code> 将是一个大小为 &lt;cite&gt;n x m&lt;/cite&gt; 的矩阵.</p>
<p>对于 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或者 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 类型的输入, 参数 &lt;cite&gt;beta&lt;/cite&gt; 和 &lt;cite&gt;alpha&lt;/cite&gt; 必须是实数, 否则他们应该是整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – 作用于 <code>mat</code> 的乘子</li>
<li><strong>mat</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要被相加的矩阵</li>
<li><strong>alpha</strong> (<em>Number__,</em> <em>optional</em>) – 作用于 <code>vec1</code> 和 <code>vec2</code> 外积计算结果的乘子</li>
<li><strong>vec1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 外积计算的第一个向量</li>
<li><strong>vec2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 外积计算的第二个向量</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出结果</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; vec1 = torch.arange(1, 4)
&gt;&gt;&gt; vec2 = torch.arange(1, 3)
&gt;&gt;&gt; M = torch.zeros(3, 2)
&gt;&gt;&gt; torch.addr(M, vec1, vec2)
 1  2
 2  4
 3  6
[torch.FloatTensor of size 3x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor</code></pre>


<p>执行保存在 <code>batch1</code> 和 <code>batch2</code> 中的矩阵的批量点乘. <code>mat</code> 被相加到最终的结果中.</p>
<p><code>batch1</code> 和 <code>batch2</code> 必须是三维的张量, 且每个包含相同数量的矩阵.</p>
<p>如果 <code>batch1</code> 是一个 &lt;cite&gt;b x n x m&lt;/cite&gt; 的张量, <code>batch2</code> 是一个 &lt;cite&gt;b x m x p<code>的张量, 那么 ::attr:</code>mat&lt;/cite&gt; 必须是 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> 且是一个 &lt;cite&gt;b x n x p&lt;/cite&gt; 的张量, 同时 attr:&lt;cite&gt;out&lt;/cite&gt; 将是一个 &lt;cite&gt;b x n x p&lt;/cite&gt; 的张量.</p>
<p>换句话说, <code>\(res_i = (beta * M_i) + (alpha * batch1_i \times batch2_i)\)</code></p>
<p>对于 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或者 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 类型的输入, 参数 &lt;cite&gt;beta&lt;/cite&gt; 和 &lt;cite&gt;alpha&lt;/cite&gt; 必须是实数, 否则他们应该是整数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – 作用于 <code>mat</code> 的乘子 (系数)</li>
<li><strong>mat</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要被相加的张量</li>
<li><strong>alpha</strong> (<em>Number__,</em> <em>optional</em>) – 作用于 &lt;cite&gt;batch1 @ batch2&lt;/cite&gt; 的乘子</li>
<li><strong>batch1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要相乘的第一批矩阵</li>
<li><strong>batch2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要相乘的第二批矩阵</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出的张量结果</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; M = torch.randn(10, 3, 5)
&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size()
torch.Size([10, 3, 5])</code></pre>


<pre class="codehilite"><code class="language-py">torch.bmm(batch1, batch2, out=None) → Tensor</code></pre>


<p>执行保存在 <code>batch1</code> 和 <code>batch2</code> 中的矩阵的批量点乘.</p>
<p><code>batch1</code> 和 <code>batch2</code> 必须是三维的张量, 且每个包含相同数量的矩阵.</p>
<p>如果 <code>batch1</code> 是一个 &lt;cite&gt;b x n x m&lt;/cite&gt; 的张量, <code>batch2</code> 是一个 &lt;cite&gt;b x m x p&lt;/cite&gt; 的张量, <code>out</code> 将是一个 &lt;cite&gt;b x n x p&lt;/cite&gt; 的张量.</p>
<p>Note</p>
<p>这个函数不能参考 <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>. 对于广播矩阵相乘, 参见 <a href="#torch.matmul" title="torch.matmul"><code>torch.matmul()</code></a>.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>batch1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要相乘的第一批矩阵</li>
<li><strong>batch2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要相乘的第二批矩阵</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – 输出结果</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; res = torch.bmm(batch1, batch2)
&gt;&gt;&gt; res.size()
torch.Size([10, 3, 5])</code></pre>


<pre class="codehilite"><code class="language-py">torch.btrifact(A, info=None, pivot=True) → Tensor, IntTensor</code></pre>


<p>批量 LU 分解.</p>
<p>返回一个包含 LU 分解和枢轴的元组. 对于每个 minibatch 示例, 如果分解成功, 可选参数 &lt;cite&gt;info&lt;/cite&gt; 将提供分解信息. &lt;cite&gt;info&lt;/cite&gt; 的值来自 dgetrf, 若是非零值, 则表示有错误发生. 如果 cuda 被使用的话, 具体的值来自 cublas, 否则来自 LAPACK. 如果设置了 pivot, 那么旋转操作将被执行.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>A</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 要分解的张量.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; A_LU = A.btrifact()</code></pre>


<pre class="codehilite"><code class="language-py">torch.btrisolve(b, LU_data, LU_pivots) → Tensor</code></pre>


<p>批量 LU 解.</p>
<p>返回线性系统 Ax = b 的 LU 解.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>b</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – RHS tensor.</li>
<li><strong>LU_data</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Pivoted LU factorization of A from btrifact.</li>
<li><strong>LU_pivots</strong> (<em>IntTensor</em>) – Pivots of the LU factorization.</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; b = torch.randn(2, 3)
&gt;&gt;&gt; A_LU = torch.btrifact(A)
&gt;&gt;&gt; x = b.btrisolve(*A_LU)
&gt;&gt;&gt; torch.norm(A.bmm(x.unsqueeze(2)) - b)
6.664001874625056e-08</code></pre>


<pre class="codehilite"><code class="language-py">torch.dot(tensor1, tensor2) → float</code></pre>


<p>计算两个张量的点乘 (内积).</p>
<p>Note</p>
<p>这个函数不支持 <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>.</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.dot(torch.Tensor([2, 3]), torch.Tensor([2, 1]))
7.0</code></pre>


<pre class="codehilite"><code class="language-py">torch.eig(a, eigenvectors=False, out=None) -&gt; (Tensor, Tensor)</code></pre>


<p>计算实数方阵的特征值和特征向量.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>a</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 一个要被计算特征值与特征向量的方阵</li>
<li><strong>eigenvectors</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 若为 <code>True</code>, 表示特征值与特征向量都被计算. 否则, 仅计算特征值.</li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>,</em> <em>optional</em>) – 输出张量</li>
</ul>
<p>|
| --- | --- |
| Returns: | 包含</p>
<p>&gt; *   <strong>e</strong> (<em>Tensor</em>): <code>a</code> 的左特征值
&gt; *   <strong>v</strong> (<em>Tensor</em>): 如果 <code>eigenvectors</code> 为 <code>True</code>, 表示 <code>a</code> 的特征向量; 否则是一个空的张量</p>
<p>|
| --- | --- |
| Return type: | 返回一个元组, (<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) |
| --- | --- |</p>
<pre class="codehilite"><code class="language-py">torch.gels(B, A, out=None) → Tensor</code></pre>


<p>计算秩为 <code>\(m\)</code> 的， 大小为 m x n 的矩阵 <code>\(A\)</code> 最小二乘和最小范数问题的解</p>
<p>如果 <code>\(m &amp;gt;= n\)</code>, <a href="#torch.gels" title="torch.gels"><code>gels()</code></a> 求解最小二乘问题:</p>
<pre class="codehilite"><code class="language-py">\[\begin{array}{ll} \mbox{minimize} &amp; \|AX-B\|_F. \end{array}\]</code></pre>


<p>如果 <code>\(m &amp;lt; n\)</code>, <a href="#torch.gels" title="torch.gels"><code>gels()</code></a> 求解最小范数问题:</p>
<pre class="codehilite"><code class="language-py">\[\begin{array}{ll} \mbox{minimize} &amp; \|X\|_F &amp; \mbox{subject to} &amp; AX = B. \end{array}\]</code></pre>


<p>返回的矩阵 <code>\(X\)</code> 的头 <code>\(n\)</code> 行包含解信息. 其余行包含剩余信息: 从第 <code>\(n\)</code> 行开始的每列的 euclidean 范数, 是对应列的剩余.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>B</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The matrix <code>\(B\)</code></li>
<li><strong>A</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The <code>\(m\)</code> by <code>\(n\)</code> matrix <code>\(A\)</code></li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>,</em> <em>optional</em>) – Optional destination tensor</li>
</ul>
<p>|
| --- | --- |
| Returns: | tuple containing:</p>
<p>&gt; *   <strong>X</strong> (<em>Tensor</em>): 最小二乘解
&gt; *   <strong>qr</strong> (<em>Tensor</em>): QR 分解的详细信息</p>
<p>|
| --- | --- |
| Return type: | (<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) |
| --- | --- |</p>
<p>Note</p>
<p>不管输入矩阵的步长如何, 返回来的矩阵将总是被转置. 也就是, 他们的步长是 &lt;cite&gt;(1, m)&lt;/cite&gt; 而不是 &lt;cite&gt;(m, 1)&lt;/cite&gt;.</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; A = torch.Tensor([[1, 1, 1],
...                   [2, 3, 4],
...                   [3, 5, 2],
...                   [4, 2, 5],
...                   [5, 4, 3]])
&gt;&gt;&gt; B = torch.Tensor([[-10, -3],
 [ 12, 14],
 [ 14, 12],
 [ 16, 16],
 [ 18, 16]])
&gt;&gt;&gt; X, _ = torch.gels(B, A)
&gt;&gt;&gt; X
2.0000  1.0000
1.0000  1.0000
1.0000  2.0000
[torch.FloatTensor of size 3x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.geqrf(input, out=None) -&gt; (Tensor, Tensor)</code></pre>


<p>这是直接调用 LAPACK 的低层函数.</p>
<p>通常您应该使用 <a href="#torch.qr" title="torch.qr"><code>torch.qr()</code></a> 来代替之.</p>
<p>计算 <code>input</code> 的 QR 分解, 但不构造 &lt;cite&gt;Q&lt;/cite&gt; 和 &lt;cite&gt;R&lt;/cite&gt; 作为显示分开的矩阵.</p>
<p>然而, 这样直接调用 LAPACK 的底层函数 &lt;cite&gt;?geqrf&lt;/cite&gt;, 会产生一连串的 ‘elementary reflectors’.</p>
<p>更多信息请参见 <a href="https://software.intel.com/en-us/node/521004">LAPACK documentation</a> .</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input matrix</li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>,</em> <em>optional</em>) – The result tuple of (Tensor, Tensor)</li>
</ul>
<p>|
| --- | --- |</p>
<pre class="codehilite"><code class="language-py">torch.ger(vec1, vec2, out=None) → Tensor</code></pre>


<p>计算 <code>vec1</code> 和 <code>vec2</code> 的外积. 如果 <code>vec1</code> 是一个长度为 &lt;cite&gt;n&lt;/cite&gt; 的向量, <code>vec2</code> 是一个长度为 &lt;cite&gt;m&lt;/cite&gt; 的向量, 那么 <code>out</code> 必须是一个 &lt;cite&gt;n x m&lt;/cite&gt; 的矩阵.</p>
<p>Note</p>
<p>这个函数不支持 <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>vec1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 1D input vector</li>
<li><strong>vec2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 1D input vector</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – optional output matrix</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; v1 = torch.arange(1, 5)
&gt;&gt;&gt; v2 = torch.arange(1, 4)
&gt;&gt;&gt; torch.ger(v1, v2)

 1   2   3
 2   4   6
 3   6   9
 4   8  12
[torch.FloatTensor of size 4x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.gesv(B, A, out=None) -&gt; (Tensor, Tensor)</code></pre>


<p>&lt;cite&gt;X, LU = torch.gesv(B, A)&lt;/cite&gt; , 该函数返回线性系统 <code>\(AX = B\)</code> 的解.</p>
<p>&lt;cite&gt;LU&lt;/cite&gt; 包含 &lt;cite&gt;A&lt;/cite&gt; 的 LU 分解因子 &lt;cite&gt;L&lt;/cite&gt; 和 &lt;cite&gt;U&lt;/cite&gt;.</p>
<p><code>A</code> 必须是方阵, 且是非奇异的 (2维可逆张量).</p>
<p>如果 &lt;cite&gt;A&lt;/cite&gt; 是一个 &lt;cite&gt;m x m&lt;/cite&gt; 矩阵, &lt;cite&gt;B&lt;/cite&gt; 是一个 &lt;cite&gt;m x k&lt;/cite&gt; 的矩阵, 那么结果 &lt;cite&gt;LU&lt;/cite&gt; 的大小为 &lt;cite&gt;m x m&lt;/cite&gt;, &lt;cite&gt;X&lt;/cite&gt; 的大小为 &lt;cite&gt;m x k&lt;/cite&gt; .</p>
<p>Note</p>
<p>Irrespective of the original strides, the returned matrices &lt;cite&gt;X&lt;/cite&gt; and &lt;cite&gt;LU&lt;/cite&gt; will be transposed, i.e. with strides &lt;cite&gt;(1, m)&lt;/cite&gt; instead of &lt;cite&gt;(m, 1)&lt;/cite&gt;.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>B</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input matrix of &lt;cite&gt;m x k&lt;/cite&gt; dimensions</li>
<li><strong>A</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input square matrix of &lt;cite&gt;m x m&lt;/cite&gt; dimensions</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – optional output matrix</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; A = torch.Tensor([[6.80, -2.11,  5.66,  5.97,  8.23],
...                   [-6.05, -3.30,  5.36, -4.44,  1.08],
...                   [-0.45,  2.58, -2.70,  0.27,  9.04],
...                   [8.32,  2.71,  4.35,  -7.17,  2.14],
...                   [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()
&gt;&gt;&gt; B = torch.Tensor([[4.02,  6.19, -8.22, -7.57, -3.03],
...                   [-1.56,  4.00, -8.67,  1.75,  2.86],
...                   [9.81, -4.09, -4.57, -8.61,  8.99]]).t()
&gt;&gt;&gt; X, LU = torch.gesv(B, A)
&gt;&gt;&gt; torch.dist(B, torch.mm(A, X))
9.250057093890353e-06</code></pre>


<pre class="codehilite"><code class="language-py">torch.inverse(input, out=None) → Tensor</code></pre>


<p>计算方阵 <code>input</code> 的逆.</p>
<p>Note</p>
<p>Irrespective of the original strides, the returned matrix will be transposed, i.e. with strides &lt;cite&gt;(1, m)&lt;/cite&gt; instead of &lt;cite&gt;(m, 1)&lt;/cite&gt;</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input 2D square &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the optional output &lt;cite&gt;Tensor&lt;/cite&gt;</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.rand(10, 10)
&gt;&gt;&gt; x

 0.7800  0.2267  0.7855  0.9479  0.5914  0.7119  0.4437  0.9131  0.1289  0.1982
 0.0045  0.0425  0.2229  0.4626  0.6210  0.0207  0.6338  0.7067  0.6381  0.8196
 0.8350  0.7810  0.8526  0.9364  0.7504  0.2737  0.0694  0.5899  0.8516  0.3883
 0.6280  0.6016  0.5357  0.2936  0.7827  0.2772  0.0744  0.2627  0.6326  0.9153
 0.7897  0.0226  0.3102  0.0198  0.9415  0.9896  0.3528  0.9397  0.2074  0.6980
 0.5235  0.6119  0.6522  0.3399  0.3205  0.5555  0.8454  0.3792  0.4927  0.6086
 0.1048  0.0328  0.5734  0.6318  0.9802  0.4458  0.0979  0.3320  0.3701  0.0909
 0.2616  0.3485  0.4370  0.5620  0.5291  0.8295  0.7693  0.1807  0.0650  0.8497
 0.1655  0.2192  0.6913  0.0093  0.0178  0.3064  0.6715  0.5101  0.2561  0.3396
 0.4370  0.4695  0.8333  0.1180  0.4266  0.4161  0.0699  0.4263  0.8865  0.2578
[torch.FloatTensor of size 10x10]

&gt;&gt;&gt; x = torch.rand(10, 10)
&gt;&gt;&gt; y = torch.inverse(x)
&gt;&gt;&gt; z = torch.mm(x, y)
&gt;&gt;&gt; z

 1.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000
 0.0000  1.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000 -0.0000 -0.0000
 0.0000  0.0000  1.0000 -0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000
 0.0000  0.0000  0.0000  1.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000  0.0000
 0.0000  0.0000 -0.0000 -0.0000  1.0000  0.0000  0.0000 -0.0000 -0.0000 -0.0000
 0.0000  0.0000  0.0000 -0.0000  0.0000  1.0000 -0.0000 -0.0000 -0.0000 -0.0000
 0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  1.0000  0.0000 -0.0000  0.0000
 0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000  1.0000 -0.0000  0.0000
-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000  1.0000 -0.0000
-0.0000  0.0000 -0.0000 -0.0000 -0.0000  0.0000 -0.0000 -0.0000  0.0000  1.0000
[torch.FloatTensor of size 10x10]

&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(10))) # Max nonzero
5.096662789583206e-07</code></pre>


<pre class="codehilite"><code class="language-py">torch.matmul(tensor1, tensor2, out=None)</code></pre>


<p>Matrix product of two tensors.</p>
<p>The behavior depends on the dimensionality of the tensors as follows:</p>
<ul>
<li>If both tensors are 1-dimensional, the dot product (scalar) is returned.</li>
<li>If both arguments are 2-dimensional, the matrix-matrix product is returned.</li>
<li>If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.</li>
<li>If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.</li>
<li>If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N &gt; 2), then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are <a href="notes/broadcasting.html#broadcasting-semantics">broadcasted</a> (and thus must be broadcastable). For example, if <code>tensor1</code> is a &lt;cite&gt;j x 1 x n x m&lt;/cite&gt; Tensor and <code>tensor2</code> is a &lt;cite&gt;k x m x p&lt;/cite&gt; Tensor, <code>out</code> will be an &lt;cite&gt;j x k x n x p&lt;/cite&gt; Tensor.</li>
</ul>
<p>Note</p>
<p>The 1-dimensional dot product version of this function does not support an <code>out</code> parameter.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>tensor1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – First tensor to be multiplied</li>
<li><strong>tensor2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Second tensor to be multiplied</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – Output tensor</li>
</ul>
<p>|
| --- | --- |</p>
<pre class="codehilite"><code class="language-py">torch.mm(mat1, mat2, out=None) → Tensor</code></pre>


<p>执行 <code>mat1</code> 和 <code>mat2</code> 的矩阵乘法.</p>
<p>如果 <code>mat1</code> 是一个 &lt;cite&gt;n x m&lt;/cite&gt; 张量, <code>mat2</code> 是一个 &lt;cite&gt;m x p&lt;/cite&gt; 张量, <code>out</code> 将是一个 &lt;cite&gt;n x p&lt;/cite&gt; 张量.</p>
<p>Note</p>
<p>这个函数不支持 <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>. 要使用支持广播矩阵乘法, 参见 <a href="#torch.matmul" title="torch.matmul"><code>torch.matmul()</code></a>.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>mat1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – First matrix to be multiplied</li>
<li><strong>mat2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Second matrix to be multiplied</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – Output tensor</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; mat1 = torch.randn(2, 3)
&gt;&gt;&gt; mat2 = torch.randn(3, 3)
&gt;&gt;&gt; torch.mm(mat1, mat2)
 0.0519 -0.3304  1.2232
 4.3910 -5.1498  2.7571
[torch.FloatTensor of size 2x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.mv(mat, vec, out=None) → Tensor</code></pre>


<p>执行矩阵 <code>mat</code> 与向量 <code>vec</code> 的乘法操作.</p>
<p>如果 <code>mat</code> 是一个 &lt;cite&gt;n x m&lt;/cite&gt; 张量, <code>vec</code> 是一个大小为 &lt;cite&gt;m&lt;/cite&gt; 的一维张量, <code>out</code> 将是一个大小为 &lt;cite&gt;n&lt;/cite&gt; 的张量.</p>
<p>Note</p>
<p>这个函数不支持 <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>mat</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – matrix to be multiplied</li>
<li><strong>vec</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – vector to be multiplied</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – Output tensor</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; mat = torch.randn(2, 3)
&gt;&gt;&gt; vec = torch.randn(3)
&gt;&gt;&gt; torch.mv(mat, vec)
-2.0939
-2.2950
[torch.FloatTensor of size 2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.orgqr()</code></pre>


<pre class="codehilite"><code class="language-py">torch.ormqr()</code></pre>


<pre class="codehilite"><code class="language-py">torch.potrf(a, out=None)</code></pre>


<p>potrf(a, upper, out=None)</p>
<p>计算半正定矩阵 <code>a</code>: 的 Cholesky 分解. 返回结果 &lt;cite&gt;u&lt;/cite&gt;, 若 &lt;cite&gt;upper&lt;/cite&gt; 设为 <code>True</code> 或未提供时, &lt;cite&gt;u&lt;/cite&gt; 是一个上三角矩阵, 使得 <code>\(a = u^T u\)</code> 成立; 若 &lt;cite&gt;upper&lt;/cite&gt; 设为 <code>False</code>, &lt;cite&gt;u&lt;/cite&gt; 是一个下三角矩阵, 使得 <code>\(a = u u^T\)</code> 成立.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>a</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input 2D &lt;cite&gt;Tensor&lt;/cite&gt;, a symmetric positive semidefinite matrix</li>
<li><strong>upper</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>,</em> <em>optional</em>) – Return upper (default) or lower triangular matrix</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – A Tensor for u</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(3,3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive definite
&gt;&gt;&gt; u = torch.potrf(a)
&gt;&gt;&gt; a

 2.3563  3.2318 -0.9406
 3.2318  4.9557 -2.1618
-0.9406 -2.1618  2.2443
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; u

 1.5350  2.1054 -0.6127
 0.0000  0.7233 -1.2053
 0.0000  0.0000  0.6451
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.mm(u.t(),u)

 2.3563  3.2318 -0.9406
 3.2318  4.9557 -2.1618
-0.9406 -2.1618  2.2443
[torch.FloatTensor of size 3x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.potri(u, out=None)</code></pre>


<p>potri(u, upper, out=None)</p>
<p>给定一个半正定矩阵的 Cholesky 分解因子 <code>u</code>, 计算该半正定矩阵的逆. 返回矩阵 &lt;cite&gt;inv&lt;/cite&gt;, 若 &lt;cite&gt;upper&lt;/cite&gt; 设为 <code>True</code> 或为提供, &lt;cite&gt;u&lt;/cite&gt; 是一个上三角矩阵, 使得 <code>\(inv = (u^T u)^{-1}\)</code> 成立; 若 &lt;cite&gt;upper&lt;/cite&gt; 设为 <code>False</code>, &lt;cite&gt;u&lt;/cite&gt; 是一个下三角矩阵, 使得 <code>\(inv = (u u^T)^{-1}\)</code> 成立.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>u</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input 2D &lt;cite&gt;Tensor&lt;/cite&gt;, a upper or lower triangular Cholesky factor</li>
<li><strong>upper</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>,</em> <em>optional</em>) – Flag if upper (default) or lower triangular matrix</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – A Tensor for inv</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(3,3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive definite
&gt;&gt;&gt; u = torch.potrf(a)
&gt;&gt;&gt; a

 2.3563  3.2318 -0.9406
 3.2318  4.9557 -2.1618
-0.9406 -2.1618  2.2443
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.potri(u)

 12.5724 -10.1765  -4.5333
-10.1765   8.5852   4.0047
 -4.5333   4.0047   2.4031
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; a.inverse()

 12.5723 -10.1765  -4.5333
-10.1765   8.5852   4.0047
 -4.5333   4.0047   2.4031
[torch.FloatTensor of size 3x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.potrs(b, u, out=None)</code></pre>


<p>potrs(b, u, upper, out=None)</p>
<p>Solves a linear system of equations with a positive semidefinite matrix to be inverted given its given a Cholesky factor matrix <code>u</code>: returns matrix &lt;cite&gt;c&lt;/cite&gt; If &lt;cite&gt;upper&lt;/cite&gt; is <code>True</code> or not provided, &lt;cite&gt;u&lt;/cite&gt; is and upper triangular such that <code>\(c = (u^T u)^{-1} b\)</code>. If &lt;cite&gt;upper&lt;/cite&gt; is <code>False</code>, &lt;cite&gt;u&lt;/cite&gt; is and lower triangular such that <code>\(c = (u u^T)^{-1} b\)</code>.</p>
<p>Note</p>
<p>&lt;cite&gt;b&lt;/cite&gt; is always a 2D &lt;cite&gt;Tensor&lt;/cite&gt;, use &lt;cite&gt;b.unsqueeze(1)&lt;/cite&gt; to convert a vector.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>b</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the right hand side 2D &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>u</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input 2D &lt;cite&gt;Tensor&lt;/cite&gt;, a upper or lower triangular Cholesky factor</li>
<li><strong>upper</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>,</em> <em>optional</em>) – Return upper (default) or lower triangular matrix</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – A Tensor for c</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(3,3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive definite
&gt;&gt;&gt; u = torch.potrf(a)
&gt;&gt;&gt; a

 2.3563  3.2318 -0.9406
 3.2318  4.9557 -2.1618
-0.9406 -2.1618  2.2443
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; b = torch.randn(3,2)
&gt;&gt;&gt; b

-0.3119 -1.8224
-0.2798  0.1789
-0.3735  1.7451
[torch.FloatTensor of size 3x2]

&gt;&gt;&gt; torch.potrs(b,u)

 0.6187 -32.6438
-0.7234  27.0703
-0.6039  13.1717
[torch.FloatTensor of size 3x2]

&gt;&gt;&gt; torch.mm(a.inverse(),b)

 0.6187 -32.6436
-0.7234  27.0702
-0.6039  13.1717
[torch.FloatTensor of size 3x2]</code></pre>


<pre class="codehilite"><code class="language-py">torch.pstrf(a, out=None)</code></pre>


<p>pstrf(a, upper, out=None)</p>
<p>Computes the pivoted Cholesky decomposition of a positive semidefinite matrix <code>a</code>: returns matrices &lt;cite&gt;u&lt;/cite&gt; and &lt;cite&gt;piv&lt;/cite&gt;. If &lt;cite&gt;upper&lt;/cite&gt; is <code>True</code> or not provided, &lt;cite&gt;u&lt;/cite&gt; is and upper triangular such that <code>\(a = p^T u^T u p\)</code>, with &lt;cite&gt;p&lt;/cite&gt; the permutation given by &lt;cite&gt;piv&lt;/cite&gt;. If &lt;cite&gt;upper&lt;/cite&gt; is <code>False</code>, &lt;cite&gt;u&lt;/cite&gt; is and lower triangular such that <code>\(a = p^T u u^T p\)</code>.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>a</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input 2D &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>upper</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>,</em> <em>optional</em>) – Return upper (default) or lower triangular matrix</li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>,</em> <em>optional</em>) – A tuple of u and piv Tensors</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.randn(3,3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive definite
&gt;&gt;&gt; a

 5.4417 -2.5280  1.3643
-2.5280  2.9689 -2.1368
 1.3643 -2.1368  4.6116
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; u,piv = torch.pstrf(a)
&gt;&gt;&gt; u

 2.3328  0.5848 -1.0837
 0.0000  2.0663 -0.7274
 0.0000  0.0000  1.1249
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; piv

 0
 2
 1
[torch.IntTensor of size 3]

&gt;&gt;&gt; p = torch.eye(3).index_select(0,piv.long()).index_select(0,piv.long()).t() # make pivot permutation
&gt;&gt;&gt; torch.mm(torch.mm(p.t(),torch.mm(u.t(),u)),p) # reconstruct

 5.4417  1.3643 -2.5280
 1.3643  4.6116 -2.1368
-2.5280 -2.1368  2.9689
[torch.FloatTensor of size 3x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.qr(input, out=None) -&gt; (Tensor, Tensor)</code></pre>


<p>计算矩阵 <code>input</code> 的 QR 分解. 返回矩阵 &lt;cite&gt;q&lt;/cite&gt; 和 &lt;cite&gt;r&lt;/cite&gt; 使得 <code>\(x = q * r\)</code>, 且 &lt;cite&gt;q&lt;/cite&gt; 是一个 正交矩阵, &lt;cite&gt;r&lt;/cite&gt; 是一个上三角矩阵.</p>
<p>This returns the thin (reduced) QR factorization.</p>
<p>Note</p>
<p>如果矩阵 &lt;cite&gt;input&lt;/cite&gt; 中的元素太大, 那么精度可能会丢失.</p>
<p>Note</p>
<p>尽管该函数总是能给您一个有效的分解, 但在不同平台上结果可能不同 - 取决于该平台上 LAPACK 的实现.</p>
<p>Note</p>
<p>Irrespective of the original strides, the returned matrix &lt;cite&gt;q&lt;/cite&gt; will be transposed, i.e. with strides &lt;cite&gt;(1, m)&lt;/cite&gt; instead of &lt;cite&gt;(m, 1)&lt;/cite&gt;.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input 2D &lt;cite&gt;Tensor&lt;/cite&gt;</li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>,</em> <em>optional</em>) – A tuple of Q and R Tensors</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.Tensor([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])
&gt;&gt;&gt; q, r = torch.qr(a)
&gt;&gt;&gt; q

-0.8571  0.3943  0.3314
-0.4286 -0.9029 -0.0343
 0.2857 -0.1714  0.9429
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; r

 -14.0000  -21.0000   14.0000
 0.0000 -175.0000   70.0000
 0.0000    0.0000  -35.0000
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.mm(q, r).round()

 12  -51    4
 6  167  -68
 -4   24  -41
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.mm(q.t(), q).round()

 1 -0  0
-0  1  0
 0  0  1
[torch.FloatTensor of size 3x3]</code></pre>


<pre class="codehilite"><code class="language-py">torch.svd(input, some=True, out=None) -&gt; (Tensor, Tensor, Tensor)</code></pre>


<p>&lt;cite&gt;U, S, V = torch.svd(A)&lt;/cite&gt; 返回大小为 &lt;cite&gt;(n x m)&lt;/cite&gt; 的实矩阵 &lt;cite&gt;A&lt;/cite&gt; 的奇异值分解, 使得 <code>\(A = USV'*\)</code>.</p>
<p>&lt;cite&gt;U&lt;/cite&gt; 的大小为 &lt;cite&gt;n x n&lt;/cite&gt;</p>
<p>&lt;cite&gt;S&lt;/cite&gt; 的大小为<code>n x m</code></p>
<p>&lt;cite&gt;V&lt;/cite&gt; 的大小为 &lt;cite&gt;m x m&lt;/cite&gt;.</p>
<p><code>some</code> 表示将被计算的奇异值的总数. 如果 &lt;cite&gt;some=True&lt;/cite&gt;, 它将计算指定的 some 数量个奇异值, 如果 &lt;cite&gt;some=False&lt;/cite&gt;, 则计算所有奇异值.</p>
<p>Note</p>
<p>Irrespective of the original strides, the returned matrix &lt;cite&gt;U&lt;/cite&gt; will be transposed, i.e. with strides &lt;cite&gt;(1, n)&lt;/cite&gt; instead of &lt;cite&gt;(n, 1)&lt;/cite&gt;.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input 2D Tensor</li>
<li><strong>some</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>,</em> <em>optional</em>) – controls the number of singular values to be computed</li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>,</em> <em>optional</em>) – the result tuple</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.Tensor([[8.79,  6.11, -9.15,  9.57, -3.49,  9.84],
...                   [9.93,  6.91, -7.93,  1.64,  4.02,  0.15],
...                   [9.83,  5.04,  4.86,  8.83,  9.80, -8.99],
...                   [5.45, -0.27,  4.85,  0.74, 10.00, -6.02],
...                   [3.16,  7.98,  3.01,  5.80,  4.27, -5.31]]).t()
&gt;&gt;&gt; a

 8.7900   9.9300   9.8300   5.4500   3.1600
 6.1100   6.9100   5.0400  -0.2700   7.9800
 -9.1500  -7.9300   4.8600   4.8500   3.0100
 9.5700   1.6400   8.8300   0.7400   5.8000
 -3.4900   4.0200   9.8000  10.0000   4.2700
 9.8400   0.1500  -8.9900  -6.0200  -5.3100
[torch.FloatTensor of size 6x5]

&gt;&gt;&gt; u, s, v = torch.svd(a)
&gt;&gt;&gt; u

-0.5911  0.2632  0.3554  0.3143  0.2299
-0.3976  0.2438 -0.2224 -0.7535 -0.3636
-0.0335 -0.6003 -0.4508  0.2334 -0.3055
-0.4297  0.2362 -0.6859  0.3319  0.1649
-0.4697 -0.3509  0.3874  0.1587 -0.5183
 0.2934  0.5763 -0.0209  0.3791 -0.6526
[torch.FloatTensor of size 6x5]

&gt;&gt;&gt; s

 27.4687
 22.6432
 8.5584
 5.9857
 2.0149
[torch.FloatTensor of size 5]

&gt;&gt;&gt; v

-0.2514  0.8148 -0.2606  0.3967 -0.2180
-0.3968  0.3587  0.7008 -0.4507  0.1402
-0.6922 -0.2489 -0.2208  0.2513  0.5891
-0.3662 -0.3686  0.3859  0.4342 -0.6265
-0.4076 -0.0980 -0.4932 -0.6227 -0.4396
[torch.FloatTensor of size 5x5]

&gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))
8.934150226306685e-06</code></pre>


<pre class="codehilite"><code class="language-py">torch.symeig(input, eigenvectors=False, upper=True, out=None) -&gt; (Tensor, Tensor)</code></pre>


<p>&lt;cite&gt;e, V = torch.symeig(input)&lt;/cite&gt; 返回实对称矩阵 <code>input</code> 的特征值和特征向量.</p>
<p>&lt;cite&gt;input&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; 是 &lt;cite&gt;m x m&lt;/cite&gt; 矩阵, &lt;cite&gt;e&lt;/cite&gt; 是一个 &lt;cite&gt;m&lt;/cite&gt; 维的向量.</p>
<p>这个函数计算矩阵 &lt;cite&gt;input&lt;/cite&gt; 的所有特征值 (和向量), 使得 &lt;cite&gt;input = V diag(e) V’&lt;/cite&gt;.</p>
<p>布尔参数 <code>eigenvectors</code> 定义了是否计算特征向量. 如果它为 <code>False</code>, 那么只有特征值会被计算. 如果它为 <code>True</code>, 特征值和特征向量都会被计算.</p>
<p>由于输入矩阵 &lt;cite&gt;input&lt;/cite&gt; 被假定是对称的, 因此默认地只有它的上三角部分会被使用.</p>
<p>如果 <code>upper</code> 是 <code>False</code>, 那么它的下三角部分会被使用.</p>
<p>Note: Irrespective of the original strides, the returned matrix &lt;cite&gt;V&lt;/cite&gt; will be transposed, i.e. with strides &lt;cite&gt;(1, m)&lt;/cite&gt; instead of &lt;cite&gt;(m, 1)&lt;/cite&gt;.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input symmetric matrix</li>
<li><strong>eigenvectors</strong> (<em>boolean__,</em> <em>optional</em>) – controls whether eigenvectors have to be computed</li>
<li><strong>upper</strong> (<em>boolean__,</em> <em>optional</em>) – controls whether to consider upper-triangular or lower-triangular region</li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>,</em> <em>optional</em>) – The result tuple of (Tensor, Tensor)</li>
</ul>
<p>|
| --- | --- |</p>
<p>Examples:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; a = torch.Tensor([[ 1.96,  0.00,  0.00,  0.00,  0.00],
...                   [-6.49,  3.80,  0.00,  0.00,  0.00],
...                   [-0.47, -6.39,  4.17,  0.00,  0.00],
...                   [-7.20,  1.50, -1.51,  5.70,  0.00],
...                   [-0.65, -6.34,  2.67,  1.80, -7.10]]).t()

&gt;&gt;&gt; e, v = torch.symeig(a, eigenvectors=True)
&gt;&gt;&gt; e

-11.0656
 -6.2287
 0.8640
 8.8655
 16.0948
[torch.FloatTensor of size 5]

&gt;&gt;&gt; v

-0.2981 -0.6075  0.4026 -0.3745  0.4896
-0.5078 -0.2880 -0.4066 -0.3572 -0.6053
-0.0816 -0.3843 -0.6600  0.5008  0.3991
-0.0036 -0.4467  0.4553  0.6204 -0.4564
-0.8041  0.4480  0.1725  0.3108  0.1622
[torch.FloatTensor of size 5x5]</code></pre>


<pre class="codehilite"><code class="language-py">torch.trtrs()</code></pre>
                
                  
                
              
              
                


              

              <hr/>
              <div align="center">
                  <p><a href="http://www.apachecn.org/" target="_blank"><font face="KaiTi" size="6" color="red">我们一直在努力</font></a><p>
                  <p><a href="https://github.com/apachecn/pytorch-doc-zh/" target="_blank">apachecn/pytorch-doc-zh</a></p>
                  <p><iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&repo=pytorch-doc-zh&type=watch&count=true&v=2" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
                  <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&repo=pytorch-doc-zh&type=star&count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
                  <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&repo=pytorch-doc-zh&type=fork&count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
                  <a target="_blank" href="//shang.qq.com/wpa/qunwpa?idkey=bcee938030cc9e1552deb3bd9617bbbf62d3ec1647e4b60d9cd6b6e8f78ddc03"><img border="0" src="//pub.idqqimg.com/wpa/images/group.png" alt="ML | ApacheCN" title="ML | ApacheCN"></a></p>
                  <div style="text-align:center;margin:0 0 10.5px;"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
                        <ins class="adsbygoogle"
                             style="display:inline-block;width:728px;height:90px"
                             data-ad-client="ca-pub-3565452474788507"
                             data-ad-slot="2543897000"></ins>
                        <script>
                        (adsbygoogle = window.adsbygoogle || []).push({});
                        </script></div>
              </div>

              <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
              <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
              <div id="gitalk-container" class="container-fluid"></div>
              <script type="text/javascript">
                  var gitalk = new Gitalk({
                  clientID: 'f27b87eb424ba43df978',
                  clientSecret: '9b3482a495c5257a1d269d8108b9bfd71f048c3c',
                  repo: 'pytorch-doc-zh',
                  owner: 'apachecn',
                  admin: ['jiangzhonglian'],
                  id: md5(location.pathname),
                  distractionFreeMode: false
                  })
                  gitalk.render('gitalk-container')
              </script>
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../52/" title="Package 参考" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                Package 参考
              </span>
            </div>
          </a>
        
        
          <a href="../54/" title="torch.Tensor" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                torch.Tensor
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.583bbe55.js"></script>
      
        
        
          
          <script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../assets/javascripts/lunr/lunr.jp.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
    
      
    
  </body>
</html>