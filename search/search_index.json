{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch \u4e2d\u6587\u6587\u6863 PyTorch \u662f\u4e00\u4e2a\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60, \u5e76\u4e14\u4f7f\u7528 GPU \u548c CPU \u6765\u4f18\u5316\u7684 tensor library (\u5f20\u91cf\u5e93) . \u5b98\u65b9\u5730\u5740: http://pytorch.org/ \u4e2d\u6587\u793e\u533a: http://pytorch.apachecn.org/ \u6ce8\u610f \u6211\u4eec\u5e76\u6ca1\u6709\u5c06\u672c\u9879\u76ee\u5347\u7ea7\u5230 0.4 \u7684\u8ba1\u5212\u3002 \u8bf7\u7b49\u5f85 1.0\uff0c\u6216\u8005\u8bbf\u95ee PyTorch 0.4 \u4e2d\u6587\u6587\u6863 \u3002 \u6587\u6863\u5730\u5740 PyTorch \u4e2d\u6587\u793e\u533a: http://pytorch.apachecn.org/ PyTorch \u4e2d\u6587\u6587\u6863: http://pytorch.apachecn.org/cn/docs/0.3.0/ PyTorch \u4e2d\u6587\u6559\u7a0b: http://pytorch.apachecn.org/cn/tutorials/ PyTorch \u82f1\u6587\u6587\u6863: http://pytorch.apachecn.org/en/docs/0.3.0/ PyTorch \u82f1\u6587\u6559\u7a0b: http://pytorch.apachecn.org/en/tutorials/ \u9879\u76ee\u8d1f\u8d23\u4eba \u7ef4\u62a4\u7ec4\u7ec7: @ApacheCN Pytorch 0.2.0 \u7b2c\u4e00\u671f (2017-04-11) @awfssv Pytorch 0.3.0 \u7b2c\u4e00\u671f (2018-01-09) @\u90a3\u4f0a\u62b9\u5fae\u7b11 Pytorch 0.3.0 \u7b2c\u4e8c\u671f (2018-04-04) @\u98de\u9f99 @\u54b8\u9c7c @\u7247\u523b \u8d21\u732e\u8005 PyTorch 0.3.0 \u4e2d\u6587\u6587\u6863\u8d21\u732e\u8005 \u6807\u9898 \u7ffb\u8bd1 \u6821\u5bf9 \u6587\u6863\u90e8\u5206 - - \u4ecb\u7ecd - - \u81ea\u52a8\u6c42\u5bfc\u673a\u5236 @\u90a3\u4f0a\u62b9\u5fae\u7b11 @Twinkle \u5e7f\u64ad\u8bed\u4e49 @\u8c22\u5bb6\u67ef @Twinkle CUDA \u8bed\u4e49 @Chris @Twinkle \u6269\u5c55 PyTorch @\u90a3\u4f0a\u62b9\u5fae\u7b11 @Twinkle \u591a\u8fdb\u7a0b\u7684\u6700\u4f73\u5b9e\u8df5 @\u51af\u6590 @Twinkle \u5e8f\u5217\u5316\u8bed\u4e49 @\u80e1\u4e1c\u7476 @Twinkle PACKAGE \u53c2\u8003 - - torch @\u90a3\u4f0a\u62b9\u5fae\u7b11 @yudong @\u5c0f\u7476 @\u7247\u523b @\u674e\u96e8\u9f99 @K @devin @\u5f20\u5047\u98de @rickllyxu @\u5f20\u5047\u98de @\u98de\u9f99 torch.Tensor @Sylvester @\u90a3\u4f0a\u62b9\u5fae\u7b11 @Sariel torch.sparse @\u738b\u5e05 @Timor torch.Storage @FanXing @Timor torch.nn @\u5c0f\u738b\u5b50 @\u90a3\u4f0a\u62b9\u5fae\u7b11 @Yang Shun @Zhu Yansen @woaichipinngguo @buldajs @\u5409\u601d\u96e8 @\u738b\u4e91\u5cf0 @\u674e\u96e8\u9f99 @Yucong Zhu @\u6797\u5609\u5e94 @QianFanCe @dabney777 @Alex @SiKai Yao @\u5c0f\u4e54 @laihongchang @\u567c\u91cc\u556a\u5566\u5623 @BarrettLi @KrokYin @MUSK1881 @clown9804 @\u98de\u9f99 torch.optim @\u4e8e\u589e\u6e90 @\u9752\u6885\u5f80\u4e8b torch.autograd @ZhenLei Xu @\u9752\u6885\u5f80\u4e8b torch.distributions @\u53f6\u8212\u6cc9 @smilesboy @Charles Xu torch.multiprocessing @\u591c\u795e\u6708 @smilesboy torch.distributed @Mu Wu9527 @smilesboy torch.legacy @\u90a3\u4f0a\u62b9\u5fae\u7b11 @smilesboy torch.cuda @\u8c08\u7b11\u98ce\u751f @smilesboy torch.utils.ffi @\u4e4b\u8317 @aleczhang torch.utils.data @\u4e4b\u8317 @aleczhang torch.utils.model_zoo @\u4e4b\u8317 @aleczhang torch.onnx @Haofan Wang @aleczhang TORCHVISION \u53c2\u8003 - - torchvision @\u90a3\u4f0a\u62b9\u5fae\u7b11 @dawenzi123 @LeeGeong @liandongze @\u54b8\u9c7c \u6559\u7a0b\u90e8\u5206 - - \u521d\u5b66\u8005\u6559\u7a0b - - PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b @\u5c0f\u738b\u5b50 @\u674e\u5b50\u6587 \u4e3a\u4ee5\u524d Torch \u7528\u6237\u63d0\u4f9b\u7684 Pytorch \u6559\u7a0b @unknown @bringtree \u8ddf\u7740\u4f8b\u5b50\u5b66\u4e60 PyTorch @yongjay13 @speedmancs @bringtree \u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b @Sylvester @Archie Yu \u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b @distant1219 @bringtree Deep Learning for NLP with Pytorch @JingTao @friedhelm739 \u4e2d\u7ea7\u6559\u7a0b - - Classifying Names with a Character-Level RNN @\u5b59\u6c38\u6770 Generating Names with a Character-Level RNN @jianchengss Translation with a Sequence to Sequence Network and Attention @EWilsen Reinforcement Learning (DQN) tutorial @Lisanaaa Writing Distributed Applications with PyTorch @Sylvester Spatial Transformer Networks Tutorial @Twinkle \u9ad8\u7ea7\u6559\u7a0b - - Neural Transfer with PyTorch @Twinkle Creating extensions using numpy and scipy @\u98de\u9f99 Transfering a model from PyTorch to Caffe2 and Mobile using ONNX @\u7247\u523b Custom C extensions for pytorch @\u98de\u9f99 \u6ce8: \u659c\u4f53\u4ee3\u8868\u672a\u5b8c\u6210 PyTorch 0.2.0 \u4e2d\u6587\u6587\u6863\u8d21\u732e\u8005 @awfssv (\u7ec4\u7ec7\u8005) @ycszen @KeithYin @kophy @swordspoet @dyl745001196 @koshinryuu @tfygg @weigp @ZijunDeng @yichuan9527 \u52a0\u5165\u6211\u4eec \u5982\u679c\u60f3\u8981\u52a0\u5165\u6211\u4eec, \u8bf7\u53c2\u9605: http://www.apachecn.org/organization/209.html . \u6b22\u8fce\u5404\u4f4d\u7231\u88c5\u903c\u7684\u5927\u4f6c\u4eec. \u5efa\u8bae\u53cd\u9988 \u8054\u7cfb\u9879\u76ee\u8d1f\u8d23\u4eba @\u90a3\u4f0a\u62b9\u5fae\u7b11 . \u5728\u6211\u4eec\u7684 apachecn/pytorch-doc-zh github \u4e0a\u63d0 issue. \u53d1\u90ae\u4ef6\u9001\u5230 Email: pytorch#apachecn.org (#\u66ff\u6362\u6210@) . \u5728\u6211\u4eec\u7684 \u7ec4\u7ec7\u5b66\u4e60\u4ea4\u6d41\u7fa4 \u4e2d\u8054\u7cfb\u7fa4\u4e3b/\u7ba1\u7406\u5458\u5373\u53ef. \u8d5e\u52a9\u6211\u4eec","title":"\u4e3b\u9875"},{"location":"#pytorch","text":"PyTorch \u662f\u4e00\u4e2a\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60, \u5e76\u4e14\u4f7f\u7528 GPU \u548c CPU \u6765\u4f18\u5316\u7684 tensor library (\u5f20\u91cf\u5e93) . \u5b98\u65b9\u5730\u5740: http://pytorch.org/ \u4e2d\u6587\u793e\u533a: http://pytorch.apachecn.org/ \u6ce8\u610f \u6211\u4eec\u5e76\u6ca1\u6709\u5c06\u672c\u9879\u76ee\u5347\u7ea7\u5230 0.4 \u7684\u8ba1\u5212\u3002 \u8bf7\u7b49\u5f85 1.0\uff0c\u6216\u8005\u8bbf\u95ee PyTorch 0.4 \u4e2d\u6587\u6587\u6863 \u3002","title":"PyTorch \u4e2d\u6587\u6587\u6863"},{"location":"#_1","text":"PyTorch \u4e2d\u6587\u793e\u533a: http://pytorch.apachecn.org/ PyTorch \u4e2d\u6587\u6587\u6863: http://pytorch.apachecn.org/cn/docs/0.3.0/ PyTorch \u4e2d\u6587\u6559\u7a0b: http://pytorch.apachecn.org/cn/tutorials/ PyTorch \u82f1\u6587\u6587\u6863: http://pytorch.apachecn.org/en/docs/0.3.0/ PyTorch \u82f1\u6587\u6559\u7a0b: http://pytorch.apachecn.org/en/tutorials/","title":"\u6587\u6863\u5730\u5740"},{"location":"#_2","text":"\u7ef4\u62a4\u7ec4\u7ec7: @ApacheCN Pytorch 0.2.0 \u7b2c\u4e00\u671f (2017-04-11) @awfssv Pytorch 0.3.0 \u7b2c\u4e00\u671f (2018-01-09) @\u90a3\u4f0a\u62b9\u5fae\u7b11 Pytorch 0.3.0 \u7b2c\u4e8c\u671f (2018-04-04) @\u98de\u9f99 @\u54b8\u9c7c @\u7247\u523b","title":"\u9879\u76ee\u8d1f\u8d23\u4eba"},{"location":"#_3","text":"","title":"\u8d21\u732e\u8005"},{"location":"#pytorch-030","text":"\u6807\u9898 \u7ffb\u8bd1 \u6821\u5bf9 \u6587\u6863\u90e8\u5206 - - \u4ecb\u7ecd - - \u81ea\u52a8\u6c42\u5bfc\u673a\u5236 @\u90a3\u4f0a\u62b9\u5fae\u7b11 @Twinkle \u5e7f\u64ad\u8bed\u4e49 @\u8c22\u5bb6\u67ef @Twinkle CUDA \u8bed\u4e49 @Chris @Twinkle \u6269\u5c55 PyTorch @\u90a3\u4f0a\u62b9\u5fae\u7b11 @Twinkle \u591a\u8fdb\u7a0b\u7684\u6700\u4f73\u5b9e\u8df5 @\u51af\u6590 @Twinkle \u5e8f\u5217\u5316\u8bed\u4e49 @\u80e1\u4e1c\u7476 @Twinkle PACKAGE \u53c2\u8003 - - torch @\u90a3\u4f0a\u62b9\u5fae\u7b11 @yudong @\u5c0f\u7476 @\u7247\u523b @\u674e\u96e8\u9f99 @K @devin @\u5f20\u5047\u98de @rickllyxu @\u5f20\u5047\u98de @\u98de\u9f99 torch.Tensor @Sylvester @\u90a3\u4f0a\u62b9\u5fae\u7b11 @Sariel torch.sparse @\u738b\u5e05 @Timor torch.Storage @FanXing @Timor torch.nn @\u5c0f\u738b\u5b50 @\u90a3\u4f0a\u62b9\u5fae\u7b11 @Yang Shun @Zhu Yansen @woaichipinngguo @buldajs @\u5409\u601d\u96e8 @\u738b\u4e91\u5cf0 @\u674e\u96e8\u9f99 @Yucong Zhu @\u6797\u5609\u5e94 @QianFanCe @dabney777 @Alex @SiKai Yao @\u5c0f\u4e54 @laihongchang @\u567c\u91cc\u556a\u5566\u5623 @BarrettLi @KrokYin @MUSK1881 @clown9804 @\u98de\u9f99 torch.optim @\u4e8e\u589e\u6e90 @\u9752\u6885\u5f80\u4e8b torch.autograd @ZhenLei Xu @\u9752\u6885\u5f80\u4e8b torch.distributions @\u53f6\u8212\u6cc9 @smilesboy @Charles Xu torch.multiprocessing @\u591c\u795e\u6708 @smilesboy torch.distributed @Mu Wu9527 @smilesboy torch.legacy @\u90a3\u4f0a\u62b9\u5fae\u7b11 @smilesboy torch.cuda @\u8c08\u7b11\u98ce\u751f @smilesboy torch.utils.ffi @\u4e4b\u8317 @aleczhang torch.utils.data @\u4e4b\u8317 @aleczhang torch.utils.model_zoo @\u4e4b\u8317 @aleczhang torch.onnx @Haofan Wang @aleczhang TORCHVISION \u53c2\u8003 - - torchvision @\u90a3\u4f0a\u62b9\u5fae\u7b11 @dawenzi123 @LeeGeong @liandongze @\u54b8\u9c7c \u6559\u7a0b\u90e8\u5206 - - \u521d\u5b66\u8005\u6559\u7a0b - - PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b @\u5c0f\u738b\u5b50 @\u674e\u5b50\u6587 \u4e3a\u4ee5\u524d Torch \u7528\u6237\u63d0\u4f9b\u7684 Pytorch \u6559\u7a0b @unknown @bringtree \u8ddf\u7740\u4f8b\u5b50\u5b66\u4e60 PyTorch @yongjay13 @speedmancs @bringtree \u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b @Sylvester @Archie Yu \u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b @distant1219 @bringtree Deep Learning for NLP with Pytorch @JingTao @friedhelm739 \u4e2d\u7ea7\u6559\u7a0b - - Classifying Names with a Character-Level RNN @\u5b59\u6c38\u6770 Generating Names with a Character-Level RNN @jianchengss Translation with a Sequence to Sequence Network and Attention @EWilsen Reinforcement Learning (DQN) tutorial @Lisanaaa Writing Distributed Applications with PyTorch @Sylvester Spatial Transformer Networks Tutorial @Twinkle \u9ad8\u7ea7\u6559\u7a0b - - Neural Transfer with PyTorch @Twinkle Creating extensions using numpy and scipy @\u98de\u9f99 Transfering a model from PyTorch to Caffe2 and Mobile using ONNX @\u7247\u523b Custom C extensions for pytorch @\u98de\u9f99 \u6ce8: \u659c\u4f53\u4ee3\u8868\u672a\u5b8c\u6210","title":"PyTorch 0.3.0 \u4e2d\u6587\u6587\u6863\u8d21\u732e\u8005"},{"location":"#pytorch-020","text":"@awfssv (\u7ec4\u7ec7\u8005) @ycszen @KeithYin @kophy @swordspoet @dyl745001196 @koshinryuu @tfygg @weigp @ZijunDeng @yichuan9527","title":"PyTorch 0.2.0 \u4e2d\u6587\u6587\u6863\u8d21\u732e\u8005"},{"location":"#_4","text":"\u5982\u679c\u60f3\u8981\u52a0\u5165\u6211\u4eec, \u8bf7\u53c2\u9605: http://www.apachecn.org/organization/209.html . \u6b22\u8fce\u5404\u4f4d\u7231\u88c5\u903c\u7684\u5927\u4f6c\u4eec.","title":"\u52a0\u5165\u6211\u4eec"},{"location":"#_5","text":"\u8054\u7cfb\u9879\u76ee\u8d1f\u8d23\u4eba @\u90a3\u4f0a\u62b9\u5fae\u7b11 . \u5728\u6211\u4eec\u7684 apachecn/pytorch-doc-zh github \u4e0a\u63d0 issue. \u53d1\u90ae\u4ef6\u9001\u5230 Email: pytorch#apachecn.org (#\u66ff\u6362\u6210@) . \u5728\u6211\u4eec\u7684 \u7ec4\u7ec7\u5b66\u4e60\u4ea4\u6d41\u7fa4 \u4e2d\u8054\u7cfb\u7fa4\u4e3b/\u7ba1\u7406\u5458\u5373\u53ef.","title":"\u5efa\u8bae\u53cd\u9988"},{"location":"#_6","text":"","title":"\u8d5e\u52a9\u6211\u4eec"},{"location":"0/","text":"\u521d\u5b66\u8005\u6559\u7a0b","title":"\u521d\u5b66\u8005\u6559\u7a0b"},{"location":"0/#_1","text":"","title":"\u521d\u5b66\u8005\u6559\u7a0b"},{"location":"1/","text":"PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b Author : Soumith Chintala \u672c\u6559\u7a0b\u7684\u76ee\u6807: \u66f4\u9ad8\u5c42\u6b21\u5730\u7406\u89e3 PyTorch \u7684 Tensor (\u5f20\u91cf) \u5e93\u4ee5\u53ca\u795e\u7ecf\u7f51\u7edc. \u5b66\u4f1a\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684\u795e\u7ecf\u7f51\u7edc\u7528\u6765\u5bf9\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b \u672c\u6559\u7a0b\u5047\u8bbe\u60a8\u5bf9 numpy \u6709\u57fa\u672c\u7684\u4e86\u89e3 Note \u8bf7\u786e\u8ba4\u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 torch \u548c torchvision \u5305. PyTorch \u662f\u4ec0\u4e48\uff1f \u81ea\u52a8\u6c42\u5bfc: \u81ea\u52a8\u5fae\u5206 \u795e\u7ecf\u7f51\u7edc \u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668 \u53ef\u9009: \u6570\u636e\u5e76\u884c","title":"PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b"},{"location":"1/#pytorch-60","text":"Author : Soumith Chintala \u672c\u6559\u7a0b\u7684\u76ee\u6807: \u66f4\u9ad8\u5c42\u6b21\u5730\u7406\u89e3 PyTorch \u7684 Tensor (\u5f20\u91cf) \u5e93\u4ee5\u53ca\u795e\u7ecf\u7f51\u7edc. \u5b66\u4f1a\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684\u795e\u7ecf\u7f51\u7edc\u7528\u6765\u5bf9\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b \u672c\u6559\u7a0b\u5047\u8bbe\u60a8\u5bf9 numpy \u6709\u57fa\u672c\u7684\u4e86\u89e3 Note \u8bf7\u786e\u8ba4\u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 torch \u548c torchvision \u5305. PyTorch \u662f\u4ec0\u4e48\uff1f \u81ea\u52a8\u6c42\u5bfc: \u81ea\u52a8\u5fae\u5206 \u795e\u7ecf\u7f51\u7edc \u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668 \u53ef\u9009: \u6570\u636e\u5e76\u884c","title":"PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b"},{"location":"10/","text":"nn package \u6211\u4eec\u91cd\u65b0\u8bbe\u8ba1\u4e86 nn package, \u4ee5\u4fbf\u4e0e autograd \u5b8c\u5168\u96c6\u6210. \u8ba9\u6211\u4eec\u6765\u56de\u987e\u4e00\u4e0b\u8fd9\u4e9b\u53d8\u5316. \u7528 autograd \u66ff\u6362 containers: \u4f60\u4e0d\u518d\u9700\u8981\u4f7f\u7528\u50cf ConcatTable \u8fd9\u6837\u7684 Containers, \u6216\u8005\u50cf CAddTable \u8fd9\u6837\u7684\u6a21\u5757, \u6216\u8005\u4f7f\u7528 nngraph \u5e76\u4e14 debug. \u6211\u4eec\u5c06\u65e0\u7f1d\u5730\u4f7f\u7528 autograd \u6765\u5b9a\u4e49\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc. \u4f8b\u5982, output = nn.CAddTable():forward({input1, input2}) \u7b80\u5316\u4e3a output = input1 + input2 output = nn.MulConstant(0.5):forward(input) \u7b80\u5316\u4e3a output = input * 0.5 \u4e2d\u95f4\u72b6\u6001\u4e0d\u518d\u5b58\u653e\u5728\u4e0a\u8ff0\u63d0\u5230\u7684\u90a3\u4e9b\u6a21\u5757\u4e2d, \u800c\u662f\u5b58\u653e\u5728\u8ba1\u7b97\u56fe\u4e2d: \u56e0\u4e3a\u8fd9\u4e2a\u539f\u56e0, \u6240\u4ee5\u4f7f\u7528\u5faa\u73af\u7f51\u7edc\u53d8\u5f97\u66f4\u52a0\u7b80\u5355. \u5982\u679c\u4f60\u60f3\u521b\u5efa\u4e00\u4e2a\u5faa\u73af\u7f51\u7edc, \u53ea\u9700\u591a\u6b21\u4f7f\u7528\u76f8\u540c\u7684 Linear \u5c42, \u800c\u4e0d\u5fc5\u8003\u8651\u5171\u4eab\u6743\u91cd. torch-nn-vs-pytorch-nn Simplified debugging: \u4f7f\u7528Python\u7684pdb\u8c03\u8bd5\u5668\u8fdb\u884c\u8c03\u8bd5\u662f\u76f4\u89c2\u7684, \u8c03\u8bd5\u5668\u548c\u5806\u6808\u8ddf\u8e2a\u5728\u53d1\u751f\u9519\u8bef\u7684\u5730\u65b9\u505c\u6b62. What you see is what you get(\u6240\u89c1\u5373\u6240\u5f97, \u8bd1\u8005\u6ce8:\u5e94\u8be5\u662f\u8bf4\u53ef\u89c6\u5316\u5427). Example 1: ConvNet \u8ba9\u6211\u4eec\u6765\u521b\u5efa\u4e00\u4e2a\u5c0f\u7684 ConvNet. \u4f60\u6240\u6709\u7684\u7f51\u7edc\u90fd\u6765\u81ea nn.Module \u57fa\u7c7b: \u5728\u6784\u9020\u51fd\u6570\u4e2d, \u58f0\u660e\u4f60\u60f3\u8981\u4f7f\u7528\u7684\u6240\u6709\u5c42. \u5728 forward \u51fd\u6570\u4e2d, \u4f60\u53ef\u4ee5\u5b9a\u4e49\u6a21\u578b\u4ece\u8f93\u5165\u5230\u8f93\u51fa\u5c06\u5982\u4f55\u8fd0\u884c import torch from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F class MNISTConvNet(nn.Module): def __init__(self): # \u8fd9\u662f\u4f60\u5b9e\u4f8b\u5316\u6240\u6709\u6a21\u5757\u7684\u5730\u65b9 # \u4f60\u53ef\u4ee5\u7a0d\u540e\u4f7f\u7528\u4f60\u5728\u6b64\u7ed9\u51fa\u7684\u76f8\u540c\u540d\u79f0\u8bbf\u95ee\u5b83\u4eec super(MNISTConvNet, self).__init__() self.conv1 = nn.Conv2d(1, 10, 5) self.pool1 = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(10, 20, 5) self.pool2 = nn.MaxPool2d(2, 2) self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) # \u8fd9\u662f forward \u51fd\u6570, \u5b83\u5b9a\u4e49\u4e86\u53ea\u63a5\u53d7\u4e00\u4e2a\u8f93\u5165\u7684\u7f51\u7edc\u7ed3\u6784, # \u5982\u679c\u4f60\u613f\u610f, \u53ef\u4ee5\u968f\u610f\u5b9a\u4e49\u652f\u6301\u4f7f\u7528\u66f4\u591a\u8f93\u5165\u7684\u7f51\u7edc\u7ed3\u6784. def forward(self, input): x = self.pool1(F.relu(self.conv1(input))) x = self.pool2(F.relu(self.conv2(x))) # \u5728\u4f60\u7684\u521b\u5efa\u6a21\u578b\u7684\u8fc7\u7a0b\u4e2d, \u4f60\u53ef\u4ee5\u75af\u72c2\u5730\u4f7f\u7528\u4efb\u610f\u7684python\u4ee3\u7801\u521b\u5efa\u4f60\u7684\u6a21\u578b\u7ed3\u6784, # \u8fd9\u4e9b\u64cd\u4f5c\u90fd\u662f\u5b8c\u5168\u5408\u6cd5\u7684, \u5e76\u4e14\u4f1a\u88abautograd\u6b63\u786e\u5904\u7406: # if x.gt(0) > x.numel() / 2: # ... # # \u4f60\u751a\u81f3\u53ef\u4ee5\u505a\u4e00\u4e2a\u5faa\u73af\u6765\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u7684\u6a21\u5757, \u6a21\u5757\u5185\u90e8\u7684\u6a21\u5757\u4e0d\u518d # \u5904\u4e8e\u4e34\u65f6\u72b6\u6001, \u6240\u4ee5\u4f60\u53ef\u4ee5\u5728 forward \u65f6\u591a\u6b21\u4f7f\u7528\u5b83\u4eec. # while x.norm(2) < 10: # x = self.conv1(x) x = x.view(x.size(0), -1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) return x \u73b0\u5728\u8ba9\u6211\u4eec\u6765\u4f7f\u7528\u5b9a\u4e49\u597d\u7684 ConvNet. \u4f60\u5e94\u8be5\u5148\u521b\u5efa\u4e00\u4e2a\u7c7b\u7684\u5b9e\u4f8b. net = MNISTConvNet() print(net) Note torch.nn \u53ea\u652f\u6301 mini-batches , \u6574\u4e2a torch.nn package \u53ea\u652f\u6301\u8f93\u5165 mini-batch \u683c\u5f0f\u7684\u6837\u672c, \u800c\u4e0d\u652f\u6301\u8f93\u5165\u5355\u4e2a\u6837\u672c. \u4f8b\u5982, nn.Conv2d \u5c06\u91c7\u7528 nSamples x nChannels x Height x Width \u7684 4D Tensor. \u5982\u679c\u4f60\u6709\u4e00\u4e2a\u5355\u4e2a\u7684\u6837\u672c, \u53ea\u9700\u4f7f\u7528 input.unsqueeze(0) \u6dfb\u52a0\u4e00\u4e2a \u865a\u5047\u7684 batch \u7ef4\u5ea6. \u521b\u5efa\u4e00\u4e2a\u5305\u542b\u968f\u673a\u6570\u636e\u7684\u5355\u4e2a\u6837\u672c\u7684 mini-batch, \u5e76\u5c06\u8be5\u6837\u672c\u4f20\u5165\u5230 ConvNet . input = Variable(torch.randn(1, 1, 28, 28)) out = net(input) print(out.size()) \u5b9a\u4e49\u4e00\u4e2a\u865a\u62df\u76ee\u6807\u6807\u7b7e, \u5e76\u4f7f\u7528\u635f\u5931\u51fd\u6570\u6765\u8ba1\u7b97 error. target = Variable(torch.LongTensor([3])) loss_fn = nn.CrossEntropyLoss() # LogSoftmax + ClassNLL Loss err = loss_fn(out, target) err.backward() print(err) ConvNet \u7684 out \u662f\u4e00\u4e2a Variable . \u6211\u4eec\u4f7f\u7528\u5b83\u6765\u8ba1\u7b97\u635f\u5931, \u8ba1\u7b97\u7ed3\u679c err \u4e5f\u662f\u4e00\u4e2a Variable . \u8c03\u7528 err \u7684 .backward \u65b9\u6cd5\u5c06\u4f1a\u901a\u8fc7 ConvNet \u5c06\u68af\u5ea6\u4f20\u64ad\u5230\u5b83\u7684\u6743\u91cd. \u8ba9\u6211\u4eec\u6765\u8bbf\u95ee\u5355\u4e2a\u5c42\u7684\u6743\u91cd\u548c\u68af\u5ea6: print(net.conv1.weight.grad.size()) print(net.conv1.weight.data.norm()) # norm of the weight print(net.conv1.weight.grad.data.norm()) # norm of the gradients Forward and Backward Function Hooks \u6211\u4eec\u5df2\u7ecf\u68c0\u67e5\u4e86\u6743\u91cd\u548c\u68af\u5ea6. \u4f46\u662f\u5982\u4f55\u68c0\u67e5 / \u4fee\u6539\u4e00\u4e2a\u5c42\u7684\u8f93\u51fa\u548c grad_output? \u6211\u4eec\u4e3a\u6b64\u5f15\u51fa\u4e86 hooks . \u4f60\u53ef\u4ee5\u5728\u4e00\u4e2a Module \u6216\u4e00\u4e2a Variable \u4e0a\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570. hook \u53ef\u4ee5\u662f forward hook \u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a backward hook. \u5f53 forward \u88ab\u6267\u884c\u540e forward hook \u5c06\u4f1a\u88ab\u6267\u884c. backward hook \u5c06\u5728\u6267\u884c backward \u9636\u6bb5\u88ab\u6267\u884c. \u8ba9\u6211\u4eec\u6765\u770b\u4e00\u4e2a\u4f8b\u5b50. \u6211\u4eec\u5728 conv2 \u6ce8\u518c\u4e00\u4e2a forward hook \u6765\u6253\u5370\u4e00\u4e9b\u4fe1\u606f def printnorm(self, input, output): # input\u662f\u5c06\u8f93\u5165\u6253\u5305\u6210\u7684 tuple \u7684input # \u8f93\u51fa\u662f\u4e00\u4e2a Variable. output.data \u662f\u6211\u4eec\u611f\u5174\u8da3\u7684 Tensor print('Inside ' + self.__class__.__name__ + ' forward') print('') print('input: ', type(input)) print('input[0]: ', type(input[0])) print('output: ', type(output)) print('') print('input size:', input[0].size()) print('output size:', output.data.size()) print('output norm:', output.data.norm()) net.conv2.register_forward_hook(printnorm) out = net(input) \u6211\u4eec\u5728 conv2 \u6ce8\u518c\u4e00\u4e2a backward hook \u6765\u6253\u5370\u4e00\u4e9b\u4fe1\u606f def printgradnorm(self, grad_input, grad_output): print('Inside ' + self.__class__.__name__ + ' backward') print('Inside class:' + self.__class__.__name__) print('') print('grad_input: ', type(grad_input)) print('grad_input[0]: ', type(grad_input[0])) print('grad_output: ', type(grad_output)) print('grad_output[0]: ', type(grad_output[0])) print('') print('grad_input size:', grad_input[0].size()) print('grad_output size:', grad_output[0].size()) print('grad_input norm:', grad_input[0].data.norm()) net.conv2.register_backward_hook(printgradnorm) out = net(input) err = loss_fn(out, target) err.backward() \u4e00\u4e2a\u5b8c\u6574\u7684\u53ef\u4ee5\u8fd0\u884c\u7684 MNIST \u4f8b\u5b50\u5728\u6b64\u94fe\u63a5\u4e2d https://github.com/pytorch/examples/tree/master/mnist Example 2: Recurrent Net \u63a5\u4e0b\u6765, \u8ba9\u6211\u4eec\u770b\u4e00\u4e0b\u7528 PyTorch \u521b\u5efa recurrent nets. \u7531\u4e8e\u7f51\u7edc\u7684\u72b6\u6001\u662f\u4fdd\u5b58\u5728\u56fe\u4e2d, \u800c\u4e0d\u662f\u5728 layer \u4e2d, \u6240\u4ee5\u60a8\u53ef\u4ee5\u7b80\u5355\u5730 \u521b\u5efa\u4e00\u4e2a nn.Linear \u5e76\u91cd\u590d\u4f7f\u7528\u5b83. class RNN(nn.Module): # \u4f60\u4e5f\u53ef\u4ee5\u5728\u4f60\u6a21\u578b\u7684\u6784\u9020\u51fd\u6570\u4e2d\u4f20\u5165\u53c2\u6570 def __init__(self, data_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size input_size = data_size + hidden_size self.i2h = nn.Linear(input_size, hidden_size) self.h2o = nn.Linear(hidden_size, output_size) def forward(self, data, last_hidden): input = torch.cat((data, last_hidden), 1) hidden = self.i2h(input) output = self.h2o(hidden) return hidden, output rnn = RNN(50, 20, 10) \u66f4\u5b8c\u6574\u7684\u4f7f\u7528 LSTMs \u548c Penn Tree-bank \u7684\u8bed\u8a00\u6a21\u578b\u4f4d\u4e8e here PyTorch \u9ed8\u8ba4\u5df2\u7ecf\u4e3a ConvNets \u548c Recurrent Nets \u63d0\u4f9b\u4e86\u65e0\u7f1d\u7684 CuDNN \u96c6\u6210. loss_fn = nn.MSELoss() batch_size = 10 TIMESTEPS = 5 # \u521b\u5efa\u4e00\u4e9b\u5047\u6570\u636e batch = Variable(torch.randn(batch_size, 50)) hidden = Variable(torch.zeros(batch_size, 20)) target = Variable(torch.zeros(batch_size, 10)) loss = 0 for t in range(TIMESTEPS): # \u662f\u7684! \u4f60\u53ef\u4ee5\u591a\u6b21\u4f7f\u7528\u540c\u4e00\u4e2a\u7f51\u7edc, # \u5c06\u635f\u5931\u76f8\u52a0, \u5e76\u4e14\u8c03\u7528 call backward! hidden, output = rnn(batch, hidden) loss += loss_fn(output, target) loss.backward() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: nn_tutorial.py Download Jupyter notebook: nn_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"nn package"},{"location":"10/#nn-package","text":"\u6211\u4eec\u91cd\u65b0\u8bbe\u8ba1\u4e86 nn package, \u4ee5\u4fbf\u4e0e autograd \u5b8c\u5168\u96c6\u6210. \u8ba9\u6211\u4eec\u6765\u56de\u987e\u4e00\u4e0b\u8fd9\u4e9b\u53d8\u5316. \u7528 autograd \u66ff\u6362 containers: \u4f60\u4e0d\u518d\u9700\u8981\u4f7f\u7528\u50cf ConcatTable \u8fd9\u6837\u7684 Containers, \u6216\u8005\u50cf CAddTable \u8fd9\u6837\u7684\u6a21\u5757, \u6216\u8005\u4f7f\u7528 nngraph \u5e76\u4e14 debug. \u6211\u4eec\u5c06\u65e0\u7f1d\u5730\u4f7f\u7528 autograd \u6765\u5b9a\u4e49\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc. \u4f8b\u5982, output = nn.CAddTable():forward({input1, input2}) \u7b80\u5316\u4e3a output = input1 + input2 output = nn.MulConstant(0.5):forward(input) \u7b80\u5316\u4e3a output = input * 0.5 \u4e2d\u95f4\u72b6\u6001\u4e0d\u518d\u5b58\u653e\u5728\u4e0a\u8ff0\u63d0\u5230\u7684\u90a3\u4e9b\u6a21\u5757\u4e2d, \u800c\u662f\u5b58\u653e\u5728\u8ba1\u7b97\u56fe\u4e2d: \u56e0\u4e3a\u8fd9\u4e2a\u539f\u56e0, \u6240\u4ee5\u4f7f\u7528\u5faa\u73af\u7f51\u7edc\u53d8\u5f97\u66f4\u52a0\u7b80\u5355. \u5982\u679c\u4f60\u60f3\u521b\u5efa\u4e00\u4e2a\u5faa\u73af\u7f51\u7edc, \u53ea\u9700\u591a\u6b21\u4f7f\u7528\u76f8\u540c\u7684 Linear \u5c42, \u800c\u4e0d\u5fc5\u8003\u8651\u5171\u4eab\u6743\u91cd. torch-nn-vs-pytorch-nn Simplified debugging: \u4f7f\u7528Python\u7684pdb\u8c03\u8bd5\u5668\u8fdb\u884c\u8c03\u8bd5\u662f\u76f4\u89c2\u7684, \u8c03\u8bd5\u5668\u548c\u5806\u6808\u8ddf\u8e2a\u5728\u53d1\u751f\u9519\u8bef\u7684\u5730\u65b9\u505c\u6b62. What you see is what you get(\u6240\u89c1\u5373\u6240\u5f97, \u8bd1\u8005\u6ce8:\u5e94\u8be5\u662f\u8bf4\u53ef\u89c6\u5316\u5427).","title":"nn package"},{"location":"10/#example-1-convnet","text":"\u8ba9\u6211\u4eec\u6765\u521b\u5efa\u4e00\u4e2a\u5c0f\u7684 ConvNet. \u4f60\u6240\u6709\u7684\u7f51\u7edc\u90fd\u6765\u81ea nn.Module \u57fa\u7c7b: \u5728\u6784\u9020\u51fd\u6570\u4e2d, \u58f0\u660e\u4f60\u60f3\u8981\u4f7f\u7528\u7684\u6240\u6709\u5c42. \u5728 forward \u51fd\u6570\u4e2d, \u4f60\u53ef\u4ee5\u5b9a\u4e49\u6a21\u578b\u4ece\u8f93\u5165\u5230\u8f93\u51fa\u5c06\u5982\u4f55\u8fd0\u884c import torch from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F class MNISTConvNet(nn.Module): def __init__(self): # \u8fd9\u662f\u4f60\u5b9e\u4f8b\u5316\u6240\u6709\u6a21\u5757\u7684\u5730\u65b9 # \u4f60\u53ef\u4ee5\u7a0d\u540e\u4f7f\u7528\u4f60\u5728\u6b64\u7ed9\u51fa\u7684\u76f8\u540c\u540d\u79f0\u8bbf\u95ee\u5b83\u4eec super(MNISTConvNet, self).__init__() self.conv1 = nn.Conv2d(1, 10, 5) self.pool1 = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(10, 20, 5) self.pool2 = nn.MaxPool2d(2, 2) self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) # \u8fd9\u662f forward \u51fd\u6570, \u5b83\u5b9a\u4e49\u4e86\u53ea\u63a5\u53d7\u4e00\u4e2a\u8f93\u5165\u7684\u7f51\u7edc\u7ed3\u6784, # \u5982\u679c\u4f60\u613f\u610f, \u53ef\u4ee5\u968f\u610f\u5b9a\u4e49\u652f\u6301\u4f7f\u7528\u66f4\u591a\u8f93\u5165\u7684\u7f51\u7edc\u7ed3\u6784. def forward(self, input): x = self.pool1(F.relu(self.conv1(input))) x = self.pool2(F.relu(self.conv2(x))) # \u5728\u4f60\u7684\u521b\u5efa\u6a21\u578b\u7684\u8fc7\u7a0b\u4e2d, \u4f60\u53ef\u4ee5\u75af\u72c2\u5730\u4f7f\u7528\u4efb\u610f\u7684python\u4ee3\u7801\u521b\u5efa\u4f60\u7684\u6a21\u578b\u7ed3\u6784, # \u8fd9\u4e9b\u64cd\u4f5c\u90fd\u662f\u5b8c\u5168\u5408\u6cd5\u7684, \u5e76\u4e14\u4f1a\u88abautograd\u6b63\u786e\u5904\u7406: # if x.gt(0) > x.numel() / 2: # ... # # \u4f60\u751a\u81f3\u53ef\u4ee5\u505a\u4e00\u4e2a\u5faa\u73af\u6765\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u7684\u6a21\u5757, \u6a21\u5757\u5185\u90e8\u7684\u6a21\u5757\u4e0d\u518d # \u5904\u4e8e\u4e34\u65f6\u72b6\u6001, \u6240\u4ee5\u4f60\u53ef\u4ee5\u5728 forward \u65f6\u591a\u6b21\u4f7f\u7528\u5b83\u4eec. # while x.norm(2) < 10: # x = self.conv1(x) x = x.view(x.size(0), -1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) return x \u73b0\u5728\u8ba9\u6211\u4eec\u6765\u4f7f\u7528\u5b9a\u4e49\u597d\u7684 ConvNet. \u4f60\u5e94\u8be5\u5148\u521b\u5efa\u4e00\u4e2a\u7c7b\u7684\u5b9e\u4f8b. net = MNISTConvNet() print(net) Note torch.nn \u53ea\u652f\u6301 mini-batches , \u6574\u4e2a torch.nn package \u53ea\u652f\u6301\u8f93\u5165 mini-batch \u683c\u5f0f\u7684\u6837\u672c, \u800c\u4e0d\u652f\u6301\u8f93\u5165\u5355\u4e2a\u6837\u672c. \u4f8b\u5982, nn.Conv2d \u5c06\u91c7\u7528 nSamples x nChannels x Height x Width \u7684 4D Tensor. \u5982\u679c\u4f60\u6709\u4e00\u4e2a\u5355\u4e2a\u7684\u6837\u672c, \u53ea\u9700\u4f7f\u7528 input.unsqueeze(0) \u6dfb\u52a0\u4e00\u4e2a \u865a\u5047\u7684 batch \u7ef4\u5ea6. \u521b\u5efa\u4e00\u4e2a\u5305\u542b\u968f\u673a\u6570\u636e\u7684\u5355\u4e2a\u6837\u672c\u7684 mini-batch, \u5e76\u5c06\u8be5\u6837\u672c\u4f20\u5165\u5230 ConvNet . input = Variable(torch.randn(1, 1, 28, 28)) out = net(input) print(out.size()) \u5b9a\u4e49\u4e00\u4e2a\u865a\u62df\u76ee\u6807\u6807\u7b7e, \u5e76\u4f7f\u7528\u635f\u5931\u51fd\u6570\u6765\u8ba1\u7b97 error. target = Variable(torch.LongTensor([3])) loss_fn = nn.CrossEntropyLoss() # LogSoftmax + ClassNLL Loss err = loss_fn(out, target) err.backward() print(err) ConvNet \u7684 out \u662f\u4e00\u4e2a Variable . \u6211\u4eec\u4f7f\u7528\u5b83\u6765\u8ba1\u7b97\u635f\u5931, \u8ba1\u7b97\u7ed3\u679c err \u4e5f\u662f\u4e00\u4e2a Variable . \u8c03\u7528 err \u7684 .backward \u65b9\u6cd5\u5c06\u4f1a\u901a\u8fc7 ConvNet \u5c06\u68af\u5ea6\u4f20\u64ad\u5230\u5b83\u7684\u6743\u91cd. \u8ba9\u6211\u4eec\u6765\u8bbf\u95ee\u5355\u4e2a\u5c42\u7684\u6743\u91cd\u548c\u68af\u5ea6: print(net.conv1.weight.grad.size()) print(net.conv1.weight.data.norm()) # norm of the weight print(net.conv1.weight.grad.data.norm()) # norm of the gradients","title":"Example 1: ConvNet"},{"location":"10/#forward-and-backward-function-hooks","text":"\u6211\u4eec\u5df2\u7ecf\u68c0\u67e5\u4e86\u6743\u91cd\u548c\u68af\u5ea6. \u4f46\u662f\u5982\u4f55\u68c0\u67e5 / \u4fee\u6539\u4e00\u4e2a\u5c42\u7684\u8f93\u51fa\u548c grad_output? \u6211\u4eec\u4e3a\u6b64\u5f15\u51fa\u4e86 hooks . \u4f60\u53ef\u4ee5\u5728\u4e00\u4e2a Module \u6216\u4e00\u4e2a Variable \u4e0a\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570. hook \u53ef\u4ee5\u662f forward hook \u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a backward hook. \u5f53 forward \u88ab\u6267\u884c\u540e forward hook \u5c06\u4f1a\u88ab\u6267\u884c. backward hook \u5c06\u5728\u6267\u884c backward \u9636\u6bb5\u88ab\u6267\u884c. \u8ba9\u6211\u4eec\u6765\u770b\u4e00\u4e2a\u4f8b\u5b50. \u6211\u4eec\u5728 conv2 \u6ce8\u518c\u4e00\u4e2a forward hook \u6765\u6253\u5370\u4e00\u4e9b\u4fe1\u606f def printnorm(self, input, output): # input\u662f\u5c06\u8f93\u5165\u6253\u5305\u6210\u7684 tuple \u7684input # \u8f93\u51fa\u662f\u4e00\u4e2a Variable. output.data \u662f\u6211\u4eec\u611f\u5174\u8da3\u7684 Tensor print('Inside ' + self.__class__.__name__ + ' forward') print('') print('input: ', type(input)) print('input[0]: ', type(input[0])) print('output: ', type(output)) print('') print('input size:', input[0].size()) print('output size:', output.data.size()) print('output norm:', output.data.norm()) net.conv2.register_forward_hook(printnorm) out = net(input) \u6211\u4eec\u5728 conv2 \u6ce8\u518c\u4e00\u4e2a backward hook \u6765\u6253\u5370\u4e00\u4e9b\u4fe1\u606f def printgradnorm(self, grad_input, grad_output): print('Inside ' + self.__class__.__name__ + ' backward') print('Inside class:' + self.__class__.__name__) print('') print('grad_input: ', type(grad_input)) print('grad_input[0]: ', type(grad_input[0])) print('grad_output: ', type(grad_output)) print('grad_output[0]: ', type(grad_output[0])) print('') print('grad_input size:', grad_input[0].size()) print('grad_output size:', grad_output[0].size()) print('grad_input norm:', grad_input[0].data.norm()) net.conv2.register_backward_hook(printgradnorm) out = net(input) err = loss_fn(out, target) err.backward() \u4e00\u4e2a\u5b8c\u6574\u7684\u53ef\u4ee5\u8fd0\u884c\u7684 MNIST \u4f8b\u5b50\u5728\u6b64\u94fe\u63a5\u4e2d https://github.com/pytorch/examples/tree/master/mnist","title":"Forward and Backward Function Hooks"},{"location":"10/#example-2-recurrent-net","text":"\u63a5\u4e0b\u6765, \u8ba9\u6211\u4eec\u770b\u4e00\u4e0b\u7528 PyTorch \u521b\u5efa recurrent nets. \u7531\u4e8e\u7f51\u7edc\u7684\u72b6\u6001\u662f\u4fdd\u5b58\u5728\u56fe\u4e2d, \u800c\u4e0d\u662f\u5728 layer \u4e2d, \u6240\u4ee5\u60a8\u53ef\u4ee5\u7b80\u5355\u5730 \u521b\u5efa\u4e00\u4e2a nn.Linear \u5e76\u91cd\u590d\u4f7f\u7528\u5b83. class RNN(nn.Module): # \u4f60\u4e5f\u53ef\u4ee5\u5728\u4f60\u6a21\u578b\u7684\u6784\u9020\u51fd\u6570\u4e2d\u4f20\u5165\u53c2\u6570 def __init__(self, data_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size input_size = data_size + hidden_size self.i2h = nn.Linear(input_size, hidden_size) self.h2o = nn.Linear(hidden_size, output_size) def forward(self, data, last_hidden): input = torch.cat((data, last_hidden), 1) hidden = self.i2h(input) output = self.h2o(hidden) return hidden, output rnn = RNN(50, 20, 10) \u66f4\u5b8c\u6574\u7684\u4f7f\u7528 LSTMs \u548c Penn Tree-bank \u7684\u8bed\u8a00\u6a21\u578b\u4f4d\u4e8e here PyTorch \u9ed8\u8ba4\u5df2\u7ecf\u4e3a ConvNets \u548c Recurrent Nets \u63d0\u4f9b\u4e86\u65e0\u7f1d\u7684 CuDNN \u96c6\u6210. loss_fn = nn.MSELoss() batch_size = 10 TIMESTEPS = 5 # \u521b\u5efa\u4e00\u4e9b\u5047\u6570\u636e batch = Variable(torch.randn(batch_size, 50)) hidden = Variable(torch.zeros(batch_size, 20)) target = Variable(torch.zeros(batch_size, 10)) loss = 0 for t in range(TIMESTEPS): # \u662f\u7684! \u4f60\u53ef\u4ee5\u591a\u6b21\u4f7f\u7528\u540c\u4e00\u4e2a\u7f51\u7edc, # \u5c06\u635f\u5931\u76f8\u52a0, \u5e76\u4e14\u8c03\u7528 call backward! hidden, output = rnn(batch, hidden) loss += loss_fn(output, target) loss.backward() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: nn_tutorial.py Download Jupyter notebook: nn_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"Example 2: Recurrent Net"},{"location":"11/","text":"Multi-GPU examples \u6570\u636e\u5e76\u884c\u662f\u6307\u5f53\u6211\u4eec\u5c06 mini-batch \u7684\u6837\u672c\u5206\u6210\u66f4\u5c0f\u7684 mini-batches, \u5e76\u884c\u5730\u8ba1\u7b97\u6bcf\u4e2a\u66f4\u5c0f\u7684 mini-batches. \u6570\u636e\u5e76\u884c\u901a\u8fc7\u4f7f\u7528 torch.nn.DataParallel \u5b9e\u73b0. \u6211\u4eec\u53ef\u4ee5\u7528 DataParallel \u5305\u88c5\u4e00\u4e2a\u6a21\u5757, \u7136\u540e\u5b83\u5c06\u5728 batch \u7ef4\u5ea6(\u9ed8\u8ba4\u662f0\u8f74) \u5e73\u5206\u6570\u636e\u7ed9\u591a\u4e2a GPUs \u8fdb\u884c\u5e76\u884c\u8ba1\u7b97. DataParallel import torch.nn as nn class DataParallelModel(nn.Module): def __init__(self): super().__init__() self.block1 = nn.Linear(10, 20) # \u7528 DataParallel \u5305\u88c5 block2 self.block2 = nn.Linear(20, 20) self.block2 = nn.DataParallel(self.block2) self.block3 = nn.Linear(20, 20) def forward(self, x): x = self.block1(x) x = self.block2(x) x = self.block3(x) return x \u8fd9\u4e2a\u4ee3\u7801\u4e0d\u505a\u4efb\u4f55\u4fee\u6539, \u5728 CPU \u6a21\u5f0f\u4e0b\u4e5f\u80fd\u8fd0\u884c. DataParallel \u7684\u6587\u6863\u4e3a here . \u5728\u5176\u4e0a\u5b9e\u73b0 DataParallel \u7684\u57fa\u5143: \u901a\u5e38, pytorch \u7684 <cite>nn.parallel</cite> \u539f\u51fd\u6570\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528. \u6211\u4eec\u5b9e\u73b0\u4e86\u7b80\u5355\u7684\u7c7b\u4f3c MPI \u7684\u539f\u51fd\u6570: replicate: \u5728\u591a\u4e2a\u8bbe\u5907\u4e0a\u590d\u5236\u6a21\u5757 scatter: \u5728\u7b2c\u4e00\u7ef4\u4e2d\u5206\u914d\u8f93\u5165 gather: \u5728\u7b2c\u4e00\u7ef4 gather \u548c concatenate \u8f93\u5165 parallel_apply: \u5c06\u4e00\u7ec4\u5df2\u7ecf\u5206\u914d\u7684\u8f93\u5165\u5e94\u7528\u4e8e\u4e00\u7ec4\u5df2\u7ecf\u5206\u914d\u7684\u6a21\u578b. \u4e3a\u4e86\u66f4\u6e05\u6670\u8d77\u89c1, \u8fd9\u91cc\u4f7f\u7528\u8fd9\u4e9b\u96c6\u5408\u7ec4\u6210\u7684\u51fd\u6570 data_parallel def data_parallel(module, input, device_ids, output_device=None): if not device_ids: return module(input) if output_device is None: output_device = device_ids[0] replicas = nn.parallel.replicate(module, device_ids) inputs = nn.parallel.scatter(input, device_ids) replicas = replicas[:len(inputs)] outputs = nn.parallel.parallel_apply(replicas, inputs) return nn.parallel.gather(outputs, output_device) Part of the model on CPU and part on the GPU \u8ba9\u6211\u4eec\u6765\u770b\u4e00\u4e2a\u7f51\u7edc\u6a21\u578b, \u4ed6\u7684\u7f51\u7edc\u4e00\u90e8\u5206\u7528 CPU \u8fd0\u7b97, \u53e6\u4e00\u90e8\u5206\u7528 GPU \u8fd0\u7b97. class DistributedModel(nn.Module): def __init__(self): super().__init__( embedding=nn.Embedding(1000, 10), rnn=nn.Linear(10, 10).cuda(0), ) def forward(self, x): # \u5728 CPU \u4e0a\u8ba1\u7b97 embedding x = self.embedding(x) # \u8fc1\u79fb\u5230 GPU x = x.cuda(0) # \u5728 GPU \u4e0a\u8fd0\u884c RNN x = self.rnn(x) return x \u8fd9\u662f\u9762\u5411 Torch \u4f7f\u7528\u8005\u7684 PyTorch \u7684\u7b80\u77ed\u4ecb\u7ecd. \u5f53\u7136\u8fd8\u6709\u66f4\u591a\u4e1c\u897f\u9700\u8981\u5b66\u4e60. \u770b\u5b8c\u8fd9\u90e8\u5206\u6559\u7a0b, \u4e5f\u53ef\u4ee5\u770b\u770b\u6211\u4eec\u66f4\u5168\u9762\u7684\u5165\u95e8\u6559\u7a0b, \u5b83\u4ecb\u7ecd\u4e86 optim package, data loaders \u7b49.: PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b . \u4e5f\u53ef\u4ee5\u770b\u770b \u8bad\u7ec3\u4e00\u4e2a\u4f1a\u73a9\u89c6\u9891\u6e38\u620f\u7684\u795e\u7ecf\u7f51\u7edc \u4f7f\u7528 imagenet \u56fe\u50cf\u6570\u636e\u6765\u8bad\u7ec3\u4e00\u4e2a\u73b0\u5728\u6700\u70ed\u95e8\u7684\u6a21\u578b \u8bad\u7ec3\u4e00\u4e2a GAN \u7f51\u7edc\u6765\u751f\u6210\u4eba\u8138 \u4f7f\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc LSTM \u6765\u8bad\u7ec3\u5355\u8bcd\u7ea7\u8bed\u8a00\u6a21\u578b \u4e86\u89e3\u66f4\u591a\u7684\u4f8b\u5b50 \u4e86\u89e3\u66f4\u591a\u7684\u6559\u7a0b \u5728\u8bba\u575b\u4e0a\u8ba8\u8bba PyTorch \u5728 slack \u548c\u5176\u4ed6\u7528\u6237\u8ba8\u8bba PyTorch Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: parallelism_tutorial.py Download Jupyter notebook: parallelism_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"Multi-GPU examples"},{"location":"11/#multi-gpu-examples","text":"\u6570\u636e\u5e76\u884c\u662f\u6307\u5f53\u6211\u4eec\u5c06 mini-batch \u7684\u6837\u672c\u5206\u6210\u66f4\u5c0f\u7684 mini-batches, \u5e76\u884c\u5730\u8ba1\u7b97\u6bcf\u4e2a\u66f4\u5c0f\u7684 mini-batches. \u6570\u636e\u5e76\u884c\u901a\u8fc7\u4f7f\u7528 torch.nn.DataParallel \u5b9e\u73b0. \u6211\u4eec\u53ef\u4ee5\u7528 DataParallel \u5305\u88c5\u4e00\u4e2a\u6a21\u5757, \u7136\u540e\u5b83\u5c06\u5728 batch \u7ef4\u5ea6(\u9ed8\u8ba4\u662f0\u8f74) \u5e73\u5206\u6570\u636e\u7ed9\u591a\u4e2a GPUs \u8fdb\u884c\u5e76\u884c\u8ba1\u7b97.","title":"Multi-GPU examples"},{"location":"11/#dataparallel","text":"import torch.nn as nn class DataParallelModel(nn.Module): def __init__(self): super().__init__() self.block1 = nn.Linear(10, 20) # \u7528 DataParallel \u5305\u88c5 block2 self.block2 = nn.Linear(20, 20) self.block2 = nn.DataParallel(self.block2) self.block3 = nn.Linear(20, 20) def forward(self, x): x = self.block1(x) x = self.block2(x) x = self.block3(x) return x \u8fd9\u4e2a\u4ee3\u7801\u4e0d\u505a\u4efb\u4f55\u4fee\u6539, \u5728 CPU \u6a21\u5f0f\u4e0b\u4e5f\u80fd\u8fd0\u884c. DataParallel \u7684\u6587\u6863\u4e3a here . \u5728\u5176\u4e0a\u5b9e\u73b0 DataParallel \u7684\u57fa\u5143: \u901a\u5e38, pytorch \u7684 <cite>nn.parallel</cite> \u539f\u51fd\u6570\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528. \u6211\u4eec\u5b9e\u73b0\u4e86\u7b80\u5355\u7684\u7c7b\u4f3c MPI \u7684\u539f\u51fd\u6570: replicate: \u5728\u591a\u4e2a\u8bbe\u5907\u4e0a\u590d\u5236\u6a21\u5757 scatter: \u5728\u7b2c\u4e00\u7ef4\u4e2d\u5206\u914d\u8f93\u5165 gather: \u5728\u7b2c\u4e00\u7ef4 gather \u548c concatenate \u8f93\u5165 parallel_apply: \u5c06\u4e00\u7ec4\u5df2\u7ecf\u5206\u914d\u7684\u8f93\u5165\u5e94\u7528\u4e8e\u4e00\u7ec4\u5df2\u7ecf\u5206\u914d\u7684\u6a21\u578b. \u4e3a\u4e86\u66f4\u6e05\u6670\u8d77\u89c1, \u8fd9\u91cc\u4f7f\u7528\u8fd9\u4e9b\u96c6\u5408\u7ec4\u6210\u7684\u51fd\u6570 data_parallel def data_parallel(module, input, device_ids, output_device=None): if not device_ids: return module(input) if output_device is None: output_device = device_ids[0] replicas = nn.parallel.replicate(module, device_ids) inputs = nn.parallel.scatter(input, device_ids) replicas = replicas[:len(inputs)] outputs = nn.parallel.parallel_apply(replicas, inputs) return nn.parallel.gather(outputs, output_device)","title":"DataParallel"},{"location":"11/#part-of-the-model-on-cpu-and-part-on-the-gpu","text":"\u8ba9\u6211\u4eec\u6765\u770b\u4e00\u4e2a\u7f51\u7edc\u6a21\u578b, \u4ed6\u7684\u7f51\u7edc\u4e00\u90e8\u5206\u7528 CPU \u8fd0\u7b97, \u53e6\u4e00\u90e8\u5206\u7528 GPU \u8fd0\u7b97. class DistributedModel(nn.Module): def __init__(self): super().__init__( embedding=nn.Embedding(1000, 10), rnn=nn.Linear(10, 10).cuda(0), ) def forward(self, x): # \u5728 CPU \u4e0a\u8ba1\u7b97 embedding x = self.embedding(x) # \u8fc1\u79fb\u5230 GPU x = x.cuda(0) # \u5728 GPU \u4e0a\u8fd0\u884c RNN x = self.rnn(x) return x \u8fd9\u662f\u9762\u5411 Torch \u4f7f\u7528\u8005\u7684 PyTorch \u7684\u7b80\u77ed\u4ecb\u7ecd. \u5f53\u7136\u8fd8\u6709\u66f4\u591a\u4e1c\u897f\u9700\u8981\u5b66\u4e60. \u770b\u5b8c\u8fd9\u90e8\u5206\u6559\u7a0b, \u4e5f\u53ef\u4ee5\u770b\u770b\u6211\u4eec\u66f4\u5168\u9762\u7684\u5165\u95e8\u6559\u7a0b, \u5b83\u4ecb\u7ecd\u4e86 optim package, data loaders \u7b49.: PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b . \u4e5f\u53ef\u4ee5\u770b\u770b \u8bad\u7ec3\u4e00\u4e2a\u4f1a\u73a9\u89c6\u9891\u6e38\u620f\u7684\u795e\u7ecf\u7f51\u7edc \u4f7f\u7528 imagenet \u56fe\u50cf\u6570\u636e\u6765\u8bad\u7ec3\u4e00\u4e2a\u73b0\u5728\u6700\u70ed\u95e8\u7684\u6a21\u578b \u8bad\u7ec3\u4e00\u4e2a GAN \u7f51\u7edc\u6765\u751f\u6210\u4eba\u8138 \u4f7f\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc LSTM \u6765\u8bad\u7ec3\u5355\u8bcd\u7ea7\u8bed\u8a00\u6a21\u578b \u4e86\u89e3\u66f4\u591a\u7684\u4f8b\u5b50 \u4e86\u89e3\u66f4\u591a\u7684\u6559\u7a0b \u5728\u8bba\u575b\u4e0a\u8ba8\u8bba PyTorch \u5728 slack \u548c\u5176\u4ed6\u7528\u6237\u8ba8\u8bba PyTorch Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: parallelism_tutorial.py Download Jupyter notebook: parallelism_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"Part of the model on CPU and part on the GPU"},{"location":"12/","text":"\u8ddf\u7740\u4f8b\u5b50\u5b66\u4e60 PyTorch Author : Justin Johnson \u8fd9\u4e2a\u6559\u7a0b\u901a\u8fc7\u4e00\u4e9b\u5355\u72ec\u7684\u793a\u4f8b\u4ecb\u7ecd\u4e86 PyTorch \u7684\u57fa\u672c\u6982\u5ff5. PyTorch \u7684\u6838\u5fc3\u90e8\u5206\u63d0\u4f9b\u4e86\u4e24\u4e2a\u4e3b\u8981\u529f\u80fd: \u4e00\u4e2a\u7c7b\u4f3c\u4e8e numpy \u7684n\u7ef4\u5f20\u91cf, \u4f46\u53ef\u4ee5\u5728 GPU \u4e0a\u8fd0\u884c \u4e3a\u5efa\u7acb\u548c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u81ea\u52a8\u5fae\u5206 \u6211\u4eec\u5c06\u4f7f\u7528\u5168\u8fde\u63a5\u7684 ReLU \u7f51\u7edc\u4f5c\u4e3a\u6211\u4eec\u7684\u8fd0\u884c\u793a\u4f8b. \u8be5\u7f51\u7edc\u5c06\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u5e76\u5c06\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u53bb\u6700\u5c0f\u5316\u968f\u673a\u6570\u5b57\u7684\u9884\u6d4b\u8f93\u51fa\u548c\u771f\u5b9e\u8f93\u51fa\u4e4b\u95f4\u7684\u6b27\u5f0f\u8ddd\u79bb. Note \u4f60\u53ef\u4ee5\u4e0b\u8f7d\u8fd9\u4e9b\u5355\u72ec\u7684\u4f8b\u5b50\u5728\u9875\u9762\u7684\u5e95\u7aef <examples-download>. \u672c\u7ae0\u5185\u5bb9\u76ee\u5f55 Tensors Warm-up: numpy PyTorch: Tensors Autograd PyTorch: Variables and autograd PyTorch: Defining new autograd functions TensorFlow: Static Graphs <cite>nn</cite> module PyTorch: nn PyTorch: optim PyTorch: Custom nn Modules PyTorch: Control Flow + Weight Sharing Examples Tensors Autograd <cite>nn</cite> module Tensors Warm-up: numpy \u5728\u4ecb\u7ecd PyTorch \u4e4b\u524d, \u6211\u4eec\u5148\u4f7f\u7528 numpy \u5b9e\u73b0\u7f51\u7edc. Numpy \u63d0\u4f9b\u4e86\u4e00\u4e2an\u7ef4\u7684\u6570\u7ec4\u5bf9\u8c61, \u5e76\u63d0\u4f9b\u4e86\u8bb8\u591a\u64cd\u7eb5\u8fd9\u4e2a\u6570\u7ec4\u5bf9\u8c61\u7684\u51fd\u6570. Numpy \u662f\u79d1\u5b66\u8ba1\u7b97\u7684\u901a\u7528\u6846\u67b6; Numpy \u6570\u7ec4\u6ca1\u6709\u8ba1\u7b97\u56fe, \u4e5f\u6ca1\u6709\u6df1\u5ea6\u5b66\u4e60, \u4e5f\u6ca1\u6709\u68af\u5ea6\u4e0b\u964d\u7b49\u65b9\u6cd5\u5b9e\u73b0\u7684\u63a5\u53e3. \u4f46\u662f\u6211\u4eec\u4ecd\u7136\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u4f7f\u7528 numpy \u751f\u6210\u968f\u673a\u6570\u636e \u5e76\u5c06\u4ea7\u751f\u7684\u6570\u636e\u4f20\u5165\u53cc\u5c42\u7684\u795e\u7ecf\u7f51\u7edc, \u5e76\u4f7f\u7528 numpy \u6765\u5b9e\u73b0\u8fd9\u4e2a\u7f51\u7edc\u7684\u6b63\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad: # -*- coding: utf-8 -*- import numpy as np # N \u662f\u4e00\u4e2abatch\u7684\u6837\u672c\u6570\u91cf; D_in\u662f\u8f93\u5165\u7ef4\u5ea6; # H \u662f\u9690\u85cf\u5c42\u5411\u91cf\u7684\u7ef4\u5ea6; D_out\u662f\u8f93\u51fa\u7ef4\u5ea6. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u7684\u8f93\u5165\u8f93\u51fa\u6570\u636e x = np.random.randn(N, D_in) y = np.random.randn(N, D_out) # \u968f\u673a\u521d\u59cb\u5316\u6743\u91cd\u53c2\u6570 w1 = np.random.randn(D_in, H) w2 = np.random.randn(H, D_out) learning_rate = 1e-6 for t in range(500): # \u524d\u5411\u8ba1\u7b97, \u7b97\u51fay\u7684\u9884\u6d4b\u503c h = x.dot(w1) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # \u8ba1\u7b97\u5e76\u6253\u5370\u8bef\u5dee\u503c loss = np.square(y_pred - y).sum() print(t, loss) # \u5728\u53cd\u5411\u4f20\u64ad\u4e2d, \u8ba1\u7b97\u51fa\u8bef\u5dee\u5173\u4e8ew1\u548cw2\u7684\u5bfc\u6570 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h < 0] = 0 grad_w1 = x.T.dot(grad_h) # \u66f4\u65b0\u6743\u91cd w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 PyTorch: Tensors Numpy \u662f\u4e00\u4e2a\u4f1f\u5927\u7684\u6846\u67b6, \u4f46\u5b83\u4e0d\u80fd\u5229\u7528 GPU \u52a0\u901f\u5b83\u6570\u503c\u8ba1\u7b97. \u5bf9\u4e8e\u73b0\u4ee3\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc, GPU \u5f80\u5f80\u662f\u63d0\u4f9b 50\u500d\u6216\u66f4\u5927\u7684\u52a0\u901f , \u6240\u4ee5\u4e0d\u5e78\u7684\u662f, numpy \u4e0d\u8db3\u4ee5\u6ee1\u8db3\u73b0\u5728\u6df1\u5ea6\u5b66\u4e60\u7684\u9700\u6c42. \u8fd9\u91cc\u6211\u4eec\u4ecb\u7ecd\u4e00\u4e0b\u6700\u57fa\u672c\u7684 PyTorch \u6982\u5ff5: Tensor . PyTorch Tensor \u5728\u6982\u5ff5\u4e0a\u4e0e numpy \u6570\u7ec4\u76f8\u540c: Tensor \u662f\u4e00\u4e2an\u7ef4\u6570\u7ec4, PyTorch \u4e5f\u63d0\u4f9b\u4e86\u5f88\u591a\u80fd\u5728\u8fd9\u4e9b Tensor \u4e0a\u64cd\u4f5c\u7684\u51fd\u6570. \u50cf numpy \u6570\u7ec4\u4e00\u6837, PyTorch Tensor \u4e5f\u548cnumpy\u7684\u6570\u7ec4\u5bf9\u8c61\u4e00\u6837\u4e0d\u4e86\u89e3\u6df1\u5ea6\u5b66\u4e60,\u8ba1\u7b97\u56fe\u548c\u68af\u5ea6\u4e0b\u964d\uff1b\u5b83\u4eec\u53ea\u662f\u79d1\u5b66\u8ba1\u7b97\u7684\u901a\u7528\u5de5\u5177. \u7136\u800c\u4e0d\u50cf numpy, PyTorch Tensor \u53ef\u4ee5\u5229\u7528 GPU \u52a0\u901f\u4ed6\u4eec\u7684\u6570\u5b57\u8ba1\u7b97. \u8981\u5728 GPU \u4e0a\u8fd0\u884c PyTorch \u5f20\u91cf, \u53ea\u9700\u5c06\u5176\u8f6c\u6362\u4e3a\u65b0\u7684\u6570\u636e\u7c7b\u578b. \u5728\u8fd9\u91cc, \u6211\u4eec\u5c06 PyTorch Tensor \u751f\u6210\u7684\u968f\u673a\u6570\u636e\u4f20\u5165\u53cc\u5c42\u7684\u795e\u7ecf\u7f51\u7edc. \u5c31\u50cf\u4e0a\u9762\u7684 numpy \u4f8b\u5b50\u4e00\u6837, \u6211\u4eec\u9700\u8981\u624b\u52a8\u5b9e\u73b0\u7f51\u7edc\u7684\u6b63\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad: # -*- coding: utf-8 -*- import torch dtype = torch.FloatTensor # dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u8f93\u5165\u548c\u8f93\u51fa\u6570\u636e x = torch.randn(N, D_in).type(dtype) y = torch.randn(N, D_out).type(dtype) # \u968f\u673a\u521d\u59cb\u5316\u6743\u91cd w1 = torch.randn(D_in, H).type(dtype) w2 = torch.randn(H, D_out).type(dtype) learning_rate = 1e-6 for t in range(500): # \u6b63\u5411\u4f20\u9012\uff1a\u8ba1\u7b97\u9884\u6d4by h = x.mm(w1) h_relu = h.clamp(min=0) y_pred = h_relu.mm(w2) # \u8ba1\u7b97\u5e76\u6253\u5370loss loss = (y_pred - y).pow(2).sum() print(t, loss) # \u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u5173\u4e8e\u635f\u5931\u7684w1\u548cw2\u7684\u68af\u5ea6 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.t().mm(grad_y_pred) grad_h_relu = grad_y_pred.mm(w2.t()) grad_h = grad_h_relu.clone() grad_h[h < 0] = 0 grad_w1 = x.t().mm(grad_h) # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 Autograd PyTorch: Variables and autograd \u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d, \u6211\u4eec\u4e0d\u5f97\u4e0d\u624b\u5199\u5b9e\u73b0\u795e\u7ecf\u7f51\u7edc\u7684\u6b63\u53cd\u5411\u4f20\u64ad\u7684\u4ee3\u7801. \u800c\u624b\u5199\u5b9e\u73b0\u53cd\u5411\u4f20\u64ad\u7684\u4ee3\u7801\u5bf9\u4e8e\u4e00\u4e2a \u5c0f\u578b\u7684\u53cc\u5c42\u7f51\u7edc\u6765\u8bf4\u662f\u6ca1\u4ec0\u4e48\u5927\u95ee\u9898\u7684, \u4f46\u662f\u5728\u9762\u5bf9\u5927\u578b\u590d\u6742\u7f51\u7edc\u624b\u5199\u65b9\u5411\u4f20\u64ad\u4ee3\u7801\u5c31\u4f1a\u53d8\u5f97\u5f88\u68d8\u624b. \u8c22\u5929\u8c22\u5730, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 \u81ea\u52a8\u5fae\u5206 \u6765\u81ea\u52a8\u5316\u7684\u8ba1\u7b97\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u540e\u5411\u4f20\u64ad. PyTorch \u4e2d\u7684 autograd \u5305\u63d0\u4f9b\u81ea\u52a8\u5fae\u5206\u4e86\u8fd9\u4e2a\u529f\u80fd. \u4f7f\u7528 autograd \u65f6, \u7f51\u7edc\u7684\u6b63\u5411\u4f20\u64ad\u5c06\u5b9a\u4e49\u4e00\u4e2a \u8ba1\u7b97\u56fe ; Tensor \u5c06\u4f1a\u6210\u4e3a\u56fe\u4e2d\u7684\u8282\u70b9,\u4ece\u8f93\u5165 Tensor \u4ea7\u751f\u8f93\u51fa Tensor \u7684\u51fd\u6570\u5c06\u4f1a\u7528\u56fe\u4e2d\u7684( Edge )\u4f9d\u8d56\u8fb9\u8868\u793a. \u901a\u8fc7\u8ba1\u7b97\u56fe\u6765\u53cd\u5411\u4f20\u64ad\u53ef\u4ee5\u8ba9\u60a8\u8f7b\u677e\u8ba1\u7b97\u68af\u5ea6. \u8fd9\u542c\u8d77\u6765\u5f88\u590d\u6742, \u4f46\u662f\u5728\u5b9e\u8df5\u4e2d\u4f7f\u7528\u8d77\u6765\u76f8\u5f53\u7b80\u5355. \u6211\u4eec\u5c06 PyTorch \u7684 Tensor \u5305\u88c5\u6210\u5728 Variable \u5bf9\u8c61\uff1b \u4e00\u4e2a Variable \u4ee3\u8868\u4e00\u4e2a\u8ba1\u7b97\u56fe\u4e2d\u7684\u8282\u70b9. \u5982\u679c x \u662f\u4e00\u4e2a Variable , \u5219 x.data \u662f\u4e00\u4e2a Tensor , \u800c x.grad \u662f\u53e6\u5916\u4e00\u4e2a\u5305\u542b\u5173\u4e8e x \u7684\u68af\u5ea6\u7684 Variable . PyTorch Variable \u4e0e PyTorch Tensor \u5177\u6709\u76f8\u540c\u7684 API: (\u51e0\u4e4e) \u4efb\u4f55\u60a8\u53ef\u4ee5\u5728 Tensor \u4e0a\u6267\u884c\u7684 \u64cd\u4f5c\u4e5f\u9002\u7528\u4e8e Variable \uff1b\u8be5\u533a\u522b\u5728\u4e8e\u5982\u679c\u4f60\u4f7f\u7528 Variable \u5b9a\u4e49\u4e86\u4e00\u4e2a\u8ba1\u7b97\u56fe, Pytorch \u5141\u8bb8\u60a8\u81ea\u52a8\u8ba1\u7b97\u68af\u5ea6. \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 PyTorch \u7684 Variable \u548c\u81ea\u52a8\u5fae\u5206\u6765\u5b9e\u73b0\u6211\u4eec\u7684\u53cc\u5c42\u7f51\u7edc\uff1b\u73b0\u5728\u6211\u4eec\u4e0d\u518d\u9700\u8981\u624b\u5199\u4efb\u4f55\u5173\u4e8e \u8ba1\u7b97\u7f51\u7edc\u53cd\u5411\u4f20\u64ad\u7684\u4ee3\u7801: # -*- coding: utf-8 -*- import torch from torch.autograd import Variable dtype = torch.FloatTensor # dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. # \u8bbe\u7f6erequires_grad = False, \u56e0\u4e3a\u5728\u540e\u5411\u4f20\u64ad\u65f6, \u6211\u4eec\u5e76\u4e0d\u9700\u8981\u8ba1\u7b97\u5173\u4e8e\u8fd9\u4e9b\u53d8\u91cf\u7684\u68af\u5ea6 x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False) y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False) # \u4e3a\u6743\u91cd\u521b\u5efa\u968f\u673a\u5f20\u91cf,\u5e76\u5c06\u5176\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. # \u8bbe\u7f6erequires_grad = True, \u56e0\u4e3a\u5728\u540e\u5411\u4f20\u64ad\u65f6, \u6211\u4eec\u9700\u8981\u8ba1\u7b97\u5173\u4e8e\u8fd9\u4e9b\u53d8\u91cf\u7684\u68af\u5ea6 w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True) w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True) learning_rate = 1e-6 for t in range(500): # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y; \u8fd9\u4e9b # \u4e0e\u6211\u4eec\u7528\u4e8e\u8ba1\u7b97\u4f7f\u7528\u5f20\u91cf\u7684\u6b63\u5411\u4f20\u9012\u5b8c\u5168\u76f8\u540c, # \u4f46\u6211\u4eec\u4e0d\u9700\u8981\u4fdd\u7559\u5bf9\u4e2d\u95f4\u503c\u7684\u5f15\u7528, # \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5b9e\u73b0\u5411\u540e\u4f20\u9012. y_pred = x.mm(w1).clamp(min=0).mm(w2) # \u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u64cd\u4f5c\u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931. # \u73b0\u5728\u635f\u5931\u662f\u5f62\u72b6\u53d8\u91cf (1,) \u5e76\u4e14 loss.data \u662f\u5f62\u72b6\u7684\u5f20\u91cf # (1,); loss.data[0] \u662f\u6301\u6709\u635f\u5931\u7684\u6807\u91cf\u503c. loss = (y_pred - y).pow(2).sum() print(t, loss.data[0]) # \u4f7f\u7528autograd\u6765\u8ba1\u7b97\u53cd\u5411\u4f20\u9012. # \u8be5\u8c03\u7528\u5c06\u4f7f\u7528requires_grad = True\u6765\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u6240\u6709\u53d8\u91cf\u7684\u635f\u5931\u68af\u5ea6. # \u5728\u8fd9\u6b21\u8c03\u7528\u4e4b\u540e w1.grad \u548c w2.grad \u5c06\u662f\u53d8\u91cf # \u5b83\u4eec\u5206\u522b\u76f8\u5bf9\u4e8ew1\u548cw2\u4fdd\u5b58\u635f\u5931\u7684\u68af\u5ea6. loss.backward() # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd; w1.data \u548c w2.data \u662f\u5f20\u91cf, # w1.grad \u548c w2.grad \u662f\u53d8\u91cf\u5e76\u4e14 w1.grad.data \u548c w2.grad.data # \u662f\u5f20\u91cf. w1.data -= learning_rate * w1.grad.data w2.data -= learning_rate * w2.grad.data # \u66f4\u65b0\u6743\u91cd\u540e\u624b\u52a8\u5c06\u68af\u5ea6\u5f52\u96f6 w1.grad.data.zero_() w2.grad.data.zero_() PyTorch: Defining new autograd functions \u5728\u8fd9\u5c42\u8986\u76d6\u4e0b, \u6bcf\u4e2a\u539f\u59cb\u7684 autograd \u64cd\u4f5c\u7b26\u5b9e\u9645\u4e0a\u662f\u4e24\u4e2a\u51fd\u6570\u5728\u5f20\u91cf\u4e0a\u8fd0\u884c. \u524d\u5411\u4f20\u64ad \u51fd\u6570\u4ece\u8f93\u5165\u7684 Tensor \u8ba1\u7b97\u5c06\u8981\u8f93\u51fa\u7684 Tensor . \u540e\u5411\u4f20\u64ad \u51fd\u6570\u63a5\u6536\u4e0a\u4e00\u4e2a Tensor \u5173\u4e8e scalar \u7684\u68af\u5ea6, \u4ee5 \u53ca\u8ba1\u7b97\u5f53\u524d\u8f93\u5165 Tensor \u5bf9\u76f8\u540c scalar \u503c\u7684\u68af\u5ea6. \u5728 PyTorch \u4e2d, \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b9a\u4e49\u4e00\u4e2a torch.autograd.Function \u7684\u5b50\u7c7b\u548c \u5b9e\u73b0 \u524d\u5411\u4f20\u64ad \u548c \u540e\u5411\u4f20\u64ad \u51fd\u6570\u6765\u8f7b\u677e\u5b9a\u4e49\u81ea\u5df1\u7684 autograd \u64cd\u4f5c\u7b26. \u7136\u540e\u6211\u4eec\u53ef\u4ee5 \u4f7f\u7528\u6211\u4eec\u65b0\u7684 autograd \u64cd\u4f5c\u7b26\u6784\u9020\u4e00\u4e2a\u5b9e\u4f8b\u5e76\u5c06\u5176\u4f5c\u4e3a\u4e00\u4e2a\u51fd\u6570\u8c03\u7528, \u4f20\u9012\u7528 Variable \u5305\u88c5\u4e86\u7684\u8f93\u5165\u6570\u636e\u7684. \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\u6211\u4eec\u5b9a\u4e49\u4e86\u6211\u4eec\u81ea\u5df1\u7684 autograd \u51fd\u6570\u6765\u6267\u884c ReLU \u975e\u7ebf\u6027\u51fd\u6570, \u5e76\u7528\u5b83\u6765\u5b9e\u73b0\u6211\u4eec\u7684\u53cc\u5c42\u7f51\u7edc: # -*- coding: utf-8 -*- import torch from torch.autograd import Variable class MyReLU(torch.autograd.Function): \"\"\" \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b50\u7c7b\u5b9e\u73b0\u6211\u4eec\u81ea\u5df1\u5b9a\u5236\u7684autograd\u51fd\u6570 torch.autograd.Function\u548c\u6267\u884c\u5728Tensors\u4e0a\u8fd0\u884c\u7684\u5411\u524d\u548c\u5411\u540e\u901a\u884c\u8bc1. \"\"\" @staticmethod def forward(ctx, input): \"\"\" \u5728\u6b63\u5411\u4f20\u9012\u4e2d,\u6211\u4eec\u6536\u5230\u4e00\u4e2a\u5305\u542b\u8f93\u5165\u548c\u8fd4\u56de\u5f20\u91cf\u7684\u5f20\u91cf,\u5176\u4e2d\u5305\u542b\u8f93\u51fa. ctx\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u5bf9\u8c61,\u53ef\u7528\u4e8e\u5b58\u50a8\u53cd\u5411\u8ba1\u7b97\u7684\u4fe1\u606f. \u60a8\u53ef\u4ee5\u4f7f\u7528ctx.save_for_backward\u65b9\u6cd5\u7f13\u5b58\u4efb\u610f\u5bf9\u8c61\u4ee5\u7528\u4e8e\u540e\u5411\u4f20\u9012. \"\"\" ctx.save_for_backward(input) return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): \"\"\" \u5728\u540e\u5411\u4f20\u9012\u4e2d,\u6211\u4eec\u6536\u5230\u4e00\u4e2a\u5f20\u91cf,\u5176\u4e2d\u5305\u542b\u76f8\u5bf9\u4e8e\u8f93\u51fa\u7684\u635f\u5931\u68af\u5ea6, \u6211\u4eec\u9700\u8981\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u8f93\u5165\u7684\u635f\u5931\u68af\u5ea6. \"\"\" input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input < 0] = 0 return grad_input dtype = torch.FloatTensor # dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False) y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False) # \u4e3a\u6743\u91cd\u521b\u5efa\u968f\u673a\u5f20\u91cf,\u5e76\u5c06\u5176\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True) w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True) learning_rate = 1e-6 for t in range(500): # \u4e3a\u4e86\u5e94\u7528\u6211\u4eec\u7684\u51fd\u6570,\u6211\u4eec\u4f7f\u7528Function.apply\u65b9\u6cd5.\u6211\u4eec\u628a\u5b83\u79f0\u4e3a'relu'. relu = MyReLU.apply # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y; # \u6211\u4eec\u4f7f\u7528\u6211\u4eec\u7684\u81ea\u5b9a\u4e49autograd\u64cd\u4f5c\u6765\u8ba1\u7b97ReLU. y_pred = relu(x.mm(w1)).mm(w2) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931 loss = (y_pred - y).pow(2).sum() print(t, loss.data[0]) # \u4f7f\u7528autograd\u6765\u8ba1\u7b97\u53cd\u5411\u4f20\u9012. loss.backward() # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd w1.data -= learning_rate * w1.grad.data w2.data -= learning_rate * w2.grad.data # \u66f4\u65b0\u6743\u91cd\u540e\u624b\u52a8\u5c06\u68af\u5ea6\u5f52\u96f6 w1.grad.data.zero_() w2.grad.data.zero_() TensorFlow: Static Graphs Pytorch \u7684 autograd \u770b\u4e0a\u53bb\u6709\u70b9\u50cf TensorFlow .\u4e24\u4e2a\u6846\u67b6\u7684\u5171\u540c\u70b9\u662f\u4ed6\u4eec\u90fd\u662f\u5b9a\u4e49\u4e86\u81ea\u5df1\u7684\u8ba1\u7b97\u56fe. \u548c\u4f7f\u7528\u81ea\u52a8\u6c42\u5fae\u5206\u7684\u65b9\u6cd5\u6765\u8ba1\u7b97\u68af\u5ea6. \u4e24\u8005\u4e4b\u95f4\u6700\u5927\u7684\u4e0d\u540c\u5728\u4e8e TensorFlow \u7684\u8ba1\u7b97\u56fe\u662f \u9759\u6001\u7684 \u548c PyTorch \u7684\u8ba1\u7b97\u56fe\u662f \u52a8\u6001\u7684 . \u5728 TensorFlow \u4e2d, \u6211\u4eec\u53ea\u5b9a\u4e49\u4e86\u4e00\u6b21\u8ba1\u7b97\u56fe,\u7136\u540e\u91cd\u590d\u6267\u884c\u540c\u4e00\u5f20\u8ba1\u7b97\u56fe, \u53ea\u662f\u8f93\u5165\u8ba1\u7b97\u56fe\u7684\u6570\u636e\u4e0d\u540c\u800c\u5df2. \u800c\u5728 PyTorch \u4e2d, \u6bcf\u4e2a\u6b63\u5411\u4f20\u64ad\u90fd\u4f1a\u5b9a\u4e49\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u56fe. \u9759\u6001\u56fe\u5f88\u597d, \u56e0\u4e3a\u60a8\u53ef\u4ee5\u9884\u5148\u4f18\u5316\u8ba1\u7b97\u56fe\uff1b\u4f8b\u5982\u4e00\u4e2a\u6846\u67b6\u53ef\u80fd\u4f1a\u4e3a\u4e86\u8ba1\u7b97\u6548\u7387\u51b3\u5b9a\u878d\u5408\u4e00\u4e9b\u8ba1\u7b97\u56fe\u64cd\u4f5c(\u50cf:Fused Graph), \u6216\u63d0\u51fa \u4e00\u4e2a\u591a\u5361\u6216\u8005\u591a\u673a\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u56fe\u7684\u7b56\u7565. \u5982\u679c\u60a8\u6b63\u5728\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u7684\u8ba1\u7b97\u56fe, \u90a3\u4e48\u8fd9\u4e2a\u6f5c\u5728\u7684 \u6602\u8d35\u7684\u524d\u671f\u4f18\u5316\u53ef\u4ee5\u4f7f\u7528\u9759\u6001\u56fe\u6765\u5f97\u4ee5\u51cf\u8f7b. \u4e00\u65b9\u9762\u6765\u8bf4, \u9759\u6001\u56fe\u548c\u52a8\u6001\u56fe\u7684\u63a7\u5236\u6d41\u662f\u4e0d\u540c\u7684. \u5bf9\u4e8e\u6709\u4e9b\u6a21\u578b\u6211\u4eec\u53ef\u80fd\u5e0c\u671b\u5bf9\u6bcf\u4e2a\u6570\u636e\u70b9\u6267\u884c\u4e0d\u540c \u7684\u8ba1\u7b97\uff1b\u4f8b\u5982\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u53ef\u80fd\u4f1a\u88ab\u5c55\u5f00\u4e3a\u5bf9\u6bcf\u4e2a\u6570\u636e\u7684\u4e0d\u540c\u7684\u957f\u5ea6\u7684\u65f6\u95f4\u6b65\u6570\uff1b\u8fd9\u4e2a\u5c55\u5f00\u53ef\u4ee5\u7528\u5faa \u73af\u6765\u5b9e\u73b0. \u5faa\u73af\u7ed3\u6784\u7684\u9759\u6001\u56fe\u9700\u8981\u6210\u4e3a\u8ba1\u7b97\u56fe\u7684\u4e00\u90e8\u5206\uff1b\u4e3a\u6b64 TensorFlow \u63d0\u4f9b tf.scan \u64cd\u4f5c\u7b26 \u7528\u4e8e\u5c06\u91cd\u590d\u7684\u7ed3\u6784\u5d4c\u5165\u5230\u8ba1\u7b97\u56fe\u4e2d. \u800c\u52a8\u6001\u8ba1\u7b97\u56fe\u7684\u60c5\u51b5\u6bd4\u8f83\u7b80\u5355: \u56e0\u4e3a\u6211\u4eec\u8bbe\u8ba1\u7684\u8ba1\u7b97\u56fe\u53ef\u4ee5\u5bf9\u6bcf\u4e2a\u4e0d\u540c\u957f\u5ea6\u7684\u8f93\u5165\u968f\u673a\u5e94\u53d8. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6b63\u5e38\u7684\u547d\u4ee4\u5f0f\u4ee3\u7801\u5bf9\u6bcf\u4e2a\u4e0d\u540c\u957f\u5ea6\u7684\u8f93\u5165\u6267\u884c\u8ba1\u7b97. \u4e3a\u4e86\u4e0e\u4e0a\u9762\u7684 PyTorch autograd \u4f8b\u5b50\u8fdb\u884c\u5bf9\u6bd4, \u6211\u4eec\u5728\u8fd9\u91cc\u4e5f\u4f7f\u7528 TensorFlow \u521b\u5efa\u7b80\u5355\u7684\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc: # -*- coding: utf-8 -*- import tensorflow as tf import numpy as np # \u9996\u5148\u6211\u4eec\u8bbe\u7f6e\u8ba1\u7b97\u56fe: # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u4e3a\u8f93\u5165\u6570\u636e\u548c\u76ee\u6807\u6570\u636e\u521b\u5efa\u5360\u4f4d\u7b26; # \u5f53\u6211\u4eec\u6267\u884c\u56fe\u65f6,\u8fd9\u4e9b\u5c06\u88ab\u586b\u5145\u771f\u5b9e\u7684\u6570\u636e. x = tf.placeholder(tf.float32, shape=(None, D_in)) y = tf.placeholder(tf.float32, shape=(None, D_out)) # \u4e3a\u6743\u91cd\u521b\u5efa\u53d8\u91cf\u5e76\u7528\u968f\u673a\u6570\u636e\u521d\u59cb\u5316\u5b83\u4eec. # \u4e00\u4e2aTensorFlow\u53d8\u91cf\u5728\u56fe\u7684\u6267\u884c\u4e2d\u4fdd\u6301\u5176\u503c. w1 = tf.Variable(tf.random_normal((D_in, H))) w2 = tf.Variable(tf.random_normal((H, D_out))) # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528TensorFlow Tensors\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y. # \u8bf7\u6ce8\u610f\u6b64\u4ee3\u7801\u5b9e\u9645\u4e0a\u5e76\u672a\u6267\u884c\u4efb\u4f55\u6570\u5b57\u64cd\u4f5c; # \u5b83\u53ea\u662f\u8bbe\u7f6e\u6211\u4eec\u7a0d\u540e\u5c06\u6267\u884c\u7684\u8ba1\u7b97\u56fe. h = tf.matmul(x, w1) h_relu = tf.maximum(h, tf.zeros(1)) y_pred = tf.matmul(h_relu, w2) # \u4f7f\u7528TensorFlow\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u8ba1\u7b97\u635f\u5931 loss = tf.reduce_sum((y - y_pred) ** 2.0) # \u8ba1\u7b97\u76f8\u5bf9\u4e8ew1\u548cw2\u7684\u635f\u5931\u68af\u5ea6. grad_w1, grad_w2 = tf.gradients(loss, [w1, w2]) # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd. # \u8981\u5b9e\u9645\u66f4\u65b0\u6743\u91cd,\u6211\u4eec\u9700\u8981\u5728\u6267\u884c\u56fe\u65f6\u8bc4\u4f30new_w1\u548cnew_w2. # \u8bf7\u6ce8\u610f,\u5728TensorFlow\u4e2d,\u66f4\u65b0\u6743\u503c\u7684\u884c\u4e3a\u662f\u8ba1\u7b97\u56fe\u7684\u4e00\u90e8\u5206 # \u5728PyTorch\u4e2d,\u8fd9\u53d1\u751f\u5728\u8ba1\u7b97\u56fe\u4e4b\u5916. learning_rate = 1e-6 new_w1 = w1.assign(w1 - learning_rate * grad_w1) new_w2 = w2.assign(w2 - learning_rate * grad_w2) # \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u6784\u5efa\u4e86\u8ba1\u7b97\u56fe,\u6240\u4ee5\u6211\u4eec\u8f93\u5165\u4e00\u4e2aTensorFlow\u4f1a\u8bdd\u6765\u5b9e\u9645\u6267\u884c\u56fe. with tf.Session() as sess: # \u8fd0\u884c\u4e00\u6b21\u56fe\u5f62\u521d\u59cb\u5316\u53d8\u91cfw1\u548cw2. sess.run(tf.global_variables_initializer()) # \u521b\u5efa\u5305\u542b\u8f93\u5165x\u548c\u76ee\u6807y\u7684\u5b9e\u9645\u6570\u636e\u7684numpy\u6570\u7ec4 x_value = np.random.randn(N, D_in) y_value = np.random.randn(N, D_out) for _ in range(500): # \u591a\u6b21\u6267\u884c\u56fe. \u6bcf\u6b21\u6267\u884c\u65f6, # \u6211\u4eec\u90fd\u60f3\u5c06x_value\u7ed1\u5b9a\u5230x,\u5c06y_value\u7ed1\u5b9a\u5230y,\u7528feed_dict\u53c2\u6570\u6307\u5b9a. # \u6bcf\u6b21\u6211\u4eec\u6267\u884c\u56fe\u65f6,\u6211\u4eec\u90fd\u60f3\u8ba1\u7b97\u635f\u5931\u503cnew_w1 \u548c new_w2; # \u8fd9\u4e9b\u5f20\u91cf\u7684\u503c\u4f5c\u4e3anumpy\u6570\u7ec4\u8fd4\u56de. loss_value, _, _ = sess.run([loss, new_w1, new_w2], feed_dict={x: x_value, y: y_value}) print(loss_value) <cite>nn</cite> module PyTorch: nn \u8ba1\u7b97\u56fe( Computational graphs )\u548c autograd \u662f\u4e00\u4e2a\u975e\u5e38\u5f3a\u5927\u7684\u5b9a\u4e49\u590d\u6742\u7684\u8fd0\u7b97\u7b26\u5e76\u81ea\u52a8\u5730\u5bfc\u51fa\u7684\u8303\u5f0f\uff1b\u7136\u800c\u5bf9\u4e8e \u5927\u578b\u7684\u795e\u7ecf\u7f51\u7edc, \u539f\u59cb\u7684 autograd \u4ecd\u7136\u663e\u5f97\u6709\u70b9\u592a\u4f4e\u7ea7. \u5f53\u6211\u4eec\u521b\u5efa\u795e\u7ecf\u7f51\u7edc\u65f6, \u6211\u4eec\u7ecf\u5e38\u601d\u8003\u5982\u4f55\u8bbe\u8ba1\u5b89\u6392 layer , \u4ee5\u53ca\u4e00\u4e9b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7f51\u7edc\u4f1a\u5b66\u4e60\u5230\u7684 learnable parameters \u5728TensorFlow\u4e2d, \u50cf Keras , TensorFlow-Slim , \u548c TFLearn \u901a\u8fc7\u6784\u5efa\u5bf9\u795e\u7ecf\u7f51\u7edc\u6709\u7528\u7684\u539f\u59cb\u8ba1\u7b97\u56fe\u63d0\u4f9b\u66f4\u9ad8\u5c42\u6b21\u7684\u62bd\u8c61. \u5728 PyTorch \u4e2d, nn \u5305\u8d77\u4e86\u540c\u6837\u7684\u4f5c\u7528. nn \u5305\u5b9a\u4e49\u4e86\u4e00\u7ec4 Modules , \u5927\u81f4\u76f8\u5f53\u4e8e\u795e\u7ecf\u7f51\u7edc\u5c42. \u6a21\u5757\u63a5\u6536\u8f93\u5165\u53d8\u91cf\u5e76\u8fdb\u884c\u8ba1\u7b97\u8f93\u51fa\u53d8\u91cf, \u4f46\u4e5f\u53ef\u4ee5\u4fdd\u6301\u5185\u90e8\u72b6\u6001, \u5982 \u7528 Variable \u5305\u88c5\u7684 learnable parameters . nn \u5305 \u4e5f\u5b9a\u4e49\u4e86\u4e00\u7cfb\u5217\u5728\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u65f6\u6bd4\u8f83\u5e38\u7528\u7684\u635f\u5931\u51fd\u6570. \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d, \u6211\u4eec\u4f7f\u7528 nn \u5305\u6765\u5b9e\u73b0\u6211\u4eec\u7684\u53cc\u5c42\u795e\u7ecf\u7f51\u7edc: # -*- coding: utf-8 -*- import torch from torch.autograd import Variable # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u4f7f\u7528nn\u5305\u5c06\u6211\u4eec\u7684\u6a21\u578b\u5b9a\u4e49\u4e3a\u4e00\u7cfb\u5217\u56fe\u5c42. # nn.Sequential\u662f\u4e00\u4e2a\u5305\u542b\u5176\u4ed6\u6a21\u5757\u7684\u6a21\u5757,\u5e76\u5c06\u5b83\u4eec\u6309\u987a\u5e8f\u5e94\u7528\u4ee5\u4ea7\u751f\u5176\u8f93\u51fa. # \u6bcf\u4e2a\u7ebf\u6027\u6a21\u5757\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u8ba1\u7b97\u6765\u81ea\u8f93\u5165\u7684\u8f93\u51fa,\u5e76\u4fdd\u5b58\u5185\u90e8\u53d8\u91cf\u7684\u6743\u91cd\u548c\u504f\u5dee. model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) # nn\u5305\u8fd8\u5305\u542b\u6d41\u884c\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49; # \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u6211\u4eec\u5c06\u4f7f\u7528\u5747\u65b9\u5dee(MSE)\u4f5c\u4e3a\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570. loss_fn = torch.nn.MSELoss(size_average=False) learning_rate = 1e-4 for t in range(500): # \u6b63\u5411\u4f20\u9012:\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y. # \u6a21\u5757\u5bf9\u8c61\u4f1a\u8986\u76d6__call__\u8fd0\u7b97\u7b26,\u56e0\u6b64\u60a8\u53ef\u4ee5\u5c06\u5b83\u4eec\u79f0\u4e3a\u51fd\u6570. # \u8fd9\u6837\u505a\u65f6,\u60a8\u5c06\u8f93\u5165\u6570\u636e\u7684\u53d8\u91cf\u4f20\u9012\u7ed9\u6a21\u5757,\u5e76\u751f\u6210\u8f93\u51fa\u6570\u636e\u7684\u53d8\u91cf. y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931. # \u6211\u4eec\u4f20\u9012\u5305\u542by\u7684\u9884\u6d4b\u503c\u548c\u771f\u503c\u7684\u53d8\u91cf,\u5e76\u4e14\u635f\u5931\u51fd\u6570\u8fd4\u56de\u5305\u542b\u635f\u5931\u7684\u53d8\u91cf. loss = loss_fn(y_pred, y) print(t, loss.data[0]) # \u5728\u8fd0\u884c\u53cd\u5411\u4f20\u9012\u4e4b\u524d\u5c06\u68af\u5ea6\u5f52\u96f6. model.zero_grad() # \u5411\u540e\u4f20\u9012:\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u6a21\u578b\u7684\u6240\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u635f\u5931\u68af\u5ea6. # \u5728\u5185\u90e8,\u6bcf\u4e2a\u6a21\u5757\u7684\u53c2\u6570\u90fd\u5b58\u50a8\u5728\u53d8\u91cfrequire_grad = True\u4e2d, # \u56e0\u6b64\u8be5\u8c03\u7528\u5c06\u8ba1\u7b97\u6a21\u578b\u4e2d\u6240\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u68af\u5ea6. loss.backward() # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd. # \u6bcf\u4e2a\u53c2\u6570\u90fd\u662f\u4e00\u4e2a\u53d8\u91cf,\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u50cf\u6211\u4eec\u4ee5\u524d\u90a3\u6837\u8bbf\u95ee\u5b83\u7684\u6570\u636e\u548c\u68af\u5ea6. for param in model.parameters(): param.data -= learning_rate * param.grad.data PyTorch: optim \u5230\u76ee\u524d\u4e3a\u6b62, \u6211\u4eec\u4e00\u76f4\u901a\u8fc7\u624b\u52a8\u66f4\u65b0\u7684\u65b9\u6cd5\u66f4\u65b0\u6a21\u578b\u7684\u53ef\u5b66\u4e60\u53c2\u6570( learnable parameters )\u7684\u6743\u91cd .data \u8fd9\u5bf9\u4e8e\u7b80\u5355\u7684\u4f18\u5316\u7b97\u6cd5\u50cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6765\u8fd8\u7b97\u8f7b\u677e, \u4f46\u662f\u5728\u5b9e\u9645\u4e2d\u6211\u4eec\u7ecf\u5e38\u4f7f\u7528\u66f4\u5de7\u5999\u7684 \u4f18\u5316\u5668\u6765\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc, \u5982 AdaGrad, RMSProp, Adam \u7b49. PyTorch \u4e2d\u7684 optim \u5305\u5305\u542b\u4e86\u4e00\u4e9b\u4f18\u5316\u5668\u7684\u7b97\u6cd5, \u5e76\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5e38\u7528\u4f18\u5316\u5668\u7684\u4f7f\u7528. \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d, \u867d\u7136\u6211\u4eec\u5c06\u50cf\u4e4b\u524d\u4e00\u6837\u4f7f\u7528 nn \u5305\u6765\u5b9a\u4e49\u6211\u4eec\u7684\u6a21\u578b, \u4f46\u662f\u6211\u4eec\u8fd9\u6b21\u5c06\u4f7f\u7528\u7531 optim \u5305\u63d0\u4f9b\u7684Adam\u7b97\u6cd5\u6765\u66f4\u65b0\u6a21\u578b: # -*- coding: utf-8 -*- import torch from torch.autograd import Variable # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u4f7f\u7528nn\u5305\u6765\u5b9a\u4e49\u6211\u4eec\u7684\u6a21\u578b\u548c\u635f\u5931\u51fd\u6570. model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) loss_fn = torch.nn.MSELoss(size_average=False) # \u4f7f\u7528\u4f18\u5316\u5305\u6765\u5b9a\u4e49\u4e00\u4e2a\u4f18\u5316\u5668,\u5b83\u5c06\u4e3a\u6211\u4eec\u66f4\u65b0\u6a21\u578b\u7684\u6743\u91cd. # \u5728\u8fd9\u91cc,\u6211\u4eec\u5c06\u4f7f\u7528 Adam;\u8fd9\u4e2a optim \u5305\u5305\u542b\u8bb8\u591a\u5176\u4ed6\u4f18\u5316\u7b97\u6cd5. # Adam\u6784\u9020\u51fd\u6570\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u544a\u8bc9\u4f18\u5316\u5668\u5e94\u8be5\u66f4\u65b0\u54ea\u4e2aVariables. learning_rate = 1e-4 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) for t in range(500): # \u6b63\u5411\u4f20\u9012:\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y. y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931\u51fd\u6570. loss = loss_fn(y_pred, y) print(t, loss.data[0]) # \u5728\u5411\u540e\u4f20\u9012\u4e4b\u524d,\u4f7f\u7528\u4f18\u5316\u5668\u5bf9\u8c61\u4e3a\u5176\u8981\u66f4\u65b0\u7684\u53d8\u91cf\uff08\u8fd9\u662f\u6a21\u578b\u7684\u53ef\u5b66\u4e60\u6743\u91cd\uff09\u7684\u6240\u6709\u68af\u5ea6\u5f52\u96f6. # \u8fd9\u662f\u56e0\u4e3a\u9ed8\u8ba4\u60c5\u51b5\u4e0b,\u53ea\u8981\u8c03\u7528.backward(),\u68af\u5ea6\u5c31\u4f1a\u5728\u7f13\u51b2\u533a\u4e2d\u7d2f\u79ef(\u5373\u4e0d\u4f1a\u88ab\u8986\u76d6). # \u67e5\u770btorch.autograd.backward\u7684\u6587\u6863\u4ee5\u83b7\u53d6\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f. optimizer.zero_grad() # \u5411\u540e\u4f20\u9012:\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u76f8\u5bf9\u4e8e\u6a21\u578b\u53c2\u6570\u7684\u68af\u5ea6 loss.backward() # \u5728\u4f18\u5316\u5668\u4e0a\u8c03\u7528step\u51fd\u6570\u4f1a\u66f4\u65b0\u5176\u53c2\u6570 optimizer.step() PyTorch: Custom nn Modules \u6709\u65f6\u4f60\u4f1a\u60f3\u8981\u4f7f\u7528\u6bd4\u73b0\u6709\u6a21\u5757\u7ec4\u5408\u66f4\u590d\u6742\u7684\u7279\u6b8a\u6a21\u578b\uff1b\u5bf9\u4e8e\u8fd9\u4e9b\u60c5\u51b5, \u4f60\u53ef\u4ee5 \u901a\u8fc7\u7ee7\u627f nn.Module \u6765\u5b9a\u4e49\u4f60\u81ea\u5df1\u7684\u6a21\u5757, \u5e76\u5b9a\u4e49\u4e00\u4e2a forward \u6765\u5b9e\u73b0\u6a21\u5757\u63a5\u6536\u8f93\u5165 Variable \u5e76\u4f7f\u7528\u5176\u4ed6\u6a21\u5757\u8f93\u51fa\u7684 Variable \u548c \u5176\u4ed6 autograd \u64cd\u4f5c. \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d, \u6211\u4eec\u4f7f\u7528\u4e86\u6211\u4eec\u4e4b\u524d\u5df2\u7ecf\u5b9e\u73b0\u7684\u53cc\u5c42\u7f51\u7edc\u6765\u4f5c\u4e3a\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684\u6a21\u5757\u5b50\u7c7b: # -*- coding: utf-8 -*- import torch from torch.autograd import Variable class TwoLayerNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" \u5728\u6784\u9020\u51fd\u6570\u4e2d,\u6211\u4eec\u5b9e\u4f8b\u5316\u4e24\u4e2ann.Linear\u6a21\u5757\u5e76\u5c06\u5b83\u4eec\u5206\u914d\u4e3a\u6210\u5458\u53d8\u91cf. \"\"\" super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" \u5728forward\u51fd\u6570\u4e2d,\u6211\u4eec\u63a5\u53d7\u4e00\u4e2a\u53d8\u91cf\u7684\u8f93\u5165\u6570\u636e,\u6211\u4eec\u5fc5\u987b\u8fd4\u56de\u4e00\u4e2a\u53d8\u91cf\u7684\u8f93\u51fa\u6570\u636e. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6784\u9020\u51fd\u6570\u4e2d\u5b9a\u4e49\u7684\u6a21\u5757\u4ee5\u53ca\u53d8\u91cf\u4e0a\u7684\u4efb\u610f\u8fd0\u7b97\u7b26. \"\"\" h_relu = self.linear1(x).clamp(min=0) y_pred = self.linear2(h_relu) return y_pred # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u901a\u8fc7\u5b9e\u4f8b\u5316\u4e0a\u9762\u5b9a\u4e49\u7684\u7c7b\u6765\u6784\u5efa\u6211\u4eec\u7684\u6a21\u578b model = TwoLayerNet(D_in, H, D_out) # \u6784\u5efa\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668. # \u5bf9SGD\u6784\u9020\u51fd\u6570\u4e2d\u7684model.parameters()\u7684\u8c03\u7528\u5c06\u5305\u542b\u4f5c\u4e3a\u6a21\u578b\u6210\u5458\u7684\u4e24\u4e2ann.Linear\u6a21\u5757\u7684\u53ef\u5b66\u4e60\u53c2\u6570. criterion = torch.nn.MSELoss(size_average=False) optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) for t in range(500): # \u6b63\u5411\u4f20\u9012\uff1a\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931 loss = criterion(y_pred, y) print(t, loss.data[0]) # \u68af\u5ea6\u7f6e\u96f6, \u6267\u884c\u53cd\u5411\u4f20\u9012\u5e76\u66f4\u65b0\u6743\u91cd. optimizer.zero_grad() loss.backward() optimizer.step() PyTorch: Control Flow + Weight Sharing \u4f5c\u4e3a\u4e00\u4e2a\u52a8\u6001\u56fe\u548c\u6743\u503c\u5171\u4eab\u7684\u4f8b\u5b50, \u6211\u4eec\u5b9e\u73b0\u4e00\u4e2a\u5947\u8469\u7684\u6a21\u578b: \u968f\u673a1-4\u6b21\u91cd\u590d\u642d\u5efa\u540c\u4e2a\u6b63\u5411\u4f20\u64ad\u7684\u5168\u8fde\u63a5 \u7684 ReLU \u7f51\u7edc, \u5e76\u4e14\u591a\u4e2a\u9690\u85cf\u5c42\u4f7f\u7528\u76f8\u540c\u7684\u6743\u91cd\u6765\u8ba1\u7b97\u6700\u5185\u5c42\u9690\u85cf\u5c42(\u8bd1\u8005\u6ce8: \u8fd9\u91cc\u7684\u76f8\u540c\u6743\u91cd,\u662f\u6307\u968f\u673a1-4\u6b21\u91cd\u590d\u642d\u5efa\u7684\u8fd9\u4e2amiddle_linear). \u5bf9\u4e8e\u8fd9\u4e2a\u6a21\u578b, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u666e\u901a\u7684 Python \u6d41\u7a0b\u63a7\u5236\u8bed\u53e5\u6765\u5b9e\u73b0\u5faa\u73af, \u800c\u4e14\u6211\u4eec\u53ef\u4ee5\u5728\u5b9a\u4e49\u524d\u5411\u4f20 \u64ad\u65f6\u901a\u8fc7\u7b80\u5355\u5730\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u7684\u6a21\u5757\u5b9e\u73b0 middle_linear \u5c42\u7684\u6743\u91cd\u5171\u4eab. \u6211\u4eec\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u5c06\u8fd9\u4e2a\u6a21\u578b\u4f5c\u4e3a Module \u5b50\u7c7b\u6765\u5b9e\u73b0: # -*- coding: utf-8 -*- import random import torch from torch.autograd import Variable class DynamicNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" \u5728\u6784\u9020\u51fd\u6570\u4e2d,\u6211\u4eec\u6784\u9020\u4e86\u4e09\u4e2ann.Linear\u5b9e\u4f8b,\u6211\u4eec\u5c06\u5728\u6b63\u5411\u4f20\u9012\u4e2d\u4f7f\u7528\u5b83\u4eec. \"\"\" super(DynamicNet, self).__init__() self.input_linear = torch.nn.Linear(D_in, H) self.middle_linear = torch.nn.Linear(H, H) self.output_linear = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" \u5bf9\u4e8e\u6a21\u578b\u7684\u6b63\u5411\u901a\u9053,\u6211\u4eec\u968f\u673a\u9009\u62e90,1,2\u62163, \u5e76\u91cd\u590d\u4f7f\u7528\u591a\u6b21\u8ba1\u7b97\u9690\u85cf\u5c42\u8868\u793a\u7684middle_linear\u6a21\u5757. \u7531\u4e8e\u6bcf\u4e2a\u6b63\u5411\u901a\u9053\u90fd\u4f1a\u751f\u6210\u4e00\u4e2a\u52a8\u6001\u8ba1\u7b97\u56fe,\u56e0\u6b64\u5728\u5b9a\u4e49\u6a21\u578b\u7684\u6b63\u5411\u901a\u9053\u65f6, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u666e\u901a\u7684Python\u63a7\u5236\u6d41\u64cd\u4f5c\u7b26(\u5982\u5faa\u73af\u6216\u6761\u4ef6\u8bed\u53e5). \u5728\u8fd9\u91cc\u6211\u4eec\u4e5f\u770b\u5230,\u5b9a\u4e49\u8ba1\u7b97\u56fe\u65f6\u591a\u6b21\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u6a21\u5757\u662f\u5b8c\u5168\u5b89\u5168\u7684. \u8fd9\u662fLua Torch\u7684\u4e00\u5927\u6539\u8fdb,\u6bcf\u4e2a\u6a21\u5757\u53ea\u80fd\u4f7f\u7528\u4e00\u6b21. \"\"\" h_relu = self.input_linear(x).clamp(min=0) for _ in range(random.randint(0, 3)): h_relu = self.middle_linear(h_relu).clamp(min=0) y_pred = self.output_linear(h_relu) return y_pred # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u901a\u8fc7\u5b9e\u4f8b\u5316\u4e0a\u9762\u5b9a\u4e49\u7684\u7c7b\u6765\u6784\u5efa\u6211\u4eec\u7684\u6a21\u578b model = DynamicNet(D_in, H, D_out) # \u6784\u5efa\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668. # \u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u8fd9\u4e2a\u5947\u602a\u7684\u6a21\u578b\u975e\u5e38\u56f0\u96be,\u6240\u4ee5\u6211\u4eec\u4f7f\u7528\u52a8\u91cf criterion = torch.nn.MSELoss(size_average=False) optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9) for t in range(500): # \u6b63\u5411\u4f20\u9012:\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931 loss = criterion(y_pred, y) print(t, loss.data[0]) # \u96f6\u68af\u5ea6\u6267\u884c\u53cd\u5411\u4f20\u9012\u5e76\u66f4\u65b0\u6743\u91cd. optimizer.zero_grad() loss.backward() optimizer.step() Examples \u4f60\u53ef\u4ee5\u5728\u8fd9\u91cc\u6d4f\u89c8\u4e0a\u7f51\u63d0\u5230\u7684\u4f8b\u5b50 Tensors Warm-up: numpy PyTorch: Tensors Autograd PyTorch: \u53d8\u91cf\u548cautograd PyTorch: \u5b9a\u4e49\u65b0\u7684autograd\u51fd\u6570 TensorFlow: \u9759\u6001\u56fe <cite>nn</cite> module PyTorch: nn\u5305 PyTorch: optim\u5305 PyTorch: \u5b9a\u5236\u5316nn\u6a21\u5757 PyTorch: \u52a8\u6001\u63a7\u5236\u6d41\u7a0b + \u6743\u91cd\u5171\u4eab","title":"\u8ddf\u7740\u4f8b\u5b50\u5b66\u4e60 PyTorch"},{"location":"12/#pytorch","text":"Author : Justin Johnson \u8fd9\u4e2a\u6559\u7a0b\u901a\u8fc7\u4e00\u4e9b\u5355\u72ec\u7684\u793a\u4f8b\u4ecb\u7ecd\u4e86 PyTorch \u7684\u57fa\u672c\u6982\u5ff5. PyTorch \u7684\u6838\u5fc3\u90e8\u5206\u63d0\u4f9b\u4e86\u4e24\u4e2a\u4e3b\u8981\u529f\u80fd: \u4e00\u4e2a\u7c7b\u4f3c\u4e8e numpy \u7684n\u7ef4\u5f20\u91cf, \u4f46\u53ef\u4ee5\u5728 GPU \u4e0a\u8fd0\u884c \u4e3a\u5efa\u7acb\u548c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u81ea\u52a8\u5fae\u5206 \u6211\u4eec\u5c06\u4f7f\u7528\u5168\u8fde\u63a5\u7684 ReLU \u7f51\u7edc\u4f5c\u4e3a\u6211\u4eec\u7684\u8fd0\u884c\u793a\u4f8b. \u8be5\u7f51\u7edc\u5c06\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u5e76\u5c06\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u53bb\u6700\u5c0f\u5316\u968f\u673a\u6570\u5b57\u7684\u9884\u6d4b\u8f93\u51fa\u548c\u771f\u5b9e\u8f93\u51fa\u4e4b\u95f4\u7684\u6b27\u5f0f\u8ddd\u79bb. Note \u4f60\u53ef\u4ee5\u4e0b\u8f7d\u8fd9\u4e9b\u5355\u72ec\u7684\u4f8b\u5b50\u5728\u9875\u9762\u7684\u5e95\u7aef <examples-download>. \u672c\u7ae0\u5185\u5bb9\u76ee\u5f55 Tensors Warm-up: numpy PyTorch: Tensors Autograd PyTorch: Variables and autograd PyTorch: Defining new autograd functions TensorFlow: Static Graphs <cite>nn</cite> module PyTorch: nn PyTorch: optim PyTorch: Custom nn Modules PyTorch: Control Flow + Weight Sharing Examples Tensors Autograd <cite>nn</cite> module","title":"\u8ddf\u7740\u4f8b\u5b50\u5b66\u4e60 PyTorch"},{"location":"12/#tensors","text":"","title":"Tensors"},{"location":"12/#warm-up-numpy","text":"\u5728\u4ecb\u7ecd PyTorch \u4e4b\u524d, \u6211\u4eec\u5148\u4f7f\u7528 numpy \u5b9e\u73b0\u7f51\u7edc. Numpy \u63d0\u4f9b\u4e86\u4e00\u4e2an\u7ef4\u7684\u6570\u7ec4\u5bf9\u8c61, \u5e76\u63d0\u4f9b\u4e86\u8bb8\u591a\u64cd\u7eb5\u8fd9\u4e2a\u6570\u7ec4\u5bf9\u8c61\u7684\u51fd\u6570. Numpy \u662f\u79d1\u5b66\u8ba1\u7b97\u7684\u901a\u7528\u6846\u67b6; Numpy \u6570\u7ec4\u6ca1\u6709\u8ba1\u7b97\u56fe, \u4e5f\u6ca1\u6709\u6df1\u5ea6\u5b66\u4e60, \u4e5f\u6ca1\u6709\u68af\u5ea6\u4e0b\u964d\u7b49\u65b9\u6cd5\u5b9e\u73b0\u7684\u63a5\u53e3. \u4f46\u662f\u6211\u4eec\u4ecd\u7136\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u4f7f\u7528 numpy \u751f\u6210\u968f\u673a\u6570\u636e \u5e76\u5c06\u4ea7\u751f\u7684\u6570\u636e\u4f20\u5165\u53cc\u5c42\u7684\u795e\u7ecf\u7f51\u7edc, \u5e76\u4f7f\u7528 numpy \u6765\u5b9e\u73b0\u8fd9\u4e2a\u7f51\u7edc\u7684\u6b63\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad: # -*- coding: utf-8 -*- import numpy as np # N \u662f\u4e00\u4e2abatch\u7684\u6837\u672c\u6570\u91cf; D_in\u662f\u8f93\u5165\u7ef4\u5ea6; # H \u662f\u9690\u85cf\u5c42\u5411\u91cf\u7684\u7ef4\u5ea6; D_out\u662f\u8f93\u51fa\u7ef4\u5ea6. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u7684\u8f93\u5165\u8f93\u51fa\u6570\u636e x = np.random.randn(N, D_in) y = np.random.randn(N, D_out) # \u968f\u673a\u521d\u59cb\u5316\u6743\u91cd\u53c2\u6570 w1 = np.random.randn(D_in, H) w2 = np.random.randn(H, D_out) learning_rate = 1e-6 for t in range(500): # \u524d\u5411\u8ba1\u7b97, \u7b97\u51fay\u7684\u9884\u6d4b\u503c h = x.dot(w1) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # \u8ba1\u7b97\u5e76\u6253\u5370\u8bef\u5dee\u503c loss = np.square(y_pred - y).sum() print(t, loss) # \u5728\u53cd\u5411\u4f20\u64ad\u4e2d, \u8ba1\u7b97\u51fa\u8bef\u5dee\u5173\u4e8ew1\u548cw2\u7684\u5bfc\u6570 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h < 0] = 0 grad_w1 = x.T.dot(grad_h) # \u66f4\u65b0\u6743\u91cd w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2","title":"Warm-up: numpy"},{"location":"12/#pytorch-tensors","text":"Numpy \u662f\u4e00\u4e2a\u4f1f\u5927\u7684\u6846\u67b6, \u4f46\u5b83\u4e0d\u80fd\u5229\u7528 GPU \u52a0\u901f\u5b83\u6570\u503c\u8ba1\u7b97. \u5bf9\u4e8e\u73b0\u4ee3\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc, GPU \u5f80\u5f80\u662f\u63d0\u4f9b 50\u500d\u6216\u66f4\u5927\u7684\u52a0\u901f , \u6240\u4ee5\u4e0d\u5e78\u7684\u662f, numpy \u4e0d\u8db3\u4ee5\u6ee1\u8db3\u73b0\u5728\u6df1\u5ea6\u5b66\u4e60\u7684\u9700\u6c42. \u8fd9\u91cc\u6211\u4eec\u4ecb\u7ecd\u4e00\u4e0b\u6700\u57fa\u672c\u7684 PyTorch \u6982\u5ff5: Tensor . PyTorch Tensor \u5728\u6982\u5ff5\u4e0a\u4e0e numpy \u6570\u7ec4\u76f8\u540c: Tensor \u662f\u4e00\u4e2an\u7ef4\u6570\u7ec4, PyTorch \u4e5f\u63d0\u4f9b\u4e86\u5f88\u591a\u80fd\u5728\u8fd9\u4e9b Tensor \u4e0a\u64cd\u4f5c\u7684\u51fd\u6570. \u50cf numpy \u6570\u7ec4\u4e00\u6837, PyTorch Tensor \u4e5f\u548cnumpy\u7684\u6570\u7ec4\u5bf9\u8c61\u4e00\u6837\u4e0d\u4e86\u89e3\u6df1\u5ea6\u5b66\u4e60,\u8ba1\u7b97\u56fe\u548c\u68af\u5ea6\u4e0b\u964d\uff1b\u5b83\u4eec\u53ea\u662f\u79d1\u5b66\u8ba1\u7b97\u7684\u901a\u7528\u5de5\u5177. \u7136\u800c\u4e0d\u50cf numpy, PyTorch Tensor \u53ef\u4ee5\u5229\u7528 GPU \u52a0\u901f\u4ed6\u4eec\u7684\u6570\u5b57\u8ba1\u7b97. \u8981\u5728 GPU \u4e0a\u8fd0\u884c PyTorch \u5f20\u91cf, \u53ea\u9700\u5c06\u5176\u8f6c\u6362\u4e3a\u65b0\u7684\u6570\u636e\u7c7b\u578b. \u5728\u8fd9\u91cc, \u6211\u4eec\u5c06 PyTorch Tensor \u751f\u6210\u7684\u968f\u673a\u6570\u636e\u4f20\u5165\u53cc\u5c42\u7684\u795e\u7ecf\u7f51\u7edc. \u5c31\u50cf\u4e0a\u9762\u7684 numpy \u4f8b\u5b50\u4e00\u6837, \u6211\u4eec\u9700\u8981\u624b\u52a8\u5b9e\u73b0\u7f51\u7edc\u7684\u6b63\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad: # -*- coding: utf-8 -*- import torch dtype = torch.FloatTensor # dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u8f93\u5165\u548c\u8f93\u51fa\u6570\u636e x = torch.randn(N, D_in).type(dtype) y = torch.randn(N, D_out).type(dtype) # \u968f\u673a\u521d\u59cb\u5316\u6743\u91cd w1 = torch.randn(D_in, H).type(dtype) w2 = torch.randn(H, D_out).type(dtype) learning_rate = 1e-6 for t in range(500): # \u6b63\u5411\u4f20\u9012\uff1a\u8ba1\u7b97\u9884\u6d4by h = x.mm(w1) h_relu = h.clamp(min=0) y_pred = h_relu.mm(w2) # \u8ba1\u7b97\u5e76\u6253\u5370loss loss = (y_pred - y).pow(2).sum() print(t, loss) # \u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u5173\u4e8e\u635f\u5931\u7684w1\u548cw2\u7684\u68af\u5ea6 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.t().mm(grad_y_pred) grad_h_relu = grad_y_pred.mm(w2.t()) grad_h = grad_h_relu.clone() grad_h[h < 0] = 0 grad_w1 = x.t().mm(grad_h) # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2","title":"PyTorch: Tensors"},{"location":"12/#autograd","text":"","title":"Autograd"},{"location":"12/#pytorch-variables-and-autograd","text":"\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d, \u6211\u4eec\u4e0d\u5f97\u4e0d\u624b\u5199\u5b9e\u73b0\u795e\u7ecf\u7f51\u7edc\u7684\u6b63\u53cd\u5411\u4f20\u64ad\u7684\u4ee3\u7801. \u800c\u624b\u5199\u5b9e\u73b0\u53cd\u5411\u4f20\u64ad\u7684\u4ee3\u7801\u5bf9\u4e8e\u4e00\u4e2a \u5c0f\u578b\u7684\u53cc\u5c42\u7f51\u7edc\u6765\u8bf4\u662f\u6ca1\u4ec0\u4e48\u5927\u95ee\u9898\u7684, \u4f46\u662f\u5728\u9762\u5bf9\u5927\u578b\u590d\u6742\u7f51\u7edc\u624b\u5199\u65b9\u5411\u4f20\u64ad\u4ee3\u7801\u5c31\u4f1a\u53d8\u5f97\u5f88\u68d8\u624b. \u8c22\u5929\u8c22\u5730, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 \u81ea\u52a8\u5fae\u5206 \u6765\u81ea\u52a8\u5316\u7684\u8ba1\u7b97\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u540e\u5411\u4f20\u64ad. PyTorch \u4e2d\u7684 autograd \u5305\u63d0\u4f9b\u81ea\u52a8\u5fae\u5206\u4e86\u8fd9\u4e2a\u529f\u80fd. \u4f7f\u7528 autograd \u65f6, \u7f51\u7edc\u7684\u6b63\u5411\u4f20\u64ad\u5c06\u5b9a\u4e49\u4e00\u4e2a \u8ba1\u7b97\u56fe ; Tensor \u5c06\u4f1a\u6210\u4e3a\u56fe\u4e2d\u7684\u8282\u70b9,\u4ece\u8f93\u5165 Tensor \u4ea7\u751f\u8f93\u51fa Tensor \u7684\u51fd\u6570\u5c06\u4f1a\u7528\u56fe\u4e2d\u7684( Edge )\u4f9d\u8d56\u8fb9\u8868\u793a. \u901a\u8fc7\u8ba1\u7b97\u56fe\u6765\u53cd\u5411\u4f20\u64ad\u53ef\u4ee5\u8ba9\u60a8\u8f7b\u677e\u8ba1\u7b97\u68af\u5ea6. \u8fd9\u542c\u8d77\u6765\u5f88\u590d\u6742, \u4f46\u662f\u5728\u5b9e\u8df5\u4e2d\u4f7f\u7528\u8d77\u6765\u76f8\u5f53\u7b80\u5355. \u6211\u4eec\u5c06 PyTorch \u7684 Tensor \u5305\u88c5\u6210\u5728 Variable \u5bf9\u8c61\uff1b \u4e00\u4e2a Variable \u4ee3\u8868\u4e00\u4e2a\u8ba1\u7b97\u56fe\u4e2d\u7684\u8282\u70b9. \u5982\u679c x \u662f\u4e00\u4e2a Variable , \u5219 x.data \u662f\u4e00\u4e2a Tensor , \u800c x.grad \u662f\u53e6\u5916\u4e00\u4e2a\u5305\u542b\u5173\u4e8e x \u7684\u68af\u5ea6\u7684 Variable . PyTorch Variable \u4e0e PyTorch Tensor \u5177\u6709\u76f8\u540c\u7684 API: (\u51e0\u4e4e) \u4efb\u4f55\u60a8\u53ef\u4ee5\u5728 Tensor \u4e0a\u6267\u884c\u7684 \u64cd\u4f5c\u4e5f\u9002\u7528\u4e8e Variable \uff1b\u8be5\u533a\u522b\u5728\u4e8e\u5982\u679c\u4f60\u4f7f\u7528 Variable \u5b9a\u4e49\u4e86\u4e00\u4e2a\u8ba1\u7b97\u56fe, Pytorch \u5141\u8bb8\u60a8\u81ea\u52a8\u8ba1\u7b97\u68af\u5ea6. \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 PyTorch \u7684 Variable \u548c\u81ea\u52a8\u5fae\u5206\u6765\u5b9e\u73b0\u6211\u4eec\u7684\u53cc\u5c42\u7f51\u7edc\uff1b\u73b0\u5728\u6211\u4eec\u4e0d\u518d\u9700\u8981\u624b\u5199\u4efb\u4f55\u5173\u4e8e \u8ba1\u7b97\u7f51\u7edc\u53cd\u5411\u4f20\u64ad\u7684\u4ee3\u7801: # -*- coding: utf-8 -*- import torch from torch.autograd import Variable dtype = torch.FloatTensor # dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. # \u8bbe\u7f6erequires_grad = False, \u56e0\u4e3a\u5728\u540e\u5411\u4f20\u64ad\u65f6, \u6211\u4eec\u5e76\u4e0d\u9700\u8981\u8ba1\u7b97\u5173\u4e8e\u8fd9\u4e9b\u53d8\u91cf\u7684\u68af\u5ea6 x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False) y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False) # \u4e3a\u6743\u91cd\u521b\u5efa\u968f\u673a\u5f20\u91cf,\u5e76\u5c06\u5176\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. # \u8bbe\u7f6erequires_grad = True, \u56e0\u4e3a\u5728\u540e\u5411\u4f20\u64ad\u65f6, \u6211\u4eec\u9700\u8981\u8ba1\u7b97\u5173\u4e8e\u8fd9\u4e9b\u53d8\u91cf\u7684\u68af\u5ea6 w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True) w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True) learning_rate = 1e-6 for t in range(500): # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y; \u8fd9\u4e9b # \u4e0e\u6211\u4eec\u7528\u4e8e\u8ba1\u7b97\u4f7f\u7528\u5f20\u91cf\u7684\u6b63\u5411\u4f20\u9012\u5b8c\u5168\u76f8\u540c, # \u4f46\u6211\u4eec\u4e0d\u9700\u8981\u4fdd\u7559\u5bf9\u4e2d\u95f4\u503c\u7684\u5f15\u7528, # \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5b9e\u73b0\u5411\u540e\u4f20\u9012. y_pred = x.mm(w1).clamp(min=0).mm(w2) # \u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u64cd\u4f5c\u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931. # \u73b0\u5728\u635f\u5931\u662f\u5f62\u72b6\u53d8\u91cf (1,) \u5e76\u4e14 loss.data \u662f\u5f62\u72b6\u7684\u5f20\u91cf # (1,); loss.data[0] \u662f\u6301\u6709\u635f\u5931\u7684\u6807\u91cf\u503c. loss = (y_pred - y).pow(2).sum() print(t, loss.data[0]) # \u4f7f\u7528autograd\u6765\u8ba1\u7b97\u53cd\u5411\u4f20\u9012. # \u8be5\u8c03\u7528\u5c06\u4f7f\u7528requires_grad = True\u6765\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u6240\u6709\u53d8\u91cf\u7684\u635f\u5931\u68af\u5ea6. # \u5728\u8fd9\u6b21\u8c03\u7528\u4e4b\u540e w1.grad \u548c w2.grad \u5c06\u662f\u53d8\u91cf # \u5b83\u4eec\u5206\u522b\u76f8\u5bf9\u4e8ew1\u548cw2\u4fdd\u5b58\u635f\u5931\u7684\u68af\u5ea6. loss.backward() # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd; w1.data \u548c w2.data \u662f\u5f20\u91cf, # w1.grad \u548c w2.grad \u662f\u53d8\u91cf\u5e76\u4e14 w1.grad.data \u548c w2.grad.data # \u662f\u5f20\u91cf. w1.data -= learning_rate * w1.grad.data w2.data -= learning_rate * w2.grad.data # \u66f4\u65b0\u6743\u91cd\u540e\u624b\u52a8\u5c06\u68af\u5ea6\u5f52\u96f6 w1.grad.data.zero_() w2.grad.data.zero_()","title":"PyTorch: Variables and autograd"},{"location":"12/#pytorch-defining-new-autograd-functions","text":"\u5728\u8fd9\u5c42\u8986\u76d6\u4e0b, \u6bcf\u4e2a\u539f\u59cb\u7684 autograd \u64cd\u4f5c\u7b26\u5b9e\u9645\u4e0a\u662f\u4e24\u4e2a\u51fd\u6570\u5728\u5f20\u91cf\u4e0a\u8fd0\u884c. \u524d\u5411\u4f20\u64ad \u51fd\u6570\u4ece\u8f93\u5165\u7684 Tensor \u8ba1\u7b97\u5c06\u8981\u8f93\u51fa\u7684 Tensor . \u540e\u5411\u4f20\u64ad \u51fd\u6570\u63a5\u6536\u4e0a\u4e00\u4e2a Tensor \u5173\u4e8e scalar \u7684\u68af\u5ea6, \u4ee5 \u53ca\u8ba1\u7b97\u5f53\u524d\u8f93\u5165 Tensor \u5bf9\u76f8\u540c scalar \u503c\u7684\u68af\u5ea6. \u5728 PyTorch \u4e2d, \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b9a\u4e49\u4e00\u4e2a torch.autograd.Function \u7684\u5b50\u7c7b\u548c \u5b9e\u73b0 \u524d\u5411\u4f20\u64ad \u548c \u540e\u5411\u4f20\u64ad \u51fd\u6570\u6765\u8f7b\u677e\u5b9a\u4e49\u81ea\u5df1\u7684 autograd \u64cd\u4f5c\u7b26. \u7136\u540e\u6211\u4eec\u53ef\u4ee5 \u4f7f\u7528\u6211\u4eec\u65b0\u7684 autograd \u64cd\u4f5c\u7b26\u6784\u9020\u4e00\u4e2a\u5b9e\u4f8b\u5e76\u5c06\u5176\u4f5c\u4e3a\u4e00\u4e2a\u51fd\u6570\u8c03\u7528, \u4f20\u9012\u7528 Variable \u5305\u88c5\u4e86\u7684\u8f93\u5165\u6570\u636e\u7684. \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\u6211\u4eec\u5b9a\u4e49\u4e86\u6211\u4eec\u81ea\u5df1\u7684 autograd \u51fd\u6570\u6765\u6267\u884c ReLU \u975e\u7ebf\u6027\u51fd\u6570, \u5e76\u7528\u5b83\u6765\u5b9e\u73b0\u6211\u4eec\u7684\u53cc\u5c42\u7f51\u7edc: # -*- coding: utf-8 -*- import torch from torch.autograd import Variable class MyReLU(torch.autograd.Function): \"\"\" \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b50\u7c7b\u5b9e\u73b0\u6211\u4eec\u81ea\u5df1\u5b9a\u5236\u7684autograd\u51fd\u6570 torch.autograd.Function\u548c\u6267\u884c\u5728Tensors\u4e0a\u8fd0\u884c\u7684\u5411\u524d\u548c\u5411\u540e\u901a\u884c\u8bc1. \"\"\" @staticmethod def forward(ctx, input): \"\"\" \u5728\u6b63\u5411\u4f20\u9012\u4e2d,\u6211\u4eec\u6536\u5230\u4e00\u4e2a\u5305\u542b\u8f93\u5165\u548c\u8fd4\u56de\u5f20\u91cf\u7684\u5f20\u91cf,\u5176\u4e2d\u5305\u542b\u8f93\u51fa. ctx\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u5bf9\u8c61,\u53ef\u7528\u4e8e\u5b58\u50a8\u53cd\u5411\u8ba1\u7b97\u7684\u4fe1\u606f. \u60a8\u53ef\u4ee5\u4f7f\u7528ctx.save_for_backward\u65b9\u6cd5\u7f13\u5b58\u4efb\u610f\u5bf9\u8c61\u4ee5\u7528\u4e8e\u540e\u5411\u4f20\u9012. \"\"\" ctx.save_for_backward(input) return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): \"\"\" \u5728\u540e\u5411\u4f20\u9012\u4e2d,\u6211\u4eec\u6536\u5230\u4e00\u4e2a\u5f20\u91cf,\u5176\u4e2d\u5305\u542b\u76f8\u5bf9\u4e8e\u8f93\u51fa\u7684\u635f\u5931\u68af\u5ea6, \u6211\u4eec\u9700\u8981\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u8f93\u5165\u7684\u635f\u5931\u68af\u5ea6. \"\"\" input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input < 0] = 0 return grad_input dtype = torch.FloatTensor # dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False) y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False) # \u4e3a\u6743\u91cd\u521b\u5efa\u968f\u673a\u5f20\u91cf,\u5e76\u5c06\u5176\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True) w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True) learning_rate = 1e-6 for t in range(500): # \u4e3a\u4e86\u5e94\u7528\u6211\u4eec\u7684\u51fd\u6570,\u6211\u4eec\u4f7f\u7528Function.apply\u65b9\u6cd5.\u6211\u4eec\u628a\u5b83\u79f0\u4e3a'relu'. relu = MyReLU.apply # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y; # \u6211\u4eec\u4f7f\u7528\u6211\u4eec\u7684\u81ea\u5b9a\u4e49autograd\u64cd\u4f5c\u6765\u8ba1\u7b97ReLU. y_pred = relu(x.mm(w1)).mm(w2) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931 loss = (y_pred - y).pow(2).sum() print(t, loss.data[0]) # \u4f7f\u7528autograd\u6765\u8ba1\u7b97\u53cd\u5411\u4f20\u9012. loss.backward() # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd w1.data -= learning_rate * w1.grad.data w2.data -= learning_rate * w2.grad.data # \u66f4\u65b0\u6743\u91cd\u540e\u624b\u52a8\u5c06\u68af\u5ea6\u5f52\u96f6 w1.grad.data.zero_() w2.grad.data.zero_()","title":"PyTorch: Defining new autograd functions"},{"location":"12/#tensorflow-static-graphs","text":"Pytorch \u7684 autograd \u770b\u4e0a\u53bb\u6709\u70b9\u50cf TensorFlow .\u4e24\u4e2a\u6846\u67b6\u7684\u5171\u540c\u70b9\u662f\u4ed6\u4eec\u90fd\u662f\u5b9a\u4e49\u4e86\u81ea\u5df1\u7684\u8ba1\u7b97\u56fe. \u548c\u4f7f\u7528\u81ea\u52a8\u6c42\u5fae\u5206\u7684\u65b9\u6cd5\u6765\u8ba1\u7b97\u68af\u5ea6. \u4e24\u8005\u4e4b\u95f4\u6700\u5927\u7684\u4e0d\u540c\u5728\u4e8e TensorFlow \u7684\u8ba1\u7b97\u56fe\u662f \u9759\u6001\u7684 \u548c PyTorch \u7684\u8ba1\u7b97\u56fe\u662f \u52a8\u6001\u7684 . \u5728 TensorFlow \u4e2d, \u6211\u4eec\u53ea\u5b9a\u4e49\u4e86\u4e00\u6b21\u8ba1\u7b97\u56fe,\u7136\u540e\u91cd\u590d\u6267\u884c\u540c\u4e00\u5f20\u8ba1\u7b97\u56fe, \u53ea\u662f\u8f93\u5165\u8ba1\u7b97\u56fe\u7684\u6570\u636e\u4e0d\u540c\u800c\u5df2. \u800c\u5728 PyTorch \u4e2d, \u6bcf\u4e2a\u6b63\u5411\u4f20\u64ad\u90fd\u4f1a\u5b9a\u4e49\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u56fe. \u9759\u6001\u56fe\u5f88\u597d, \u56e0\u4e3a\u60a8\u53ef\u4ee5\u9884\u5148\u4f18\u5316\u8ba1\u7b97\u56fe\uff1b\u4f8b\u5982\u4e00\u4e2a\u6846\u67b6\u53ef\u80fd\u4f1a\u4e3a\u4e86\u8ba1\u7b97\u6548\u7387\u51b3\u5b9a\u878d\u5408\u4e00\u4e9b\u8ba1\u7b97\u56fe\u64cd\u4f5c(\u50cf:Fused Graph), \u6216\u63d0\u51fa \u4e00\u4e2a\u591a\u5361\u6216\u8005\u591a\u673a\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u56fe\u7684\u7b56\u7565. \u5982\u679c\u60a8\u6b63\u5728\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u7684\u8ba1\u7b97\u56fe, \u90a3\u4e48\u8fd9\u4e2a\u6f5c\u5728\u7684 \u6602\u8d35\u7684\u524d\u671f\u4f18\u5316\u53ef\u4ee5\u4f7f\u7528\u9759\u6001\u56fe\u6765\u5f97\u4ee5\u51cf\u8f7b. \u4e00\u65b9\u9762\u6765\u8bf4, \u9759\u6001\u56fe\u548c\u52a8\u6001\u56fe\u7684\u63a7\u5236\u6d41\u662f\u4e0d\u540c\u7684. \u5bf9\u4e8e\u6709\u4e9b\u6a21\u578b\u6211\u4eec\u53ef\u80fd\u5e0c\u671b\u5bf9\u6bcf\u4e2a\u6570\u636e\u70b9\u6267\u884c\u4e0d\u540c \u7684\u8ba1\u7b97\uff1b\u4f8b\u5982\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u53ef\u80fd\u4f1a\u88ab\u5c55\u5f00\u4e3a\u5bf9\u6bcf\u4e2a\u6570\u636e\u7684\u4e0d\u540c\u7684\u957f\u5ea6\u7684\u65f6\u95f4\u6b65\u6570\uff1b\u8fd9\u4e2a\u5c55\u5f00\u53ef\u4ee5\u7528\u5faa \u73af\u6765\u5b9e\u73b0. \u5faa\u73af\u7ed3\u6784\u7684\u9759\u6001\u56fe\u9700\u8981\u6210\u4e3a\u8ba1\u7b97\u56fe\u7684\u4e00\u90e8\u5206\uff1b\u4e3a\u6b64 TensorFlow \u63d0\u4f9b tf.scan \u64cd\u4f5c\u7b26 \u7528\u4e8e\u5c06\u91cd\u590d\u7684\u7ed3\u6784\u5d4c\u5165\u5230\u8ba1\u7b97\u56fe\u4e2d. \u800c\u52a8\u6001\u8ba1\u7b97\u56fe\u7684\u60c5\u51b5\u6bd4\u8f83\u7b80\u5355: \u56e0\u4e3a\u6211\u4eec\u8bbe\u8ba1\u7684\u8ba1\u7b97\u56fe\u53ef\u4ee5\u5bf9\u6bcf\u4e2a\u4e0d\u540c\u957f\u5ea6\u7684\u8f93\u5165\u968f\u673a\u5e94\u53d8. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6b63\u5e38\u7684\u547d\u4ee4\u5f0f\u4ee3\u7801\u5bf9\u6bcf\u4e2a\u4e0d\u540c\u957f\u5ea6\u7684\u8f93\u5165\u6267\u884c\u8ba1\u7b97. \u4e3a\u4e86\u4e0e\u4e0a\u9762\u7684 PyTorch autograd \u4f8b\u5b50\u8fdb\u884c\u5bf9\u6bd4, \u6211\u4eec\u5728\u8fd9\u91cc\u4e5f\u4f7f\u7528 TensorFlow \u521b\u5efa\u7b80\u5355\u7684\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc: # -*- coding: utf-8 -*- import tensorflow as tf import numpy as np # \u9996\u5148\u6211\u4eec\u8bbe\u7f6e\u8ba1\u7b97\u56fe: # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u4e3a\u8f93\u5165\u6570\u636e\u548c\u76ee\u6807\u6570\u636e\u521b\u5efa\u5360\u4f4d\u7b26; # \u5f53\u6211\u4eec\u6267\u884c\u56fe\u65f6,\u8fd9\u4e9b\u5c06\u88ab\u586b\u5145\u771f\u5b9e\u7684\u6570\u636e. x = tf.placeholder(tf.float32, shape=(None, D_in)) y = tf.placeholder(tf.float32, shape=(None, D_out)) # \u4e3a\u6743\u91cd\u521b\u5efa\u53d8\u91cf\u5e76\u7528\u968f\u673a\u6570\u636e\u521d\u59cb\u5316\u5b83\u4eec. # \u4e00\u4e2aTensorFlow\u53d8\u91cf\u5728\u56fe\u7684\u6267\u884c\u4e2d\u4fdd\u6301\u5176\u503c. w1 = tf.Variable(tf.random_normal((D_in, H))) w2 = tf.Variable(tf.random_normal((H, D_out))) # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528TensorFlow Tensors\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y. # \u8bf7\u6ce8\u610f\u6b64\u4ee3\u7801\u5b9e\u9645\u4e0a\u5e76\u672a\u6267\u884c\u4efb\u4f55\u6570\u5b57\u64cd\u4f5c; # \u5b83\u53ea\u662f\u8bbe\u7f6e\u6211\u4eec\u7a0d\u540e\u5c06\u6267\u884c\u7684\u8ba1\u7b97\u56fe. h = tf.matmul(x, w1) h_relu = tf.maximum(h, tf.zeros(1)) y_pred = tf.matmul(h_relu, w2) # \u4f7f\u7528TensorFlow\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u8ba1\u7b97\u635f\u5931 loss = tf.reduce_sum((y - y_pred) ** 2.0) # \u8ba1\u7b97\u76f8\u5bf9\u4e8ew1\u548cw2\u7684\u635f\u5931\u68af\u5ea6. grad_w1, grad_w2 = tf.gradients(loss, [w1, w2]) # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd. # \u8981\u5b9e\u9645\u66f4\u65b0\u6743\u91cd,\u6211\u4eec\u9700\u8981\u5728\u6267\u884c\u56fe\u65f6\u8bc4\u4f30new_w1\u548cnew_w2. # \u8bf7\u6ce8\u610f,\u5728TensorFlow\u4e2d,\u66f4\u65b0\u6743\u503c\u7684\u884c\u4e3a\u662f\u8ba1\u7b97\u56fe\u7684\u4e00\u90e8\u5206 # \u5728PyTorch\u4e2d,\u8fd9\u53d1\u751f\u5728\u8ba1\u7b97\u56fe\u4e4b\u5916. learning_rate = 1e-6 new_w1 = w1.assign(w1 - learning_rate * grad_w1) new_w2 = w2.assign(w2 - learning_rate * grad_w2) # \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u6784\u5efa\u4e86\u8ba1\u7b97\u56fe,\u6240\u4ee5\u6211\u4eec\u8f93\u5165\u4e00\u4e2aTensorFlow\u4f1a\u8bdd\u6765\u5b9e\u9645\u6267\u884c\u56fe. with tf.Session() as sess: # \u8fd0\u884c\u4e00\u6b21\u56fe\u5f62\u521d\u59cb\u5316\u53d8\u91cfw1\u548cw2. sess.run(tf.global_variables_initializer()) # \u521b\u5efa\u5305\u542b\u8f93\u5165x\u548c\u76ee\u6807y\u7684\u5b9e\u9645\u6570\u636e\u7684numpy\u6570\u7ec4 x_value = np.random.randn(N, D_in) y_value = np.random.randn(N, D_out) for _ in range(500): # \u591a\u6b21\u6267\u884c\u56fe. \u6bcf\u6b21\u6267\u884c\u65f6, # \u6211\u4eec\u90fd\u60f3\u5c06x_value\u7ed1\u5b9a\u5230x,\u5c06y_value\u7ed1\u5b9a\u5230y,\u7528feed_dict\u53c2\u6570\u6307\u5b9a. # \u6bcf\u6b21\u6211\u4eec\u6267\u884c\u56fe\u65f6,\u6211\u4eec\u90fd\u60f3\u8ba1\u7b97\u635f\u5931\u503cnew_w1 \u548c new_w2; # \u8fd9\u4e9b\u5f20\u91cf\u7684\u503c\u4f5c\u4e3anumpy\u6570\u7ec4\u8fd4\u56de. loss_value, _, _ = sess.run([loss, new_w1, new_w2], feed_dict={x: x_value, y: y_value}) print(loss_value)","title":"TensorFlow: Static Graphs"},{"location":"12/#citenncite-module","text":"","title":"&lt;cite&gt;nn&lt;/cite&gt; module"},{"location":"12/#pytorch-nn","text":"\u8ba1\u7b97\u56fe( Computational graphs )\u548c autograd \u662f\u4e00\u4e2a\u975e\u5e38\u5f3a\u5927\u7684\u5b9a\u4e49\u590d\u6742\u7684\u8fd0\u7b97\u7b26\u5e76\u81ea\u52a8\u5730\u5bfc\u51fa\u7684\u8303\u5f0f\uff1b\u7136\u800c\u5bf9\u4e8e \u5927\u578b\u7684\u795e\u7ecf\u7f51\u7edc, \u539f\u59cb\u7684 autograd \u4ecd\u7136\u663e\u5f97\u6709\u70b9\u592a\u4f4e\u7ea7. \u5f53\u6211\u4eec\u521b\u5efa\u795e\u7ecf\u7f51\u7edc\u65f6, \u6211\u4eec\u7ecf\u5e38\u601d\u8003\u5982\u4f55\u8bbe\u8ba1\u5b89\u6392 layer , \u4ee5\u53ca\u4e00\u4e9b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7f51\u7edc\u4f1a\u5b66\u4e60\u5230\u7684 learnable parameters \u5728TensorFlow\u4e2d, \u50cf Keras , TensorFlow-Slim , \u548c TFLearn \u901a\u8fc7\u6784\u5efa\u5bf9\u795e\u7ecf\u7f51\u7edc\u6709\u7528\u7684\u539f\u59cb\u8ba1\u7b97\u56fe\u63d0\u4f9b\u66f4\u9ad8\u5c42\u6b21\u7684\u62bd\u8c61. \u5728 PyTorch \u4e2d, nn \u5305\u8d77\u4e86\u540c\u6837\u7684\u4f5c\u7528. nn \u5305\u5b9a\u4e49\u4e86\u4e00\u7ec4 Modules , \u5927\u81f4\u76f8\u5f53\u4e8e\u795e\u7ecf\u7f51\u7edc\u5c42. \u6a21\u5757\u63a5\u6536\u8f93\u5165\u53d8\u91cf\u5e76\u8fdb\u884c\u8ba1\u7b97\u8f93\u51fa\u53d8\u91cf, \u4f46\u4e5f\u53ef\u4ee5\u4fdd\u6301\u5185\u90e8\u72b6\u6001, \u5982 \u7528 Variable \u5305\u88c5\u7684 learnable parameters . nn \u5305 \u4e5f\u5b9a\u4e49\u4e86\u4e00\u7cfb\u5217\u5728\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u65f6\u6bd4\u8f83\u5e38\u7528\u7684\u635f\u5931\u51fd\u6570. \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d, \u6211\u4eec\u4f7f\u7528 nn \u5305\u6765\u5b9e\u73b0\u6211\u4eec\u7684\u53cc\u5c42\u795e\u7ecf\u7f51\u7edc: # -*- coding: utf-8 -*- import torch from torch.autograd import Variable # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u4f7f\u7528nn\u5305\u5c06\u6211\u4eec\u7684\u6a21\u578b\u5b9a\u4e49\u4e3a\u4e00\u7cfb\u5217\u56fe\u5c42. # nn.Sequential\u662f\u4e00\u4e2a\u5305\u542b\u5176\u4ed6\u6a21\u5757\u7684\u6a21\u5757,\u5e76\u5c06\u5b83\u4eec\u6309\u987a\u5e8f\u5e94\u7528\u4ee5\u4ea7\u751f\u5176\u8f93\u51fa. # \u6bcf\u4e2a\u7ebf\u6027\u6a21\u5757\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u8ba1\u7b97\u6765\u81ea\u8f93\u5165\u7684\u8f93\u51fa,\u5e76\u4fdd\u5b58\u5185\u90e8\u53d8\u91cf\u7684\u6743\u91cd\u548c\u504f\u5dee. model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) # nn\u5305\u8fd8\u5305\u542b\u6d41\u884c\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49; # \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u6211\u4eec\u5c06\u4f7f\u7528\u5747\u65b9\u5dee(MSE)\u4f5c\u4e3a\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570. loss_fn = torch.nn.MSELoss(size_average=False) learning_rate = 1e-4 for t in range(500): # \u6b63\u5411\u4f20\u9012:\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y. # \u6a21\u5757\u5bf9\u8c61\u4f1a\u8986\u76d6__call__\u8fd0\u7b97\u7b26,\u56e0\u6b64\u60a8\u53ef\u4ee5\u5c06\u5b83\u4eec\u79f0\u4e3a\u51fd\u6570. # \u8fd9\u6837\u505a\u65f6,\u60a8\u5c06\u8f93\u5165\u6570\u636e\u7684\u53d8\u91cf\u4f20\u9012\u7ed9\u6a21\u5757,\u5e76\u751f\u6210\u8f93\u51fa\u6570\u636e\u7684\u53d8\u91cf. y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931. # \u6211\u4eec\u4f20\u9012\u5305\u542by\u7684\u9884\u6d4b\u503c\u548c\u771f\u503c\u7684\u53d8\u91cf,\u5e76\u4e14\u635f\u5931\u51fd\u6570\u8fd4\u56de\u5305\u542b\u635f\u5931\u7684\u53d8\u91cf. loss = loss_fn(y_pred, y) print(t, loss.data[0]) # \u5728\u8fd0\u884c\u53cd\u5411\u4f20\u9012\u4e4b\u524d\u5c06\u68af\u5ea6\u5f52\u96f6. model.zero_grad() # \u5411\u540e\u4f20\u9012:\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u6a21\u578b\u7684\u6240\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u635f\u5931\u68af\u5ea6. # \u5728\u5185\u90e8,\u6bcf\u4e2a\u6a21\u5757\u7684\u53c2\u6570\u90fd\u5b58\u50a8\u5728\u53d8\u91cfrequire_grad = True\u4e2d, # \u56e0\u6b64\u8be5\u8c03\u7528\u5c06\u8ba1\u7b97\u6a21\u578b\u4e2d\u6240\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u68af\u5ea6. loss.backward() # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd. # \u6bcf\u4e2a\u53c2\u6570\u90fd\u662f\u4e00\u4e2a\u53d8\u91cf,\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u50cf\u6211\u4eec\u4ee5\u524d\u90a3\u6837\u8bbf\u95ee\u5b83\u7684\u6570\u636e\u548c\u68af\u5ea6. for param in model.parameters(): param.data -= learning_rate * param.grad.data","title":"PyTorch: nn"},{"location":"12/#pytorch-optim","text":"\u5230\u76ee\u524d\u4e3a\u6b62, \u6211\u4eec\u4e00\u76f4\u901a\u8fc7\u624b\u52a8\u66f4\u65b0\u7684\u65b9\u6cd5\u66f4\u65b0\u6a21\u578b\u7684\u53ef\u5b66\u4e60\u53c2\u6570( learnable parameters )\u7684\u6743\u91cd .data \u8fd9\u5bf9\u4e8e\u7b80\u5355\u7684\u4f18\u5316\u7b97\u6cd5\u50cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6765\u8fd8\u7b97\u8f7b\u677e, \u4f46\u662f\u5728\u5b9e\u9645\u4e2d\u6211\u4eec\u7ecf\u5e38\u4f7f\u7528\u66f4\u5de7\u5999\u7684 \u4f18\u5316\u5668\u6765\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc, \u5982 AdaGrad, RMSProp, Adam \u7b49. PyTorch \u4e2d\u7684 optim \u5305\u5305\u542b\u4e86\u4e00\u4e9b\u4f18\u5316\u5668\u7684\u7b97\u6cd5, \u5e76\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5e38\u7528\u4f18\u5316\u5668\u7684\u4f7f\u7528. \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d, \u867d\u7136\u6211\u4eec\u5c06\u50cf\u4e4b\u524d\u4e00\u6837\u4f7f\u7528 nn \u5305\u6765\u5b9a\u4e49\u6211\u4eec\u7684\u6a21\u578b, \u4f46\u662f\u6211\u4eec\u8fd9\u6b21\u5c06\u4f7f\u7528\u7531 optim \u5305\u63d0\u4f9b\u7684Adam\u7b97\u6cd5\u6765\u66f4\u65b0\u6a21\u578b: # -*- coding: utf-8 -*- import torch from torch.autograd import Variable # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u4f7f\u7528nn\u5305\u6765\u5b9a\u4e49\u6211\u4eec\u7684\u6a21\u578b\u548c\u635f\u5931\u51fd\u6570. model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) loss_fn = torch.nn.MSELoss(size_average=False) # \u4f7f\u7528\u4f18\u5316\u5305\u6765\u5b9a\u4e49\u4e00\u4e2a\u4f18\u5316\u5668,\u5b83\u5c06\u4e3a\u6211\u4eec\u66f4\u65b0\u6a21\u578b\u7684\u6743\u91cd. # \u5728\u8fd9\u91cc,\u6211\u4eec\u5c06\u4f7f\u7528 Adam;\u8fd9\u4e2a optim \u5305\u5305\u542b\u8bb8\u591a\u5176\u4ed6\u4f18\u5316\u7b97\u6cd5. # Adam\u6784\u9020\u51fd\u6570\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u544a\u8bc9\u4f18\u5316\u5668\u5e94\u8be5\u66f4\u65b0\u54ea\u4e2aVariables. learning_rate = 1e-4 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) for t in range(500): # \u6b63\u5411\u4f20\u9012:\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y. y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931\u51fd\u6570. loss = loss_fn(y_pred, y) print(t, loss.data[0]) # \u5728\u5411\u540e\u4f20\u9012\u4e4b\u524d,\u4f7f\u7528\u4f18\u5316\u5668\u5bf9\u8c61\u4e3a\u5176\u8981\u66f4\u65b0\u7684\u53d8\u91cf\uff08\u8fd9\u662f\u6a21\u578b\u7684\u53ef\u5b66\u4e60\u6743\u91cd\uff09\u7684\u6240\u6709\u68af\u5ea6\u5f52\u96f6. # \u8fd9\u662f\u56e0\u4e3a\u9ed8\u8ba4\u60c5\u51b5\u4e0b,\u53ea\u8981\u8c03\u7528.backward(),\u68af\u5ea6\u5c31\u4f1a\u5728\u7f13\u51b2\u533a\u4e2d\u7d2f\u79ef(\u5373\u4e0d\u4f1a\u88ab\u8986\u76d6). # \u67e5\u770btorch.autograd.backward\u7684\u6587\u6863\u4ee5\u83b7\u53d6\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f. optimizer.zero_grad() # \u5411\u540e\u4f20\u9012:\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u76f8\u5bf9\u4e8e\u6a21\u578b\u53c2\u6570\u7684\u68af\u5ea6 loss.backward() # \u5728\u4f18\u5316\u5668\u4e0a\u8c03\u7528step\u51fd\u6570\u4f1a\u66f4\u65b0\u5176\u53c2\u6570 optimizer.step()","title":"PyTorch: optim"},{"location":"12/#pytorch-custom-nn-modules","text":"\u6709\u65f6\u4f60\u4f1a\u60f3\u8981\u4f7f\u7528\u6bd4\u73b0\u6709\u6a21\u5757\u7ec4\u5408\u66f4\u590d\u6742\u7684\u7279\u6b8a\u6a21\u578b\uff1b\u5bf9\u4e8e\u8fd9\u4e9b\u60c5\u51b5, \u4f60\u53ef\u4ee5 \u901a\u8fc7\u7ee7\u627f nn.Module \u6765\u5b9a\u4e49\u4f60\u81ea\u5df1\u7684\u6a21\u5757, \u5e76\u5b9a\u4e49\u4e00\u4e2a forward \u6765\u5b9e\u73b0\u6a21\u5757\u63a5\u6536\u8f93\u5165 Variable \u5e76\u4f7f\u7528\u5176\u4ed6\u6a21\u5757\u8f93\u51fa\u7684 Variable \u548c \u5176\u4ed6 autograd \u64cd\u4f5c. \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d, \u6211\u4eec\u4f7f\u7528\u4e86\u6211\u4eec\u4e4b\u524d\u5df2\u7ecf\u5b9e\u73b0\u7684\u53cc\u5c42\u7f51\u7edc\u6765\u4f5c\u4e3a\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684\u6a21\u5757\u5b50\u7c7b: # -*- coding: utf-8 -*- import torch from torch.autograd import Variable class TwoLayerNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" \u5728\u6784\u9020\u51fd\u6570\u4e2d,\u6211\u4eec\u5b9e\u4f8b\u5316\u4e24\u4e2ann.Linear\u6a21\u5757\u5e76\u5c06\u5b83\u4eec\u5206\u914d\u4e3a\u6210\u5458\u53d8\u91cf. \"\"\" super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" \u5728forward\u51fd\u6570\u4e2d,\u6211\u4eec\u63a5\u53d7\u4e00\u4e2a\u53d8\u91cf\u7684\u8f93\u5165\u6570\u636e,\u6211\u4eec\u5fc5\u987b\u8fd4\u56de\u4e00\u4e2a\u53d8\u91cf\u7684\u8f93\u51fa\u6570\u636e. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6784\u9020\u51fd\u6570\u4e2d\u5b9a\u4e49\u7684\u6a21\u5757\u4ee5\u53ca\u53d8\u91cf\u4e0a\u7684\u4efb\u610f\u8fd0\u7b97\u7b26. \"\"\" h_relu = self.linear1(x).clamp(min=0) y_pred = self.linear2(h_relu) return y_pred # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u901a\u8fc7\u5b9e\u4f8b\u5316\u4e0a\u9762\u5b9a\u4e49\u7684\u7c7b\u6765\u6784\u5efa\u6211\u4eec\u7684\u6a21\u578b model = TwoLayerNet(D_in, H, D_out) # \u6784\u5efa\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668. # \u5bf9SGD\u6784\u9020\u51fd\u6570\u4e2d\u7684model.parameters()\u7684\u8c03\u7528\u5c06\u5305\u542b\u4f5c\u4e3a\u6a21\u578b\u6210\u5458\u7684\u4e24\u4e2ann.Linear\u6a21\u5757\u7684\u53ef\u5b66\u4e60\u53c2\u6570. criterion = torch.nn.MSELoss(size_average=False) optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) for t in range(500): # \u6b63\u5411\u4f20\u9012\uff1a\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931 loss = criterion(y_pred, y) print(t, loss.data[0]) # \u68af\u5ea6\u7f6e\u96f6, \u6267\u884c\u53cd\u5411\u4f20\u9012\u5e76\u66f4\u65b0\u6743\u91cd. optimizer.zero_grad() loss.backward() optimizer.step()","title":"PyTorch: Custom nn Modules"},{"location":"12/#pytorch-control-flow-weight-sharing","text":"\u4f5c\u4e3a\u4e00\u4e2a\u52a8\u6001\u56fe\u548c\u6743\u503c\u5171\u4eab\u7684\u4f8b\u5b50, \u6211\u4eec\u5b9e\u73b0\u4e00\u4e2a\u5947\u8469\u7684\u6a21\u578b: \u968f\u673a1-4\u6b21\u91cd\u590d\u642d\u5efa\u540c\u4e2a\u6b63\u5411\u4f20\u64ad\u7684\u5168\u8fde\u63a5 \u7684 ReLU \u7f51\u7edc, \u5e76\u4e14\u591a\u4e2a\u9690\u85cf\u5c42\u4f7f\u7528\u76f8\u540c\u7684\u6743\u91cd\u6765\u8ba1\u7b97\u6700\u5185\u5c42\u9690\u85cf\u5c42(\u8bd1\u8005\u6ce8: \u8fd9\u91cc\u7684\u76f8\u540c\u6743\u91cd,\u662f\u6307\u968f\u673a1-4\u6b21\u91cd\u590d\u642d\u5efa\u7684\u8fd9\u4e2amiddle_linear). \u5bf9\u4e8e\u8fd9\u4e2a\u6a21\u578b, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u666e\u901a\u7684 Python \u6d41\u7a0b\u63a7\u5236\u8bed\u53e5\u6765\u5b9e\u73b0\u5faa\u73af, \u800c\u4e14\u6211\u4eec\u53ef\u4ee5\u5728\u5b9a\u4e49\u524d\u5411\u4f20 \u64ad\u65f6\u901a\u8fc7\u7b80\u5355\u5730\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u7684\u6a21\u5757\u5b9e\u73b0 middle_linear \u5c42\u7684\u6743\u91cd\u5171\u4eab. \u6211\u4eec\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u5c06\u8fd9\u4e2a\u6a21\u578b\u4f5c\u4e3a Module \u5b50\u7c7b\u6765\u5b9e\u73b0: # -*- coding: utf-8 -*- import random import torch from torch.autograd import Variable class DynamicNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" \u5728\u6784\u9020\u51fd\u6570\u4e2d,\u6211\u4eec\u6784\u9020\u4e86\u4e09\u4e2ann.Linear\u5b9e\u4f8b,\u6211\u4eec\u5c06\u5728\u6b63\u5411\u4f20\u9012\u4e2d\u4f7f\u7528\u5b83\u4eec. \"\"\" super(DynamicNet, self).__init__() self.input_linear = torch.nn.Linear(D_in, H) self.middle_linear = torch.nn.Linear(H, H) self.output_linear = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" \u5bf9\u4e8e\u6a21\u578b\u7684\u6b63\u5411\u901a\u9053,\u6211\u4eec\u968f\u673a\u9009\u62e90,1,2\u62163, \u5e76\u91cd\u590d\u4f7f\u7528\u591a\u6b21\u8ba1\u7b97\u9690\u85cf\u5c42\u8868\u793a\u7684middle_linear\u6a21\u5757. \u7531\u4e8e\u6bcf\u4e2a\u6b63\u5411\u901a\u9053\u90fd\u4f1a\u751f\u6210\u4e00\u4e2a\u52a8\u6001\u8ba1\u7b97\u56fe,\u56e0\u6b64\u5728\u5b9a\u4e49\u6a21\u578b\u7684\u6b63\u5411\u901a\u9053\u65f6, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u666e\u901a\u7684Python\u63a7\u5236\u6d41\u64cd\u4f5c\u7b26(\u5982\u5faa\u73af\u6216\u6761\u4ef6\u8bed\u53e5). \u5728\u8fd9\u91cc\u6211\u4eec\u4e5f\u770b\u5230,\u5b9a\u4e49\u8ba1\u7b97\u56fe\u65f6\u591a\u6b21\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u6a21\u5757\u662f\u5b8c\u5168\u5b89\u5168\u7684. \u8fd9\u662fLua Torch\u7684\u4e00\u5927\u6539\u8fdb,\u6bcf\u4e2a\u6a21\u5757\u53ea\u80fd\u4f7f\u7528\u4e00\u6b21. \"\"\" h_relu = self.input_linear(x).clamp(min=0) for _ in range(random.randint(0, 3)): h_relu = self.middle_linear(h_relu).clamp(min=0) y_pred = self.output_linear(h_relu) return y_pred # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u901a\u8fc7\u5b9e\u4f8b\u5316\u4e0a\u9762\u5b9a\u4e49\u7684\u7c7b\u6765\u6784\u5efa\u6211\u4eec\u7684\u6a21\u578b model = DynamicNet(D_in, H, D_out) # \u6784\u5efa\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668. # \u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u8fd9\u4e2a\u5947\u602a\u7684\u6a21\u578b\u975e\u5e38\u56f0\u96be,\u6240\u4ee5\u6211\u4eec\u4f7f\u7528\u52a8\u91cf criterion = torch.nn.MSELoss(size_average=False) optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9) for t in range(500): # \u6b63\u5411\u4f20\u9012:\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931 loss = criterion(y_pred, y) print(t, loss.data[0]) # \u96f6\u68af\u5ea6\u6267\u884c\u53cd\u5411\u4f20\u9012\u5e76\u66f4\u65b0\u6743\u91cd. optimizer.zero_grad() loss.backward() optimizer.step()","title":"PyTorch: Control Flow + Weight Sharing"},{"location":"12/#examples","text":"\u4f60\u53ef\u4ee5\u5728\u8fd9\u91cc\u6d4f\u89c8\u4e0a\u7f51\u63d0\u5230\u7684\u4f8b\u5b50","title":"Examples"},{"location":"12/#tensors_1","text":"Warm-up: numpy PyTorch: Tensors","title":"Tensors"},{"location":"12/#autograd_1","text":"PyTorch: \u53d8\u91cf\u548cautograd PyTorch: \u5b9a\u4e49\u65b0\u7684autograd\u51fd\u6570 TensorFlow: \u9759\u6001\u56fe","title":"Autograd"},{"location":"12/#citenncite-module_1","text":"PyTorch: nn\u5305 PyTorch: optim\u5305 PyTorch: \u5b9a\u5236\u5316nn\u6a21\u5757 PyTorch: \u52a8\u6001\u63a7\u5236\u6d41\u7a0b + \u6743\u91cd\u5171\u4eab","title":"&lt;cite&gt;nn&lt;/cite&gt; module"},{"location":"13/","text":"Warm-up: numpy \u672c\u4f8b\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u8bef\u5dee\u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u6211\u4eec\u53ea\u7528\u5230\u4e86numpy, \u5b8c\u5168\u624b\u5199\u5b9e\u73b0\u795e\u7ecf\u7f51\u7edc, \u5305\u62ec\u524d\u5411\u8ba1\u7b97, \u8bef\u5dee\u8ba1\u7b97\u548c\u540e\u5411\u4f20\u64ad. numpy\u7684\u6570\u7ec4\u7c7b\u578b\u662f\u4e00\u79cd\u901a\u7528\u7684N\u7ef4\u6570\u7ec4; \u5b83\u6ca1\u6709\u5185\u7f6e\u6df1\u5ea6\u5b66\u4e60\u7684\u51fd\u6570, \u65e2\u4e0d\u77e5\u9053\u600e\u4e48\u6c42\u5bfc, \u4e5f\u6ca1\u6709\u8ba1\u7b97\u56fe\u7684\u6982\u5ff5, \u53ea\u80fd\u505a\u4e00\u4e9b\u901a\u7528\u7684\u6570\u503c\u8ba1\u7b97. import numpy as np # N \u662f\u4e00\u4e2abatch\u7684\u6837\u672c\u6570\u91cf; D_in\u662f\u8f93\u5165\u7ef4\u5ea6; # H \u662f\u9690\u85cf\u5c42\u5411\u91cf\u7684\u7ef4\u5ea6; D_out\u662f\u8f93\u51fa\u7ef4\u5ea6. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u7684\u8f93\u5165\u8f93\u51fa\u6570\u636e x = np.random.randn(N, D_in) y = np.random.randn(N, D_out) # \u968f\u673a\u521d\u59cb\u5316\u6743\u91cd\u53c2\u6570 w1 = np.random.randn(D_in, H) w2 = np.random.randn(H, D_out) learning_rate = 1e-6 for t in range(500): # \u524d\u5411\u8ba1\u7b97, \u7b97\u51fay\u7684\u9884\u6d4b\u503c h = x.dot(w1) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # \u8ba1\u7b97\u5e76\u6253\u5370\u8bef\u5dee\u503c loss = np.square(y_pred - y).sum() print(t, loss) # \u5728\u53cd\u5411\u4f20\u64ad\u4e2d, \u8ba1\u7b97\u51fa\u8bef\u5dee\u5173\u4e8ew1\u548cw2\u7684\u5bfc\u6570 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h < 0] = 0 grad_w1 = x.T.dot(grad_h) # \u66f4\u65b0\u6743\u91cd w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_numpy.py Download Jupyter notebook: two_layer_net_numpy.ipynb Gallery generated by Sphinx-Gallery","title":"Warm-up: numpy"},{"location":"13/#warm-up-numpy","text":"\u672c\u4f8b\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u8bef\u5dee\u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u6211\u4eec\u53ea\u7528\u5230\u4e86numpy, \u5b8c\u5168\u624b\u5199\u5b9e\u73b0\u795e\u7ecf\u7f51\u7edc, \u5305\u62ec\u524d\u5411\u8ba1\u7b97, \u8bef\u5dee\u8ba1\u7b97\u548c\u540e\u5411\u4f20\u64ad. numpy\u7684\u6570\u7ec4\u7c7b\u578b\u662f\u4e00\u79cd\u901a\u7528\u7684N\u7ef4\u6570\u7ec4; \u5b83\u6ca1\u6709\u5185\u7f6e\u6df1\u5ea6\u5b66\u4e60\u7684\u51fd\u6570, \u65e2\u4e0d\u77e5\u9053\u600e\u4e48\u6c42\u5bfc, \u4e5f\u6ca1\u6709\u8ba1\u7b97\u56fe\u7684\u6982\u5ff5, \u53ea\u80fd\u505a\u4e00\u4e9b\u901a\u7528\u7684\u6570\u503c\u8ba1\u7b97. import numpy as np # N \u662f\u4e00\u4e2abatch\u7684\u6837\u672c\u6570\u91cf; D_in\u662f\u8f93\u5165\u7ef4\u5ea6; # H \u662f\u9690\u85cf\u5c42\u5411\u91cf\u7684\u7ef4\u5ea6; D_out\u662f\u8f93\u51fa\u7ef4\u5ea6. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u7684\u8f93\u5165\u8f93\u51fa\u6570\u636e x = np.random.randn(N, D_in) y = np.random.randn(N, D_out) # \u968f\u673a\u521d\u59cb\u5316\u6743\u91cd\u53c2\u6570 w1 = np.random.randn(D_in, H) w2 = np.random.randn(H, D_out) learning_rate = 1e-6 for t in range(500): # \u524d\u5411\u8ba1\u7b97, \u7b97\u51fay\u7684\u9884\u6d4b\u503c h = x.dot(w1) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # \u8ba1\u7b97\u5e76\u6253\u5370\u8bef\u5dee\u503c loss = np.square(y_pred - y).sum() print(t, loss) # \u5728\u53cd\u5411\u4f20\u64ad\u4e2d, \u8ba1\u7b97\u51fa\u8bef\u5dee\u5173\u4e8ew1\u548cw2\u7684\u5bfc\u6570 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h < 0] = 0 grad_w1 = x.T.dot(grad_h) # \u66f4\u65b0\u6743\u91cd w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_numpy.py Download Jupyter notebook: two_layer_net_numpy.ipynb Gallery generated by Sphinx-Gallery","title":"Warm-up: numpy"},{"location":"14/","text":"PyTorch: Tensors \u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u4f7f\u7528\u6b27\u5f0f\u8ddd\u79bb\u5e73\u65b9\u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u5b9e\u73b0\u4e2d\u6211\u4eec\u4f7f\u7528\u4e86PyTorch\u7684\u5f20\u91cf\u6765\u8fdb\u884c\u524d\u5411\u8ba1\u7b97, \u8bef\u5dee\u8ba1\u7b97\u548c\u540e\u5411\u4f20\u64ad. PyTorch\u7684\u5f20\u91cfTensor\u57fa\u672c\u4e0a\u548cnumpy\u7684\u6570\u7ec4\u4e00\u6837, \u4e5f\u6ca1\u6709\u4efb\u4f55\u5185\u7f6e\u7684\u6df1\u5ea6\u5b66\u4e60\u51fd\u6570, \u4e0d\u77e5\u9053\u8ba1\u7b97\u56fe\u7684\u6982\u5ff5, \u4e5f\u65e0\u6cd5\u6c42\u5bfc, \u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u7684N\u7ef4\u6570\u7ec4, \u5b83\u53ea\u7528\u505a\u4efb\u610f\u7684\u6570\u503c\u8ba1\u7b97. \u548cnumpy\u6570\u7ec4\u6700\u5927\u7684\u533a\u522b\u5728\u4e8e, PyTorch\u5f20\u91cf\u65e2\u53ef\u4ee5\u8dd1\u5728CPU\u4e0a, \u4e5f\u53ef\u4ee5\u5728GPU\u4e0a\u4f5c\u8fd0\u7b97. \u4e3a\u4e86\u5728GPU\u4e0a\u8fdb\u884c\u8ba1\u7b97, \u53ea\u8981\u628a\u5f20\u91cf\u7c7b\u578b\u8f6c\u6210cuda\u6570\u636e\u7c7b\u578b\u5373\u53ef\u3002 import torch dtype = torch.FloatTensor # dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u8f93\u5165\u548c\u8f93\u51fa\u6570\u636e x = torch.randn(N, D_in).type(dtype) y = torch.randn(N, D_out).type(dtype) # \u968f\u673a\u521d\u59cb\u5316\u6743\u91cd w1 = torch.randn(D_in, H).type(dtype) w2 = torch.randn(H, D_out).type(dtype) learning_rate = 1e-6 for t in range(500): # \u6b63\u5411\u4f20\u9012\uff1a\u8ba1\u7b97\u9884\u6d4by h = x.mm(w1) h_relu = h.clamp(min=0) y_pred = h_relu.mm(w2) # \u8ba1\u7b97\u5e76\u6253\u5370loss loss = (y_pred - y).pow(2).sum() print(t, loss) # \u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u5173\u4e8e\u635f\u5931\u7684w1\u548cw2\u7684\u68af\u5ea6 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.t().mm(grad_y_pred) grad_h_relu = grad_y_pred.mm(w2.t()) grad_h = grad_h_relu.clone() grad_h[h < 0] = 0 grad_w1 = x.t().mm(grad_h) # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_tensor.py Download Jupyter notebook: two_layer_net_tensor.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: Tensors"},{"location":"14/#pytorch-tensors","text":"\u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u4f7f\u7528\u6b27\u5f0f\u8ddd\u79bb\u5e73\u65b9\u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u5b9e\u73b0\u4e2d\u6211\u4eec\u4f7f\u7528\u4e86PyTorch\u7684\u5f20\u91cf\u6765\u8fdb\u884c\u524d\u5411\u8ba1\u7b97, \u8bef\u5dee\u8ba1\u7b97\u548c\u540e\u5411\u4f20\u64ad. PyTorch\u7684\u5f20\u91cfTensor\u57fa\u672c\u4e0a\u548cnumpy\u7684\u6570\u7ec4\u4e00\u6837, \u4e5f\u6ca1\u6709\u4efb\u4f55\u5185\u7f6e\u7684\u6df1\u5ea6\u5b66\u4e60\u51fd\u6570, \u4e0d\u77e5\u9053\u8ba1\u7b97\u56fe\u7684\u6982\u5ff5, \u4e5f\u65e0\u6cd5\u6c42\u5bfc, \u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u7684N\u7ef4\u6570\u7ec4, \u5b83\u53ea\u7528\u505a\u4efb\u610f\u7684\u6570\u503c\u8ba1\u7b97. \u548cnumpy\u6570\u7ec4\u6700\u5927\u7684\u533a\u522b\u5728\u4e8e, PyTorch\u5f20\u91cf\u65e2\u53ef\u4ee5\u8dd1\u5728CPU\u4e0a, \u4e5f\u53ef\u4ee5\u5728GPU\u4e0a\u4f5c\u8fd0\u7b97. \u4e3a\u4e86\u5728GPU\u4e0a\u8fdb\u884c\u8ba1\u7b97, \u53ea\u8981\u628a\u5f20\u91cf\u7c7b\u578b\u8f6c\u6210cuda\u6570\u636e\u7c7b\u578b\u5373\u53ef\u3002 import torch dtype = torch.FloatTensor # dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u8f93\u5165\u548c\u8f93\u51fa\u6570\u636e x = torch.randn(N, D_in).type(dtype) y = torch.randn(N, D_out).type(dtype) # \u968f\u673a\u521d\u59cb\u5316\u6743\u91cd w1 = torch.randn(D_in, H).type(dtype) w2 = torch.randn(H, D_out).type(dtype) learning_rate = 1e-6 for t in range(500): # \u6b63\u5411\u4f20\u9012\uff1a\u8ba1\u7b97\u9884\u6d4by h = x.mm(w1) h_relu = h.clamp(min=0) y_pred = h_relu.mm(w2) # \u8ba1\u7b97\u5e76\u6253\u5370loss loss = (y_pred - y).pow(2).sum() print(t, loss) # \u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u5173\u4e8e\u635f\u5931\u7684w1\u548cw2\u7684\u68af\u5ea6 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.t().mm(grad_y_pred) grad_h_relu = grad_y_pred.mm(w2.t()) grad_h = grad_h_relu.clone() grad_h[h < 0] = 0 grad_w1 = x.t().mm(grad_h) # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_tensor.py Download Jupyter notebook: two_layer_net_tensor.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: Tensors"},{"location":"15/","text":"PyTorch: \u53d8\u91cf\u548cautograd \u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u5728\u5b9e\u73b0\u4e2d, \u6211\u4eec\u5c06\u4f7f\u7528PyTorch\u53d8\u91cf\u7684\u51fd\u6570\u6765\u8fdb\u884c\u524d\u5411\u8ba1\u7b97, \u5e76\u7528PyTorch\u7684autograd\u8ba1\u7b97\u68af\u5ea6 PyTorch\u53d8\u91cf\u662fPyTorch\u5f20\u91cf\u7684\u5c01\u88c5, \u8868\u793a\u8ba1\u7b97\u56fe\u4e2d\u7684\u4e00\u4e2a\u8282\u70b9. \u5982\u679cx\u662f\u53d8\u91cf, \u90a3\u4e48x.data\u5c31\u662f \u8868\u793a\u5176\u503c\u7684\u5f20\u91cf, \u800cx.grad\u5219\u662f\u53e6\u4e00\u4e2a\u53d8\u91cf, \u5176\u4e2d\u5305\u542b\u67d0\u4e2a\u6807\u91cf\u5173\u4e8ex\u7684\u68af\u5ea6. PyTorch\u53d8\u91cf\u7684API\u548c\u5f20\u91cf\u662f\u4e00\u6837\u7684: \u51e0\u4e4e\u6240\u6709Tensor\u4e0a\u80fd\u505a\u7684\u64cd\u4f5c, \u4f60\u5728\u53d8\u91cf\u4e0a\u4e5f\u53ef\u4ee5\u8c03\u7528. \u533a\u522b \u5728\u4e8e\u7528\u53d8\u91cf\u65f6, autograd\u53ef\u4ee5\u81ea\u52a8\u8ba1\u7b97\u68af\u5ea6. import torch from torch.autograd import Variable dtype = torch.FloatTensor # dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. # \u8bbe\u7f6erequires_grad = False, \u56e0\u4e3a\u5728\u540e\u5411\u4f20\u64ad\u65f6, \u6211\u4eec\u5e76\u4e0d\u9700\u8981\u8ba1\u7b97\u5173\u4e8e\u8fd9\u4e9b\u53d8\u91cf\u7684\u68af\u5ea6 x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False) y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False) # \u4e3a\u6743\u91cd\u521b\u5efa\u968f\u673a\u5f20\u91cf,\u5e76\u5c06\u5176\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. # \u8bbe\u7f6erequires_grad = True, \u56e0\u4e3a\u5728\u540e\u5411\u4f20\u64ad\u65f6, \u6211\u4eec\u9700\u8981\u8ba1\u7b97\u5173\u4e8e\u8fd9\u4e9b\u53d8\u91cf\u7684\u68af\u5ea6 w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True) w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True) learning_rate = 1e-6 for t in range(500): # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y; \u8fd9\u4e9b # \u4e0e\u6211\u4eec\u7528\u4e8e\u8ba1\u7b97\u4f7f\u7528\u5f20\u91cf\u7684\u6b63\u5411\u4f20\u9012\u5b8c\u5168\u76f8\u540c, # \u4f46\u6211\u4eec\u4e0d\u9700\u8981\u4fdd\u7559\u5bf9\u4e2d\u95f4\u503c\u7684\u5f15\u7528, # \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5b9e\u73b0\u5411\u540e\u4f20\u9012. y_pred = x.mm(w1).clamp(min=0).mm(w2) # \u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u64cd\u4f5c\u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931. # \u73b0\u5728\u635f\u5931\u662f\u5f62\u72b6\u53d8\u91cf (1,) \u5e76\u4e14 loss.data \u662f\u5f62\u72b6\u7684\u5f20\u91cf # (1,); loss.data[0] \u662f\u6301\u6709\u635f\u5931\u7684\u6807\u91cf\u503c. loss = (y_pred - y).pow(2).sum() print(t, loss.data[0]) # \u4f7f\u7528autograd\u6765\u8ba1\u7b97\u53cd\u5411\u4f20\u9012. # \u8be5\u8c03\u7528\u5c06\u4f7f\u7528requires_grad = True\u6765\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u6240\u6709\u53d8\u91cf\u7684\u635f\u5931\u68af\u5ea6. # \u5728\u8fd9\u6b21\u8c03\u7528\u4e4b\u540e w1.grad \u548c w2.grad \u5c06\u662f\u53d8\u91cf # \u5b83\u4eec\u5206\u522b\u76f8\u5bf9\u4e8ew1\u548cw2\u4fdd\u5b58\u635f\u5931\u7684\u68af\u5ea6. loss.backward() # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd; w1.data \u548c w2.data \u662f\u5f20\u91cf, # w1.grad \u548c w2.grad \u662f\u53d8\u91cf\u5e76\u4e14 w1.grad.data \u548c w2.grad.data # \u662f\u5f20\u91cf. w1.data -= learning_rate * w1.grad.data w2.data -= learning_rate * w2.grad.data # \u66f4\u65b0\u6743\u91cd\u540e\u624b\u52a8\u5c06\u68af\u5ea6\u5f52\u96f6 w1.grad.data.zero_() w2.grad.data.zero_() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_autograd.py Download Jupyter notebook: two_layer_net_autograd.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: \u53d8\u91cf\u548cautograd"},{"location":"15/#pytorch-autograd","text":"\u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u5728\u5b9e\u73b0\u4e2d, \u6211\u4eec\u5c06\u4f7f\u7528PyTorch\u53d8\u91cf\u7684\u51fd\u6570\u6765\u8fdb\u884c\u524d\u5411\u8ba1\u7b97, \u5e76\u7528PyTorch\u7684autograd\u8ba1\u7b97\u68af\u5ea6 PyTorch\u53d8\u91cf\u662fPyTorch\u5f20\u91cf\u7684\u5c01\u88c5, \u8868\u793a\u8ba1\u7b97\u56fe\u4e2d\u7684\u4e00\u4e2a\u8282\u70b9. \u5982\u679cx\u662f\u53d8\u91cf, \u90a3\u4e48x.data\u5c31\u662f \u8868\u793a\u5176\u503c\u7684\u5f20\u91cf, \u800cx.grad\u5219\u662f\u53e6\u4e00\u4e2a\u53d8\u91cf, \u5176\u4e2d\u5305\u542b\u67d0\u4e2a\u6807\u91cf\u5173\u4e8ex\u7684\u68af\u5ea6. PyTorch\u53d8\u91cf\u7684API\u548c\u5f20\u91cf\u662f\u4e00\u6837\u7684: \u51e0\u4e4e\u6240\u6709Tensor\u4e0a\u80fd\u505a\u7684\u64cd\u4f5c, \u4f60\u5728\u53d8\u91cf\u4e0a\u4e5f\u53ef\u4ee5\u8c03\u7528. \u533a\u522b \u5728\u4e8e\u7528\u53d8\u91cf\u65f6, autograd\u53ef\u4ee5\u81ea\u52a8\u8ba1\u7b97\u68af\u5ea6. import torch from torch.autograd import Variable dtype = torch.FloatTensor # dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. # \u8bbe\u7f6erequires_grad = False, \u56e0\u4e3a\u5728\u540e\u5411\u4f20\u64ad\u65f6, \u6211\u4eec\u5e76\u4e0d\u9700\u8981\u8ba1\u7b97\u5173\u4e8e\u8fd9\u4e9b\u53d8\u91cf\u7684\u68af\u5ea6 x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False) y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False) # \u4e3a\u6743\u91cd\u521b\u5efa\u968f\u673a\u5f20\u91cf,\u5e76\u5c06\u5176\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. # \u8bbe\u7f6erequires_grad = True, \u56e0\u4e3a\u5728\u540e\u5411\u4f20\u64ad\u65f6, \u6211\u4eec\u9700\u8981\u8ba1\u7b97\u5173\u4e8e\u8fd9\u4e9b\u53d8\u91cf\u7684\u68af\u5ea6 w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True) w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True) learning_rate = 1e-6 for t in range(500): # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y; \u8fd9\u4e9b # \u4e0e\u6211\u4eec\u7528\u4e8e\u8ba1\u7b97\u4f7f\u7528\u5f20\u91cf\u7684\u6b63\u5411\u4f20\u9012\u5b8c\u5168\u76f8\u540c, # \u4f46\u6211\u4eec\u4e0d\u9700\u8981\u4fdd\u7559\u5bf9\u4e2d\u95f4\u503c\u7684\u5f15\u7528, # \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5b9e\u73b0\u5411\u540e\u4f20\u9012. y_pred = x.mm(w1).clamp(min=0).mm(w2) # \u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u64cd\u4f5c\u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931. # \u73b0\u5728\u635f\u5931\u662f\u5f62\u72b6\u53d8\u91cf (1,) \u5e76\u4e14 loss.data \u662f\u5f62\u72b6\u7684\u5f20\u91cf # (1,); loss.data[0] \u662f\u6301\u6709\u635f\u5931\u7684\u6807\u91cf\u503c. loss = (y_pred - y).pow(2).sum() print(t, loss.data[0]) # \u4f7f\u7528autograd\u6765\u8ba1\u7b97\u53cd\u5411\u4f20\u9012. # \u8be5\u8c03\u7528\u5c06\u4f7f\u7528requires_grad = True\u6765\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u6240\u6709\u53d8\u91cf\u7684\u635f\u5931\u68af\u5ea6. # \u5728\u8fd9\u6b21\u8c03\u7528\u4e4b\u540e w1.grad \u548c w2.grad \u5c06\u662f\u53d8\u91cf # \u5b83\u4eec\u5206\u522b\u76f8\u5bf9\u4e8ew1\u548cw2\u4fdd\u5b58\u635f\u5931\u7684\u68af\u5ea6. loss.backward() # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd; w1.data \u548c w2.data \u662f\u5f20\u91cf, # w1.grad \u548c w2.grad \u662f\u53d8\u91cf\u5e76\u4e14 w1.grad.data \u548c w2.grad.data # \u662f\u5f20\u91cf. w1.data -= learning_rate * w1.grad.data w2.data -= learning_rate * w2.grad.data # \u66f4\u65b0\u6743\u91cd\u540e\u624b\u52a8\u5c06\u68af\u5ea6\u5f52\u96f6 w1.grad.data.zero_() w2.grad.data.zero_() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_autograd.py Download Jupyter notebook: two_layer_net_autograd.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: \u53d8\u91cf\u548cautograd"},{"location":"16/","text":"PyTorch: \u5b9a\u4e49\u65b0\u7684autograd\u51fd\u6570 \u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u5728\u6b64\u5b9e\u73b0\u4e2d, \u6211\u4eec\u4f7f\u7528PyTorch\u53d8\u91cf\u4e0a\u7684\u51fd\u6570\u6765\u8fdb\u884c\u524d\u5411\u8ba1\u7b97, \u7136\u540e\u7528PyTorch\u7684autograd\u8ba1\u7b97\u68af\u5ea6 \u6211\u4eec\u8fd8\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5b9a\u5236\u5316\u7684autograd\u51fd\u6570, \u7528\u4e8eReLU\u51fd\u6570. import torch from torch.autograd import Variable class MyReLU(torch.autograd.Function): \"\"\" \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b50\u7c7b\u5b9e\u73b0\u6211\u4eec\u81ea\u5df1\u5b9a\u5236\u7684autograd\u51fd\u6570 torch.autograd.Function\u548c\u6267\u884c\u5728Tensors\u4e0a\u8fd0\u884c\u7684\u5411\u524d\u548c\u5411\u540e\u901a\u884c\u8bc1. \"\"\" @staticmethod def forward(ctx, input): \"\"\" \u5728\u6b63\u5411\u4f20\u9012\u4e2d,\u6211\u4eec\u6536\u5230\u4e00\u4e2a\u5305\u542b\u8f93\u5165\u548c\u8fd4\u56de\u5f20\u91cf\u7684\u5f20\u91cf,\u5176\u4e2d\u5305\u542b\u8f93\u51fa. ctx\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u5bf9\u8c61,\u53ef\u7528\u4e8e\u5b58\u50a8\u53cd\u5411\u8ba1\u7b97\u7684\u4fe1\u606f. \u60a8\u53ef\u4ee5\u4f7f\u7528ctx.save_for_backward\u65b9\u6cd5\u7f13\u5b58\u4efb\u610f\u5bf9\u8c61\u4ee5\u7528\u4e8e\u540e\u5411\u4f20\u9012. \"\"\" ctx.save_for_backward(input) return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): \"\"\" \u5728\u540e\u5411\u4f20\u9012\u4e2d,\u6211\u4eec\u6536\u5230\u4e00\u4e2a\u5f20\u91cf,\u5176\u4e2d\u5305\u542b\u76f8\u5bf9\u4e8e\u8f93\u51fa\u7684\u635f\u5931\u68af\u5ea6, \u6211\u4eec\u9700\u8981\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u8f93\u5165\u7684\u635f\u5931\u68af\u5ea6. \"\"\" input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input < 0] = 0 return grad_input dtype = torch.FloatTensor # dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False) y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False) # \u4e3a\u6743\u91cd\u521b\u5efa\u968f\u673a\u5f20\u91cf,\u5e76\u5c06\u5176\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True) w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True) learning_rate = 1e-6 for t in range(500): # \u4e3a\u4e86\u5e94\u7528\u6211\u4eec\u7684\u51fd\u6570,\u6211\u4eec\u4f7f\u7528Function.apply\u65b9\u6cd5.\u6211\u4eec\u628a\u5b83\u79f0\u4e3a'relu'. relu = MyReLU.apply # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y; # \u6211\u4eec\u4f7f\u7528\u6211\u4eec\u7684\u81ea\u5b9a\u4e49autograd\u64cd\u4f5c\u6765\u8ba1\u7b97ReLU. y_pred = relu(x.mm(w1)).mm(w2) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931 loss = (y_pred - y).pow(2).sum() print(t, loss.data[0]) # \u4f7f\u7528autograd\u6765\u8ba1\u7b97\u53cd\u5411\u4f20\u9012. loss.backward() # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd w1.data -= learning_rate * w1.grad.data w2.data -= learning_rate * w2.grad.data # \u66f4\u65b0\u6743\u91cd\u540e\u624b\u52a8\u5c06\u68af\u5ea6\u5f52\u96f6 w1.grad.data.zero_() w2.grad.data.zero_() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_custom_function.py Download Jupyter notebook: two_layer_net_custom_function.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: \u5b9a\u4e49\u65b0\u7684autograd\u51fd\u6570"},{"location":"16/#pytorch-autograd","text":"\u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u5728\u6b64\u5b9e\u73b0\u4e2d, \u6211\u4eec\u4f7f\u7528PyTorch\u53d8\u91cf\u4e0a\u7684\u51fd\u6570\u6765\u8fdb\u884c\u524d\u5411\u8ba1\u7b97, \u7136\u540e\u7528PyTorch\u7684autograd\u8ba1\u7b97\u68af\u5ea6 \u6211\u4eec\u8fd8\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5b9a\u5236\u5316\u7684autograd\u51fd\u6570, \u7528\u4e8eReLU\u51fd\u6570. import torch from torch.autograd import Variable class MyReLU(torch.autograd.Function): \"\"\" \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b50\u7c7b\u5b9e\u73b0\u6211\u4eec\u81ea\u5df1\u5b9a\u5236\u7684autograd\u51fd\u6570 torch.autograd.Function\u548c\u6267\u884c\u5728Tensors\u4e0a\u8fd0\u884c\u7684\u5411\u524d\u548c\u5411\u540e\u901a\u884c\u8bc1. \"\"\" @staticmethod def forward(ctx, input): \"\"\" \u5728\u6b63\u5411\u4f20\u9012\u4e2d,\u6211\u4eec\u6536\u5230\u4e00\u4e2a\u5305\u542b\u8f93\u5165\u548c\u8fd4\u56de\u5f20\u91cf\u7684\u5f20\u91cf,\u5176\u4e2d\u5305\u542b\u8f93\u51fa. ctx\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u5bf9\u8c61,\u53ef\u7528\u4e8e\u5b58\u50a8\u53cd\u5411\u8ba1\u7b97\u7684\u4fe1\u606f. \u60a8\u53ef\u4ee5\u4f7f\u7528ctx.save_for_backward\u65b9\u6cd5\u7f13\u5b58\u4efb\u610f\u5bf9\u8c61\u4ee5\u7528\u4e8e\u540e\u5411\u4f20\u9012. \"\"\" ctx.save_for_backward(input) return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): \"\"\" \u5728\u540e\u5411\u4f20\u9012\u4e2d,\u6211\u4eec\u6536\u5230\u4e00\u4e2a\u5f20\u91cf,\u5176\u4e2d\u5305\u542b\u76f8\u5bf9\u4e8e\u8f93\u51fa\u7684\u635f\u5931\u68af\u5ea6, \u6211\u4eec\u9700\u8981\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u8f93\u5165\u7684\u635f\u5931\u68af\u5ea6. \"\"\" input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input < 0] = 0 return grad_input dtype = torch.FloatTensor # dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False) y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False) # \u4e3a\u6743\u91cd\u521b\u5efa\u968f\u673a\u5f20\u91cf,\u5e76\u5c06\u5176\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True) w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True) learning_rate = 1e-6 for t in range(500): # \u4e3a\u4e86\u5e94\u7528\u6211\u4eec\u7684\u51fd\u6570,\u6211\u4eec\u4f7f\u7528Function.apply\u65b9\u6cd5.\u6211\u4eec\u628a\u5b83\u79f0\u4e3a'relu'. relu = MyReLU.apply # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y; # \u6211\u4eec\u4f7f\u7528\u6211\u4eec\u7684\u81ea\u5b9a\u4e49autograd\u64cd\u4f5c\u6765\u8ba1\u7b97ReLU. y_pred = relu(x.mm(w1)).mm(w2) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931 loss = (y_pred - y).pow(2).sum() print(t, loss.data[0]) # \u4f7f\u7528autograd\u6765\u8ba1\u7b97\u53cd\u5411\u4f20\u9012. loss.backward() # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd w1.data -= learning_rate * w1.grad.data w2.data -= learning_rate * w2.grad.data # \u66f4\u65b0\u6743\u91cd\u540e\u624b\u52a8\u5c06\u68af\u5ea6\u5f52\u96f6 w1.grad.data.zero_() w2.grad.data.zero_() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_custom_function.py Download Jupyter notebook: two_layer_net_custom_function.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: \u5b9a\u4e49\u65b0\u7684autograd\u51fd\u6570"},{"location":"17/","text":"TensorFlow: \u9759\u6001\u56fe \u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u5728\u5b9e\u73b0\u4e2d, \u6211\u4eec\u4f1a\u7528\u57fa\u672c\u7684TensorFlow\u64cd\u4f5c\u6765\u5efa\u7acb\u4e00\u4e2a\u8ba1\u7b97\u56fe, \u968f\u540e\u591a\u6b21\u6267\u884c\u8fd9\u4e2a\u56fe\u6765\u8bad\u7ec3\u7f51\u7edc. TensorFlow\u548cPyTorch\u6709\u4e00\u4e2a\u5f88\u5927\u7684\u533a\u522b, \u5c31\u662fTensorFlow\u7528\u7684\u662f\u9759\u6001\u8ba1\u7b97\u56fe, \u800cPyTorch\u5219\u7528\u52a8\u6001\u8ba1\u7b97\u56fe. \u7528TensorFlow\u6211\u4eec\u5148\u5efa\u7acb\u8ba1\u7b97\u56fe, \u7136\u540e\u5728\u591a\u6b21\u6267\u884c\u8fc7\u7a0b\u4e2d, \u8ba1\u7b97\u56fe\u56fa\u5b9a\u4e0d\u53d8. import tensorflow as tf import numpy as np # \u9996\u5148\u6211\u4eec\u8bbe\u7f6e\u8ba1\u7b97\u56fe: # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u4e3a\u8f93\u5165\u6570\u636e\u548c\u76ee\u6807\u6570\u636e\u521b\u5efa\u5360\u4f4d\u7b26; # \u5f53\u6211\u4eec\u6267\u884c\u56fe\u65f6,\u8fd9\u4e9b\u5c06\u88ab\u586b\u5145\u771f\u5b9e\u7684\u6570\u636e. x = tf.placeholder(tf.float32, shape=(None, D_in)) y = tf.placeholder(tf.float32, shape=(None, D_out)) # \u4e3a\u6743\u91cd\u521b\u5efa\u53d8\u91cf\u5e76\u7528\u968f\u673a\u6570\u636e\u521d\u59cb\u5316\u5b83\u4eec. # \u4e00\u4e2aTensorFlow\u53d8\u91cf\u5728\u56fe\u7684\u6267\u884c\u4e2d\u4fdd\u6301\u5176\u503c. w1 = tf.Variable(tf.random_normal((D_in, H))) w2 = tf.Variable(tf.random_normal((H, D_out))) # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528TensorFlow Tensors\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y. # \u8bf7\u6ce8\u610f\u6b64\u4ee3\u7801\u5b9e\u9645\u4e0a\u5e76\u672a\u6267\u884c\u4efb\u4f55\u6570\u5b57\u64cd\u4f5c; # \u5b83\u53ea\u662f\u8bbe\u7f6e\u6211\u4eec\u7a0d\u540e\u5c06\u6267\u884c\u7684\u8ba1\u7b97\u56fe. h = tf.matmul(x, w1) h_relu = tf.maximum(h, tf.zeros(1)) y_pred = tf.matmul(h_relu, w2) # \u4f7f\u7528TensorFlow\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u8ba1\u7b97\u635f\u5931 loss = tf.reduce_sum((y - y_pred) ** 2.0) # \u8ba1\u7b97\u76f8\u5bf9\u4e8ew1\u548cw2\u7684\u635f\u5931\u68af\u5ea6. grad_w1, grad_w2 = tf.gradients(loss, [w1, w2]) # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd. # \u8981\u5b9e\u9645\u66f4\u65b0\u6743\u91cd,\u6211\u4eec\u9700\u8981\u5728\u6267\u884c\u56fe\u65f6\u8bc4\u4f30new_w1\u548cnew_w2. # \u8bf7\u6ce8\u610f,\u5728TensorFlow\u4e2d,\u66f4\u65b0\u6743\u503c\u7684\u884c\u4e3a\u662f\u8ba1\u7b97\u56fe\u7684\u4e00\u90e8\u5206 # \u5728PyTorch\u4e2d,\u8fd9\u53d1\u751f\u5728\u8ba1\u7b97\u56fe\u4e4b\u5916. learning_rate = 1e-6 new_w1 = w1.assign(w1 - learning_rate * grad_w1) new_w2 = w2.assign(w2 - learning_rate * grad_w2) # \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u6784\u5efa\u4e86\u8ba1\u7b97\u56fe,\u6240\u4ee5\u6211\u4eec\u8f93\u5165\u4e00\u4e2aTensorFlow\u4f1a\u8bdd\u6765\u5b9e\u9645\u6267\u884c\u56fe. with tf.Session() as sess: # \u8fd0\u884c\u4e00\u6b21\u56fe\u5f62\u521d\u59cb\u5316\u53d8\u91cfw1\u548cw2. sess.run(tf.global_variables_initializer()) # \u521b\u5efa\u5305\u542b\u8f93\u5165x\u548c\u76ee\u6807y\u7684\u5b9e\u9645\u6570\u636e\u7684numpy\u6570\u7ec4 x_value = np.random.randn(N, D_in) y_value = np.random.randn(N, D_out) for _ in range(500): # \u591a\u6b21\u6267\u884c\u56fe. \u6bcf\u6b21\u6267\u884c\u65f6, # \u6211\u4eec\u90fd\u60f3\u5c06x_value\u7ed1\u5b9a\u5230x,\u5c06y_value\u7ed1\u5b9a\u5230y,\u7528feed_dict\u53c2\u6570\u6307\u5b9a. # \u6bcf\u6b21\u6211\u4eec\u6267\u884c\u56fe\u65f6,\u6211\u4eec\u90fd\u60f3\u8ba1\u7b97\u635f\u5931\u503cnew_w1 \u548c new_w2; # \u8fd9\u4e9b\u5f20\u91cf\u7684\u503c\u4f5c\u4e3anumpy\u6570\u7ec4\u8fd4\u56de. loss_value, _, _ = sess.run([loss, new_w1, new_w2], feed_dict={x: x_value, y: y_value}) print(loss_value) Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: tf_two_layer_net.py Download Jupyter notebook: tf_two_layer_net.ipynb Gallery generated by Sphinx-Gallery","title":"TensorFlow: \u9759\u6001\u56fe"},{"location":"17/#tensorflow","text":"\u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u5728\u5b9e\u73b0\u4e2d, \u6211\u4eec\u4f1a\u7528\u57fa\u672c\u7684TensorFlow\u64cd\u4f5c\u6765\u5efa\u7acb\u4e00\u4e2a\u8ba1\u7b97\u56fe, \u968f\u540e\u591a\u6b21\u6267\u884c\u8fd9\u4e2a\u56fe\u6765\u8bad\u7ec3\u7f51\u7edc. TensorFlow\u548cPyTorch\u6709\u4e00\u4e2a\u5f88\u5927\u7684\u533a\u522b, \u5c31\u662fTensorFlow\u7528\u7684\u662f\u9759\u6001\u8ba1\u7b97\u56fe, \u800cPyTorch\u5219\u7528\u52a8\u6001\u8ba1\u7b97\u56fe. \u7528TensorFlow\u6211\u4eec\u5148\u5efa\u7acb\u8ba1\u7b97\u56fe, \u7136\u540e\u5728\u591a\u6b21\u6267\u884c\u8fc7\u7a0b\u4e2d, \u8ba1\u7b97\u56fe\u56fa\u5b9a\u4e0d\u53d8. import tensorflow as tf import numpy as np # \u9996\u5148\u6211\u4eec\u8bbe\u7f6e\u8ba1\u7b97\u56fe: # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u4e3a\u8f93\u5165\u6570\u636e\u548c\u76ee\u6807\u6570\u636e\u521b\u5efa\u5360\u4f4d\u7b26; # \u5f53\u6211\u4eec\u6267\u884c\u56fe\u65f6,\u8fd9\u4e9b\u5c06\u88ab\u586b\u5145\u771f\u5b9e\u7684\u6570\u636e. x = tf.placeholder(tf.float32, shape=(None, D_in)) y = tf.placeholder(tf.float32, shape=(None, D_out)) # \u4e3a\u6743\u91cd\u521b\u5efa\u53d8\u91cf\u5e76\u7528\u968f\u673a\u6570\u636e\u521d\u59cb\u5316\u5b83\u4eec. # \u4e00\u4e2aTensorFlow\u53d8\u91cf\u5728\u56fe\u7684\u6267\u884c\u4e2d\u4fdd\u6301\u5176\u503c. w1 = tf.Variable(tf.random_normal((D_in, H))) w2 = tf.Variable(tf.random_normal((H, D_out))) # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528TensorFlow Tensors\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y. # \u8bf7\u6ce8\u610f\u6b64\u4ee3\u7801\u5b9e\u9645\u4e0a\u5e76\u672a\u6267\u884c\u4efb\u4f55\u6570\u5b57\u64cd\u4f5c; # \u5b83\u53ea\u662f\u8bbe\u7f6e\u6211\u4eec\u7a0d\u540e\u5c06\u6267\u884c\u7684\u8ba1\u7b97\u56fe. h = tf.matmul(x, w1) h_relu = tf.maximum(h, tf.zeros(1)) y_pred = tf.matmul(h_relu, w2) # \u4f7f\u7528TensorFlow\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u8ba1\u7b97\u635f\u5931 loss = tf.reduce_sum((y - y_pred) ** 2.0) # \u8ba1\u7b97\u76f8\u5bf9\u4e8ew1\u548cw2\u7684\u635f\u5931\u68af\u5ea6. grad_w1, grad_w2 = tf.gradients(loss, [w1, w2]) # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd. # \u8981\u5b9e\u9645\u66f4\u65b0\u6743\u91cd,\u6211\u4eec\u9700\u8981\u5728\u6267\u884c\u56fe\u65f6\u8bc4\u4f30new_w1\u548cnew_w2. # \u8bf7\u6ce8\u610f,\u5728TensorFlow\u4e2d,\u66f4\u65b0\u6743\u503c\u7684\u884c\u4e3a\u662f\u8ba1\u7b97\u56fe\u7684\u4e00\u90e8\u5206 # \u5728PyTorch\u4e2d,\u8fd9\u53d1\u751f\u5728\u8ba1\u7b97\u56fe\u4e4b\u5916. learning_rate = 1e-6 new_w1 = w1.assign(w1 - learning_rate * grad_w1) new_w2 = w2.assign(w2 - learning_rate * grad_w2) # \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u6784\u5efa\u4e86\u8ba1\u7b97\u56fe,\u6240\u4ee5\u6211\u4eec\u8f93\u5165\u4e00\u4e2aTensorFlow\u4f1a\u8bdd\u6765\u5b9e\u9645\u6267\u884c\u56fe. with tf.Session() as sess: # \u8fd0\u884c\u4e00\u6b21\u56fe\u5f62\u521d\u59cb\u5316\u53d8\u91cfw1\u548cw2. sess.run(tf.global_variables_initializer()) # \u521b\u5efa\u5305\u542b\u8f93\u5165x\u548c\u76ee\u6807y\u7684\u5b9e\u9645\u6570\u636e\u7684numpy\u6570\u7ec4 x_value = np.random.randn(N, D_in) y_value = np.random.randn(N, D_out) for _ in range(500): # \u591a\u6b21\u6267\u884c\u56fe. \u6bcf\u6b21\u6267\u884c\u65f6, # \u6211\u4eec\u90fd\u60f3\u5c06x_value\u7ed1\u5b9a\u5230x,\u5c06y_value\u7ed1\u5b9a\u5230y,\u7528feed_dict\u53c2\u6570\u6307\u5b9a. # \u6bcf\u6b21\u6211\u4eec\u6267\u884c\u56fe\u65f6,\u6211\u4eec\u90fd\u60f3\u8ba1\u7b97\u635f\u5931\u503cnew_w1 \u548c new_w2; # \u8fd9\u4e9b\u5f20\u91cf\u7684\u503c\u4f5c\u4e3anumpy\u6570\u7ec4\u8fd4\u56de. loss_value, _, _ = sess.run([loss, new_w1, new_w2], feed_dict={x: x_value, y: y_value}) print(loss_value) Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: tf_two_layer_net.py Download Jupyter notebook: tf_two_layer_net.ipynb Gallery generated by Sphinx-Gallery","title":"TensorFlow: \u9759\u6001\u56fe"},{"location":"18/","text":"PyTorch: nn\u5305 \u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u5b9e\u73b0\u4e2d\u7528PyTorch\u7684nn\u5305\u6765\u642d\u5efa\u795e\u7ecf\u7f51\u7edc. \u5982\u679c\u4f7f\u7528PyTorch\u7684autograd\u5305, \u5b9a\u4e49\u8ba1\u7b97\u56fe\u548c\u68af\u5ea6\u8ba1\u7b97\u5c06\u53d8\u5f97\u975e\u5e38\u5bb9\u6613. \u4f46\u662f\u5bf9\u4e8e\u4e00\u4e9b\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u6765\u8bf4, \u53ea\u7528autograd\u8fd8\u662f\u6709\u70b9\u5e95\u5c42\u4e86. \u8fd9\u6b63\u662fnn\u5305\u7684\u7528\u6b66\u4e4b\u5730. nn\u5305\u5b9a\u4e49\u4e86\u5f88\u591a\u6a21\u5757, \u4f60\u53ef\u4ee5\u628a\u5b83\u4eec\u5f53\u4f5c\u4e00\u4e2a\u4e2a\u7684\u795e\u7ecf\u7f51\u7edc\u5c42. \u6bcf\u4e2a\u6a21\u5757\u90fd\u6709\u8f93\u5165\u8f93\u51fa, \u5e76\u53ef\u80fd\u6709\u4e00\u4e9b\u53ef\u8bad\u7ec3\u6743\u91cd. import torch from torch.autograd import Variable # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u4f7f\u7528nn\u5305\u5c06\u6211\u4eec\u7684\u6a21\u578b\u5b9a\u4e49\u4e3a\u4e00\u7cfb\u5217\u56fe\u5c42. # nn.Sequential\u662f\u4e00\u4e2a\u5305\u542b\u5176\u4ed6\u6a21\u5757\u7684\u6a21\u5757,\u5e76\u5c06\u5b83\u4eec\u6309\u987a\u5e8f\u5e94\u7528\u4ee5\u4ea7\u751f\u5176\u8f93\u51fa. # \u6bcf\u4e2a\u7ebf\u6027\u6a21\u5757\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u8ba1\u7b97\u6765\u81ea\u8f93\u5165\u7684\u8f93\u51fa,\u5e76\u4fdd\u5b58\u5185\u90e8\u53d8\u91cf\u7684\u6743\u91cd\u548c\u504f\u5dee. model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) # nn\u5305\u8fd8\u5305\u542b\u6d41\u884c\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49; # \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u6211\u4eec\u5c06\u4f7f\u7528\u5747\u65b9\u5dee(MSE)\u4f5c\u4e3a\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570. loss_fn = torch.nn.MSELoss(size_average=False) learning_rate = 1e-4 for t in range(500): # \u6b63\u5411\u4f20\u9012:\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y. # \u6a21\u5757\u5bf9\u8c61\u4f1a\u8986\u76d6__call__\u8fd0\u7b97\u7b26,\u56e0\u6b64\u60a8\u53ef\u4ee5\u5c06\u5b83\u4eec\u79f0\u4e3a\u51fd\u6570. # \u8fd9\u6837\u505a\u65f6,\u60a8\u5c06\u8f93\u5165\u6570\u636e\u7684\u53d8\u91cf\u4f20\u9012\u7ed9\u6a21\u5757,\u5e76\u751f\u6210\u8f93\u51fa\u6570\u636e\u7684\u53d8\u91cf. y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931. # \u6211\u4eec\u4f20\u9012\u5305\u542by\u7684\u9884\u6d4b\u503c\u548c\u771f\u503c\u7684\u53d8\u91cf,\u5e76\u4e14\u635f\u5931\u51fd\u6570\u8fd4\u56de\u5305\u542b\u635f\u5931\u7684\u53d8\u91cf. loss = loss_fn(y_pred, y) print(t, loss.data[0]) # \u5728\u8fd0\u884c\u53cd\u5411\u4f20\u9012\u4e4b\u524d\u5c06\u68af\u5ea6\u5f52\u96f6. model.zero_grad() # \u5411\u540e\u4f20\u9012:\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u6a21\u578b\u7684\u6240\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u635f\u5931\u68af\u5ea6. # \u5728\u5185\u90e8,\u6bcf\u4e2a\u6a21\u5757\u7684\u53c2\u6570\u90fd\u5b58\u50a8\u5728\u53d8\u91cfrequire_grad = True\u4e2d, # \u56e0\u6b64\u8be5\u8c03\u7528\u5c06\u8ba1\u7b97\u6a21\u578b\u4e2d\u6240\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u68af\u5ea6. loss.backward() # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd. # \u6bcf\u4e2a\u53c2\u6570\u90fd\u662f\u4e00\u4e2a\u53d8\u91cf,\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u50cf\u6211\u4eec\u4ee5\u524d\u90a3\u6837\u8bbf\u95ee\u5b83\u7684\u6570\u636e\u548c\u68af\u5ea6. for param in model.parameters(): param.data -= learning_rate * param.grad.data Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_nn.py Download Jupyter notebook: two_layer_net_nn.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: nn\u5305"},{"location":"18/#pytorch-nn","text":"\u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u5b9e\u73b0\u4e2d\u7528PyTorch\u7684nn\u5305\u6765\u642d\u5efa\u795e\u7ecf\u7f51\u7edc. \u5982\u679c\u4f7f\u7528PyTorch\u7684autograd\u5305, \u5b9a\u4e49\u8ba1\u7b97\u56fe\u548c\u68af\u5ea6\u8ba1\u7b97\u5c06\u53d8\u5f97\u975e\u5e38\u5bb9\u6613. \u4f46\u662f\u5bf9\u4e8e\u4e00\u4e9b\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u6765\u8bf4, \u53ea\u7528autograd\u8fd8\u662f\u6709\u70b9\u5e95\u5c42\u4e86. \u8fd9\u6b63\u662fnn\u5305\u7684\u7528\u6b66\u4e4b\u5730. nn\u5305\u5b9a\u4e49\u4e86\u5f88\u591a\u6a21\u5757, \u4f60\u53ef\u4ee5\u628a\u5b83\u4eec\u5f53\u4f5c\u4e00\u4e2a\u4e2a\u7684\u795e\u7ecf\u7f51\u7edc\u5c42. \u6bcf\u4e2a\u6a21\u5757\u90fd\u6709\u8f93\u5165\u8f93\u51fa, \u5e76\u53ef\u80fd\u6709\u4e00\u4e9b\u53ef\u8bad\u7ec3\u6743\u91cd. import torch from torch.autograd import Variable # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u4f7f\u7528nn\u5305\u5c06\u6211\u4eec\u7684\u6a21\u578b\u5b9a\u4e49\u4e3a\u4e00\u7cfb\u5217\u56fe\u5c42. # nn.Sequential\u662f\u4e00\u4e2a\u5305\u542b\u5176\u4ed6\u6a21\u5757\u7684\u6a21\u5757,\u5e76\u5c06\u5b83\u4eec\u6309\u987a\u5e8f\u5e94\u7528\u4ee5\u4ea7\u751f\u5176\u8f93\u51fa. # \u6bcf\u4e2a\u7ebf\u6027\u6a21\u5757\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u8ba1\u7b97\u6765\u81ea\u8f93\u5165\u7684\u8f93\u51fa,\u5e76\u4fdd\u5b58\u5185\u90e8\u53d8\u91cf\u7684\u6743\u91cd\u548c\u504f\u5dee. model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) # nn\u5305\u8fd8\u5305\u542b\u6d41\u884c\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49; # \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u6211\u4eec\u5c06\u4f7f\u7528\u5747\u65b9\u5dee(MSE)\u4f5c\u4e3a\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570. loss_fn = torch.nn.MSELoss(size_average=False) learning_rate = 1e-4 for t in range(500): # \u6b63\u5411\u4f20\u9012:\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y. # \u6a21\u5757\u5bf9\u8c61\u4f1a\u8986\u76d6__call__\u8fd0\u7b97\u7b26,\u56e0\u6b64\u60a8\u53ef\u4ee5\u5c06\u5b83\u4eec\u79f0\u4e3a\u51fd\u6570. # \u8fd9\u6837\u505a\u65f6,\u60a8\u5c06\u8f93\u5165\u6570\u636e\u7684\u53d8\u91cf\u4f20\u9012\u7ed9\u6a21\u5757,\u5e76\u751f\u6210\u8f93\u51fa\u6570\u636e\u7684\u53d8\u91cf. y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931. # \u6211\u4eec\u4f20\u9012\u5305\u542by\u7684\u9884\u6d4b\u503c\u548c\u771f\u503c\u7684\u53d8\u91cf,\u5e76\u4e14\u635f\u5931\u51fd\u6570\u8fd4\u56de\u5305\u542b\u635f\u5931\u7684\u53d8\u91cf. loss = loss_fn(y_pred, y) print(t, loss.data[0]) # \u5728\u8fd0\u884c\u53cd\u5411\u4f20\u9012\u4e4b\u524d\u5c06\u68af\u5ea6\u5f52\u96f6. model.zero_grad() # \u5411\u540e\u4f20\u9012:\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u6a21\u578b\u7684\u6240\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u635f\u5931\u68af\u5ea6. # \u5728\u5185\u90e8,\u6bcf\u4e2a\u6a21\u5757\u7684\u53c2\u6570\u90fd\u5b58\u50a8\u5728\u53d8\u91cfrequire_grad = True\u4e2d, # \u56e0\u6b64\u8be5\u8c03\u7528\u5c06\u8ba1\u7b97\u6a21\u578b\u4e2d\u6240\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u68af\u5ea6. loss.backward() # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd. # \u6bcf\u4e2a\u53c2\u6570\u90fd\u662f\u4e00\u4e2a\u53d8\u91cf,\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u50cf\u6211\u4eec\u4ee5\u524d\u90a3\u6837\u8bbf\u95ee\u5b83\u7684\u6570\u636e\u548c\u68af\u5ea6. for param in model.parameters(): param.data -= learning_rate * param.grad.data Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_nn.py Download Jupyter notebook: two_layer_net_nn.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: nn\u5305"},{"location":"19/","text":"PyTorch: optim\u5305 \u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04 \u5728\u6b64\u5b9e\u73b0\u4e2d, \u6211\u4eec\u5c06\u5f03\u7528\u4e4b\u524d\u624b\u5de5\u66f4\u65b0\u6743\u503c\u7684\u505a\u6cd5, \u8f6c\u800c\u7528PyTorch\u7684nn\u5305\u6765\u642d\u5efa\u795e\u7ecf\u7f51\u7edc. optim\u5305\u5219\u7528\u6765\u5b9a\u4e49\u66f4\u65b0\u6743\u503c\u7684\u4f18\u5316\u5668. optim\u5305\u6709\u4f17\u591a\u6df1\u5ea6\u5b66\u4e60\u5e38\u7528\u7684\u4f18\u5316\u7b97\u6cd5, \u5305\u62ecSGD+momentum, RMSProp, Adam\u7b49. import torch from torch.autograd import Variable # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u4f7f\u7528nn\u5305\u6765\u5b9a\u4e49\u6211\u4eec\u7684\u6a21\u578b\u548c\u635f\u5931\u51fd\u6570. model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) loss_fn = torch.nn.MSELoss(size_average=False) # \u4f7f\u7528\u4f18\u5316\u5305\u6765\u5b9a\u4e49\u4e00\u4e2a\u4f18\u5316\u5668,\u5b83\u5c06\u4e3a\u6211\u4eec\u66f4\u65b0\u6a21\u578b\u7684\u6743\u91cd. # \u5728\u8fd9\u91cc,\u6211\u4eec\u5c06\u4f7f\u7528 Adam;\u8fd9\u4e2a optim \u5305\u5305\u542b\u8bb8\u591a\u5176\u4ed6\u4f18\u5316\u7b97\u6cd5. # Adam\u6784\u9020\u51fd\u6570\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u544a\u8bc9\u4f18\u5316\u5668\u5e94\u8be5\u66f4\u65b0\u54ea\u4e2aVariables. learning_rate = 1e-4 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) for t in range(500): # \u6b63\u5411\u4f20\u9012:\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y. y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931\u51fd\u6570. loss = loss_fn(y_pred, y) print(t, loss.data[0]) # \u5728\u5411\u540e\u4f20\u9012\u4e4b\u524d,\u4f7f\u7528\u4f18\u5316\u5668\u5bf9\u8c61\u4e3a\u5176\u8981\u66f4\u65b0\u7684\u53d8\u91cf\uff08\u8fd9\u662f\u6a21\u578b\u7684\u53ef\u5b66\u4e60\u6743\u91cd\uff09\u7684\u6240\u6709\u68af\u5ea6\u5f52\u96f6. # \u8fd9\u662f\u56e0\u4e3a\u9ed8\u8ba4\u60c5\u51b5\u4e0b,\u53ea\u8981\u8c03\u7528.backward(),\u68af\u5ea6\u5c31\u4f1a\u5728\u7f13\u51b2\u533a\u4e2d\u7d2f\u79ef(\u5373\u4e0d\u4f1a\u88ab\u8986\u76d6). # \u67e5\u770btorch.autograd.backward\u7684\u6587\u6863\u4ee5\u83b7\u53d6\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f. optimizer.zero_grad() # \u5411\u540e\u4f20\u9012:\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u76f8\u5bf9\u4e8e\u6a21\u578b\u53c2\u6570\u7684\u68af\u5ea6 loss.backward() # \u5728\u4f18\u5316\u5668\u4e0a\u8c03\u7528step\u51fd\u6570\u4f1a\u66f4\u65b0\u5176\u53c2\u6570 optimizer.step() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_optim.py Download Jupyter notebook: two_layer_net_optim.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: optim\u5305"},{"location":"19/#pytorch-optim","text":"\u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04 \u5728\u6b64\u5b9e\u73b0\u4e2d, \u6211\u4eec\u5c06\u5f03\u7528\u4e4b\u524d\u624b\u5de5\u66f4\u65b0\u6743\u503c\u7684\u505a\u6cd5, \u8f6c\u800c\u7528PyTorch\u7684nn\u5305\u6765\u642d\u5efa\u795e\u7ecf\u7f51\u7edc. optim\u5305\u5219\u7528\u6765\u5b9a\u4e49\u66f4\u65b0\u6743\u503c\u7684\u4f18\u5316\u5668. optim\u5305\u6709\u4f17\u591a\u6df1\u5ea6\u5b66\u4e60\u5e38\u7528\u7684\u4f18\u5316\u7b97\u6cd5, \u5305\u62ecSGD+momentum, RMSProp, Adam\u7b49. import torch from torch.autograd import Variable # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u4f7f\u7528nn\u5305\u6765\u5b9a\u4e49\u6211\u4eec\u7684\u6a21\u578b\u548c\u635f\u5931\u51fd\u6570. model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) loss_fn = torch.nn.MSELoss(size_average=False) # \u4f7f\u7528\u4f18\u5316\u5305\u6765\u5b9a\u4e49\u4e00\u4e2a\u4f18\u5316\u5668,\u5b83\u5c06\u4e3a\u6211\u4eec\u66f4\u65b0\u6a21\u578b\u7684\u6743\u91cd. # \u5728\u8fd9\u91cc,\u6211\u4eec\u5c06\u4f7f\u7528 Adam;\u8fd9\u4e2a optim \u5305\u5305\u542b\u8bb8\u591a\u5176\u4ed6\u4f18\u5316\u7b97\u6cd5. # Adam\u6784\u9020\u51fd\u6570\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u544a\u8bc9\u4f18\u5316\u5668\u5e94\u8be5\u66f4\u65b0\u54ea\u4e2aVariables. learning_rate = 1e-4 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) for t in range(500): # \u6b63\u5411\u4f20\u9012:\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y. y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931\u51fd\u6570. loss = loss_fn(y_pred, y) print(t, loss.data[0]) # \u5728\u5411\u540e\u4f20\u9012\u4e4b\u524d,\u4f7f\u7528\u4f18\u5316\u5668\u5bf9\u8c61\u4e3a\u5176\u8981\u66f4\u65b0\u7684\u53d8\u91cf\uff08\u8fd9\u662f\u6a21\u578b\u7684\u53ef\u5b66\u4e60\u6743\u91cd\uff09\u7684\u6240\u6709\u68af\u5ea6\u5f52\u96f6. # \u8fd9\u662f\u56e0\u4e3a\u9ed8\u8ba4\u60c5\u51b5\u4e0b,\u53ea\u8981\u8c03\u7528.backward(),\u68af\u5ea6\u5c31\u4f1a\u5728\u7f13\u51b2\u533a\u4e2d\u7d2f\u79ef(\u5373\u4e0d\u4f1a\u88ab\u8986\u76d6). # \u67e5\u770btorch.autograd.backward\u7684\u6587\u6863\u4ee5\u83b7\u53d6\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f. optimizer.zero_grad() # \u5411\u540e\u4f20\u9012:\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u76f8\u5bf9\u4e8e\u6a21\u578b\u53c2\u6570\u7684\u68af\u5ea6 loss.backward() # \u5728\u4f18\u5316\u5668\u4e0a\u8c03\u7528step\u51fd\u6570\u4f1a\u66f4\u65b0\u5176\u53c2\u6570 optimizer.step() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_optim.py Download Jupyter notebook: two_layer_net_optim.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: optim\u5305"},{"location":"2/","text":"PyTorch \u662f\u4ec0\u4e48\uff1f \u5b83\u662f\u4e00\u4e2a\u57fa\u4e8e Python \u7684\u79d1\u5b66\u8ba1\u7b97\u5305, \u5176\u4e3b\u8981\u662f\u4e3a\u4e86\u89e3\u51b3\u4e24\u7c7b\u573a\u666f: NumPy \u7684\u66ff\u4ee3\u54c1, \u4ee5\u4f7f\u7528 GPU \u7684\u5f3a\u5927\u52a0\u901f\u529f\u80fd \u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u5e73\u53f0, \u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u901f\u5ea6 \u65b0\u624b\u5165\u95e8 Tensors\uff08\u5f20\u91cf\uff09 Tensors \u4e0e NumPy \u7684 ndarrays \u975e\u5e38\u76f8\u4f3c, \u9664\u6b64\u4e4b\u5916\u8fd8\u53ef\u4ee5\u5728 GPU \u4e0a\u4f7f\u7528\u5f20\u91cf\u6765\u52a0\u901f\u8ba1\u7b97. from __future__ import print_function import torch \u6784\u5efa\u4e00\u4e2a 5x3 \u7684\u77e9\u9635, \u672a\u521d\u59cb\u5316\u7684: x = torch.Tensor(5, 3) print(x) \u6784\u5efa\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u7684\u77e9\u9635: x = torch.rand(5, 3) print(x) \u83b7\u5f97 size: print(x.size()) Note torch.Size \u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a tuple\uff08\u5143\u7ec4\uff09, \u6240\u4ee5\u5b83\u652f\u6301\u6240\u6709 tuple\uff08\u5143\u7ec4\uff09\u7684\u64cd\u4f5c. \u64cd\u4f5c \u9488\u5bf9\u64cd\u4f5c\u6709\u8bb8\u591a\u8bed\u6cd5. \u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d, \u6211\u4eec\u6765\u770b\u770b\u52a0\u6cd5\u8fd0\u7b97. \u52a0\u6cd5: \u8bed\u6cd5 1 y = torch.rand(5, 3) print(x + y) \u52a0\u6cd5: \u8bed\u6cd5 2 print(torch.add(x, y)) \u52a0\u6cd5: \u63d0\u4f9b\u4e00\u4e2a\u8f93\u51fa tensor \u4f5c\u4e3a\u53c2\u6570 result = torch.Tensor(5, 3) torch.add(x, y, out = result) print(result) \u52a0\u6cd5: in-place\uff08\u5c31\u5730\u64cd\u4f5c\uff09 # adds x to y y.add_(x) print(y) Note \u4efb\u4f55\u6539\u53d8\u5f20\u91cf\u7684\u64cd\u4f5c\u65b9\u6cd5\u90fd\u662f\u4ee5\u540e\u7f00 _ \u7ed3\u5c3e\u7684. \u4f8b\u5982: x.copy_(y) , x.t_() , \u5c06\u6539\u53d8\u5f20\u91cf x . \u4f60\u53ef\u4ee5\u7528\u7c7b\u4f3cNumpy\u7684\u7d22\u5f15\u6765\u5904\u7406\u6240\u6709\u7684\u5f20\u91cf\uff01 print(x[:, 1]) \u6539\u53d8\u5927\u5c0f: \u5982\u679c\u4f60\u60f3\u8981\u53bb\u6539\u53d8tensor\u7684\u5927\u5c0f, \u53ef\u4ee5\u4f7f\u7528 torch.view : x = torch.randn(4, 4) y = x.view(16) z = x.view(-1, 8) # the size -1 is inferred from other dimensions print(x.size(), y.size(), z.size()) \u7a0d\u5019\u9605\u8bfb: 100+ Tensor \u64cd\u4f5c, \u5305\u62ec\u6362\u4f4d, \u7d22\u5f15, \u5207\u7247, \u6570\u5b66\u8fd0\u7b97, \u7ebf\u6027\u4ee3\u6570, \u968f\u673a\u6570, \u7b49\u7b49, \u90fd\u5728 \u8fd9\u91cc \u6709\u63cf\u8ff0. NumPy Bridge \u5c06\u4e00\u4e2a Torch Tensor \u8f6c\u6362\u4e3a NumPy \u6570\u7ec4, \u53cd\u4e4b\u4ea6\u7136. Torch Tensor \u548c NumPy \u6570\u7ec4\u5c06\u4f1a\u5171\u4eab\u5b83\u4eec\u7684\u5b9e\u9645\u7684\u5185\u5b58\u4f4d\u7f6e, \u6539\u53d8\u4e00\u4e2a\u53e6\u4e00\u4e2a\u4e5f\u4f1a\u8ddf\u7740\u6539\u53d8. \u8f6c\u6362\u4e00\u4e2a Torch Tensor \u4e3a NumPy \u6570\u7ec4 a = torch.ones(5) print(a) b = a.numpy() print(b) \u67e5\u770b numpy \u6570\u7ec4\u662f\u5982\u4f55\u6539\u53d8\u7684. a.add_(1) print(a) print(b) \u8f6c\u6362 NumPy \u6570\u7ec4\u4e3a Torch Tensor \u770b\u770b\u6539\u53d8 np \u6570\u7ec4\u4e4b\u540e Torch Tensor \u662f\u5982\u4f55\u81ea\u52a8\u6539\u53d8\u7684 import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out = a) print(a) print(b) \u9664\u4e86 CharTensor \u4e4b\u5916, CPU \u4e0a\u7684\u6240\u6709 Tensor \u90fd\u652f\u6301\u4e0eNumpy\u8fdb\u884c\u4e92\u76f8\u8f6c\u6362 CUDA Tensors \u53ef\u4ee5\u4f7f\u7528 .cuda \u65b9\u6cd5\u5c06 Tensors \u5728GPU\u4e0a\u8fd0\u884c. # \u53ea\u8981\u5728 CUDA \u662f\u53ef\u7528\u7684\u60c5\u51b5\u4e0b, \u6211\u4eec\u53ef\u4ee5\u8fd0\u884c\u8fd9\u6bb5\u4ee3\u7801 if torch.cuda.is_available(): x = x.cuda() y = y.cuda() x + y Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: tensor_tutorial.py Download Jupyter notebook: tensor_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch \u662f\u4ec0\u4e48\uff1f"},{"location":"2/#pytorch","text":"\u5b83\u662f\u4e00\u4e2a\u57fa\u4e8e Python \u7684\u79d1\u5b66\u8ba1\u7b97\u5305, \u5176\u4e3b\u8981\u662f\u4e3a\u4e86\u89e3\u51b3\u4e24\u7c7b\u573a\u666f: NumPy \u7684\u66ff\u4ee3\u54c1, \u4ee5\u4f7f\u7528 GPU \u7684\u5f3a\u5927\u52a0\u901f\u529f\u80fd \u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u5e73\u53f0, \u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u901f\u5ea6","title":"PyTorch \u662f\u4ec0\u4e48\uff1f"},{"location":"2/#_1","text":"","title":"\u65b0\u624b\u5165\u95e8"},{"location":"2/#tensors","text":"Tensors \u4e0e NumPy \u7684 ndarrays \u975e\u5e38\u76f8\u4f3c, \u9664\u6b64\u4e4b\u5916\u8fd8\u53ef\u4ee5\u5728 GPU \u4e0a\u4f7f\u7528\u5f20\u91cf\u6765\u52a0\u901f\u8ba1\u7b97. from __future__ import print_function import torch \u6784\u5efa\u4e00\u4e2a 5x3 \u7684\u77e9\u9635, \u672a\u521d\u59cb\u5316\u7684: x = torch.Tensor(5, 3) print(x) \u6784\u5efa\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u7684\u77e9\u9635: x = torch.rand(5, 3) print(x) \u83b7\u5f97 size: print(x.size()) Note torch.Size \u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a tuple\uff08\u5143\u7ec4\uff09, \u6240\u4ee5\u5b83\u652f\u6301\u6240\u6709 tuple\uff08\u5143\u7ec4\uff09\u7684\u64cd\u4f5c.","title":"Tensors\uff08\u5f20\u91cf\uff09"},{"location":"2/#_2","text":"\u9488\u5bf9\u64cd\u4f5c\u6709\u8bb8\u591a\u8bed\u6cd5. \u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d, \u6211\u4eec\u6765\u770b\u770b\u52a0\u6cd5\u8fd0\u7b97. \u52a0\u6cd5: \u8bed\u6cd5 1 y = torch.rand(5, 3) print(x + y) \u52a0\u6cd5: \u8bed\u6cd5 2 print(torch.add(x, y)) \u52a0\u6cd5: \u63d0\u4f9b\u4e00\u4e2a\u8f93\u51fa tensor \u4f5c\u4e3a\u53c2\u6570 result = torch.Tensor(5, 3) torch.add(x, y, out = result) print(result) \u52a0\u6cd5: in-place\uff08\u5c31\u5730\u64cd\u4f5c\uff09 # adds x to y y.add_(x) print(y) Note \u4efb\u4f55\u6539\u53d8\u5f20\u91cf\u7684\u64cd\u4f5c\u65b9\u6cd5\u90fd\u662f\u4ee5\u540e\u7f00 _ \u7ed3\u5c3e\u7684. \u4f8b\u5982: x.copy_(y) , x.t_() , \u5c06\u6539\u53d8\u5f20\u91cf x . \u4f60\u53ef\u4ee5\u7528\u7c7b\u4f3cNumpy\u7684\u7d22\u5f15\u6765\u5904\u7406\u6240\u6709\u7684\u5f20\u91cf\uff01 print(x[:, 1]) \u6539\u53d8\u5927\u5c0f: \u5982\u679c\u4f60\u60f3\u8981\u53bb\u6539\u53d8tensor\u7684\u5927\u5c0f, \u53ef\u4ee5\u4f7f\u7528 torch.view : x = torch.randn(4, 4) y = x.view(16) z = x.view(-1, 8) # the size -1 is inferred from other dimensions print(x.size(), y.size(), z.size()) \u7a0d\u5019\u9605\u8bfb: 100+ Tensor \u64cd\u4f5c, \u5305\u62ec\u6362\u4f4d, \u7d22\u5f15, \u5207\u7247, \u6570\u5b66\u8fd0\u7b97, \u7ebf\u6027\u4ee3\u6570, \u968f\u673a\u6570, \u7b49\u7b49, \u90fd\u5728 \u8fd9\u91cc \u6709\u63cf\u8ff0.","title":"\u64cd\u4f5c"},{"location":"2/#numpy-bridge","text":"\u5c06\u4e00\u4e2a Torch Tensor \u8f6c\u6362\u4e3a NumPy \u6570\u7ec4, \u53cd\u4e4b\u4ea6\u7136. Torch Tensor \u548c NumPy \u6570\u7ec4\u5c06\u4f1a\u5171\u4eab\u5b83\u4eec\u7684\u5b9e\u9645\u7684\u5185\u5b58\u4f4d\u7f6e, \u6539\u53d8\u4e00\u4e2a\u53e6\u4e00\u4e2a\u4e5f\u4f1a\u8ddf\u7740\u6539\u53d8.","title":"NumPy Bridge"},{"location":"2/#torch-tensor-numpy","text":"a = torch.ones(5) print(a) b = a.numpy() print(b) \u67e5\u770b numpy \u6570\u7ec4\u662f\u5982\u4f55\u6539\u53d8\u7684. a.add_(1) print(a) print(b)","title":"\u8f6c\u6362\u4e00\u4e2a Torch Tensor \u4e3a NumPy \u6570\u7ec4"},{"location":"2/#numpy-torch-tensor","text":"\u770b\u770b\u6539\u53d8 np \u6570\u7ec4\u4e4b\u540e Torch Tensor \u662f\u5982\u4f55\u81ea\u52a8\u6539\u53d8\u7684 import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out = a) print(a) print(b) \u9664\u4e86 CharTensor \u4e4b\u5916, CPU \u4e0a\u7684\u6240\u6709 Tensor \u90fd\u652f\u6301\u4e0eNumpy\u8fdb\u884c\u4e92\u76f8\u8f6c\u6362","title":"\u8f6c\u6362 NumPy \u6570\u7ec4\u4e3a Torch Tensor"},{"location":"2/#cuda-tensors","text":"\u53ef\u4ee5\u4f7f\u7528 .cuda \u65b9\u6cd5\u5c06 Tensors \u5728GPU\u4e0a\u8fd0\u884c. # \u53ea\u8981\u5728 CUDA \u662f\u53ef\u7528\u7684\u60c5\u51b5\u4e0b, \u6211\u4eec\u53ef\u4ee5\u8fd0\u884c\u8fd9\u6bb5\u4ee3\u7801 if torch.cuda.is_available(): x = x.cuda() y = y.cuda() x + y Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: tensor_tutorial.py Download Jupyter notebook: tensor_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"CUDA Tensors"},{"location":"20/","text":"PyTorch: \u5b9a\u5236\u5316nn\u6a21\u5757 \u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u5728\u5b9e\u73b0\u4e2d\u6211\u4eec\u5c06\u5b9a\u4e49\u4e00\u4e2a\u5b9a\u5236\u5316\u7684\u6a21\u5757\u5b50\u7c7b. \u5982\u679c\u5df2\u6709\u6a21\u5757\u4e32\u8d77\u6765\u4e0d\u80fd\u6ee1\u8db3\u4f60\u7684\u590d\u6742\u9700\u6c42, \u90a3\u4e48\u4f60\u5c31\u80fd\u4ee5\u8fd9\u79cd\u65b9\u5f0f\u6765\u5b9a\u4e49\u81ea\u5df1\u7684\u6a21\u5757\u3002 import torch from torch.autograd import Variable class TwoLayerNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" \u5728\u6784\u9020\u51fd\u6570\u4e2d,\u6211\u4eec\u5b9e\u4f8b\u5316\u4e24\u4e2ann.Linear\u6a21\u5757\u5e76\u5c06\u5b83\u4eec\u5206\u914d\u4e3a\u6210\u5458\u53d8\u91cf. \"\"\" super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" \u5728forward\u51fd\u6570\u4e2d,\u6211\u4eec\u63a5\u53d7\u4e00\u4e2a\u53d8\u91cf\u7684\u8f93\u5165\u6570\u636e,\u6211\u4eec\u5fc5\u987b\u8fd4\u56de\u4e00\u4e2a\u53d8\u91cf\u7684\u8f93\u51fa\u6570\u636e. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6784\u9020\u51fd\u6570\u4e2d\u5b9a\u4e49\u7684\u6a21\u5757\u4ee5\u53ca\u53d8\u91cf\u4e0a\u7684\u4efb\u610f\u8fd0\u7b97\u7b26. \"\"\" h_relu = self.linear1(x).clamp(min=0) y_pred = self.linear2(h_relu) return y_pred # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u901a\u8fc7\u5b9e\u4f8b\u5316\u4e0a\u9762\u5b9a\u4e49\u7684\u7c7b\u6765\u6784\u5efa\u6211\u4eec\u7684\u6a21\u578b model = TwoLayerNet(D_in, H, D_out) # \u6784\u5efa\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668. # \u5bf9SGD\u6784\u9020\u51fd\u6570\u4e2d\u7684model.parameters()\u7684\u8c03\u7528\u5c06\u5305\u542b\u4f5c\u4e3a\u6a21\u578b\u6210\u5458\u7684\u4e24\u4e2ann.Linear\u6a21\u5757\u7684\u53ef\u5b66\u4e60\u53c2\u6570. criterion = torch.nn.MSELoss(size_average=False) optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) for t in range(500): # \u6b63\u5411\u4f20\u9012\uff1a\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931 loss = criterion(y_pred, y) print(t, loss.data[0]) # \u68af\u5ea6\u7f6e\u96f6, \u6267\u884c\u53cd\u5411\u4f20\u9012\u5e76\u66f4\u65b0\u6743\u91cd. optimizer.zero_grad() loss.backward() optimizer.step() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_module.py Download Jupyter notebook: two_layer_net_module.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: \u5b9a\u5236\u5316nn\u6a21\u5757"},{"location":"20/#pytorch-nn","text":"\u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04. \u5728\u5b9e\u73b0\u4e2d\u6211\u4eec\u5c06\u5b9a\u4e49\u4e00\u4e2a\u5b9a\u5236\u5316\u7684\u6a21\u5757\u5b50\u7c7b. \u5982\u679c\u5df2\u6709\u6a21\u5757\u4e32\u8d77\u6765\u4e0d\u80fd\u6ee1\u8db3\u4f60\u7684\u590d\u6742\u9700\u6c42, \u90a3\u4e48\u4f60\u5c31\u80fd\u4ee5\u8fd9\u79cd\u65b9\u5f0f\u6765\u5b9a\u4e49\u81ea\u5df1\u7684\u6a21\u5757\u3002 import torch from torch.autograd import Variable class TwoLayerNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" \u5728\u6784\u9020\u51fd\u6570\u4e2d,\u6211\u4eec\u5b9e\u4f8b\u5316\u4e24\u4e2ann.Linear\u6a21\u5757\u5e76\u5c06\u5b83\u4eec\u5206\u914d\u4e3a\u6210\u5458\u53d8\u91cf. \"\"\" super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" \u5728forward\u51fd\u6570\u4e2d,\u6211\u4eec\u63a5\u53d7\u4e00\u4e2a\u53d8\u91cf\u7684\u8f93\u5165\u6570\u636e,\u6211\u4eec\u5fc5\u987b\u8fd4\u56de\u4e00\u4e2a\u53d8\u91cf\u7684\u8f93\u51fa\u6570\u636e. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6784\u9020\u51fd\u6570\u4e2d\u5b9a\u4e49\u7684\u6a21\u5757\u4ee5\u53ca\u53d8\u91cf\u4e0a\u7684\u4efb\u610f\u8fd0\u7b97\u7b26. \"\"\" h_relu = self.linear1(x).clamp(min=0) y_pred = self.linear2(h_relu) return y_pred # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u901a\u8fc7\u5b9e\u4f8b\u5316\u4e0a\u9762\u5b9a\u4e49\u7684\u7c7b\u6765\u6784\u5efa\u6211\u4eec\u7684\u6a21\u578b model = TwoLayerNet(D_in, H, D_out) # \u6784\u5efa\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668. # \u5bf9SGD\u6784\u9020\u51fd\u6570\u4e2d\u7684model.parameters()\u7684\u8c03\u7528\u5c06\u5305\u542b\u4f5c\u4e3a\u6a21\u578b\u6210\u5458\u7684\u4e24\u4e2ann.Linear\u6a21\u5757\u7684\u53ef\u5b66\u4e60\u53c2\u6570. criterion = torch.nn.MSELoss(size_average=False) optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) for t in range(500): # \u6b63\u5411\u4f20\u9012\uff1a\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931 loss = criterion(y_pred, y) print(t, loss.data[0]) # \u68af\u5ea6\u7f6e\u96f6, \u6267\u884c\u53cd\u5411\u4f20\u9012\u5e76\u66f4\u65b0\u6743\u91cd. optimizer.zero_grad() loss.backward() optimizer.step() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: two_layer_net_module.py Download Jupyter notebook: two_layer_net_module.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: \u5b9a\u5236\u5316nn\u6a21\u5757"},{"location":"21/","text":"PyTorch: \u52a8\u6001\u63a7\u5236\u6d41\u7a0b + \u6743\u91cd\u5171\u4eab \u4e3a\u4e86\u5c55\u793aPyTorch\u7684\u52a8\u6001\u56fe\u7684\u5f3a\u5927, \u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u975e\u5e38\u5947\u5f02\u7684\u6a21\u578b: \u4e00\u4e2a\u5168\u8fde\u63a5\u7684ReLU\u6fc0\u6d3b\u7684\u795e\u7ecf\u7f51\u7edc, \u6bcf\u6b21\u524d\u5411\u8ba1\u7b97\u65f6\u90fd\u968f\u673a\u9009\u4e00\u4e2a1\u52304\u4e4b\u95f4\u7684\u6570\u5b57n, \u7136\u540e\u63a5\u4e0b\u6765\u5c31\u6709n\u5c42\u9690\u85cf\u5c42, \u6bcf\u4e2a\u9690\u85cf\u5c42\u7684\u8fde\u63a5\u6743\u91cd\u5171\u4eab. import random import torch from torch.autograd import Variable class DynamicNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" \u5728\u6784\u9020\u51fd\u6570\u4e2d,\u6211\u4eec\u6784\u9020\u4e86\u4e09\u4e2ann.Linear\u5b9e\u4f8b,\u6211\u4eec\u5c06\u5728\u6b63\u5411\u4f20\u9012\u4e2d\u4f7f\u7528\u5b83\u4eec. \"\"\" super(DynamicNet, self).__init__() self.input_linear = torch.nn.Linear(D_in, H) self.middle_linear = torch.nn.Linear(H, H) self.output_linear = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" \u5bf9\u4e8e\u6a21\u578b\u7684\u6b63\u5411\u901a\u9053,\u6211\u4eec\u968f\u673a\u9009\u62e90,1,2\u62163, \u5e76\u91cd\u590d\u4f7f\u7528\u591a\u6b21\u8ba1\u7b97\u9690\u85cf\u5c42\u8868\u793a\u7684middle_linear\u6a21\u5757. \u7531\u4e8e\u6bcf\u4e2a\u6b63\u5411\u901a\u9053\u90fd\u4f1a\u751f\u6210\u4e00\u4e2a\u52a8\u6001\u8ba1\u7b97\u56fe,\u56e0\u6b64\u5728\u5b9a\u4e49\u6a21\u578b\u7684\u6b63\u5411\u901a\u9053\u65f6, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u666e\u901a\u7684Python\u63a7\u5236\u6d41\u64cd\u4f5c\u7b26(\u5982\u5faa\u73af\u6216\u6761\u4ef6\u8bed\u53e5). \u5728\u8fd9\u91cc\u6211\u4eec\u4e5f\u770b\u5230,\u5b9a\u4e49\u8ba1\u7b97\u56fe\u65f6\u591a\u6b21\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u6a21\u5757\u662f\u5b8c\u5168\u5b89\u5168\u7684. \u8fd9\u662fLua Torch\u7684\u4e00\u5927\u6539\u8fdb,\u6bcf\u4e2a\u6a21\u5757\u53ea\u80fd\u4f7f\u7528\u4e00\u6b21. \"\"\" h_relu = self.input_linear(x).clamp(min=0) for _ in range(random.randint(0, 3)): h_relu = self.middle_linear(h_relu).clamp(min=0) y_pred = self.output_linear(h_relu) return y_pred # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u901a\u8fc7\u5b9e\u4f8b\u5316\u4e0a\u9762\u5b9a\u4e49\u7684\u7c7b\u6765\u6784\u5efa\u6211\u4eec\u7684\u6a21\u578b model = DynamicNet(D_in, H, D_out) # \u6784\u5efa\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668. # \u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u8fd9\u4e2a\u5947\u602a\u7684\u6a21\u578b\u975e\u5e38\u56f0\u96be,\u6240\u4ee5\u6211\u4eec\u4f7f\u7528\u52a8\u91cf criterion = torch.nn.MSELoss(size_average=False) optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9) for t in range(500): # \u6b63\u5411\u4f20\u9012:\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931 loss = criterion(y_pred, y) print(t, loss.data[0]) # \u96f6\u68af\u5ea6\u6267\u884c\u53cd\u5411\u4f20\u9012\u5e76\u66f4\u65b0\u6743\u91cd. optimizer.zero_grad() loss.backward() optimizer.step() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: dynamic_net.py Download Jupyter notebook: dynamic_net.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: \u52a8\u6001\u63a7\u5236\u6d41\u7a0b + \u6743\u91cd\u5171\u4eab"},{"location":"21/#pytorch","text":"\u4e3a\u4e86\u5c55\u793aPyTorch\u7684\u52a8\u6001\u56fe\u7684\u5f3a\u5927, \u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u975e\u5e38\u5947\u5f02\u7684\u6a21\u578b: \u4e00\u4e2a\u5168\u8fde\u63a5\u7684ReLU\u6fc0\u6d3b\u7684\u795e\u7ecf\u7f51\u7edc, \u6bcf\u6b21\u524d\u5411\u8ba1\u7b97\u65f6\u90fd\u968f\u673a\u9009\u4e00\u4e2a1\u52304\u4e4b\u95f4\u7684\u6570\u5b57n, \u7136\u540e\u63a5\u4e0b\u6765\u5c31\u6709n\u5c42\u9690\u85cf\u5c42, \u6bcf\u4e2a\u9690\u85cf\u5c42\u7684\u8fde\u63a5\u6743\u91cd\u5171\u4eab. import random import torch from torch.autograd import Variable class DynamicNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" \u5728\u6784\u9020\u51fd\u6570\u4e2d,\u6211\u4eec\u6784\u9020\u4e86\u4e09\u4e2ann.Linear\u5b9e\u4f8b,\u6211\u4eec\u5c06\u5728\u6b63\u5411\u4f20\u9012\u4e2d\u4f7f\u7528\u5b83\u4eec. \"\"\" super(DynamicNet, self).__init__() self.input_linear = torch.nn.Linear(D_in, H) self.middle_linear = torch.nn.Linear(H, H) self.output_linear = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" \u5bf9\u4e8e\u6a21\u578b\u7684\u6b63\u5411\u901a\u9053,\u6211\u4eec\u968f\u673a\u9009\u62e90,1,2\u62163, \u5e76\u91cd\u590d\u4f7f\u7528\u591a\u6b21\u8ba1\u7b97\u9690\u85cf\u5c42\u8868\u793a\u7684middle_linear\u6a21\u5757. \u7531\u4e8e\u6bcf\u4e2a\u6b63\u5411\u901a\u9053\u90fd\u4f1a\u751f\u6210\u4e00\u4e2a\u52a8\u6001\u8ba1\u7b97\u56fe,\u56e0\u6b64\u5728\u5b9a\u4e49\u6a21\u578b\u7684\u6b63\u5411\u901a\u9053\u65f6, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u666e\u901a\u7684Python\u63a7\u5236\u6d41\u64cd\u4f5c\u7b26(\u5982\u5faa\u73af\u6216\u6761\u4ef6\u8bed\u53e5). \u5728\u8fd9\u91cc\u6211\u4eec\u4e5f\u770b\u5230,\u5b9a\u4e49\u8ba1\u7b97\u56fe\u65f6\u591a\u6b21\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u6a21\u5757\u662f\u5b8c\u5168\u5b89\u5168\u7684. \u8fd9\u662fLua Torch\u7684\u4e00\u5927\u6539\u8fdb,\u6bcf\u4e2a\u6a21\u5757\u53ea\u80fd\u4f7f\u7528\u4e00\u6b21. \"\"\" h_relu = self.input_linear(x).clamp(min=0) for _ in range(random.randint(0, 3)): h_relu = self.middle_linear(h_relu).clamp(min=0) y_pred = self.output_linear(h_relu) return y_pred # N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8; # H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8. N, D_in, H, D_out = 64, 1000, 100, 10 # \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d. x = Variable(torch.randn(N, D_in)) y = Variable(torch.randn(N, D_out), requires_grad=False) # \u901a\u8fc7\u5b9e\u4f8b\u5316\u4e0a\u9762\u5b9a\u4e49\u7684\u7c7b\u6765\u6784\u5efa\u6211\u4eec\u7684\u6a21\u578b model = DynamicNet(D_in, H, D_out) # \u6784\u5efa\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668. # \u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u8fd9\u4e2a\u5947\u602a\u7684\u6a21\u578b\u975e\u5e38\u56f0\u96be,\u6240\u4ee5\u6211\u4eec\u4f7f\u7528\u52a8\u91cf criterion = torch.nn.MSELoss(size_average=False) optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9) for t in range(500): # \u6b63\u5411\u4f20\u9012:\u901a\u8fc7\u5c06x\u4f20\u9012\u7ed9\u6a21\u578b\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y y_pred = model(x) # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931 loss = criterion(y_pred, y) print(t, loss.data[0]) # \u96f6\u68af\u5ea6\u6267\u884c\u53cd\u5411\u4f20\u9012\u5e76\u66f4\u65b0\u6743\u91cd. optimizer.zero_grad() loss.backward() optimizer.step() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: dynamic_net.py Download Jupyter notebook: dynamic_net.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch: \u52a8\u6001\u63a7\u5236\u6d41\u7a0b + \u6743\u91cd\u5171\u4eab"},{"location":"22/","text":"\u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b \u4f5c\u8005 : Sasank Chilamkurthy \u8fd9\u4e2a\u6559\u7a0b\u5c06\u6559\u4f60\u5982\u4f55\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u8bad\u7ec3\u4f60\u7684\u7f51\u7edc. \u4f60\u53ef\u4ee5\u5728 cs231n \u7b14\u8bb0 \u4e2d \u9605\u8bfb\u66f4\u591a\u6709\u5173\u8fc1\u79fb\u5b66\u4e60\u7684\u4fe1\u606f. \u5f15\u7528\u81ea\u8be5\u7b14\u8bb0, \u4e8b\u5b9e\u4e0a, \u5f88\u5c11\u6709\u4eba\u4ece\u5934(\u968f\u673a\u521d\u59cb\u5316)\u5f00\u59cb\u8bad\u7ec3\u4e00\u4e2a\u5377\u79ef\u7f51\u7edc, \u56e0\u4e3a\u62e5\u6709\u4e00\u4e2a\u8db3\u591f\u5927\u7684\u6570\u636e\u5e93\u662f\u6bd4\u8f83\u5c11\u89c1\u7684. \u66ff\u4ee3\u7684\u662f, \u901a\u5e38\u4f1a\u4ece\u4e00\u4e2a\u5927\u7684\u6570\u636e\u96c6(\u4f8b\u5982 ImageNet, \u5305\u542b120\u4e07\u7684\u56fe\u7247\u548c1000\u4e2a\u5206\u7c7b)\u9884\u8bad\u7ec3\u4e00\u4e2a\u5377\u79ef\u7f51\u7edc, \u7136\u540e\u5c06\u8fd9\u4e2a\u5377\u79ef\u7f51\u7edc\u4f5c\u4e3a\u521d\u59cb\u5316\u7684\u7f51\u7edc, \u6216\u8005\u662f\u611f\u5174\u8da3\u4efb\u52a1\u7684\u56fa\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u5668. \u5982\u4e0b\u662f\u4e24\u79cd\u4e3b\u8981\u7684\u8fc1\u79fb\u5b66\u4e60\u7684\u4f7f\u7528\u573a\u666f: \u5fae\u8c03\u5377\u79ef\u7f51\u7edc : \u53d6\u4ee3\u968f\u673a\u521d\u59cb\u5316\u7f51\u7edc, \u6211\u4eec\u4ece\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u7f51\u7edc\u521d\u59cb\u5316, \u6bd4\u5982\u4ece imagenet 1000 \u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684\u7f51\u7edc. \u5176\u4f59\u7684\u8bad\u7ec3\u5c31\u50cf\u5f80\u5e38\u4e00\u6837. \u5377\u79ef\u7f51\u7edc\u4f5c\u4e3a\u56fa\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u5668 : \u5728\u8fd9\u91cc, \u6211\u4eec\u56fa\u5b9a\u7f51\u7edc\u4e2d\u7684\u6240\u6709\u6743\u91cd, \u6700\u540e\u7684\u5168\u8fde\u63a5\u5c42\u9664\u5916. \u6700\u540e\u7684\u5168\u8fde\u63a5\u5c42\u88ab\u65b0\u7684\u968f\u673a\u6743\u91cd\u66ff\u6362, \u5e76\u4e14, \u53ea\u6709\u8fd9\u4e00\u5c42\u662f\u88ab\u8bad\u7ec3\u7684. # License: BSD # Author: Sasank Chilamkurthy from __future__ import print_function, division import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler from torch.autograd import Variable import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os import copy plt.ion() # interactive mode \u52a0\u8f7d\u6570\u636e \u6211\u4eec\u7528 torchvision \u548c torch.utils.data \u5305\u52a0\u8f7d\u6570\u636e. \u6211\u4eec\u4eca\u5929\u8981\u89e3\u51b3\u7684\u95ee\u9898\u662f, \u8bad\u7ec3\u4e00\u4e2a\u53ef\u4ee5\u533a\u5206 ants (\u8682\u8681) \u548c bees (\u871c\u8702) \u7684\u6a21\u578b. \u7528\u4e8e\u8bad\u7ec3\u7684 ants \u548c bees \u56fe\u7247\u5404120\u5f20. \u6bcf\u4e00\u7c7b\u7528\u4e8e\u9a8c\u8bc1\u7684\u56fe\u7247\u540475\u5f20. \u901a\u5e38, \u5982\u679c\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3, \u8fd9\u4e2a\u975e\u5e38\u5c0f\u7684\u6570\u636e\u96c6\u4e0d\u8db3\u4ee5\u8fdb\u884c\u6cdb\u5316. \u4f46\u662f, \u56e0\u4e3a\u6211\u4eec\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60, \u5e94\u8be5\u53ef\u4ee5\u53d6\u5f97\u5f88\u597d\u7684\u6cdb\u5316\u6548\u679c. \u8fd9\u4e2a\u6570\u636e\u96c6\u662f\u4e00\u4e2a\u975e\u5e38\u5c0f\u7684 imagenet \u5b50\u96c6 Note \u4ece \u8fd9\u91cc &lt;[https://download.pytorch.org/tutorial/hymenoptera_data.zip](https://download.pytorch.org/tutorial/hymenoptera_data.zip)&gt; _ \u4e0b\u8f7d\u6570\u636e, \u7136\u540e\u89e3\u538b\u5230\u5f53\u524d\u76ee\u5f55. # \u8bad\u7ec3\u8981\u505a\u6570\u636e\u589e\u5f3a\u548c\u6570\u636e\u6807\u51c6\u5316 # \u9a8c\u8bc1\u53ea\u505a\u6570\u636e\u6807\u51c6\u5316 data_transforms = { 'train': transforms.Compose([ transforms.RandomSizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), 'val': transforms.Compose([ transforms.Scale(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = 'hymenoptera_data' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']} dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']} class_names = image_datasets['train'].classes use_gpu = torch.cuda.is_available() \u663e\u793a\u4e00\u4e9b\u56fe\u7247 \u8ba9\u6211\u4eec\u663e\u793a\u4e00\u4e9b\u8bad\u7ec3\u4e2d\u7684\u56fe\u7247, \u4ee5\u4fbf\u4e86\u89e3\u6570\u636e\u589e\u5f3a. def imshow(inp, title=None): \"\"\"Imshow for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # \u6682\u505c\u4e00\u4f1a, \u8ba9 plots \u66f4\u65b0 # \u83b7\u5f97\u4e00\u6279\u8bad\u7ec3\u6570\u636e inputs, classes = next(iter(dataloaders['train'])) # \u4ece\u8fd9\u6279\u6570\u636e\u751f\u6210\u4e00\u4e2a\u65b9\u683c out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) \u8bad\u7ec3\u6a21\u578b \u73b0\u5728, \u8ba9\u6211\u4eec\u5199\u4e00\u4e2a\u901a\u7528\u7684\u51fd\u6570\u6765\u8bad\u7ec3\u6a21\u578b. \u8fd9\u91cc, \u6211\u4eec\u5c06\u4f1a\u4e3e\u4f8b\u8bf4\u660e: \u8c03\u5ea6\u5b66\u4e60\u7387 \u4fdd\u5b58\u6700\u4f73\u7684\u5b66\u4e60\u6a21\u578b \u4e0b\u9762\u51fd\u6570\u4e2d, scheduler \u53c2\u6570\u662f torch.optim.lr_scheduler \u4e2d\u7684 LR scheduler \u5bf9\u8c61. def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print('Epoch {}/{}'.format(epoch, num_epochs - 1)) print('-' * 10) # \u6bcf\u4e00\u4e2a\u8fed\u4ee3\u90fd\u6709\u8bad\u7ec3\u548c\u9a8c\u8bc1\u9636\u6bb5 for phase in ['train', 'val']: if phase == 'train': scheduler.step() model.train(True) # \u8bbe\u7f6e model \u4e3a\u8bad\u7ec3 (training) \u6a21\u5f0f else: model.train(False) # \u8bbe\u7f6e model \u4e3a\u8bc4\u4f30 (evaluate) \u6a21\u5f0f running_loss = 0.0 running_corrects = 0 # \u904d\u5386\u6570\u636e for data in dataloaders[phase]: # \u83b7\u53d6\u8f93\u5165 inputs, labels = data # \u7528 Variable \u5305\u88c5\u8f93\u5165\u6570\u636e if use_gpu: inputs = Variable(inputs.cuda()) labels = Variable(labels.cuda()) else: inputs, labels = Variable(inputs), Variable(labels) # \u8bbe\u7f6e\u68af\u5ea6\u53c2\u6570\u4e3a 0 optimizer.zero_grad() # \u6b63\u5411\u4f20\u9012 outputs = model(inputs) _, preds = torch.max(outputs.data, 1) loss = criterion(outputs, labels) # \u5982\u679c\u662f\u8bad\u7ec3\u9636\u6bb5, \u5411\u540e\u4f20\u9012\u548c\u4f18\u5316 if phase == 'train': loss.backward() optimizer.step() # \u7edf\u8ba1 running_loss += loss.data[0] * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects / dataset_sizes[phase] print('{} Loss: {:.4f} Acc: {:.4f}'.format( phase, epoch_loss, epoch_acc)) # \u6df1\u62f7\u8d1d model if phase == 'val' and epoch_acc > best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print('Training complete in {:.0f}m {:.0f}s'.format( time_elapsed // 60, time_elapsed % 60)) print('Best val Acc: {:4f}'.format(best_acc)) # \u52a0\u8f7d\u6700\u4f73\u6a21\u578b\u7684\u6743\u91cd model.load_state_dict(best_model_wts) return model \u663e\u793a\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c \u5199\u4e00\u4e2a\u5904\u7406\u5c11\u91cf\u56fe\u7247, \u5e76\u663e\u793a\u9884\u6d4b\u7ed3\u679c\u7684\u901a\u7528\u51fd\u6570 def visualize_model(model, num_images=6): images_so_far = 0 fig = plt.figure() for i, data in enumerate(dataloaders['val']): inputs, labels = data if use_gpu: inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda()) else: inputs, labels = Variable(inputs), Variable(labels) outputs = model(inputs) _, preds = torch.max(outputs.data, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis('off') ax.set_title('predicted: {}'.format(class_names[preds[j]])) imshow(inputs.cpu().data[j]) if images_so_far == num_images: return \u8c03\u6574\u5377\u79ef\u7f51\u7edc \u52a0\u8f7d\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u7f51\u7edc, \u5e76\u91cd\u7f6e\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42. model_ft = models.resnet18(pretrained=True) num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs, 2) if use_gpu: model_ft = model_ft.cuda() criterion = nn.CrossEntropyLoss() # \u5982\u4f60\u6240\u89c1, \u6240\u6709\u53c2\u6570\u90fd\u5c06\u88ab\u4f18\u5316 optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # \u6bcf 7 \u4e2a\u8fed\u4ee3, \u8ba9 LR \u8870\u51cf 0.1 \u56e0\u7d20 exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) \u8bad\u7ec3\u548c\u8bc4\u4f30 \u5982\u679c\u4f7f\u7528 CPU, \u8fd9\u5c06\u82b1\u8d39 15-25 \u5206\u949f. \u4f46\u4f7f\u7528 GPU \u7684\u8bdd, \u9700\u8981\u7684\u65f6\u95f4\u5c06\u5c11\u4e8e1\u5206\u949f. model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) visualize_model(model_ft) \u5377\u79ef\u7f51\u7edc\u4f5c\u4e3a\u56fa\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u5668 \u8fd9\u91cc, \u6211\u4eec\u56fa\u5b9a\u7f51\u7edc\u4e2d\u9664\u6700\u540e\u4e00\u5c42\u5916\u7684\u6240\u6709\u6743\u91cd. \u4e3a\u4e86\u56fa\u5b9a\u8fd9\u4e9b\u53c2\u6570, \u6211\u4eec\u9700\u8981\u8bbe\u7f6e requires_grad == False , \u7136\u540e\u5728 backward() \u4e2d\u5c31\u4e0d\u4f1a\u8ba1\u7b97\u68af\u5ea6. \u4f60\u53ef\u4ee5\u5728 \u8fd9\u91cc \u9605\u8bfb\u66f4\u591a\u76f8\u5173\u4fe1\u606f. model_conv = torchvision.models.resnet18(pretrained=True) for param in model_conv.parameters(): param.requires_grad = False # \u65b0\u6784\u5efa\u7684 module \u7684\u53c2\u6570\u4e2d, \u9ed8\u8ba4\u8bbe\u7f6e\u4e86 requires_grad=True. num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) if use_gpu: model_conv = model_conv.cuda() criterion = nn.CrossEntropyLoss() # \u5982\u4f60\u6240\u89c1, \u548c\u6211\u4eec\u524d\u9762\u63d0\u51fa\u7684\u4e00\u6837, \u53ea\u6709\u6700\u540e\u4e00\u5c42\u7684\u53c2\u6570\u88ab\u4f18\u5316. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # \u6bcf 7 \u4e2a\u8fed\u4ee3, \u8ba9 LR \u8870\u51cf 0.1 \u56e0\u7d20 exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) \u8bad\u7ec3\u548c\u8bc4\u4f30 \u5728\u4f7f\u7528 CPU \u7684\u60c5\u51b5\u4e0b, \u548c\u524d\u4e00\u4e2a\u65b9\u6848\u76f8\u6bd4, \u8fd9\u5c06\u82b1\u8d39\u7684\u65f6\u95f4\u662f\u5b83\u7684\u4e00\u534a. \u671f\u671b\u4e2d, \u7f51\u7edc\u7684\u5927\u90e8\u5206\u662f\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\u7684. \u524d\u5411\u4f20\u9012\u4f9d\u7136\u8981\u8ba1\u7b97\u68af\u5ea6. model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) visualize_model(model_conv) plt.ioff() plt.show() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: transfer_learning_tutorial.py Download Jupyter notebook: transfer_learning_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b"},{"location":"22/#_1","text":"\u4f5c\u8005 : Sasank Chilamkurthy \u8fd9\u4e2a\u6559\u7a0b\u5c06\u6559\u4f60\u5982\u4f55\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u8bad\u7ec3\u4f60\u7684\u7f51\u7edc. \u4f60\u53ef\u4ee5\u5728 cs231n \u7b14\u8bb0 \u4e2d \u9605\u8bfb\u66f4\u591a\u6709\u5173\u8fc1\u79fb\u5b66\u4e60\u7684\u4fe1\u606f. \u5f15\u7528\u81ea\u8be5\u7b14\u8bb0, \u4e8b\u5b9e\u4e0a, \u5f88\u5c11\u6709\u4eba\u4ece\u5934(\u968f\u673a\u521d\u59cb\u5316)\u5f00\u59cb\u8bad\u7ec3\u4e00\u4e2a\u5377\u79ef\u7f51\u7edc, \u56e0\u4e3a\u62e5\u6709\u4e00\u4e2a\u8db3\u591f\u5927\u7684\u6570\u636e\u5e93\u662f\u6bd4\u8f83\u5c11\u89c1\u7684. \u66ff\u4ee3\u7684\u662f, \u901a\u5e38\u4f1a\u4ece\u4e00\u4e2a\u5927\u7684\u6570\u636e\u96c6(\u4f8b\u5982 ImageNet, \u5305\u542b120\u4e07\u7684\u56fe\u7247\u548c1000\u4e2a\u5206\u7c7b)\u9884\u8bad\u7ec3\u4e00\u4e2a\u5377\u79ef\u7f51\u7edc, \u7136\u540e\u5c06\u8fd9\u4e2a\u5377\u79ef\u7f51\u7edc\u4f5c\u4e3a\u521d\u59cb\u5316\u7684\u7f51\u7edc, \u6216\u8005\u662f\u611f\u5174\u8da3\u4efb\u52a1\u7684\u56fa\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u5668. \u5982\u4e0b\u662f\u4e24\u79cd\u4e3b\u8981\u7684\u8fc1\u79fb\u5b66\u4e60\u7684\u4f7f\u7528\u573a\u666f: \u5fae\u8c03\u5377\u79ef\u7f51\u7edc : \u53d6\u4ee3\u968f\u673a\u521d\u59cb\u5316\u7f51\u7edc, \u6211\u4eec\u4ece\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u7f51\u7edc\u521d\u59cb\u5316, \u6bd4\u5982\u4ece imagenet 1000 \u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684\u7f51\u7edc. \u5176\u4f59\u7684\u8bad\u7ec3\u5c31\u50cf\u5f80\u5e38\u4e00\u6837. \u5377\u79ef\u7f51\u7edc\u4f5c\u4e3a\u56fa\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u5668 : \u5728\u8fd9\u91cc, \u6211\u4eec\u56fa\u5b9a\u7f51\u7edc\u4e2d\u7684\u6240\u6709\u6743\u91cd, \u6700\u540e\u7684\u5168\u8fde\u63a5\u5c42\u9664\u5916. \u6700\u540e\u7684\u5168\u8fde\u63a5\u5c42\u88ab\u65b0\u7684\u968f\u673a\u6743\u91cd\u66ff\u6362, \u5e76\u4e14, \u53ea\u6709\u8fd9\u4e00\u5c42\u662f\u88ab\u8bad\u7ec3\u7684. # License: BSD # Author: Sasank Chilamkurthy from __future__ import print_function, division import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler from torch.autograd import Variable import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os import copy plt.ion() # interactive mode","title":"\u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b"},{"location":"22/#_2","text":"\u6211\u4eec\u7528 torchvision \u548c torch.utils.data \u5305\u52a0\u8f7d\u6570\u636e. \u6211\u4eec\u4eca\u5929\u8981\u89e3\u51b3\u7684\u95ee\u9898\u662f, \u8bad\u7ec3\u4e00\u4e2a\u53ef\u4ee5\u533a\u5206 ants (\u8682\u8681) \u548c bees (\u871c\u8702) \u7684\u6a21\u578b. \u7528\u4e8e\u8bad\u7ec3\u7684 ants \u548c bees \u56fe\u7247\u5404120\u5f20. \u6bcf\u4e00\u7c7b\u7528\u4e8e\u9a8c\u8bc1\u7684\u56fe\u7247\u540475\u5f20. \u901a\u5e38, \u5982\u679c\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3, \u8fd9\u4e2a\u975e\u5e38\u5c0f\u7684\u6570\u636e\u96c6\u4e0d\u8db3\u4ee5\u8fdb\u884c\u6cdb\u5316. \u4f46\u662f, \u56e0\u4e3a\u6211\u4eec\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60, \u5e94\u8be5\u53ef\u4ee5\u53d6\u5f97\u5f88\u597d\u7684\u6cdb\u5316\u6548\u679c. \u8fd9\u4e2a\u6570\u636e\u96c6\u662f\u4e00\u4e2a\u975e\u5e38\u5c0f\u7684 imagenet \u5b50\u96c6 Note \u4ece \u8fd9\u91cc &lt;[https://download.pytorch.org/tutorial/hymenoptera_data.zip](https://download.pytorch.org/tutorial/hymenoptera_data.zip)&gt; _ \u4e0b\u8f7d\u6570\u636e, \u7136\u540e\u89e3\u538b\u5230\u5f53\u524d\u76ee\u5f55. # \u8bad\u7ec3\u8981\u505a\u6570\u636e\u589e\u5f3a\u548c\u6570\u636e\u6807\u51c6\u5316 # \u9a8c\u8bc1\u53ea\u505a\u6570\u636e\u6807\u51c6\u5316 data_transforms = { 'train': transforms.Compose([ transforms.RandomSizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), 'val': transforms.Compose([ transforms.Scale(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = 'hymenoptera_data' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']} dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']} class_names = image_datasets['train'].classes use_gpu = torch.cuda.is_available()","title":"\u52a0\u8f7d\u6570\u636e"},{"location":"22/#_3","text":"\u8ba9\u6211\u4eec\u663e\u793a\u4e00\u4e9b\u8bad\u7ec3\u4e2d\u7684\u56fe\u7247, \u4ee5\u4fbf\u4e86\u89e3\u6570\u636e\u589e\u5f3a. def imshow(inp, title=None): \"\"\"Imshow for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # \u6682\u505c\u4e00\u4f1a, \u8ba9 plots \u66f4\u65b0 # \u83b7\u5f97\u4e00\u6279\u8bad\u7ec3\u6570\u636e inputs, classes = next(iter(dataloaders['train'])) # \u4ece\u8fd9\u6279\u6570\u636e\u751f\u6210\u4e00\u4e2a\u65b9\u683c out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes])","title":"\u663e\u793a\u4e00\u4e9b\u56fe\u7247"},{"location":"22/#_4","text":"\u73b0\u5728, \u8ba9\u6211\u4eec\u5199\u4e00\u4e2a\u901a\u7528\u7684\u51fd\u6570\u6765\u8bad\u7ec3\u6a21\u578b. \u8fd9\u91cc, \u6211\u4eec\u5c06\u4f1a\u4e3e\u4f8b\u8bf4\u660e: \u8c03\u5ea6\u5b66\u4e60\u7387 \u4fdd\u5b58\u6700\u4f73\u7684\u5b66\u4e60\u6a21\u578b \u4e0b\u9762\u51fd\u6570\u4e2d, scheduler \u53c2\u6570\u662f torch.optim.lr_scheduler \u4e2d\u7684 LR scheduler \u5bf9\u8c61. def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print('Epoch {}/{}'.format(epoch, num_epochs - 1)) print('-' * 10) # \u6bcf\u4e00\u4e2a\u8fed\u4ee3\u90fd\u6709\u8bad\u7ec3\u548c\u9a8c\u8bc1\u9636\u6bb5 for phase in ['train', 'val']: if phase == 'train': scheduler.step() model.train(True) # \u8bbe\u7f6e model \u4e3a\u8bad\u7ec3 (training) \u6a21\u5f0f else: model.train(False) # \u8bbe\u7f6e model \u4e3a\u8bc4\u4f30 (evaluate) \u6a21\u5f0f running_loss = 0.0 running_corrects = 0 # \u904d\u5386\u6570\u636e for data in dataloaders[phase]: # \u83b7\u53d6\u8f93\u5165 inputs, labels = data # \u7528 Variable \u5305\u88c5\u8f93\u5165\u6570\u636e if use_gpu: inputs = Variable(inputs.cuda()) labels = Variable(labels.cuda()) else: inputs, labels = Variable(inputs), Variable(labels) # \u8bbe\u7f6e\u68af\u5ea6\u53c2\u6570\u4e3a 0 optimizer.zero_grad() # \u6b63\u5411\u4f20\u9012 outputs = model(inputs) _, preds = torch.max(outputs.data, 1) loss = criterion(outputs, labels) # \u5982\u679c\u662f\u8bad\u7ec3\u9636\u6bb5, \u5411\u540e\u4f20\u9012\u548c\u4f18\u5316 if phase == 'train': loss.backward() optimizer.step() # \u7edf\u8ba1 running_loss += loss.data[0] * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects / dataset_sizes[phase] print('{} Loss: {:.4f} Acc: {:.4f}'.format( phase, epoch_loss, epoch_acc)) # \u6df1\u62f7\u8d1d model if phase == 'val' and epoch_acc > best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print('Training complete in {:.0f}m {:.0f}s'.format( time_elapsed // 60, time_elapsed % 60)) print('Best val Acc: {:4f}'.format(best_acc)) # \u52a0\u8f7d\u6700\u4f73\u6a21\u578b\u7684\u6743\u91cd model.load_state_dict(best_model_wts) return model","title":"\u8bad\u7ec3\u6a21\u578b"},{"location":"22/#_5","text":"\u5199\u4e00\u4e2a\u5904\u7406\u5c11\u91cf\u56fe\u7247, \u5e76\u663e\u793a\u9884\u6d4b\u7ed3\u679c\u7684\u901a\u7528\u51fd\u6570 def visualize_model(model, num_images=6): images_so_far = 0 fig = plt.figure() for i, data in enumerate(dataloaders['val']): inputs, labels = data if use_gpu: inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda()) else: inputs, labels = Variable(inputs), Variable(labels) outputs = model(inputs) _, preds = torch.max(outputs.data, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis('off') ax.set_title('predicted: {}'.format(class_names[preds[j]])) imshow(inputs.cpu().data[j]) if images_so_far == num_images: return","title":"\u663e\u793a\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c"},{"location":"22/#_6","text":"\u52a0\u8f7d\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u7f51\u7edc, \u5e76\u91cd\u7f6e\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42. model_ft = models.resnet18(pretrained=True) num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs, 2) if use_gpu: model_ft = model_ft.cuda() criterion = nn.CrossEntropyLoss() # \u5982\u4f60\u6240\u89c1, \u6240\u6709\u53c2\u6570\u90fd\u5c06\u88ab\u4f18\u5316 optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # \u6bcf 7 \u4e2a\u8fed\u4ee3, \u8ba9 LR \u8870\u51cf 0.1 \u56e0\u7d20 exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)","title":"\u8c03\u6574\u5377\u79ef\u7f51\u7edc"},{"location":"22/#_7","text":"\u5982\u679c\u4f7f\u7528 CPU, \u8fd9\u5c06\u82b1\u8d39 15-25 \u5206\u949f. \u4f46\u4f7f\u7528 GPU \u7684\u8bdd, \u9700\u8981\u7684\u65f6\u95f4\u5c06\u5c11\u4e8e1\u5206\u949f. model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) visualize_model(model_ft)","title":"\u8bad\u7ec3\u548c\u8bc4\u4f30"},{"location":"22/#_8","text":"\u8fd9\u91cc, \u6211\u4eec\u56fa\u5b9a\u7f51\u7edc\u4e2d\u9664\u6700\u540e\u4e00\u5c42\u5916\u7684\u6240\u6709\u6743\u91cd. \u4e3a\u4e86\u56fa\u5b9a\u8fd9\u4e9b\u53c2\u6570, \u6211\u4eec\u9700\u8981\u8bbe\u7f6e requires_grad == False , \u7136\u540e\u5728 backward() \u4e2d\u5c31\u4e0d\u4f1a\u8ba1\u7b97\u68af\u5ea6. \u4f60\u53ef\u4ee5\u5728 \u8fd9\u91cc \u9605\u8bfb\u66f4\u591a\u76f8\u5173\u4fe1\u606f. model_conv = torchvision.models.resnet18(pretrained=True) for param in model_conv.parameters(): param.requires_grad = False # \u65b0\u6784\u5efa\u7684 module \u7684\u53c2\u6570\u4e2d, \u9ed8\u8ba4\u8bbe\u7f6e\u4e86 requires_grad=True. num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) if use_gpu: model_conv = model_conv.cuda() criterion = nn.CrossEntropyLoss() # \u5982\u4f60\u6240\u89c1, \u548c\u6211\u4eec\u524d\u9762\u63d0\u51fa\u7684\u4e00\u6837, \u53ea\u6709\u6700\u540e\u4e00\u5c42\u7684\u53c2\u6570\u88ab\u4f18\u5316. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # \u6bcf 7 \u4e2a\u8fed\u4ee3, \u8ba9 LR \u8870\u51cf 0.1 \u56e0\u7d20 exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)","title":"\u5377\u79ef\u7f51\u7edc\u4f5c\u4e3a\u56fa\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u5668"},{"location":"22/#_9","text":"\u5728\u4f7f\u7528 CPU \u7684\u60c5\u51b5\u4e0b, \u548c\u524d\u4e00\u4e2a\u65b9\u6848\u76f8\u6bd4, \u8fd9\u5c06\u82b1\u8d39\u7684\u65f6\u95f4\u662f\u5b83\u7684\u4e00\u534a. \u671f\u671b\u4e2d, \u7f51\u7edc\u7684\u5927\u90e8\u5206\u662f\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\u7684. \u524d\u5411\u4f20\u9012\u4f9d\u7136\u8981\u8ba1\u7b97\u68af\u5ea6. model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) visualize_model(model_conv) plt.ioff() plt.show() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: transfer_learning_tutorial.py Download Jupyter notebook: transfer_learning_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u8bad\u7ec3\u548c\u8bc4\u4f30"},{"location":"23/","text":"\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b \u4f5c\u8005 : Sasank Chilamkurthy \u5728\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u95ee\u9898\u65f6, \u6211\u4eec\u9700\u8981\u4ed8\u51fa\u5f88\u591a\u52aa\u529b\u6765\u51c6\u5907\u6570\u636e, \u4e3a\u4e86\u4f7f\u4ee3\u7801\u66f4\u5177\u53ef\u8bfb\u6027, PyTorch\u63d0\u4f9b\u4e86\u8bb8\u591a\u5de5\u5177\u6765\u4f7f\u6570\u636e\u52a0\u8f7d\u53d8\u5f97\u7b80\u5355\u6613\u884c. \u5728\u672c\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u8981\u5b66\u4e60\u5982\u4f55\u5bf9 \u4e00\u4e2a\u91cd\u8981\u7684\u6570\u636e\u96c6\u8fdb\u884c\u52a0\u8f7d\u3001\u9884\u5904\u7406\u6570\u636e\u589e\u5f3a. \u4e3a\u4e86\u8fd0\u884c\u672c\u6559\u7a0b, \u8bf7\u786e\u4fdd\u4ee5\u4e0b\u5305\u5df2\u7ecf\u5b89\u88c5: scikit-image : \u7528\u6765\u8bfb\u53d6\u56fe\u7247\u548c\u56fe\u50cf\u53d8\u6362 pandas : \u66f4\u65b9\u4fbf\u5730\u89e3\u6790csv\u6587\u4ef6 from __future__ import print_function, division import os import torch import pandas as pd from skimage import io, transform import numpy as np import matplotlib.pyplot as plt from torch.utils.data import Dataset, DataLoader from torchvision import transforms, utils # \u5ffd\u7565\u8b66\u544a import warnings warnings.filterwarnings(\"ignore\") plt.ion() # \u4ea4\u4e92\u6a21\u5f0f \u6211\u4eec\u5c06\u8981\u5904\u7406\u7684\u662f\u9762\u90e8\u59ff\u6001\u7684\u6570\u636e\u96c6, \u5176\u4e2d\u4e00\u5f20\u4eba\u8138\u56fe\u50cf\u5982\u4e0b\u56fe\u4e00\u6837\u88ab\u6807\u6ce8\u51fa\u6765. \u4e0b\u9762\u662f\u4e00\u5f20\u8138\u7684\u6807\u6ce8: \u6bcf\u5f20\u4eba\u8138\u56fe\u50cf\u4e0a, \u603b\u5171\u670968\u4e2a\u4e0d\u540c\u7684\u6807\u6ce8\u70b9\u88ab\u6807\u8bb0\u51fa\u6765. Note \u70b9\u51fb \u8fd9\u91cc <https://download.pytorch.org/tutorial/faces.zip> \u4e0b\u8f7d\u6570\u636e\u96c6, \u8fd9\u4e9b\u56fe\u50cf\u5728\u76ee\u5f55 \u2018 faces/ \u2018\u4e0b. \u8fd9\u4e2a\u6570\u636e\u96c6\u5b9e\u9645\u4e0a\u662f\u4eceimagenet\u6570\u636e\u96c6\u4e2d\u9009\u53d6\u6807\u8bb0\u4e3a\u4eba\u8138\u7684\u4e00\u4e9b\u56fe\u7247, \u4f7f\u7528 dlib\u2019s pose estimation \u65b9\u6cd5\u751f\u6210\u7684. \u6570\u636e\u96c6\u4e2d\u7684csv\u6587\u4ef6\u8bb0\u5f55\u7740\u6807\u6ce8\u4fe1\u606f, \u50cf\u4e0b\u9762\u8fd9\u6837: image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y 0805personali01.jpg,27,83,27,98, ... 84,134 1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312 \u8ba9\u6211\u4eec\u5feb\u901f\u5730\u8bfb\u53d6CSV\u6587\u4ef6, \u4ee5(N,2)\u7684\u6570\u7ec4\u5f62\u5f0f\u83b7\u5f97\u6807\u8bb0\u70b9, \u5176\u4e2dN\u8868\u793a\u6807\u8bb0\u70b9\u7684\u4e2a\u6570. landmarks_frame = pd.read_csv('faces/face_landmarks.csv') n = 65 img_name = landmarks_frame.iloc[n, 0] landmarks = landmarks_frame.iloc[n, 1:].as_matrix() landmarks = landmarks.astype('float').reshape(-1, 2) print('Image name: {}'.format(img_name)) print('Landmarks shape: {}'.format(landmarks.shape)) print('First 4 Landmarks: {}'.format(landmarks[:4])) \u6211\u4eec\u5199\u4e00\u4e2a\u51fd\u6570\u6765\u663e\u793a\u4e00\u5f20\u56fe\u7247\u548c\u5b83\u7684\u6807\u8bb0\u70b9, \u7136\u540e\u7528\u8fd9\u4e2a\u51fd\u6570\u6765\u663e\u793a\u4e00\u4e2a\u6837\u672c. def show_landmarks(image, landmarks): \"\"\"\u663e\u793a\u5e26\u6807\u8bb0\u70b9\u7684\u56fe\u7247\"\"\" plt.imshow(image) plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r') plt.pause(0.001) # \u6682\u505c\u4e00\u4e0b, \u4f7fplots\u66f4\u65b0 plt.figure() show_landmarks(io.imread(os.path.join('faces/', img_name)), landmarks) plt.show() Dataset\u7c7b torch.utils.data.Dataset \u662f\u4e00\u4e2a\u8868\u793a\u6570\u636e\u96c6\u7684\u62bd\u8c61\u7c7b. \u4f60\u81ea\u5df1\u7684\u6570\u636e\u96c6\u4e00\u822c\u5e94\u8be5\u7ee7\u627f Dataset , \u5e76\u4e14\u91cd\u5199\u4e0b\u9762\u7684\u65b9\u6cd5: __len__ \u4f7f\u7528 len(dataset) \u53ef\u4ee5\u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f __getitem__ \u652f\u6301\u7d22\u5f15, \u4ee5\u4fbf\u4e8e\u4f7f\u7528 dataset[i] \u53ef\u4ee5 \u83b7\u53d6\u7b2c:math:<cite>i</cite>\u4e2a\u6837\u672c(0\u7d22\u5f15) \u6211\u4eec\u4e3a\u6211\u4eec\u7684\u4eba\u8138\u6570\u636e\u96c6\u521b\u5efa\u4e00\u4e2a\u6570\u636e\u96c6\u7c7b. \u6211\u4eec\u4f7f\u7528 [ ](#id2)__init__ \u65b9\u6cd5\u6765\u8bfb\u53d6csv\u6587\u4ef6, \u4f7f\u7528 [ ](#id4)__getitem__ \u8bfb\u53d6\u56fe\u7247. \u8fd9\u6837\u53ef\u4ee5\u4f7f\u5185\u5b58\u9ad8\u6548\u5229\u7528, \u56e0\u4e3a\u6211\u4eec\u5e76\u4e0d\u9700\u8981\u5728\u5185\u5b58\u4e2d\u4e00\u6b21\u5b58\u50a8\u6240\u6709\u56fe\u7247, \u800c\u662f\u6309\u7167\u9700\u8981\u8bfb\u53d6. \u6570\u636e\u96c6\u7684\u4e00\u4e2a\u6837\u4f8b\u662f\u4e00\u4e2a {\u2018image\u2019: image, \u2018landmarks\u2019: landmarks} \u6837\u7684\u5b57\u5178. \u6570\u636e\u96c6\u7c7b\u4e2d\u6709\u4e00\u4e2a\u53ef\u9009\u7684\u53c2\u6570 transform \u8fd9\u6837\u53ef\u4ee5\u5bf9\u6570\u636e\u96c6\u505a\u4efb\u4f55\u9700\u8981\u7684\u5904\u7406. \u6211\u4eec\u5c06\u5728\u4e0b\u8282\u770b\u5230 transform \u7684\u7528\u5904. class FaceLandmarksDataset(Dataset): \"\"\"\u4eba\u8138\u6807\u8bb0\u6570\u636e\u96c6\"\"\" def __init__(self, csv_file, root_dir, transform=None): \"\"\" Args: csv_file (string): \u5e26\u6709\u6807\u8bb0\u70b9\u7684csv\u6587\u4ef6\u8def\u5f84 root_dir (string): \u56fe\u7247\u8def\u5f84 transform (callable, optional):\u53ef\u9009\u62e9\u8fdb\u884c\u7684\u56fe\u50cf\u53d8\u6362 \"\"\" self.landmarks_frame = pd.read_csv(csv_file) self.root_dir = root_dir self.transform = transform def __len__(self): return len(self.landmarks_frame) def __getitem__(self, idx): img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0]) image = io.imread(img_name) landmarks = self.landmarks_frame.iloc[idx, 1:].as_matrix() landmarks = landmarks.astype('float').reshape(-1, 2) sample = {'image': image, 'landmarks': landmarks} if self.transform: sample = self.transform(sample) return sample \u8ba9\u6211\u4eec\u5b9e\u4f8b\u5316\u8fd9\u4e2a\u7c7b \u5e76\u4e14\u8fed\u4ee3\u6240\u6709\u7684\u6570\u636e\u6837\u672c. \u6211\u4eec\u5c06\u6253\u5370\u524d4\u4e2a\u6837\u672c, \u5e76\u663e\u793a\u5b83\u4eec\u7684\u6807\u8bb0\u70b9. face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv', root_dir='faces/') fig = plt.figure() for i in range(len(face_dataset)): sample = face_dataset[i] print(i, sample['image'].shape, sample['landmarks'].shape) ax = plt.subplot(1, 4, i + 1) plt.tight_layout() ax.set_title('Sample #{}'.format(i)) ax.axis('off') show_landmarks(**sample) if i == 3: plt.show() break Transforms \u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e0a\u9762\u8f93\u51fa\u7684\u6837\u4f8b\u4e2d\u7684\u56fe\u50cf\u5e76\u4e0d\u662f\u540c\u4e00\u5c3a\u5bf8\u7684\u56fe\u7247. \u5927\u591a\u6570\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u8f93\u5165 \u4e00\u4e2a\u56fa\u5b9a\u5927\u5c0f\u7684\u56fe\u50cf, \u56e0\u6b64\u6211\u4eec\u9700\u8981\u5199\u4ee3\u7801\u6765\u5904\u7406. \u8ba9\u6211\u4eec\u521b\u5efa\u4e09\u4e2atransform\u64cd\u4f5c: Rescale : \u4fee\u6539\u56fe\u7247\u5c3a\u5bf8 RandomCrop : \u968f\u673a\u88c1\u5207\u56fe\u7247, \u8fd9\u662f\u6570\u636e\u589e\u5f3a\u7684\u65b9\u6cd5 ToTensor : \u5c06numpy\u683c\u5f0f\u7684\u56fe\u7247\u8f6c\u4e3atorch\u683c\u5f0f\u7684\u56fe\u7247\uff08\u6211\u4eec\u9700\u8981\u4ea4\u6362\u5750\u6807\u8f74\uff09 \u6211\u4eec\u4e0d\u5c06\u5b83\u4eec\u5199\u6210\u7b80\u5355\u7684\u51fd\u6570, \u800c\u662f\u5199\u6210\u53ef\u4ee5\u8c03\u7528\u7684\u7c7b, \u8fd9\u6837transform\u7684\u53c2\u6570\u4e0d\u9700\u8981\u6bcf\u6b21\u90fd\u4f20\u9012 \u5982\u679c\u9700\u8981\u7684\u8bdd, \u6211\u4eec\u53ea\u9700\u5b9e\u73b0 __call__ \u65b9\u6cd5\u548c __init__ \u65b9\u6cd5.\u4e4b\u540e\u6211\u4eec\u53ef\u4ee5\u50cf\u4e0b\u9762\u8fd9 \u6837\u4f7f\u7528transform: tsfm = Transform(params) transformed_sample = tsfm(sample) \u89c2\u5bdf\u4e0b\u9762\u8fd9\u4e9b\u53d8\u6362\u5982\u4f55\u540c\u65f6\u5bf9\u56fe\u50cf\u548c\u6807\u8bb0\u70b9\u6539\u53d8\u7684. class Rescale(object): \"\"\"\u6309\u7167\u7ed9\u5b9a\u5c3a\u5bf8\u66f4\u6539\u4e00\u4e2a\u56fe\u50cf\u7684\u5c3a\u5bf8 Args: output_size (tuple or int): \u8981\u6c42\u8f93\u51fa\u7684\u5c3a\u5bf8. \u5982\u679c\u662f\u4e2a\u5143\u7ec4\u7c7b\u578b, \u8f93\u51fa \u548coutput_size\u5339\u914d. \u5982\u679c\u65f6int\u7c7b\u578b,\u56fe\u7247\u7684\u77ed\u8fb9\u548coutput_size\u5339\u914d, \u56fe\u7247\u7684 \u957f\u5bbd\u6bd4\u4fdd\u6301\u4e0d\u53d8. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] if isinstance(self.output_size, int): if h > w: new_h, new_w = self.output_size * h / w, self.output_size else: new_h, new_w = self.output_size, self.output_size * w / h else: new_h, new_w = self.output_size new_h, new_w = int(new_h), int(new_w) img = transform.resize(image, (new_h, new_w)) # \u5bf9\u4e8e\u6807\u8bb0\u70b9, h\u548cw\u9700\u8981\u4ea4\u6362\u4f4d\u7f6e, \u56e0\u4e3a\u5bf9\u4e8e\u56fe\u50cf, x\u548cy\u5206\u522b\u65f6\u7b2c1\u7ef4\u548c\u7b2c0\u7ef4 landmarks = landmarks * [new_w / w, new_h / h] return {'image': img, 'landmarks': landmarks} class RandomCrop(object): \"\"\"\u968f\u673a\u88c1\u526a\u56fe\u7247 Args: output_size (tuple or int): \u671f\u671b\u8f93\u51fa\u7684\u5c3a\u5bf8, \u5982\u679c\u65f6int\u7c7b\u578b, \u88c1\u5207\u6210\u6b63\u65b9\u5f62. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) if isinstance(output_size, int): self.output_size = (output_size, output_size) else: assert len(output_size) == 2 self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] new_h, new_w = self.output_size top = np.random.randint(0, h - new_h) left = np.random.randint(0, w - new_w) image = image[top: top + new_h, left: left + new_w] landmarks = landmarks - [left, top] return {'image': image, 'landmarks': landmarks} class ToTensor(object): \"\"\"\u5c06ndarrays\u7684\u6837\u672c\u8f6c\u5316\u4e3aTensors\u7684\u6837\u672c\"\"\" def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] # \u4ea4\u6362\u989c\u8272\u901a\u9053, \u56e0\u4e3a # numpy\u56fe\u7247: H x W x C # torch\u56fe\u7247 : C X H X W image = image.transpose((2, 0, 1)) return {'image': torch.from_numpy(image), 'landmarks': torch.from_numpy(landmarks)} Compose transforms \u73b0\u5728\u6211\u4eec\u5c31\u5c06 transform \u5e94\u7528\u5728\u4e00\u4e2a\u6837\u672c\u4e0a. \u5982\u679c\u6211\u4eec\u60f3\u5c06\u56fe\u7247\u7684\u77ed\u8fb9\u53d8\u4e3a256\u50cf\u7d20, \u5e76\u4e14\u968f\u540e\u968f\u673a\u88c1\u5207\u6210224\u50cf\u7d20\u7684\u6b63\u65b9\u5f62. i.e, \u6211\u4eec\u53ef\u4ee5\u7ec4\u5408 Rescale \u548c RandomCrop \u53d8\u6362. torchvision.transforms.Compose \u5c31\u662f\u4e00\u4e2a\u53ef\u4ee5\u505a\u8fd9\u6837\u4e00\u4e2a\u7ec4\u5408\u7684\u53ef\u8c03\u7528\u7684\u7c7b. scale = Rescale(256) crop = RandomCrop(128) composed = transforms.Compose([Rescale(256), RandomCrop(224)]) # \u5bf9\u6bcf\u4e2a\u6837\u672c\u8fdb\u884c\u4e0a\u9762\u7684\u6bcf\u4e00\u4e2a\u64cd\u4f5c. fig = plt.figure() sample = face_dataset[65] for i, tsfrm in enumerate([scale, crop, composed]): transformed_sample = tsfrm(sample) ax = plt.subplot(1, 3, i + 1) plt.tight_layout() ax.set_title(type(tsfrm).__name__) show_landmarks(**transformed_sample) plt.show() \u8fed\u4ee3\u6574\u4e2a\u6570\u636e\u96c6 \u8ba9\u6211\u4eec\u8fdb\u884c\u6240\u6709\u64cd\u4f5c, \u6765\u521b\u5efa\u7ed3\u5408\u4e86\u56fe\u50cf\u53d8\u6362\u7684\u6570\u636e\u96c6 \u603b\u4e4b, \u6bcf\u6b21\u8fed\u4ee3\u7684\u6570\u636e: \u4ece\u6587\u4ef6\u4e2d\u8bfb\u53d6\u56fe\u50cf \u5bf9\u6240\u8bfb\u7684\u56fe\u50cf\u4e0a\u5e94\u7528\u53d8\u6362 \u5176\u4e2d\u4e00\u4e2a\u53d8\u6362\u662f\u968f\u673a\u7684, \u6240\u4ee5\u53ef\u4ee5\u589e\u5f3a\u6570\u636e\u6837\u672c \u6211\u4eec\u53ef\u4ee5\u50cf\u4e4b\u524d\u4e00\u6837\u7528 for i in range \u5faa\u73af\u4ece\u5df2\u521b\u5efa\u7684\u6570\u636e\u96c6\u4e2d\u8fed\u4ee3 transformed_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv', root_dir='faces/', transform=transforms.Compose([ Rescale(256), RandomCrop(224), ToTensor() ])) for i in range(len(transformed_dataset)): sample = transformed_dataset[i] print(i, sample['image'].size(), sample['landmarks'].size()) if i == 3: break \u7136\u800c\u6211\u4eec\u7528\u7b80\u5355\u7684 for \u5faa\u73af\u6765\u8fed\u4ee3\u6574\u4e2a\u6570\u636e\u96c6\u4f1a\u4e22\u5931\u5f88\u591a\u7279\u70b9, \u7279\u522b\u5730, \u6211\u4eec\u4f1a\u4e22\u5931: \u6279\u8bfb\u53d6\u6570\u636e \u6253\u4e71\u6570\u636e\u987a\u5e8f \u4f7f\u7528 multiprocessing \u5e76\u884c\u52a0\u8f7d\u6570\u636e torch.utils.data.DataLoader \u662f\u63d0\u4f9b\u4e86\u6240\u6709\u4e0a\u8ff0\u7279\u70b9\u7684\u8fed\u4ee3\u5668. \u4e0b\u9762\u4f7f\u7528\u7684\u53c2\u6570\u5e94\u8be5\u5f88\u6e05\u6670. \u5176\u4e2d\u4e00\u4e2a\u6709\u8da3\u7684\u53c2\u6570\u662f collate_fn . \u4f60\u53ef\u4ee5\u4f7f\u7528 collate_fn \u6765\u6307\u5b9a\u5982\u4f55\u7cbe\u786e\u5730\u8bfb\u53d6\u4e00\u6279\u7684\u6837\u672c. \u7136\u800c, \u9ed8\u8ba4\u7684collate\u5728\u5927\u90e8\u5206\u7684\u60c5\u51b5\u4e0b\u90fd\u8868\u73b0\u5f97\u5f88\u597d dataloader = DataLoader(transformed_dataset, batch_size=4, shuffle=True, num_workers=4) # \u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\u6765\u67e5\u770b\u67d0\u4e2a batch \u7684\u6570\u636e\u6837\u672c\u56fe\u7247\u548c\u6807\u8bb0\u70b9 def show_landmarks_batch(sample_batched): \"\"\"\u663e\u793a\u6307\u5b9a batch \u7684\u6570\u636e\u6837\u672c\u7684\u56fe\u7247\u548c\u6807\u8bb0\u70b9\"\"\" images_batch, landmarks_batch = \\ sample_batched['image'], sample_batched['landmarks'] batch_size = len(images_batch) im_size = images_batch.size(2) grid = utils.make_grid(images_batch) plt.imshow(grid.numpy().transpose((1, 2, 0))) for i in range(batch_size): plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size, landmarks_batch[i, :, 1].numpy(), s=10, marker='.', c='r') plt.title('Batch from dataloader') for i_batch, sample_batched in enumerate(dataloader): print(i_batch, sample_batched['image'].size(), sample_batched['landmarks'].size()) # \u89c2\u5bdf\u5230\u7b2c\u56db\u6279\u6570\u636e\u65f6\u505c\u6b62 if i_batch == 3: plt.figure() show_landmarks_batch(sample_batched) plt.axis('off') plt.ioff() plt.show() break \u540e\u8bb0: torchvision \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d, \u6211\u4eec\u5b66\u4e60\u4e86\u5982\u4f55\u5199\u548c\u4f7f\u7528\u6570\u636e\u96c6, \u56fe\u50cf\u53d8\u6362\u548cdataloder. torchvision \u63d0\u4f9b\u4e86\u5e38\u7528\u7684\u6570\u636e\u96c6\u548c\u56fe\u50cf\u53d8\u6362, \u6216\u8bb8\u4f60\u751a\u81f3\u4e0d\u5fc5\u5199\u81ea\u5b9a\u4e49\u7684\u7c7b\u548c\u53d8\u6362. \u5728torchvision\u4e2d\u4e00\u4e2a\u6700\u7ecf\u5e38\u7528\u7684\u6570\u636e\u96c6\u662f ImageFolder . \u5b83\u8981\u6c42\u6570\u636e\u6309\u4e0b\u9762\u7684\u5f62\u5f0f\u5b58\u653e: root/ants/xxx.png root/ants/xxy.jpeg root/ants/xxz.png . . . root/bees/123.jpg root/bees/nsdf3.png root/bees/asd932_.png \u2018ants\u2019, \u2018bees\u2019 \u7b49\u662f\u56fe\u50cf\u7684\u7c7b\u6807. \u540c\u6837, PIL.Image \u4e2d\u51fa\u73b0\u7684\u4e00\u822c\u7684\u56fe\u50cf\u53d8\u6362\u50cf RandomHorizontalFlip , Scale \u4e5f\u662f\u53ef\u4ee5\u4f7f\u7528\u7684. \u4f60\u53ef\u4ee5\u50cf\u4e0b\u9762\u8fd9\u6837\u7528\u8fd9\u4e9b\u51fd\u6570\u6765\u5199dataloader: import torch from torchvision import transforms, datasets data_transform = transforms.Compose([ transforms.RandomSizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) hymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train', transform=data_transform) dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset, batch_size=4, shuffle=True, num_workers=4) \u5173\u4e8e\u8bad\u7ec3\u4ee3\u7801\u7684\u4f8b\u5b50, \u8bf7\u770b \u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b . Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: data_loading_tutorial.py Download Jupyter notebook: data_loading_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b"},{"location":"23/#_1","text":"\u4f5c\u8005 : Sasank Chilamkurthy \u5728\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u95ee\u9898\u65f6, \u6211\u4eec\u9700\u8981\u4ed8\u51fa\u5f88\u591a\u52aa\u529b\u6765\u51c6\u5907\u6570\u636e, \u4e3a\u4e86\u4f7f\u4ee3\u7801\u66f4\u5177\u53ef\u8bfb\u6027, PyTorch\u63d0\u4f9b\u4e86\u8bb8\u591a\u5de5\u5177\u6765\u4f7f\u6570\u636e\u52a0\u8f7d\u53d8\u5f97\u7b80\u5355\u6613\u884c. \u5728\u672c\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u8981\u5b66\u4e60\u5982\u4f55\u5bf9 \u4e00\u4e2a\u91cd\u8981\u7684\u6570\u636e\u96c6\u8fdb\u884c\u52a0\u8f7d\u3001\u9884\u5904\u7406\u6570\u636e\u589e\u5f3a. \u4e3a\u4e86\u8fd0\u884c\u672c\u6559\u7a0b, \u8bf7\u786e\u4fdd\u4ee5\u4e0b\u5305\u5df2\u7ecf\u5b89\u88c5: scikit-image : \u7528\u6765\u8bfb\u53d6\u56fe\u7247\u548c\u56fe\u50cf\u53d8\u6362 pandas : \u66f4\u65b9\u4fbf\u5730\u89e3\u6790csv\u6587\u4ef6 from __future__ import print_function, division import os import torch import pandas as pd from skimage import io, transform import numpy as np import matplotlib.pyplot as plt from torch.utils.data import Dataset, DataLoader from torchvision import transforms, utils # \u5ffd\u7565\u8b66\u544a import warnings warnings.filterwarnings(\"ignore\") plt.ion() # \u4ea4\u4e92\u6a21\u5f0f \u6211\u4eec\u5c06\u8981\u5904\u7406\u7684\u662f\u9762\u90e8\u59ff\u6001\u7684\u6570\u636e\u96c6, \u5176\u4e2d\u4e00\u5f20\u4eba\u8138\u56fe\u50cf\u5982\u4e0b\u56fe\u4e00\u6837\u88ab\u6807\u6ce8\u51fa\u6765. \u4e0b\u9762\u662f\u4e00\u5f20\u8138\u7684\u6807\u6ce8: \u6bcf\u5f20\u4eba\u8138\u56fe\u50cf\u4e0a, \u603b\u5171\u670968\u4e2a\u4e0d\u540c\u7684\u6807\u6ce8\u70b9\u88ab\u6807\u8bb0\u51fa\u6765. Note \u70b9\u51fb \u8fd9\u91cc <https://download.pytorch.org/tutorial/faces.zip> \u4e0b\u8f7d\u6570\u636e\u96c6, \u8fd9\u4e9b\u56fe\u50cf\u5728\u76ee\u5f55 \u2018 faces/ \u2018\u4e0b. \u8fd9\u4e2a\u6570\u636e\u96c6\u5b9e\u9645\u4e0a\u662f\u4eceimagenet\u6570\u636e\u96c6\u4e2d\u9009\u53d6\u6807\u8bb0\u4e3a\u4eba\u8138\u7684\u4e00\u4e9b\u56fe\u7247, \u4f7f\u7528 dlib\u2019s pose estimation \u65b9\u6cd5\u751f\u6210\u7684. \u6570\u636e\u96c6\u4e2d\u7684csv\u6587\u4ef6\u8bb0\u5f55\u7740\u6807\u6ce8\u4fe1\u606f, \u50cf\u4e0b\u9762\u8fd9\u6837: image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y 0805personali01.jpg,27,83,27,98, ... 84,134 1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312 \u8ba9\u6211\u4eec\u5feb\u901f\u5730\u8bfb\u53d6CSV\u6587\u4ef6, \u4ee5(N,2)\u7684\u6570\u7ec4\u5f62\u5f0f\u83b7\u5f97\u6807\u8bb0\u70b9, \u5176\u4e2dN\u8868\u793a\u6807\u8bb0\u70b9\u7684\u4e2a\u6570. landmarks_frame = pd.read_csv('faces/face_landmarks.csv') n = 65 img_name = landmarks_frame.iloc[n, 0] landmarks = landmarks_frame.iloc[n, 1:].as_matrix() landmarks = landmarks.astype('float').reshape(-1, 2) print('Image name: {}'.format(img_name)) print('Landmarks shape: {}'.format(landmarks.shape)) print('First 4 Landmarks: {}'.format(landmarks[:4])) \u6211\u4eec\u5199\u4e00\u4e2a\u51fd\u6570\u6765\u663e\u793a\u4e00\u5f20\u56fe\u7247\u548c\u5b83\u7684\u6807\u8bb0\u70b9, \u7136\u540e\u7528\u8fd9\u4e2a\u51fd\u6570\u6765\u663e\u793a\u4e00\u4e2a\u6837\u672c. def show_landmarks(image, landmarks): \"\"\"\u663e\u793a\u5e26\u6807\u8bb0\u70b9\u7684\u56fe\u7247\"\"\" plt.imshow(image) plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r') plt.pause(0.001) # \u6682\u505c\u4e00\u4e0b, \u4f7fplots\u66f4\u65b0 plt.figure() show_landmarks(io.imread(os.path.join('faces/', img_name)), landmarks) plt.show()","title":"\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b"},{"location":"23/#dataset","text":"torch.utils.data.Dataset \u662f\u4e00\u4e2a\u8868\u793a\u6570\u636e\u96c6\u7684\u62bd\u8c61\u7c7b. \u4f60\u81ea\u5df1\u7684\u6570\u636e\u96c6\u4e00\u822c\u5e94\u8be5\u7ee7\u627f Dataset , \u5e76\u4e14\u91cd\u5199\u4e0b\u9762\u7684\u65b9\u6cd5: __len__ \u4f7f\u7528 len(dataset) \u53ef\u4ee5\u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f __getitem__ \u652f\u6301\u7d22\u5f15, \u4ee5\u4fbf\u4e8e\u4f7f\u7528 dataset[i] \u53ef\u4ee5 \u83b7\u53d6\u7b2c:math:<cite>i</cite>\u4e2a\u6837\u672c(0\u7d22\u5f15) \u6211\u4eec\u4e3a\u6211\u4eec\u7684\u4eba\u8138\u6570\u636e\u96c6\u521b\u5efa\u4e00\u4e2a\u6570\u636e\u96c6\u7c7b. \u6211\u4eec\u4f7f\u7528 [ ](#id2)__init__ \u65b9\u6cd5\u6765\u8bfb\u53d6csv\u6587\u4ef6, \u4f7f\u7528 [ ](#id4)__getitem__ \u8bfb\u53d6\u56fe\u7247. \u8fd9\u6837\u53ef\u4ee5\u4f7f\u5185\u5b58\u9ad8\u6548\u5229\u7528, \u56e0\u4e3a\u6211\u4eec\u5e76\u4e0d\u9700\u8981\u5728\u5185\u5b58\u4e2d\u4e00\u6b21\u5b58\u50a8\u6240\u6709\u56fe\u7247, \u800c\u662f\u6309\u7167\u9700\u8981\u8bfb\u53d6. \u6570\u636e\u96c6\u7684\u4e00\u4e2a\u6837\u4f8b\u662f\u4e00\u4e2a {\u2018image\u2019: image, \u2018landmarks\u2019: landmarks} \u6837\u7684\u5b57\u5178. \u6570\u636e\u96c6\u7c7b\u4e2d\u6709\u4e00\u4e2a\u53ef\u9009\u7684\u53c2\u6570 transform \u8fd9\u6837\u53ef\u4ee5\u5bf9\u6570\u636e\u96c6\u505a\u4efb\u4f55\u9700\u8981\u7684\u5904\u7406. \u6211\u4eec\u5c06\u5728\u4e0b\u8282\u770b\u5230 transform \u7684\u7528\u5904. class FaceLandmarksDataset(Dataset): \"\"\"\u4eba\u8138\u6807\u8bb0\u6570\u636e\u96c6\"\"\" def __init__(self, csv_file, root_dir, transform=None): \"\"\" Args: csv_file (string): \u5e26\u6709\u6807\u8bb0\u70b9\u7684csv\u6587\u4ef6\u8def\u5f84 root_dir (string): \u56fe\u7247\u8def\u5f84 transform (callable, optional):\u53ef\u9009\u62e9\u8fdb\u884c\u7684\u56fe\u50cf\u53d8\u6362 \"\"\" self.landmarks_frame = pd.read_csv(csv_file) self.root_dir = root_dir self.transform = transform def __len__(self): return len(self.landmarks_frame) def __getitem__(self, idx): img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0]) image = io.imread(img_name) landmarks = self.landmarks_frame.iloc[idx, 1:].as_matrix() landmarks = landmarks.astype('float').reshape(-1, 2) sample = {'image': image, 'landmarks': landmarks} if self.transform: sample = self.transform(sample) return sample \u8ba9\u6211\u4eec\u5b9e\u4f8b\u5316\u8fd9\u4e2a\u7c7b \u5e76\u4e14\u8fed\u4ee3\u6240\u6709\u7684\u6570\u636e\u6837\u672c. \u6211\u4eec\u5c06\u6253\u5370\u524d4\u4e2a\u6837\u672c, \u5e76\u663e\u793a\u5b83\u4eec\u7684\u6807\u8bb0\u70b9. face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv', root_dir='faces/') fig = plt.figure() for i in range(len(face_dataset)): sample = face_dataset[i] print(i, sample['image'].shape, sample['landmarks'].shape) ax = plt.subplot(1, 4, i + 1) plt.tight_layout() ax.set_title('Sample #{}'.format(i)) ax.axis('off') show_landmarks(**sample) if i == 3: plt.show() break","title":"Dataset\u7c7b"},{"location":"23/#transforms","text":"\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e0a\u9762\u8f93\u51fa\u7684\u6837\u4f8b\u4e2d\u7684\u56fe\u50cf\u5e76\u4e0d\u662f\u540c\u4e00\u5c3a\u5bf8\u7684\u56fe\u7247. \u5927\u591a\u6570\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u8f93\u5165 \u4e00\u4e2a\u56fa\u5b9a\u5927\u5c0f\u7684\u56fe\u50cf, \u56e0\u6b64\u6211\u4eec\u9700\u8981\u5199\u4ee3\u7801\u6765\u5904\u7406. \u8ba9\u6211\u4eec\u521b\u5efa\u4e09\u4e2atransform\u64cd\u4f5c: Rescale : \u4fee\u6539\u56fe\u7247\u5c3a\u5bf8 RandomCrop : \u968f\u673a\u88c1\u5207\u56fe\u7247, \u8fd9\u662f\u6570\u636e\u589e\u5f3a\u7684\u65b9\u6cd5 ToTensor : \u5c06numpy\u683c\u5f0f\u7684\u56fe\u7247\u8f6c\u4e3atorch\u683c\u5f0f\u7684\u56fe\u7247\uff08\u6211\u4eec\u9700\u8981\u4ea4\u6362\u5750\u6807\u8f74\uff09 \u6211\u4eec\u4e0d\u5c06\u5b83\u4eec\u5199\u6210\u7b80\u5355\u7684\u51fd\u6570, \u800c\u662f\u5199\u6210\u53ef\u4ee5\u8c03\u7528\u7684\u7c7b, \u8fd9\u6837transform\u7684\u53c2\u6570\u4e0d\u9700\u8981\u6bcf\u6b21\u90fd\u4f20\u9012 \u5982\u679c\u9700\u8981\u7684\u8bdd, \u6211\u4eec\u53ea\u9700\u5b9e\u73b0 __call__ \u65b9\u6cd5\u548c __init__ \u65b9\u6cd5.\u4e4b\u540e\u6211\u4eec\u53ef\u4ee5\u50cf\u4e0b\u9762\u8fd9 \u6837\u4f7f\u7528transform: tsfm = Transform(params) transformed_sample = tsfm(sample) \u89c2\u5bdf\u4e0b\u9762\u8fd9\u4e9b\u53d8\u6362\u5982\u4f55\u540c\u65f6\u5bf9\u56fe\u50cf\u548c\u6807\u8bb0\u70b9\u6539\u53d8\u7684. class Rescale(object): \"\"\"\u6309\u7167\u7ed9\u5b9a\u5c3a\u5bf8\u66f4\u6539\u4e00\u4e2a\u56fe\u50cf\u7684\u5c3a\u5bf8 Args: output_size (tuple or int): \u8981\u6c42\u8f93\u51fa\u7684\u5c3a\u5bf8. \u5982\u679c\u662f\u4e2a\u5143\u7ec4\u7c7b\u578b, \u8f93\u51fa \u548coutput_size\u5339\u914d. \u5982\u679c\u65f6int\u7c7b\u578b,\u56fe\u7247\u7684\u77ed\u8fb9\u548coutput_size\u5339\u914d, \u56fe\u7247\u7684 \u957f\u5bbd\u6bd4\u4fdd\u6301\u4e0d\u53d8. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] if isinstance(self.output_size, int): if h > w: new_h, new_w = self.output_size * h / w, self.output_size else: new_h, new_w = self.output_size, self.output_size * w / h else: new_h, new_w = self.output_size new_h, new_w = int(new_h), int(new_w) img = transform.resize(image, (new_h, new_w)) # \u5bf9\u4e8e\u6807\u8bb0\u70b9, h\u548cw\u9700\u8981\u4ea4\u6362\u4f4d\u7f6e, \u56e0\u4e3a\u5bf9\u4e8e\u56fe\u50cf, x\u548cy\u5206\u522b\u65f6\u7b2c1\u7ef4\u548c\u7b2c0\u7ef4 landmarks = landmarks * [new_w / w, new_h / h] return {'image': img, 'landmarks': landmarks} class RandomCrop(object): \"\"\"\u968f\u673a\u88c1\u526a\u56fe\u7247 Args: output_size (tuple or int): \u671f\u671b\u8f93\u51fa\u7684\u5c3a\u5bf8, \u5982\u679c\u65f6int\u7c7b\u578b, \u88c1\u5207\u6210\u6b63\u65b9\u5f62. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) if isinstance(output_size, int): self.output_size = (output_size, output_size) else: assert len(output_size) == 2 self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] new_h, new_w = self.output_size top = np.random.randint(0, h - new_h) left = np.random.randint(0, w - new_w) image = image[top: top + new_h, left: left + new_w] landmarks = landmarks - [left, top] return {'image': image, 'landmarks': landmarks} class ToTensor(object): \"\"\"\u5c06ndarrays\u7684\u6837\u672c\u8f6c\u5316\u4e3aTensors\u7684\u6837\u672c\"\"\" def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] # \u4ea4\u6362\u989c\u8272\u901a\u9053, \u56e0\u4e3a # numpy\u56fe\u7247: H x W x C # torch\u56fe\u7247 : C X H X W image = image.transpose((2, 0, 1)) return {'image': torch.from_numpy(image), 'landmarks': torch.from_numpy(landmarks)}","title":"Transforms"},{"location":"23/#compose-transforms","text":"\u73b0\u5728\u6211\u4eec\u5c31\u5c06 transform \u5e94\u7528\u5728\u4e00\u4e2a\u6837\u672c\u4e0a. \u5982\u679c\u6211\u4eec\u60f3\u5c06\u56fe\u7247\u7684\u77ed\u8fb9\u53d8\u4e3a256\u50cf\u7d20, \u5e76\u4e14\u968f\u540e\u968f\u673a\u88c1\u5207\u6210224\u50cf\u7d20\u7684\u6b63\u65b9\u5f62. i.e, \u6211\u4eec\u53ef\u4ee5\u7ec4\u5408 Rescale \u548c RandomCrop \u53d8\u6362. torchvision.transforms.Compose \u5c31\u662f\u4e00\u4e2a\u53ef\u4ee5\u505a\u8fd9\u6837\u4e00\u4e2a\u7ec4\u5408\u7684\u53ef\u8c03\u7528\u7684\u7c7b. scale = Rescale(256) crop = RandomCrop(128) composed = transforms.Compose([Rescale(256), RandomCrop(224)]) # \u5bf9\u6bcf\u4e2a\u6837\u672c\u8fdb\u884c\u4e0a\u9762\u7684\u6bcf\u4e00\u4e2a\u64cd\u4f5c. fig = plt.figure() sample = face_dataset[65] for i, tsfrm in enumerate([scale, crop, composed]): transformed_sample = tsfrm(sample) ax = plt.subplot(1, 3, i + 1) plt.tight_layout() ax.set_title(type(tsfrm).__name__) show_landmarks(**transformed_sample) plt.show()","title":"Compose transforms"},{"location":"23/#_2","text":"\u8ba9\u6211\u4eec\u8fdb\u884c\u6240\u6709\u64cd\u4f5c, \u6765\u521b\u5efa\u7ed3\u5408\u4e86\u56fe\u50cf\u53d8\u6362\u7684\u6570\u636e\u96c6 \u603b\u4e4b, \u6bcf\u6b21\u8fed\u4ee3\u7684\u6570\u636e: \u4ece\u6587\u4ef6\u4e2d\u8bfb\u53d6\u56fe\u50cf \u5bf9\u6240\u8bfb\u7684\u56fe\u50cf\u4e0a\u5e94\u7528\u53d8\u6362 \u5176\u4e2d\u4e00\u4e2a\u53d8\u6362\u662f\u968f\u673a\u7684, \u6240\u4ee5\u53ef\u4ee5\u589e\u5f3a\u6570\u636e\u6837\u672c \u6211\u4eec\u53ef\u4ee5\u50cf\u4e4b\u524d\u4e00\u6837\u7528 for i in range \u5faa\u73af\u4ece\u5df2\u521b\u5efa\u7684\u6570\u636e\u96c6\u4e2d\u8fed\u4ee3 transformed_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv', root_dir='faces/', transform=transforms.Compose([ Rescale(256), RandomCrop(224), ToTensor() ])) for i in range(len(transformed_dataset)): sample = transformed_dataset[i] print(i, sample['image'].size(), sample['landmarks'].size()) if i == 3: break \u7136\u800c\u6211\u4eec\u7528\u7b80\u5355\u7684 for \u5faa\u73af\u6765\u8fed\u4ee3\u6574\u4e2a\u6570\u636e\u96c6\u4f1a\u4e22\u5931\u5f88\u591a\u7279\u70b9, \u7279\u522b\u5730, \u6211\u4eec\u4f1a\u4e22\u5931: \u6279\u8bfb\u53d6\u6570\u636e \u6253\u4e71\u6570\u636e\u987a\u5e8f \u4f7f\u7528 multiprocessing \u5e76\u884c\u52a0\u8f7d\u6570\u636e torch.utils.data.DataLoader \u662f\u63d0\u4f9b\u4e86\u6240\u6709\u4e0a\u8ff0\u7279\u70b9\u7684\u8fed\u4ee3\u5668. \u4e0b\u9762\u4f7f\u7528\u7684\u53c2\u6570\u5e94\u8be5\u5f88\u6e05\u6670. \u5176\u4e2d\u4e00\u4e2a\u6709\u8da3\u7684\u53c2\u6570\u662f collate_fn . \u4f60\u53ef\u4ee5\u4f7f\u7528 collate_fn \u6765\u6307\u5b9a\u5982\u4f55\u7cbe\u786e\u5730\u8bfb\u53d6\u4e00\u6279\u7684\u6837\u672c. \u7136\u800c, \u9ed8\u8ba4\u7684collate\u5728\u5927\u90e8\u5206\u7684\u60c5\u51b5\u4e0b\u90fd\u8868\u73b0\u5f97\u5f88\u597d dataloader = DataLoader(transformed_dataset, batch_size=4, shuffle=True, num_workers=4) # \u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\u6765\u67e5\u770b\u67d0\u4e2a batch \u7684\u6570\u636e\u6837\u672c\u56fe\u7247\u548c\u6807\u8bb0\u70b9 def show_landmarks_batch(sample_batched): \"\"\"\u663e\u793a\u6307\u5b9a batch \u7684\u6570\u636e\u6837\u672c\u7684\u56fe\u7247\u548c\u6807\u8bb0\u70b9\"\"\" images_batch, landmarks_batch = \\ sample_batched['image'], sample_batched['landmarks'] batch_size = len(images_batch) im_size = images_batch.size(2) grid = utils.make_grid(images_batch) plt.imshow(grid.numpy().transpose((1, 2, 0))) for i in range(batch_size): plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size, landmarks_batch[i, :, 1].numpy(), s=10, marker='.', c='r') plt.title('Batch from dataloader') for i_batch, sample_batched in enumerate(dataloader): print(i_batch, sample_batched['image'].size(), sample_batched['landmarks'].size()) # \u89c2\u5bdf\u5230\u7b2c\u56db\u6279\u6570\u636e\u65f6\u505c\u6b62 if i_batch == 3: plt.figure() show_landmarks_batch(sample_batched) plt.axis('off') plt.ioff() plt.show() break","title":"\u8fed\u4ee3\u6574\u4e2a\u6570\u636e\u96c6"},{"location":"23/#torchvision","text":"\u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d, \u6211\u4eec\u5b66\u4e60\u4e86\u5982\u4f55\u5199\u548c\u4f7f\u7528\u6570\u636e\u96c6, \u56fe\u50cf\u53d8\u6362\u548cdataloder. torchvision \u63d0\u4f9b\u4e86\u5e38\u7528\u7684\u6570\u636e\u96c6\u548c\u56fe\u50cf\u53d8\u6362, \u6216\u8bb8\u4f60\u751a\u81f3\u4e0d\u5fc5\u5199\u81ea\u5b9a\u4e49\u7684\u7c7b\u548c\u53d8\u6362. \u5728torchvision\u4e2d\u4e00\u4e2a\u6700\u7ecf\u5e38\u7528\u7684\u6570\u636e\u96c6\u662f ImageFolder . \u5b83\u8981\u6c42\u6570\u636e\u6309\u4e0b\u9762\u7684\u5f62\u5f0f\u5b58\u653e: root/ants/xxx.png root/ants/xxy.jpeg root/ants/xxz.png . . . root/bees/123.jpg root/bees/nsdf3.png root/bees/asd932_.png \u2018ants\u2019, \u2018bees\u2019 \u7b49\u662f\u56fe\u50cf\u7684\u7c7b\u6807. \u540c\u6837, PIL.Image \u4e2d\u51fa\u73b0\u7684\u4e00\u822c\u7684\u56fe\u50cf\u53d8\u6362\u50cf RandomHorizontalFlip , Scale \u4e5f\u662f\u53ef\u4ee5\u4f7f\u7528\u7684. \u4f60\u53ef\u4ee5\u50cf\u4e0b\u9762\u8fd9\u6837\u7528\u8fd9\u4e9b\u51fd\u6570\u6765\u5199dataloader: import torch from torchvision import transforms, datasets data_transform = transforms.Compose([ transforms.RandomSizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) hymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train', transform=data_transform) dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset, batch_size=4, shuffle=True, num_workers=4) \u5173\u4e8e\u8bad\u7ec3\u4ee3\u7801\u7684\u4f8b\u5b50, \u8bf7\u770b \u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b . Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: data_loading_tutorial.py Download Jupyter notebook: data_loading_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u540e\u8bb0: torchvision"},{"location":"24/","text":"\u9488\u5bf9NLP\u7684Pytorch\u6df1\u5ea6\u5b66\u4e60 \u4f5c\u8005 : Robert Guthrie \u672c\u6559\u7a0b\u5c06\u5e26\u4f60\u6d4f\u89c8\u57fa\u4e8ePytorch\u6df1\u5ea6\u5b66\u4e60\u7f16\u7a0b\u7684\u6838\u5fc3\u601d\u60f3.\u5176\u4e2d\u5f88\u591a\u601d\u60f3(\u4f8b\u5982\u8ba1\u7b97\u56fe\u5f62\u62bd\u8c61\u5316\u4ee5\u53ca\u81ea\u52a8\u6c42\u5bfc) \u5e76\u4e0d\u662fPytorch\u7279\u6709\u7684,\u4ed6\u4eec\u548c\u4efb\u4f55\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305\u90fd\u662f\u76f8\u5173\u7684. \u672c\u6559\u7a0b\u9488\u5bf9\u90a3\u4e9b\u4ece\u672a\u5728\u4efb\u4f55\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e0b\u7f16\u5199\u8fc7\u4ee3\u7801\u7684\u4eba(\u4f8b\u5982TensorFlow,Theano, Keras, Dynet),\u5e76 \u4e13\u6ce8\u4e8eNLP.\u5b83\u63d0\u51fa\u4e86\u5e94\u7528\u77e5\u8bc6\u4e2dNLP\u7684\u6838\u5fc3\u95ee\u9898:\u8bcd\u6027\u6807\u6ce8,\u8bed\u8a00\u5efa\u6a21\u7b49.\u5b83\u540c\u6837\u63d0\u51fa\u4e86\u5728AI\u5165\u95e8\u7ea7\u522b\u719f\u6089\u795e\u7ecf \u7f51\u7edc(\u4f8b\u5982Russel\u548cNorvig\u7684\u4e66).\u901a\u5e38\u60c5\u51b5\u4e0b, \u8fd9\u4e9b\u8bfe\u7a0b\u5305\u62ec\u4e86\u57fa\u4e8e\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u672c\u7684\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5, \u5e76\u4f7f\u4f60\u4e86\u89e3\u5230\u5b83\u4eec\u662f\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u7ec4\u6210\u7684\u94fe\u6761.\u672c\u6559\u7a0b\u76ee\u7684\u4f7f\u4f60\u5f00\u59cb\u7f16\u5199\u6df1\u5ea6\u5b66\u4e60\u4ee3\u7801\u5e76\u7ed9\u4f60\u9996\u8981\u5fc5\u5907\u7684\u77e5\u8bc6. \u63d0\u793a\u4e00\u4e0b, \u8fd9\u4ec5\u5173\u4e4e\u4e8e \u6a21\u578b , \u5e76\u975e\u662f\u6570\u636e.\u9488\u5bf9\u6240\u6709\u6a21\u578b,\u6211\u4ec5\u4ec5\u63d0\u51fa\u4e86\u4e00\u4e9b\u4f4e\u7eac\u5ea6\u7684\u4f8b\u5b50 \u4ee5\u4fbf\u4e8e\u4f60\u53ef\u4ee5\u89c2\u5bdf\u5f53\u8bad\u7ec3\u65f6\u6743\u91cd\u7684\u53d8\u5316.\u5982\u679c\u4f60\u6709\u4e00\u4e9b\u771f\u5b9e\u6570\u636e\u53bb\u5c1d\u8bd5,\u53ef\u4ee5\u5c06\u672c\u6559\u7a0b\u4e2d\u6a21\u578b \u590d\u5236\u4e0b\u5e76\u5c06\u6570\u636e\u5e94\u7528\u5230\u6a21\u578b\u4e0a. PyTorch\u4ecb\u7ecd PyTorch\u6df1\u5ea6\u5b66\u4e60 \u8bcd\u6c47\u5d4c\u5165:\u7f16\u7801\u8bcd\u6c47\u8bed\u4e49 \u5e8f\u5217\u6a21\u578b\u548c LSTM \u7f51\u7edc\uff08\u957f\u77ed\u8bb0\u5fc6\u7f51\u7edc\uff09 \u9ad8\u7ea7\u6559\u7a0b: \u4f5c\u51fa\u52a8\u6001\u51b3\u7b56\u548c Bi-LSTM CRF","title":"\u9488\u5bf9NLP\u7684Pytorch\u6df1\u5ea6\u5b66\u4e60"},{"location":"24/#nlppytorch","text":"\u4f5c\u8005 : Robert Guthrie \u672c\u6559\u7a0b\u5c06\u5e26\u4f60\u6d4f\u89c8\u57fa\u4e8ePytorch\u6df1\u5ea6\u5b66\u4e60\u7f16\u7a0b\u7684\u6838\u5fc3\u601d\u60f3.\u5176\u4e2d\u5f88\u591a\u601d\u60f3(\u4f8b\u5982\u8ba1\u7b97\u56fe\u5f62\u62bd\u8c61\u5316\u4ee5\u53ca\u81ea\u52a8\u6c42\u5bfc) \u5e76\u4e0d\u662fPytorch\u7279\u6709\u7684,\u4ed6\u4eec\u548c\u4efb\u4f55\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305\u90fd\u662f\u76f8\u5173\u7684. \u672c\u6559\u7a0b\u9488\u5bf9\u90a3\u4e9b\u4ece\u672a\u5728\u4efb\u4f55\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e0b\u7f16\u5199\u8fc7\u4ee3\u7801\u7684\u4eba(\u4f8b\u5982TensorFlow,Theano, Keras, Dynet),\u5e76 \u4e13\u6ce8\u4e8eNLP.\u5b83\u63d0\u51fa\u4e86\u5e94\u7528\u77e5\u8bc6\u4e2dNLP\u7684\u6838\u5fc3\u95ee\u9898:\u8bcd\u6027\u6807\u6ce8,\u8bed\u8a00\u5efa\u6a21\u7b49.\u5b83\u540c\u6837\u63d0\u51fa\u4e86\u5728AI\u5165\u95e8\u7ea7\u522b\u719f\u6089\u795e\u7ecf \u7f51\u7edc(\u4f8b\u5982Russel\u548cNorvig\u7684\u4e66).\u901a\u5e38\u60c5\u51b5\u4e0b, \u8fd9\u4e9b\u8bfe\u7a0b\u5305\u62ec\u4e86\u57fa\u4e8e\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u672c\u7684\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5, \u5e76\u4f7f\u4f60\u4e86\u89e3\u5230\u5b83\u4eec\u662f\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u7ec4\u6210\u7684\u94fe\u6761.\u672c\u6559\u7a0b\u76ee\u7684\u4f7f\u4f60\u5f00\u59cb\u7f16\u5199\u6df1\u5ea6\u5b66\u4e60\u4ee3\u7801\u5e76\u7ed9\u4f60\u9996\u8981\u5fc5\u5907\u7684\u77e5\u8bc6. \u63d0\u793a\u4e00\u4e0b, \u8fd9\u4ec5\u5173\u4e4e\u4e8e \u6a21\u578b , \u5e76\u975e\u662f\u6570\u636e.\u9488\u5bf9\u6240\u6709\u6a21\u578b,\u6211\u4ec5\u4ec5\u63d0\u51fa\u4e86\u4e00\u4e9b\u4f4e\u7eac\u5ea6\u7684\u4f8b\u5b50 \u4ee5\u4fbf\u4e8e\u4f60\u53ef\u4ee5\u89c2\u5bdf\u5f53\u8bad\u7ec3\u65f6\u6743\u91cd\u7684\u53d8\u5316.\u5982\u679c\u4f60\u6709\u4e00\u4e9b\u771f\u5b9e\u6570\u636e\u53bb\u5c1d\u8bd5,\u53ef\u4ee5\u5c06\u672c\u6559\u7a0b\u4e2d\u6a21\u578b \u590d\u5236\u4e0b\u5e76\u5c06\u6570\u636e\u5e94\u7528\u5230\u6a21\u578b\u4e0a. PyTorch\u4ecb\u7ecd PyTorch\u6df1\u5ea6\u5b66\u4e60 \u8bcd\u6c47\u5d4c\u5165:\u7f16\u7801\u8bcd\u6c47\u8bed\u4e49 \u5e8f\u5217\u6a21\u578b\u548c LSTM \u7f51\u7edc\uff08\u957f\u77ed\u8bb0\u5fc6\u7f51\u7edc\uff09 \u9ad8\u7ea7\u6559\u7a0b: \u4f5c\u51fa\u52a8\u6001\u51b3\u7b56\u548c Bi-LSTM CRF","title":"\u9488\u5bf9NLP\u7684Pytorch\u6df1\u5ea6\u5b66\u4e60"},{"location":"25/","text":"PyTorch\u4ecb\u7ecd Torch\u5f20\u91cf\u5e93\u4ecb\u7ecd \u6240\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u90fd\u662f\u5728\u5f20\u91cf\u4e0a\u8ba1\u7b97\u7684,\u5176\u4e2d\u5f20\u91cf\u662f\u4e00\u4e2a\u53ef\u4ee5\u88ab\u8d85\u8fc7\u4e8c\u7ef4\u7d22\u5f15\u7684\u77e9\u9635\u7684\u4e00\u822c\u5316. \u7a0d\u540e\u6211\u4eec\u5c06\u8be6\u7ec6\u8ba8\u8bba\u8fd9\u610f\u5473\u7740\u4ec0\u4e48.\u9996\u5148,\u6211\u4eec\u5148\u6765\u770b\u4e00\u4e0b\u6211\u4eec\u53ef\u4ee5\u7528\u5f20\u91cf\u6765\u5e72\u4ec0\u4e48. # \u4f5c\u8005: Robert Guthrie import torch import torch.autograd as autograd import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) Creating Tensors(\u521b\u5efa\u5f20\u91cf) \u5f20\u91cf\u53ef\u4ee5\u5728Python list\u5f62\u5f0f\u4e0b\u901a\u8fc7torch.Tensor()\u51fd\u6570\u521b\u5efa. # \u5229\u7528\u7ed9\u5b9a\u6570\u636e\u521b\u5efa\u4e00\u4e2atorch.Tensor\u5bf9\u8c61.\u8fd9\u662f\u4e00\u4e2a\u4e00\u7ef4\u5411\u91cf V_data = [1., 2., 3.] V = torch.Tensor(V_data) print(V) # \u521b\u5efa\u4e00\u4e2a\u77e9\u9635 M_data = [[1., 2., 3.], [4., 5., 6]] M = torch.Tensor(M_data) print(M) # \u521b\u5efa2x2x2\u5f62\u5f0f\u7684\u4e09\u7ef4\u5f20\u91cf. T_data = [[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]] T = torch.Tensor(T_data) print(T) \u4ec0\u4e48\u662f\u4e09\u7ef4\u5f20\u91cf? \u8ba9\u6211\u4eec\u8fd9\u6837\u60f3\u8c61.\u5982\u679c\u4f60\u6709\u4e00\u4e2a\u5411\u91cf,\u90a3\u4e48\u5bf9\u5411\u91cf\u7d22\u5f15\u5c31\u4f1a\u5f97\u5230\u4e00\u4e2a\u6807\u91cf. \u5982\u679c\u4f60\u6709\u4e00\u4e2a\u77e9\u9635,\u5bf9\u77e9\u9635\u7d22\u5f15\u90a3\u4e48\u5c31\u4f1a\u5f97\u5230\u4e00\u4e2a\u5411\u91cf.\u5982\u679c\u4f60\u6709\u4e00\u4e2a\u4e09\u7ef4\u5f20\u91cf,\u90a3\u4e48\u5bf9\u5176\u7d22\u5f15 \u5c31\u4f1a\u5f97\u5230\u4e00\u4e2a\u77e9\u9635! \u9488\u5bf9\u672f\u8bed\u7684\u8bf4\u660e: \u5f53\u6211\u5728\u672c\u6559\u7a0b\u5185\u4f7f\u7528\u201dtensor\u201d,\u5b83\u9488\u5bf9\u7684\u662f\u6240\u6709torch.Tensor\u5bf9\u8c61.\u77e9\u9635\u548c\u5411\u91cf\u662f\u7279\u6b8a\u7684torch.Tensors, \u4ed6\u4eec\u7684\u7ef4\u5ea6\u5206\u522b\u662f1\u548c2.\u5f53\u6211\u8bf4\u5230\u4e09\u7ef4\u5f20\u91cf,\u6211\u4f1a\u7b80\u6d01\u7684\u4f7f\u7528\u201d3D tensor\u201d. # \u7d22\u5f15V\u5f97\u5230\u4e00\u4e2a\u6807\u91cf print(V[0]) # \u7d22\u5f15M\u5f97\u5230\u4e00\u4e2a\u5411\u91cf print(M[0]) # \u7d22\u5f15T\u5f97\u5230\u4e00\u4e2a\u77e9\u9635 print(T[0]) \u4f60\u4e5f\u53ef\u4ee5\u521b\u5efa\u5176\u4ed6\u6570\u636e\u7c7b\u578b\u7684tensors.\u9ed8\u8ba4\u7684\u6570\u636e\u7c7b\u578b\u4e3aFloat(\u6d6e\u70b9\u578b). \u53ef\u4ee5\u4f7f\u7528torch.LongTensor()\u6765 \u521b\u5efa\u4e00\u4e2a\u6574\u6570\u7c7b\u578b\u7684tensor.\u4f60\u53ef\u4ee5\u5728\u6587\u4ef6\u4e2d\u5bfb\u627e\u66f4\u591a\u7684\u6570\u636e\u7c7b\u578b,\u4f46\u662fFloat(\u6d6e\u70b9\u578b)\u548cLong(\u957f\u6574\u5f62)\u6700\u5e38\u7528\u7684. \u4f60\u53ef\u4ee5\u4f7f\u7528torch.randn()\u521b\u5efa\u4e00\u4e2a\u968f\u673a\u6570\u636e\u548c\u9700\u8981\u63d0\u4f9b\u7ef4\u5ea6\u7684tensor. x = torch.randn((3, 4, 5)) print(x) Operations with Tensors(\u5bf9tensor\u8fdb\u884c\u64cd\u4f5c) \u4f60\u53ef\u4ee5\u4ee5\u4f60\u60f3\u8981\u7684\u65b9\u5f0f\u64cd\u4f5ctensor. x = torch.Tensor([1., 2., 3.]) y = torch.Tensor([4., 5., 6.]) z = x + y print(z) \u53ef\u4ee5\u67e5\u9605 \u6587\u6863 \u83b7\u53d6\u5927\u91cf\u53ef\u7528\u64cd\u4f5c\u7684\u5b8c\u6574\u5217\u8868, \u6269\u5c55\u5230\u4e86\u975e\u6570\u5b66\u64cd\u4f5c. \u63a5\u4e0b\u6765\u4e00\u4e2a\u5f88\u6709\u5e2e\u52a9\u7684\u64cd\u4f5c\u5c31\u662f\u8fde\u63a5. # \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u5b83\u6cbf\u7740\u7b2c\u4e00\u8f74\u8fde\u63a5 (\u8fde\u63a5\u884c) x_1 = torch.randn(2, 5) y_1 = torch.randn(3, 5) z_1 = torch.cat([x_1, y_1]) print(z_1) # \u8fde\u63a5\u5217: x_2 = torch.randn(2, 3) y_2 = torch.randn(2, 5) # \u7b2c\u4e8c\u4e2a\u6570\u6307\u5b9a\u4e86\u6cbf\u7740\u54ea\u6761\u8f74\u8fde\u63a5 z_2 = torch.cat([x_2, y_2], 1) print(z_2) # \u5982\u679c\u4f60\u7684tensors\u662f\u4e0d\u517c\u5bb9\u7684,torch\u4f1a\u62a5\u9519.\u53d6\u6d88\u6ce8\u91ca\u6765\u67e5\u770b\u9519\u8bef. # torch.cat([x_1, x_2]) Reshaping Tensors(\u91cd\u6784Tensors) \u4f7f\u7528.view()\u53bb\u91cd\u6784tensor.\u8fd9\u662f\u4e00\u4e2a\u9ad8\u9891\u65b9\u6cd5, \u56e0\u4e3a\u8bb8\u591a\u795e\u7ecf\u7f51\u7edc\u7684\u795e\u7ecf\u5143\u5bf9\u8f93\u5165\u683c\u5f0f \u6709\u660e\u786e\u7684\u8981\u6c42. \u4f60\u901a\u5e38\u9700\u8981\u5148\u5c06\u6570\u636e\u91cd\u6784\u518d\u8f93\u5165\u5230\u795e\u7ecf\u5143\u4e2d. x = torch.randn(2, 3, 4) print(x) print(x.view(2, 12)) # \u91cd\u6784\u4e3a2\u884c12\u5217 # \u540c\u4e0a.\u5982\u679c\u7ef4\u5ea6\u4e3a-1,\u90a3\u4e48\u5b83\u7684\u7ef4\u5ea6\u6839\u636e\u6570\u636e\u63a8\u65ad\u51fa\u6765 print(x.view(2, -1)) Computation Graphs and Automatic Differentiation(\u8ba1\u7b97\u56fe\u548c\u81ea\u52a8\u6c42\u5bfc) \u8ba1\u7b97\u56fe\u7684\u601d\u60f3\u5bf9\u4e8e\u6709\u6548\u7387\u7684\u6df1\u5ea6\u5b66\u4e60\u7f16\u7a0b\u662f\u5f88\u91cd\u8981\u7684, \u56e0\u4e3a\u5b83\u5141\u8bb8\u4f60\u4e0d\u5fc5\u53bb\u81ea\u5df1\u5199\u53cd\u5411\u68af\u5ea6\u4f20\u64ad. \u8ba1\u7b97\u56fe\u53ea\u662f\u7b80\u5355\u5730\u8bf4\u660e\u4e86\u5982\u4f55\u5c06\u6570\u636e\u7ec4\u5408\u5728\u4e00\u8d77\u4ee5\u8f93\u51fa\u7ed3\u679c.\u56e0\u4e3a\u56fe\u5b8c\u5168\u6307\u5b9a\u4e86\u64cd\u4f5c\u6240\u5305\u542b\u7684\u53c2\u6570, \u56e0\u6b64\u5b83\u5305\u542b\u4e86\u8db3\u591f\u7684\u4fe1\u606f\u53bb\u6c42\u5bfc.\u8fd9\u53ef\u80fd\u542c\u8d77\u6765\u5f88\u6a21\u7cca, \u6240\u4ee5\u8ba9\u6211\u4eec\u770b\u770b\u4f7f\u7528Pytorch\u7684\u57fa\u672c\u7c7b: autograd.Variable. \u9996\u5148, \u4ece\u7a0b\u5e8f\u5458\u7684\u89d2\u5ea6\u6765\u601d\u8003.\u5728torch\u4e2d\u5b58\u50a8\u4e86\u4ec0\u4e48, \u662f\u6211\u4eec\u5728\u4e0a\u9762\u521b\u5efa\u7684Tensor\u5bf9\u8c61\u5417? \u663e\u7136, \u662f\u6570\u636e\u548c \u7ed3\u6784, \u4e5f\u5f88\u53ef\u80fd\u662f\u5176\u4ed6\u7684\u4e1c\u897f. \u4f46\u662f\u5f53\u6211\u4eec\u5c06\u4e24\u4e2atensors\u76f8\u52a0\u540e, \u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4e2a\u8f93\u51fatensor.\u8fd9\u4e2a\u8f93\u51fa\u6240\u80fd \u4f53\u73b0\u51fa\u7684\u53ea\u6709\u6570\u636e\u548c\u7ed3\u6784, \u5e76\u4e0d\u80fd\u4f53\u73b0\u51fa\u662f\u7531\u4e24\u4e2atensors\u52a0\u548c\u5f97\u5230\u7684(\u56e0\u4e3a\u5b83\u53ef\u80fd\u662f\u4ece\u4e00\u4e2a\u6587\u4ef6\u4e2d\u8bfb\u53d6\u7684, \u4e5f\u53ef\u80fd\u662f \u5176\u4ed6\u64cd\u4f5c\u7684\u7ed3\u679c\u7b49). \u53d8\u91cf\u7c7b\u522b\u53ef\u4ee5\u4e00\u76f4\u8ddf\u8e2a\u5b83\u662f\u5982\u4f55\u521b\u5efa\u7684.\u8ba9\u6211\u4eec\u5728\u5b9e\u9645\u4e2d\u6765\u770b. # \u53d8\u91cf\u56f4\u7ed5tensor\u5bf9\u8c61 x = autograd.Variable(torch.Tensor([1., 2., 3]), requires_grad=True) # \u60a8\u53ef\u4ee5\u4f7f\u7528.data\u5c5e\u6027\u8bbf\u95ee\u6570\u636e. print(x.data) # \u4f60\u4e5f\u53ef\u4ee5\u7528\u53d8\u91cf\u6765\u505a\u4e0e\u5f20\u91cf\u76f8\u540c\u7684\u8fd0\u7b97. y = autograd.Variable(torch.Tensor([4., 5., 6]), requires_grad=True) z = x + y print(z.data) # \u4f46\u662fz\u77e5\u9053\u4e00\u4e9b\u989d\u5916\u7684\u4e1c\u897f. print(z.grad_fn) \u65e2\u7136\u53d8\u91cf\u77e5\u9053\u600e\u4e48\u521b\u5efa\u7684\u5b83\u4eec. z\u77e5\u9053\u5b83\u5e76\u975e\u662f\u4ece\u6587\u4ef6\u8bfb\u53d6\u7684, \u4e5f\u4e0d\u662f\u4e58\u6cd5\u6216\u6307\u6570\u6216\u5176\u4ed6\u8fd0\u7b97\u7684\u7ed3\u679c. \u5982\u679c\u4f60\u7ee7\u7eed\u8ddf\u8e2a z.grad_fn, \u4f60\u4f1a\u4ece\u4e2d\u627e\u5230x\u548cy\u7684\u75d5\u8ff9. \u4f46\u662f\u5b83\u5982\u4f55\u5e2e\u52a9\u6211\u4eec\u8ba1\u7b97\u68af\u5ea6? # \u6211\u4eec\u6765\u5c06z\u4e2d\u6240\u6709\u9879\u4f5c\u548c\u8fd0\u7b97 s = z.sum() print(s) print(s.grad_fn) \u90a3\u4e48\u8fd9\u4e2a\u8ba1\u7b97\u548c\u5bf9x\u7684\u7b2c\u4e00\u4e2a\u5206\u91cf\u7684\u5bfc\u6570\u7b49\u4e8e\u591a\u5c11? \u5728\u6570\u5b66\u4e0a,\u6211\u4eec\u6c42 \\[\\frac{\\partial s}{\\partial x_0}\\] s\u77e5\u9053\u5b83\u662f\u88abtensor z\u7684\u548c\u521b\u5efa\u7684.z \u77e5\u9053\u5b83\u662fx+y\u7684\u548c \\[s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\\] \u5e76\u4e14s\u5305\u542b\u4e86\u8db3\u591f\u7684\u4fe1\u606f\u53bb\u51b3\u5b9a\u6211\u4eec\u9700\u8981\u7684\u5bfc\u6570\u4e3a1! \u5f53\u7136\u5b83\u63a9\u76d6\u4e86\u5982\u4f55\u8ba1\u7b97\u5bfc\u6570\u7684\u6311\u6218.\u8fd9\u662f\u56e0\u4e3as\u643a\u5e26\u4e86\u8db3\u591f\u591a\u7684\u4fe1\u606f\u6240\u4ee5\u5bfc\u6570\u53ef\u4ee5\u88ab\u8ba1\u7b97.\u73b0\u5b9e\u4e2d,Pytorch \u7a0b\u5e8f\u7684\u5f00\u53d1\u4eba\u5458\u7528\u7a0b\u5e8f\u6307\u4ee4sum()\u548c + \u64cd\u4f5c\u4ee5\u77e5\u9053\u5982\u4f55\u8ba1\u7b97\u5b83\u4eec\u7684\u68af\u5ea6\u5e76\u4e14\u8fd0\u884c\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5.\u6df1\u5165\u8ba8\u8bba\u6b64\u7b97\u6cd5 \u8d85\u51fa\u4e86\u672c\u6559\u7a0b\u7684\u8303\u56f4. \u8ba9\u6211\u4eec\u7528Pytorch\u8ba1\u7b97\u68af\u5ea6,\u53d1\u73b0\u6211\u4eec\u662f\u5bf9\u7684:(\u5982\u679c\u4f60\u8fd0\u884c\u8fd9\u4e2a\u65b9\u5757\u5f88\u591a\u6b21,\u68af\u5ea6\u4f1a\u4e0a\u5347,\u8fd9\u662f\u56e0\u4e3aPytorch accumulates (\u7d2f\u79ef) \u6e10\u53d8\u4e3a.grad\u5c5e\u6027, \u56e0\u4e3a\u5bf9\u4e8e\u5f88\u591a\u6a21\u578b\u5b83\u662f\u5f88\u65b9\u4fbf\u7684.) # \u5728\u4efb\u610f\u53d8\u91cf\u4e0a\u4f7f\u7528 .backward()\u5c06\u4f1a\u8fd0\u884c\u53cd\u5411,\u4ece\u5b83\u5f00\u59cb. s.backward() print(x.grad) \u5bf9\u4e8e\u4e00\u4e2a\u6210\u529f\u7684\u6df1\u5ea6\u5b66\u4e60\u7a0b\u5e8f\u5458\u4e86\u89e3\u4e0b\u9762\u7684\u65b9\u5757\u5982\u4f55\u8fd0\u884c\u662f\u81f3\u5173\u91cd\u8981\u7684. x = torch.randn((2, 2)) y = torch.randn((2, 2)) z = x + y # \u8fd9\u4e9b\u662fTensor\u7c7b\u578b,\u53cd\u5411\u662f\u4e0d\u53ef\u80fd\u7684 var_x = autograd.Variable(x, requires_grad=True) var_y = autograd.Variable(y, requires_grad=True) # var_z \u5305\u542b\u4e86\u8db3\u591f\u7684\u4fe1\u606f\u53bb\u8ba1\u7b97\u68af\u5ea6,\u5982\u4e0b\u6240\u793a var_z = var_x + var_y print(var_z.grad_fn) var_z_data = var_z.data # \u4ece var_z\u4e2d\u5f97\u5230\u5305\u88f9Tensor\u5bf9\u8c61... # \u5728\u4e00\u4e2a\u65b0\u7684\u53d8\u91cf\u4e2d\u91cd\u65b0\u5305\u88f9tensor new_var_z = autograd.Variable(var_z_data) # new_var_z \u6709\u53bb\u53cd\u5411x\u548cy\u7684\u4fe1\u606f\u5417? # \u6ca1\u6709! print(new_var_z.grad_fn) # \u600e\u4e48\u4f1a\u8fd9\u6837? \u6211\u4eec\u5c06 tensor \u4ece var_z \u4e2d\u63d0\u53d6 (\u63d0\u53d6\u4e3avar_z.data). \u8fd9\u4e2a\u5f20\u91cf\u4e0d\u77e5\u9053\u5b83\u662f\u5982 # \u4f55\u8ba1\u7b97\u7684.\u6211\u4eec\u628a\u5b83\u4f20\u9012\u7ed9 new_var_z. # \u8fd9\u5c31\u662fnew_var_z\u5f97\u5230\u7684\u6240\u6709\u4fe1\u606f. \u5982\u679c var_z_data \u4e0d\u77e5\u9053\u5b83\u662f\u5982\u4f55\u8ba1\u7b97\u7684, \u90a3\u4e48\u5c31\u4e0d\u4f1a\u6709 new_var_z \u7684\u65b9\u6cd5. # \u4ece\u672c\u8d28\u4e0a\u8bb2, \u6211\u4eec\u5df2\u7ecf\u628a\u8fd9\u4e2a\u53d8\u91cf\u4ece\u8fc7\u53bb\u7684\u5386\u53f2\u4e2d\u5206\u79bb\u51fa\u6765\u4e86. # \u8fd9\u5c31\u662f\u57fa\u7840\u7684,\u4f46\u662f\u5bf9\u4e8e\u8ba1\u7b97\u81ea\u52a8\u6c42\u5bfc\u662f\u7279\u522b\u91cd\u8981\u7684\u89c4\u5219 (\u8fd9\u6bd4Pytorch\u66f4\u901a\u7528,\u5728\u6bcf\u4e2a\u4e3b\u8981\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u7bb1\u4e2d\u90fd\u6709\u4e00\u4e2a\u76f8\u540c\u7684\u5bf9\u8c61): \u5982\u679c\u4f60\u60f3\u8981\u4ece\u635f\u5931\u51fd\u6570\u8fd4\u56de\u5230\u795e\u7ecf\u7f51\u7edc\u7684\u67d0\u4e2a\u795e\u7ecf\u5143\u5f97\u5230\u9519\u8bef,\u90a3\u4e48\u4f60\u5c31\u4e0d\u80fd\u5c06\u65ad\u5f00\u4ece\u8be5\u7ec4\u4ef6\u5230\u4f60\u7684\u4e22\u5931\u53d8\u91cf\u7684\u53d8\u91cf\u94fe.\u5982\u679c\u4f60\u8fd9\u6837\u505a, \u635f\u5931\u5c06\u4e0d\u77e5\u9053\u4f60\u7684\u7ec4\u4ef6\u5b58\u5728, \u5e76\u4e14\u5b83\u7684\u53c2\u6570\u4e0d\u80fd\u88ab\u66f4\u65b0. \u6211\u7528\u7c97\u4f53\u8868\u793a, \u56e0\u4e3a\u8fd9\u4e2a\u9519\u8bef\u4f1a\u5728\u4e0d\u7ecf\u610f\u95f4\u53d1\u751f(\u6211\u5c06\u5728\u4e0b\u9762\u5c55\u793a\u4e00\u4e9b\u8fd9\u6837\u7684\u65b9\u6cd5), \u5e76\u4e14\u5b83\u4e0d\u4f1a\u5bfc\u81f4\u60a8\u7684\u4ee3\u7801\u5d29\u6e83\u6216\u62a5\u9519, \u6240\u4ee5\u60a8\u5fc5\u987b\u5c0f\u5fc3. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: pytorch_tutorial.py Download Jupyter notebook: pytorch_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch\u4ecb\u7ecd"},{"location":"25/#pytorch","text":"","title":"PyTorch\u4ecb\u7ecd"},{"location":"25/#torch","text":"\u6240\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u90fd\u662f\u5728\u5f20\u91cf\u4e0a\u8ba1\u7b97\u7684,\u5176\u4e2d\u5f20\u91cf\u662f\u4e00\u4e2a\u53ef\u4ee5\u88ab\u8d85\u8fc7\u4e8c\u7ef4\u7d22\u5f15\u7684\u77e9\u9635\u7684\u4e00\u822c\u5316. \u7a0d\u540e\u6211\u4eec\u5c06\u8be6\u7ec6\u8ba8\u8bba\u8fd9\u610f\u5473\u7740\u4ec0\u4e48.\u9996\u5148,\u6211\u4eec\u5148\u6765\u770b\u4e00\u4e0b\u6211\u4eec\u53ef\u4ee5\u7528\u5f20\u91cf\u6765\u5e72\u4ec0\u4e48. # \u4f5c\u8005: Robert Guthrie import torch import torch.autograd as autograd import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1)","title":"Torch\u5f20\u91cf\u5e93\u4ecb\u7ecd"},{"location":"25/#creating-tensors","text":"\u5f20\u91cf\u53ef\u4ee5\u5728Python list\u5f62\u5f0f\u4e0b\u901a\u8fc7torch.Tensor()\u51fd\u6570\u521b\u5efa. # \u5229\u7528\u7ed9\u5b9a\u6570\u636e\u521b\u5efa\u4e00\u4e2atorch.Tensor\u5bf9\u8c61.\u8fd9\u662f\u4e00\u4e2a\u4e00\u7ef4\u5411\u91cf V_data = [1., 2., 3.] V = torch.Tensor(V_data) print(V) # \u521b\u5efa\u4e00\u4e2a\u77e9\u9635 M_data = [[1., 2., 3.], [4., 5., 6]] M = torch.Tensor(M_data) print(M) # \u521b\u5efa2x2x2\u5f62\u5f0f\u7684\u4e09\u7ef4\u5f20\u91cf. T_data = [[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]] T = torch.Tensor(T_data) print(T) \u4ec0\u4e48\u662f\u4e09\u7ef4\u5f20\u91cf? \u8ba9\u6211\u4eec\u8fd9\u6837\u60f3\u8c61.\u5982\u679c\u4f60\u6709\u4e00\u4e2a\u5411\u91cf,\u90a3\u4e48\u5bf9\u5411\u91cf\u7d22\u5f15\u5c31\u4f1a\u5f97\u5230\u4e00\u4e2a\u6807\u91cf. \u5982\u679c\u4f60\u6709\u4e00\u4e2a\u77e9\u9635,\u5bf9\u77e9\u9635\u7d22\u5f15\u90a3\u4e48\u5c31\u4f1a\u5f97\u5230\u4e00\u4e2a\u5411\u91cf.\u5982\u679c\u4f60\u6709\u4e00\u4e2a\u4e09\u7ef4\u5f20\u91cf,\u90a3\u4e48\u5bf9\u5176\u7d22\u5f15 \u5c31\u4f1a\u5f97\u5230\u4e00\u4e2a\u77e9\u9635! \u9488\u5bf9\u672f\u8bed\u7684\u8bf4\u660e: \u5f53\u6211\u5728\u672c\u6559\u7a0b\u5185\u4f7f\u7528\u201dtensor\u201d,\u5b83\u9488\u5bf9\u7684\u662f\u6240\u6709torch.Tensor\u5bf9\u8c61.\u77e9\u9635\u548c\u5411\u91cf\u662f\u7279\u6b8a\u7684torch.Tensors, \u4ed6\u4eec\u7684\u7ef4\u5ea6\u5206\u522b\u662f1\u548c2.\u5f53\u6211\u8bf4\u5230\u4e09\u7ef4\u5f20\u91cf,\u6211\u4f1a\u7b80\u6d01\u7684\u4f7f\u7528\u201d3D tensor\u201d. # \u7d22\u5f15V\u5f97\u5230\u4e00\u4e2a\u6807\u91cf print(V[0]) # \u7d22\u5f15M\u5f97\u5230\u4e00\u4e2a\u5411\u91cf print(M[0]) # \u7d22\u5f15T\u5f97\u5230\u4e00\u4e2a\u77e9\u9635 print(T[0]) \u4f60\u4e5f\u53ef\u4ee5\u521b\u5efa\u5176\u4ed6\u6570\u636e\u7c7b\u578b\u7684tensors.\u9ed8\u8ba4\u7684\u6570\u636e\u7c7b\u578b\u4e3aFloat(\u6d6e\u70b9\u578b). \u53ef\u4ee5\u4f7f\u7528torch.LongTensor()\u6765 \u521b\u5efa\u4e00\u4e2a\u6574\u6570\u7c7b\u578b\u7684tensor.\u4f60\u53ef\u4ee5\u5728\u6587\u4ef6\u4e2d\u5bfb\u627e\u66f4\u591a\u7684\u6570\u636e\u7c7b\u578b,\u4f46\u662fFloat(\u6d6e\u70b9\u578b)\u548cLong(\u957f\u6574\u5f62)\u6700\u5e38\u7528\u7684. \u4f60\u53ef\u4ee5\u4f7f\u7528torch.randn()\u521b\u5efa\u4e00\u4e2a\u968f\u673a\u6570\u636e\u548c\u9700\u8981\u63d0\u4f9b\u7ef4\u5ea6\u7684tensor. x = torch.randn((3, 4, 5)) print(x)","title":"Creating Tensors(\u521b\u5efa\u5f20\u91cf)"},{"location":"25/#operations-with-tensorstensor","text":"\u4f60\u53ef\u4ee5\u4ee5\u4f60\u60f3\u8981\u7684\u65b9\u5f0f\u64cd\u4f5ctensor. x = torch.Tensor([1., 2., 3.]) y = torch.Tensor([4., 5., 6.]) z = x + y print(z) \u53ef\u4ee5\u67e5\u9605 \u6587\u6863 \u83b7\u53d6\u5927\u91cf\u53ef\u7528\u64cd\u4f5c\u7684\u5b8c\u6574\u5217\u8868, \u6269\u5c55\u5230\u4e86\u975e\u6570\u5b66\u64cd\u4f5c. \u63a5\u4e0b\u6765\u4e00\u4e2a\u5f88\u6709\u5e2e\u52a9\u7684\u64cd\u4f5c\u5c31\u662f\u8fde\u63a5. # \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u5b83\u6cbf\u7740\u7b2c\u4e00\u8f74\u8fde\u63a5 (\u8fde\u63a5\u884c) x_1 = torch.randn(2, 5) y_1 = torch.randn(3, 5) z_1 = torch.cat([x_1, y_1]) print(z_1) # \u8fde\u63a5\u5217: x_2 = torch.randn(2, 3) y_2 = torch.randn(2, 5) # \u7b2c\u4e8c\u4e2a\u6570\u6307\u5b9a\u4e86\u6cbf\u7740\u54ea\u6761\u8f74\u8fde\u63a5 z_2 = torch.cat([x_2, y_2], 1) print(z_2) # \u5982\u679c\u4f60\u7684tensors\u662f\u4e0d\u517c\u5bb9\u7684,torch\u4f1a\u62a5\u9519.\u53d6\u6d88\u6ce8\u91ca\u6765\u67e5\u770b\u9519\u8bef. # torch.cat([x_1, x_2])","title":"Operations with Tensors(\u5bf9tensor\u8fdb\u884c\u64cd\u4f5c)"},{"location":"25/#reshaping-tensorstensors","text":"\u4f7f\u7528.view()\u53bb\u91cd\u6784tensor.\u8fd9\u662f\u4e00\u4e2a\u9ad8\u9891\u65b9\u6cd5, \u56e0\u4e3a\u8bb8\u591a\u795e\u7ecf\u7f51\u7edc\u7684\u795e\u7ecf\u5143\u5bf9\u8f93\u5165\u683c\u5f0f \u6709\u660e\u786e\u7684\u8981\u6c42. \u4f60\u901a\u5e38\u9700\u8981\u5148\u5c06\u6570\u636e\u91cd\u6784\u518d\u8f93\u5165\u5230\u795e\u7ecf\u5143\u4e2d. x = torch.randn(2, 3, 4) print(x) print(x.view(2, 12)) # \u91cd\u6784\u4e3a2\u884c12\u5217 # \u540c\u4e0a.\u5982\u679c\u7ef4\u5ea6\u4e3a-1,\u90a3\u4e48\u5b83\u7684\u7ef4\u5ea6\u6839\u636e\u6570\u636e\u63a8\u65ad\u51fa\u6765 print(x.view(2, -1))","title":"Reshaping Tensors(\u91cd\u6784Tensors)"},{"location":"25/#computation-graphs-and-automatic-differentiation","text":"\u8ba1\u7b97\u56fe\u7684\u601d\u60f3\u5bf9\u4e8e\u6709\u6548\u7387\u7684\u6df1\u5ea6\u5b66\u4e60\u7f16\u7a0b\u662f\u5f88\u91cd\u8981\u7684, \u56e0\u4e3a\u5b83\u5141\u8bb8\u4f60\u4e0d\u5fc5\u53bb\u81ea\u5df1\u5199\u53cd\u5411\u68af\u5ea6\u4f20\u64ad. \u8ba1\u7b97\u56fe\u53ea\u662f\u7b80\u5355\u5730\u8bf4\u660e\u4e86\u5982\u4f55\u5c06\u6570\u636e\u7ec4\u5408\u5728\u4e00\u8d77\u4ee5\u8f93\u51fa\u7ed3\u679c.\u56e0\u4e3a\u56fe\u5b8c\u5168\u6307\u5b9a\u4e86\u64cd\u4f5c\u6240\u5305\u542b\u7684\u53c2\u6570, \u56e0\u6b64\u5b83\u5305\u542b\u4e86\u8db3\u591f\u7684\u4fe1\u606f\u53bb\u6c42\u5bfc.\u8fd9\u53ef\u80fd\u542c\u8d77\u6765\u5f88\u6a21\u7cca, \u6240\u4ee5\u8ba9\u6211\u4eec\u770b\u770b\u4f7f\u7528Pytorch\u7684\u57fa\u672c\u7c7b: autograd.Variable. \u9996\u5148, \u4ece\u7a0b\u5e8f\u5458\u7684\u89d2\u5ea6\u6765\u601d\u8003.\u5728torch\u4e2d\u5b58\u50a8\u4e86\u4ec0\u4e48, \u662f\u6211\u4eec\u5728\u4e0a\u9762\u521b\u5efa\u7684Tensor\u5bf9\u8c61\u5417? \u663e\u7136, \u662f\u6570\u636e\u548c \u7ed3\u6784, \u4e5f\u5f88\u53ef\u80fd\u662f\u5176\u4ed6\u7684\u4e1c\u897f. \u4f46\u662f\u5f53\u6211\u4eec\u5c06\u4e24\u4e2atensors\u76f8\u52a0\u540e, \u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4e2a\u8f93\u51fatensor.\u8fd9\u4e2a\u8f93\u51fa\u6240\u80fd \u4f53\u73b0\u51fa\u7684\u53ea\u6709\u6570\u636e\u548c\u7ed3\u6784, \u5e76\u4e0d\u80fd\u4f53\u73b0\u51fa\u662f\u7531\u4e24\u4e2atensors\u52a0\u548c\u5f97\u5230\u7684(\u56e0\u4e3a\u5b83\u53ef\u80fd\u662f\u4ece\u4e00\u4e2a\u6587\u4ef6\u4e2d\u8bfb\u53d6\u7684, \u4e5f\u53ef\u80fd\u662f \u5176\u4ed6\u64cd\u4f5c\u7684\u7ed3\u679c\u7b49). \u53d8\u91cf\u7c7b\u522b\u53ef\u4ee5\u4e00\u76f4\u8ddf\u8e2a\u5b83\u662f\u5982\u4f55\u521b\u5efa\u7684.\u8ba9\u6211\u4eec\u5728\u5b9e\u9645\u4e2d\u6765\u770b. # \u53d8\u91cf\u56f4\u7ed5tensor\u5bf9\u8c61 x = autograd.Variable(torch.Tensor([1., 2., 3]), requires_grad=True) # \u60a8\u53ef\u4ee5\u4f7f\u7528.data\u5c5e\u6027\u8bbf\u95ee\u6570\u636e. print(x.data) # \u4f60\u4e5f\u53ef\u4ee5\u7528\u53d8\u91cf\u6765\u505a\u4e0e\u5f20\u91cf\u76f8\u540c\u7684\u8fd0\u7b97. y = autograd.Variable(torch.Tensor([4., 5., 6]), requires_grad=True) z = x + y print(z.data) # \u4f46\u662fz\u77e5\u9053\u4e00\u4e9b\u989d\u5916\u7684\u4e1c\u897f. print(z.grad_fn) \u65e2\u7136\u53d8\u91cf\u77e5\u9053\u600e\u4e48\u521b\u5efa\u7684\u5b83\u4eec. z\u77e5\u9053\u5b83\u5e76\u975e\u662f\u4ece\u6587\u4ef6\u8bfb\u53d6\u7684, \u4e5f\u4e0d\u662f\u4e58\u6cd5\u6216\u6307\u6570\u6216\u5176\u4ed6\u8fd0\u7b97\u7684\u7ed3\u679c. \u5982\u679c\u4f60\u7ee7\u7eed\u8ddf\u8e2a z.grad_fn, \u4f60\u4f1a\u4ece\u4e2d\u627e\u5230x\u548cy\u7684\u75d5\u8ff9. \u4f46\u662f\u5b83\u5982\u4f55\u5e2e\u52a9\u6211\u4eec\u8ba1\u7b97\u68af\u5ea6? # \u6211\u4eec\u6765\u5c06z\u4e2d\u6240\u6709\u9879\u4f5c\u548c\u8fd0\u7b97 s = z.sum() print(s) print(s.grad_fn) \u90a3\u4e48\u8fd9\u4e2a\u8ba1\u7b97\u548c\u5bf9x\u7684\u7b2c\u4e00\u4e2a\u5206\u91cf\u7684\u5bfc\u6570\u7b49\u4e8e\u591a\u5c11? \u5728\u6570\u5b66\u4e0a,\u6211\u4eec\u6c42 \\[\\frac{\\partial s}{\\partial x_0}\\] s\u77e5\u9053\u5b83\u662f\u88abtensor z\u7684\u548c\u521b\u5efa\u7684.z \u77e5\u9053\u5b83\u662fx+y\u7684\u548c \\[s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\\] \u5e76\u4e14s\u5305\u542b\u4e86\u8db3\u591f\u7684\u4fe1\u606f\u53bb\u51b3\u5b9a\u6211\u4eec\u9700\u8981\u7684\u5bfc\u6570\u4e3a1! \u5f53\u7136\u5b83\u63a9\u76d6\u4e86\u5982\u4f55\u8ba1\u7b97\u5bfc\u6570\u7684\u6311\u6218.\u8fd9\u662f\u56e0\u4e3as\u643a\u5e26\u4e86\u8db3\u591f\u591a\u7684\u4fe1\u606f\u6240\u4ee5\u5bfc\u6570\u53ef\u4ee5\u88ab\u8ba1\u7b97.\u73b0\u5b9e\u4e2d,Pytorch \u7a0b\u5e8f\u7684\u5f00\u53d1\u4eba\u5458\u7528\u7a0b\u5e8f\u6307\u4ee4sum()\u548c + \u64cd\u4f5c\u4ee5\u77e5\u9053\u5982\u4f55\u8ba1\u7b97\u5b83\u4eec\u7684\u68af\u5ea6\u5e76\u4e14\u8fd0\u884c\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5.\u6df1\u5165\u8ba8\u8bba\u6b64\u7b97\u6cd5 \u8d85\u51fa\u4e86\u672c\u6559\u7a0b\u7684\u8303\u56f4. \u8ba9\u6211\u4eec\u7528Pytorch\u8ba1\u7b97\u68af\u5ea6,\u53d1\u73b0\u6211\u4eec\u662f\u5bf9\u7684:(\u5982\u679c\u4f60\u8fd0\u884c\u8fd9\u4e2a\u65b9\u5757\u5f88\u591a\u6b21,\u68af\u5ea6\u4f1a\u4e0a\u5347,\u8fd9\u662f\u56e0\u4e3aPytorch accumulates (\u7d2f\u79ef) \u6e10\u53d8\u4e3a.grad\u5c5e\u6027, \u56e0\u4e3a\u5bf9\u4e8e\u5f88\u591a\u6a21\u578b\u5b83\u662f\u5f88\u65b9\u4fbf\u7684.) # \u5728\u4efb\u610f\u53d8\u91cf\u4e0a\u4f7f\u7528 .backward()\u5c06\u4f1a\u8fd0\u884c\u53cd\u5411,\u4ece\u5b83\u5f00\u59cb. s.backward() print(x.grad) \u5bf9\u4e8e\u4e00\u4e2a\u6210\u529f\u7684\u6df1\u5ea6\u5b66\u4e60\u7a0b\u5e8f\u5458\u4e86\u89e3\u4e0b\u9762\u7684\u65b9\u5757\u5982\u4f55\u8fd0\u884c\u662f\u81f3\u5173\u91cd\u8981\u7684. x = torch.randn((2, 2)) y = torch.randn((2, 2)) z = x + y # \u8fd9\u4e9b\u662fTensor\u7c7b\u578b,\u53cd\u5411\u662f\u4e0d\u53ef\u80fd\u7684 var_x = autograd.Variable(x, requires_grad=True) var_y = autograd.Variable(y, requires_grad=True) # var_z \u5305\u542b\u4e86\u8db3\u591f\u7684\u4fe1\u606f\u53bb\u8ba1\u7b97\u68af\u5ea6,\u5982\u4e0b\u6240\u793a var_z = var_x + var_y print(var_z.grad_fn) var_z_data = var_z.data # \u4ece var_z\u4e2d\u5f97\u5230\u5305\u88f9Tensor\u5bf9\u8c61... # \u5728\u4e00\u4e2a\u65b0\u7684\u53d8\u91cf\u4e2d\u91cd\u65b0\u5305\u88f9tensor new_var_z = autograd.Variable(var_z_data) # new_var_z \u6709\u53bb\u53cd\u5411x\u548cy\u7684\u4fe1\u606f\u5417? # \u6ca1\u6709! print(new_var_z.grad_fn) # \u600e\u4e48\u4f1a\u8fd9\u6837? \u6211\u4eec\u5c06 tensor \u4ece var_z \u4e2d\u63d0\u53d6 (\u63d0\u53d6\u4e3avar_z.data). \u8fd9\u4e2a\u5f20\u91cf\u4e0d\u77e5\u9053\u5b83\u662f\u5982 # \u4f55\u8ba1\u7b97\u7684.\u6211\u4eec\u628a\u5b83\u4f20\u9012\u7ed9 new_var_z. # \u8fd9\u5c31\u662fnew_var_z\u5f97\u5230\u7684\u6240\u6709\u4fe1\u606f. \u5982\u679c var_z_data \u4e0d\u77e5\u9053\u5b83\u662f\u5982\u4f55\u8ba1\u7b97\u7684, \u90a3\u4e48\u5c31\u4e0d\u4f1a\u6709 new_var_z \u7684\u65b9\u6cd5. # \u4ece\u672c\u8d28\u4e0a\u8bb2, \u6211\u4eec\u5df2\u7ecf\u628a\u8fd9\u4e2a\u53d8\u91cf\u4ece\u8fc7\u53bb\u7684\u5386\u53f2\u4e2d\u5206\u79bb\u51fa\u6765\u4e86. # \u8fd9\u5c31\u662f\u57fa\u7840\u7684,\u4f46\u662f\u5bf9\u4e8e\u8ba1\u7b97\u81ea\u52a8\u6c42\u5bfc\u662f\u7279\u522b\u91cd\u8981\u7684\u89c4\u5219 (\u8fd9\u6bd4Pytorch\u66f4\u901a\u7528,\u5728\u6bcf\u4e2a\u4e3b\u8981\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u7bb1\u4e2d\u90fd\u6709\u4e00\u4e2a\u76f8\u540c\u7684\u5bf9\u8c61): \u5982\u679c\u4f60\u60f3\u8981\u4ece\u635f\u5931\u51fd\u6570\u8fd4\u56de\u5230\u795e\u7ecf\u7f51\u7edc\u7684\u67d0\u4e2a\u795e\u7ecf\u5143\u5f97\u5230\u9519\u8bef,\u90a3\u4e48\u4f60\u5c31\u4e0d\u80fd\u5c06\u65ad\u5f00\u4ece\u8be5\u7ec4\u4ef6\u5230\u4f60\u7684\u4e22\u5931\u53d8\u91cf\u7684\u53d8\u91cf\u94fe.\u5982\u679c\u4f60\u8fd9\u6837\u505a, \u635f\u5931\u5c06\u4e0d\u77e5\u9053\u4f60\u7684\u7ec4\u4ef6\u5b58\u5728, \u5e76\u4e14\u5b83\u7684\u53c2\u6570\u4e0d\u80fd\u88ab\u66f4\u65b0. \u6211\u7528\u7c97\u4f53\u8868\u793a, \u56e0\u4e3a\u8fd9\u4e2a\u9519\u8bef\u4f1a\u5728\u4e0d\u7ecf\u610f\u95f4\u53d1\u751f(\u6211\u5c06\u5728\u4e0b\u9762\u5c55\u793a\u4e00\u4e9b\u8fd9\u6837\u7684\u65b9\u6cd5), \u5e76\u4e14\u5b83\u4e0d\u4f1a\u5bfc\u81f4\u60a8\u7684\u4ee3\u7801\u5d29\u6e83\u6216\u62a5\u9519, \u6240\u4ee5\u60a8\u5fc5\u987b\u5c0f\u5fc3. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: pytorch_tutorial.py Download Jupyter notebook: pytorch_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"Computation Graphs and Automatic Differentiation(\u8ba1\u7b97\u56fe\u548c\u81ea\u52a8\u6c42\u5bfc)"},{"location":"26/","text":"PyTorch\u6df1\u5ea6\u5b66\u4e60 \u6df1\u5ea6\u5b66\u4e60\u6784\u5efa\u6a21\u5757: Affine maps, non-linearities and objectives \u6df1\u5ea6\u5b66\u4e60\u4ee5\u5de7\u5999\u7684\u65b9\u5f0f\u5c06non-linearities\u548clinearities\u7ec4\u5408\u5728\u4e00\u8d77.non-linearities\u7684\u5f15\u5165\u5141\u8bb8\u5f3a\u5927\u7684\u6a21\u578b. \u5728\u672c\u8282\u4e2d, \u6211\u4eec\u5c06\u4f7f\u7528\u8fd9\u4e9b\u6838\u5fc3\u7ec4\u4ef6, \u6784\u5efa\u4e00\u4e2aobjective\u51fd\u6570, \u5e76\u4e14\u770b\u770b\u6a21\u578b\u662f\u5982\u4f55\u8bad\u7ec3\u7684. Affine Maps \u6df1\u5ea6\u5b66\u4e60\u7684\u6838\u5fc3\u5de5\u4f5c\u4e4b\u4e00\u662faffine map, \u8fd9\u662f\u4e00\u4e2a\u51fd\u6570 \\(f(x)\\) \u5176\u4e2d \\[f(x) = Ax + b\\] \u5bf9\u4e8e\u77e9\u9635 \\(A\\) \u548c\u5411\u91cf \\(x, b\\) . \u8fd9\u91cc\u5b66\u4e60\u7684\u53c2\u6570\u662f \\(A\\) and \\(b\\) . \u901a\u5e38, \\(b\\) \u88ab\u79f0\u4e3a \u504f\u5dee \u9879. Pytorch\u548c\u5927\u591a\u6570\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e0e\u4f20\u7edf\u7684\u7ebf\u6027\u4ee3\u6570\u6709\u6240\u4e0d\u540c.\u5b83\u6620\u5c04\u8f93\u5165\u7684\u662f\u884c\u800c\u4e0d\u662f\u5217. \u4e5f\u5c31\u662f\u8bf4, \u4e0b\u9762\u7684\u8f93\u51fa\u7684\u7b2c \\(i\\) \u884c\u662f \\(A\\) \u7684\u8f93\u5165\u7684\u7b2c \\(i\\) \u884c\u52a0\u4e0a\u504f\u7f6e\u9879\u7684\u6620\u5c04. \u770b\u4e0b\u9762\u7684\u4f8b\u5b50. # \u4f5c\u8005: Robert Guthrie import torch import torch.autograd as autograd import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) lin = nn.Linear(5, 3) # \u4ece R^5 \u6620\u5c04\u5230 R^3, \u53c2\u6570 A, b # \u6570\u636e\u662f 2x5\\. A \u662f\u4ece 5 \u6620\u5c04\u5230 3... \u6211\u4eec\u80fd\u5728A\u4e0b\u6620\u5c04\u5f97\u5230\u60f3\u8981\u7684\u6570\u636e\u5417? data = autograd.Variable(torch.randn(2, 5)) print(lin(data)) # \u662f\u7684 Non-Linearities(\u975e\u7ebf\u6027) \u9996\u5148, \u6ce8\u610f\u4ee5\u4e0b\u5c06\u89e3\u91ca\u4e3a\u4ec0\u4e48\u6211\u4eec\u9996\u5148\u9700\u8981 non-linearities.\u5047\u8bbe\u6211\u4eec\u6709\u4e24\u4e2a affine maps \\(f(x) = Ax + b\\) and \\(g(x) = Cx + d\\) . \u4ec0\u4e48\u662f \\(f(g(x))\\) ? \\[f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)\\] \\(AC\\) \u662f\u4e00\u4e2a\u77e9\u9635, \\(Ad + b\\) \u662f\u4e00\u4e2a\u5411\u91cf, \u6240\u4ee5\u6211\u4eec\u770b\u5230\u7ec4\u5408\u4e24\u4e2aaffine maps\u4f1a\u5f97\u5230\u4e00\u4e2aaffine map \u7531\u6b64\u53ef\u4ee5\u770b\u51fa, \u5982\u679c\u4f60\u60f3\u8ba9\u4f60\u7684\u795e\u7ecf\u7f51\u7edc\u6210\u4e3aaffine \u7ec4\u5408\u7684\u957f\u94fe\u6761, \u90a3\u4e48\u76f8\u6bd4\u4e8e\u505a\u4e00\u4e2a\u7b80\u5355\u7684affine map, \u6b64\u4e3e\u4e0d\u4f1a\u7ed9\u4f60\u7684\u6a21\u578b\u589e\u52a0\u65b0\u7684\u4f5c\u7528. \u5982\u679c\u6211\u4eec\u5728affine\u5c42\u4e4b\u95f4\u5f15\u5165non-linearities, \u5219\u4e0d\u518d\u662f\u8fd9\u79cd\u60c5\u51b5, \u6211\u4eec\u53ef\u4ee5\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u6a21\u578b. \u63a5\u4e0b\u6765\u6709\u4e00\u4e9b\u91cd\u8981\u7684non-linearities. \\(\\tanh(x), \\sigma(x), \\text{ReLU}(x)\\) \u662f\u6700\u5e38\u89c1\u7684. \u4f60\u53ef\u80fd\u60f3\u77e5\u9053: \u201c\u4e3a\u4ec0\u4e48\u8fd9\u4e9b\u51fd\u6570\uff1f\u6211\u53ef\u4ee5\u60f3\u5230\u5f88\u591a\u5176\u4ed6\u7684non-linearities \u51fd\u6570.\u201d \u5176\u539f\u56e0\u662f\u4ed6\u4eec\u7684\u68af\u5ea6\u5bb9\u6613\u8ba1\u7b97, \u5e76\u4e14\u8ba1\u7b97\u68af\u5ea6\u5bf9\u5b66\u4e60\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684. \u4f8b\u5982 \\[\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))\\] \u4e00\u4e2a\u7b80\u5355\u7684\u63d0\u793a: \u867d\u7136\u4f60\u53ef\u80fd\u5df2\u7ecf\u5728\u5165\u95e8AI\u4e2d\u5b66\u4e60\u5230\u4e86\u4e00\u4e9b\u795e\u7ecf\u7f51\u7edc, \u5176\u4e2d \\(\\sigma(x)\\) \u662f\u9ed8\u8ba4\u7684non-linearity, \u4f46\u901a\u5e38\u4eba\u4eec\u5728\u5b9e\u8df5\u4e2d\u4f1a\u907f\u514d\u5b83. \u8fd9\u662f\u56e0\u4e3a\u968f\u7740\u53c2\u6570\u7edd\u5bf9\u503c\u7684\u589e\u957f, \u68af\u5ea6\u4f1a\u5f88\u5feb \u6d88\u5931 . \u5c0f\u68af\u5ea6\u610f\u5473\u7740\u5f88\u96be\u5b66\u4e60. \u5927\u591a\u6570\u4eba\u9ed8\u8ba4tanh\u6216ReLU. # \u5728pytorch\u4e2d, \u5f88\u591a\u7684non-linearities\u662f\u5728torch\u4e2d.\u662f\u529f\u80fd\u6027\u7684 (\u6211\u4eec\u5c06\u5b83\u8bb0\u4e3a F) # \u8fd9\u610f\u5473\u7740 non-linearites \u4e0d\u50cf affine maps \u4e00\u6837\u62e5\u6709\u53c2\u6570.\u4e5f\u610f\u5473\u7740\u5b83\u4eec\u518d\u8bad\u7ec3\u65f6\u6ca1\u6709\u53ef\u4ee5\u66f4\u65b0\u7684\u53c2\u6570. # data = autograd.Variable(torch.randn(2, 2)) print(data) print(F.relu(data)) Softmax and Probabilities(Softmax\u548c\u6982\u7387\u5206\u5e03) \u51fd\u6570 \\(\\text{Softmax}(x)\\) \u4e5f\u662f\u4e00\u4e2a non-linearity, \u4f46\u5b83\u7684\u7279\u6b8a\u4e4b\u5904\u5728\u4e8e\u5b83\u901a\u5e38\u662f\u7f51\u7edc\u4e2d\u4e00\u6b21\u64cd\u4f5c. \u8fd9\u662f\u56e0\u4e3a\u5b83\u63a5\u53d7\u4e86\u4e00\u4e2a\u5b9e\u6570\u5411\u91cf\u5e76\u8fd4\u56de\u4e00\u4e2a\u6982\u7387\u5206\u5e03.\u5176\u5b9a\u4e49\u5982\u4e0b. \u5b9a\u4e49 \\(x\\) \u662f\u4e00\u4e2a\u5b9e\u6570\u7684\u5411\u91cf(\u6b63\u6570\u6216\u8d1f\u6570\u90fd\u65e0\u6240\u8c13, \u6ca1\u6709\u9650\u5236). \u7136\u540e, \u7b2ci\u4e2a \\(\\text{Softmax}(x)\\) \u7684\u7ec4\u6210\u662f \\[\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\] \u5e94\u8be5\u6e05\u695a\u7684\u662f, \u8f93\u51fa\u662f\u4e00\u4e2a\u6982\u7387\u5206\u5e03: \u6bcf\u4e2a\u5143\u7d20\u90fd\u662f\u975e\u8d1f\u7684, \u5e76\u4e14\u6240\u6709\u5143\u7d20\u7684\u603b\u548c\u90fd\u662f1. \u4f60\u4e5f\u53ef\u4ee5\u628a\u5b83\u770b\u4f5c\u53ea\u662f\u5c06\u4e00\u4e2a\u5143\u7d20\u660e\u786e\u7684\u6307\u6570\u8fd0\u7b97\u7b26\u5e94\u7528\u4e8e\u8f93\u5165, \u4ee5\u4f7f\u6240\u6709\u5185\u5bb9\u90fd\u4e3a\u975e\u8d1f\u503c, \u7136\u540e\u9664\u4ee5\u5f52\u4e00\u5316\u5e38\u6570. # Softmax\u4e5f\u5728 torch.nn.functional data = autograd.Variable(torch.randn(5)) print(data) print(F.softmax(data, dim=0)) print(F.softmax(data, dim=0).sum()) #\u603b\u548c\u4e3a\u4e00\u56e0\u4e3a\u4ed6\u662f\u6982\u7387\u5206\u5e03! print(F.log_softmax(data, dim=0)) # \u4ed6\u4e5f\u662f log_softmax Objective Functions(\u76ee\u6807\u51fd\u6570) Objective function \u662f\u4e00\u4e2a\u76ee\u6807\u51fd\u6570\uff0c\u4f60\u8bad\u7ec3\u7f51\u7edc\u7684\u76ee\u7684\u662f\u4f7f\u5176\u6700\u5c0f(\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u5b83\u901a\u5e38\u88ab\u79f0\u4e3a \u635f\u5931\u51fd\u6570 \u6216 \u6210\u672c\u51fd\u6570 ). \u9996\u5148\u9009\u62e9\u4e00\u4e2a\u8bad\u7ec3\u5b9e\u4f8b, \u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8fd0\u884c\u5b83, \u8ba1\u7b97\u8f93\u51fa\u7684\u635f\u5931. \u7136\u540e\u5229\u7528\u635f\u5931\u51fd\u6570\u7684\u5bfc\u6570\u6765\u66f4\u65b0\u6a21\u578b\u7684\u53c2\u6570. \u76f4\u89c2\u5730\u8bf4, \u5982\u679c\u4f60\u7684\u6a21\u578b\u5bf9\u7b54\u6848\u5b8c\u5168\u6709\u4fe1\u5fc3, \u4f46\u7b54\u6848\u662f\u9519\u8bef\u7684, \u4f60\u7684\u635f\u5931\u5c31\u4f1a\u5f88\u9ad8. \u5982\u679c\u5b83\u7684\u7b54\u6848\u975e\u5e38\u6709\u4fe1\u5fc3, \u800c\u4e14\u7b54\u6848\u662f\u6b63\u786e\u7684, \u90a3\u4e48\u635f\u5931\u5c31\u4f1a\u5f88\u4f4e. \u5c06\u8bad\u7ec3\u6837\u4f8b\u7684\u635f\u5931\u51fd\u6570\u6700\u5c0f\u5316\u7684\u60f3\u6cd5\u662f, \u4f60\u7684\u7f51\u7edc\u5e0c\u671b\u80fd\u591f\u5f88\u597d\u5730\u4ea7\u751f, \u5e76\u4e14\u5728\u5f00\u53d1\u96c6, \u6d4b\u8bd5\u96c6\u6216\u751f\u4ea7\u73af\u5883\u4e2d\u672a\u77e5\u7684\u793a\u4f8b\u6709\u5c0f\u7684\u635f\u5931. \u4e00\u4e2a\u793a\u4f8b\u635f\u5931\u51fd\u6570\u662f \u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931 , \u8fd9\u662f\u591a\u7c7b\u5206\u7c7b\u7684\u4e00\u4e2a\u975e\u5e38\u666e\u904d\u7684\u76ee\u6807\u51fd\u6570. \u5bf9\u4e8e\u6709\u76d1\u7763\u7684\u591a\u7c7b\u522b\u5206\u7c7b, \u8fd9\u610f\u5473\u7740\u8bad\u7ec3\u7f51\u7edc\u4ee5\u6700\u5c0f\u5316\u6b63\u786e\u8f93\u51fa\u7684\u8d1f\u5bf9\u6570\u6982\u7387(\u6216\u7b49\u540c\u5730, \u6700\u5927\u5316\u6b63\u786e\u8f93\u51fa\u7684\u5bf9\u6570\u6982\u7387). Optimization and Training(\u4f18\u5316\u548c\u8bad\u7ec3) \u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u4e00\u4e2a\u5b9e\u4f8b\u7684\u635f\u5931\u51fd\u6570?\u6211\u4eec\u8be5\u600e\u4e48\u505a?\u6211\u4eec\u4e4b\u524d\u770b\u5230autograd.\u53d8\u91cf\u77e5\u9053\u5982\u4f55\u8ba1\u7b97\u4e0e\u8ba1\u7b97\u68af\u5ea6\u6709\u5173\u7684\u4e8b\u7269.\u90a3\u4e48, \u56e0\u4e3a\u6211\u4eec\u7684\u635f\u5931\u662f\u4e00\u4e2a autograd. \u5bf9\u4e8e Variable, \u6211\u4eec\u53ef\u4ee5\u5bf9\u6240\u6709\u7528\u4e8e\u8ba1\u7b97\u7684\u53c2\u6570\u8ba1\u7b97\u68af\u5ea6\uff01\u7136\u540e\u6211\u4eec\u53ef\u4ee5\u6267\u884c\u6807\u51c6\u6e10\u53d8\u66f4\u65b0. \u4ee4 \\(\\theta\\) \u662f\u6211\u4eec\u7684\u53c2\u6570, \\(L(\\theta)\\) \u635f\u5931\u51fd\u6570, \u4ee5\u53ca: \\(\\eta\\) \u662f\u4e00\u4e2a\u6b63\u7684\u7684\u5b66\u4e60\u7387. \u7136\u540e: \\[\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\\] \u6709\u5927\u91cf\u7684\u7b97\u6cd5\u548c\u79ef\u6781\u7684\u7814\u7a76\u53bb\u5c1d\u8bd5\u6bd4vanilla\u68af\u5ea6\u66f4\u65b0\u66f4\u51fa\u8272\u7684\u65b9\u6cd5. \u8bb8\u591a\u4eba\u8bd5\u56fe\u6839\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u6539\u53d8\u5b66\u4e60\u7387. \u9664\u975e\u4f60\u771f\u7684\u611f\u5174\u8da3, \u5426\u5219\u4f60\u4e0d\u5fc5\u62c5\u5fc3\u8fd9\u4e9b\u7b97\u6cd5\u5177\u4f53\u505a\u4ec0\u4e48. Torch\u63d0\u4f9b\u4e86\u8bb8\u591a torch.optim \u5305, \u5b83\u4eec\u90fd\u662f\u5f00\u6e90\u7684.\u4f7f\u7528\u6700\u7b80\u5355\u7684\u68af\u5ea6\u66f4\u65b0\u4e0e\u66f4\u590d\u6742\u7684\u7b97\u6cd5\u6548\u679c\u76f8\u540c.\u5c1d\u8bd5\u4e0d\u540c\u7684\u66f4\u65b0\u7b97\u6cd5\u548c\u66f4\u65b0\u7b97\u6cd5\u7684\u4e0d\u540c\u53c2\u6570(\u5982\u4e0d\u540c\u7684\u521d\u59cb\u5b66\u4e60\u901f\u7387)\u5bf9\u4e8e\u4f18\u5316\u7f51\u7edc\u6027\u80fd\u975e\u5e38\u91cd\u8981. \u901a\u5e38, \u53ea\u9700\u7528Adam\u6216RMSProp\u7b49\u4f18\u5316\u5668\u66ff\u6362vanilla SGD \u5373\u53ef\u663e\u7740\u63d0\u5347\u6027\u80fd. Creating Network Components in Pytorch(\u5728Pytorch\u4e2d\u521b\u5efa\u795e\u7ecf\u5143) \u5728\u6211\u4eec\u5f00\u59cb\u5173\u6ce8NLP\u4e4b\u524d, \u8ba9\u6211\u4eec\u505a\u4e00\u4e2a\u6ce8\u91ca\u7684\u4f8b\u5b50, \u5728Pytorch\u4e2d\u53ea\u4f7f\u7528affine maps\u548cnon-linearities\u6784\u5efa\u7f51\u7edc.\u6211\u4eec\u8fd8\u5c06\u770b\u5230\u5982\u4f55\u4f7f\u7528Pytorch\u5efa\u7acb\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u8ba1\u7b97\u635f\u5931\u51fd\u6570, \u5e76\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u53c2\u6570. \u6240\u6709\u795e\u7ecf\u5143\u90fd\u5e94\u8be5\u4ecenn.Module\u7ee7\u627f\u5e76\u8986\u76d6forward()\u65b9\u6cd5.\u5c31\u6837\u677f\u800c\u8a00\u5c31\u662f\u8fd9\u6837.\u4ecenn.Module\u7ee7\u627f\u80fd\u4e3a\u4f60\u7684\u795e\u7ecf\u5143\u63d0\u4f9b\u529f\u80fd.\u4f8b\u5982, \u5b83\u53ef\u4ee5\u8ddf\u8e2a\u5176\u53ef\u8bad\u7ec3\u7684\u53c2\u6570, \u53ef\u4ee5\u4f7f\u7528.cuda()\u6216.cpu()\u51fd\u6570\u7b49\u5728CPU\u548cGPU\u4e4b\u95f4\u4ea4\u6362, \u7b49\u7b49. \u6211\u4eec\u6765\u7f16\u5199\u4e00\u4e2a\u5e26\u6709\u6ce8\u91ca\u7684\u7f51\u7edc\u793a\u4f8b, \u8be5\u7f51\u7edc\u91c7\u7528\u7a00\u758f\u7684\u8bcd\u888b\u8868\u793a\u6cd5, \u5e76\u8f93\u51fa\u6982\u7387\u5206\u5e03\u5728\u4e24\u4e2a\u6807\u7b7e\u4e0a: \u201c\u82f1\u8bed\u201d\u548c\u201c\u897f\u73ed\u7259\u8bed\u201d.\u4f7f\u7528\u7684\u6a21\u578b\u662f\u903b\u8f91\u56de\u5f52. Example: Logistic Regression Bag-of-Words classifier(\u4f8b\u5b50: \u57fa\u4e8e\u8bcd\u888b\u8868\u793a\u6cd5\u7684\u903b\u8f91\u65af\u8482\u56de\u5f52\u5206\u7c7b\u5668) \u6211\u4eec\u7684\u6a21\u578b\u5c06\u6620\u5c04\u4e00\u4e2a\u7a00\u758f\u7684BOW\u8868\u793a\u6765\u8bb0\u5f55\u6807\u7b7e\u4e0a\u7684\u6982\u7387.\u6211\u4eec\u4e3a\u8bcd\u6c47\u8868\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd\u5206\u914d\u4e00\u4e2a\u7d22\u5f15. \u4f8b\u5982, \u6211\u4eec\u7684\u5b8c\u6574\u7684\u8bcd\u6c47\u8868\u6709\u4e24\u4e2a\u5355\u8bcd: \u201c\u4f60\u597d\u201d \u548c \u201c\u4e16\u754c\u201d, \u8fd9\u4e24\u4e2a\u5355\u8bcd\u7684\u7d22\u5f15\u5206\u522b\u4e3a0\u548c1. \u53e5\u5b50\u4e3a \u201chello hello hello hello\u201d \u7684BoW\u5411\u91cf\u4e3a \\[\\left[ 4, 0 \\right]\\] \u5bf9\u4e8e \u201chello world world hello\u201d , \u5b83\u662f \\[\\left[ 2, 2 \\right]\\] \u7b49\u7b49.\u4e00\u822c\u6765\u8bf4, \u5b83\u662f \\[\\left[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world}) \\right]\\] \u5c06\u8fd9\u4e2aBOW\u5411\u91cf\u8868\u793a\u4e3a \\(x\\) . \u6211\u4eec\u7684\u7f51\u7edc\u8f93\u51fa\u662f: \\[\\log \\text{Softmax}(Ax + b)\\] \u4e5f\u5c31\u662f\u8bf4, \u6211\u4eec\u901a\u8fc7affine map\u4f20\u9012\u8f93\u5165, \u7136\u540e\u8fdb\u884csoftmax. data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"), (\"Give it to me\".split(), \"ENGLISH\"), (\"No creo que sea una buena idea\".split(), \"SPANISH\"), (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")] test_data = [(\"Yo creo que si\".split(), \"SPANISH\"), (\"it is lost on me\".split(), \"ENGLISH\")] # word_to_ix \u5c06\u5728\u8bcd\u6c47\u4e2d\u7684\u5355\u8bcd\u6620\u5c04\u4e3a\u4e00\u4e2a\u7279\u5f81\u6570, # \u8fd9\u4e2a\u7279\u5f81\u6570\u5c31\u662f\u5355\u8bcd\u5728\u8bcd\u888b\u4e2d\u7684\u7d22\u5f15 word_to_ix = {} for sent, _ in data + test_data: for word in sent: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix) print(word_to_ix) VOCAB_SIZE = len(word_to_ix) NUM_LABELS = 2 class BoWClassifier(nn.Module): # \u4ece nn.Module\u7ee7\u627f! def __init__(self, num_labels, vocab_size): # \u5728 nn.Module\u4e2d\u8c03\u7528\u521d\u59cb\u5316\u51fd\u6570. \u4e0d\u8981\u88ab\u8fd9\u4e2a\u56f0\u60d1, # \u8fd9\u4e2a\u505a\u6cd5\u7ecf\u5e38\u5728 nn.Module\u89c1\u5230 super(BoWClassifier, self).__init__() # \u5b9a\u4e49\u4f60\u9700\u8981\u7684\u53d8\u91cf. \u5728\u672c\u4f8b\u4e2d, \u6211\u4eec\u9700\u8981affine mapping\u7684\u7cfb\u6570 A \u548c b. # Torch \u5b9a\u4e49\u4e86\u53ef\u63d0\u4f9b affine map\u7684nn.Linear(). # \u786e\u5b9a\u4f60\u7406\u89e3\u4e86\u4e3a\u4ec0\u4e48\u8f93\u5165\u77e9\u9635\u7684\u7ef4\u5ea6\u662f vocab_size\u800c\u8f93\u51fa\u7684\u662fnum_labels! self.linear = nn.Linear(vocab_size, num_labels) # \u6ce8\u610f! non-linearity log softmax \u6ca1\u6709\u7cfb\u6570! # \u6240\u4ee5\u6211\u4eec\u5728\u8fd9\u5e76\u4e0d\u9700\u8981\u62c5\u5fc3 def forward(self, bow_vec): # \u5c06\u8f93\u5165\u5f15\u5165\u5230\u7ebf\u6027\u795e\u7ecf\u5143\u5c42\u4e2d, \u968f\u540e\u5f15\u5165\u5230log_softmax. # \u5728torch.nn.functional\u4e2d\u6709\u5f88\u591a\u975e\u7ebf\u6027\u548c\u5176\u4ed6\u7684\u51fd\u6570 return F.log_softmax(self.linear(bow_vec), dim=1) def make_bow_vector(sentence, word_to_ix): vec = torch.zeros(len(word_to_ix)) for word in sentence: vec[word_to_ix[word]] += 1 return vec.view(1, -1) def make_target(label, label_to_ix): return torch.LongTensor([label_to_ix[label]]) model = BoWClassifier(NUM_LABELS, VOCAB_SIZE) # model\u77e5\u9053\u5b83\u7684\u7cfb\u6570.\u7b2c\u4e00\u4e2a\u8f93\u51fa\u7684\u662fA, \u7b2c\u4e8c\u4e2a\u662fb. # \u5f53\u4f60\u5728\u6a21\u5757__init__\u51fd\u6570\u4e2d\u6307\u5b9a\u4e00\u4e2a\u795e\u7ecf\u5143\u53bb\u5206\u7c7b\u53d8\u91cf, self.linear = nn.Linear(...)\u88ab\u6267\u884c # \u968f\u540e\u4ecePytorch devs\u901a\u8fc7Python magic, \u4f60\u7684\u6a21\u5757(\u5728\u672c\u4f8b\u4e2d, BoWClassifier) \u5c06\u4f1a\u5b58\u50a8 nn.Linear\u7684\u7cfb\u6570 for param in model.parameters(): print(param) # \u8981\u8fd0\u884c\u8be5\u6a21\u578b, \u8bf7\u4f20\u5165\u4e00\u4e2aBoW vector, \u4f46\u8981\u5c06\u5176\u5c01\u88c5\u5728\u4e00\u4e2aautograd.Variable\u4e2d. sample = data[0] bow_vector = make_bow_vector(sample[0], word_to_ix) log_probs = model(autograd.Variable(bow_vector)) print(log_probs) \u4ee5\u4e0a\u54ea\u4e2a\u503c\u5bf9\u5e94\u4e8e\u201d\u82f1\u8bed\u201d\u7684\u6982\u7387, \u4ee5\u53ca\u54ea\u4e2a\u503c\u662f\u201d\u897f\u73ed\u7259\u8bed\u201d?\u6211\u4eec\u4ece\u6765\u6ca1\u6709\u5b9a\u4e49\u8fc7\u5b83, \u4f46\u5982\u679c\u6211\u4eec\u60f3\u8981\u8bad\u7ec3\u8fd9\u4e2a\u6a21\u578b, \u6211\u4eec\u9700\u8981\u53bb\u5b9a\u4e49. label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1} \u6765\u505a\u8bad\u7ec3\u5427\uff01\u8981\u505a\u5230\u8fd9\u4e00\u70b9, \u6211\u4eec\u901a\u8fc7\u5b9e\u4f8b\u6765\u83b7\u53d6\u6982\u7387, \u8ba1\u7b97\u635f\u5931\u51fd\u6570, \u8ba1\u7b97\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6, \u7136\u540e\u7528\u68af\u5ea6\u6b65\u9aa4\u66f4\u65b0\u53c2\u6570.Torch\u5728nn\u8f6f\u4ef6\u5305\u4e2d\u63d0\u4f9b\u4e86\u635f\u5931\u51fd\u6570.nn.NLLLoss()\u662f\u6211\u4eec\u60f3\u8981\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931.\u5b83\u8fd8\u5b9a\u4e49\u4e86torch.optim\u4e2d\u7684\u4f18\u5316\u51fd\u6570.\u5728\u8fd9\u91cc, \u6211\u4eec\u53ea\u4f7f\u7528SGD. \u8bf7\u6ce8\u610f, NLLLoss \u7684 \u8f93\u5165 \u662f\u4e00\u4e2a\u5bf9\u6570\u6982\u7387\u5411\u91cf\u548c\u4e00\u4e2a\u76ee\u6807\u6807\u7b7e. \u5b83\u4e0d\u4f1a\u4e3a\u6211\u4eec\u8ba1\u7b97\u5bf9\u6570\u6982\u7387. \u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u6211\u4eec\u7f51\u7edc\u7684\u6700\u540e\u4e00\u5c42\u662flog softmax. \u635f\u5931\u51fd\u6570 nn.CrossEntropyLoss() \u4e0e NLLLoss() \u76f8\u540c, \u552f\u4e00\u7684\u4e0d\u540c\u662f\u5b83\u4e3a\u4f60\u53bb\u505a softmax. # \u5728\u6211\u4eec\u8bad\u7ec3\u524d\u8fd0\u884c\u8bad\u7ec3\u96c6, \u53bb\u770b\u770b\u524d\u540e\u7684\u53d8\u5316 for instance, label in test_data: bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix)) log_probs = model(bow_vec) print(log_probs) # \u5728\u77e9\u9635\u4e2d\u8f93\u51fa\"creo\"\u5217 print(next(model.parameters())[:, word_to_ix[\"creo\"]]) loss_function = nn.NLLLoss() optimizer = optim.SGD(model.parameters(), lr=0.1) # \u901a\u5e38\u4f60\u60f3\u8981\u591a\u6b21\u6d4f\u89c8\u8bad\u7ec3\u96c6.100\u6bd4\u8d77\u5b9e\u9645\u6570\u636e\u96c6\u662f\u5f88\u591a\u7684, \u4f46\u5b9e\u9645\u6570\u636e\u96c6\u4f1a\u591a\u4e8e2\u4e2a\u5b9e\u4f8b. # \u901a\u5e38, \u57285\u523030\u4e4b\u95f4\u662f\u5408\u7406\u7684. for epoch in range(100): for instance, label in data: # \u6b65\u9aa4 1\\. \u7262\u8bb0 Pytorch \u4f1a\u79ef\u7d2f\u68af\u5ea6. # \u6211\u4eec\u9700\u8981\u5728\u6bcf\u4e00\u4f8b\u524d\u6e05\u7406\u6389 model.zero_grad() # \u6b65\u9aa4 2\\. \u5236\u4f5c\u6211\u4eec\u7684 BOW \u5411\u91cf \u5e76\u4e14\u6211\u4eec\u5fc5\u987b\u5c06\u76ee\u6807\u5c01\u88c5\u5728\u53d8\u91cf\u4e2d\u5e76\u4e14\u4e3a\u6574\u6570 . # \u4f8b\u5982, \u5982\u679c\u76ee\u6807\u662f\"\u897f\u73ed\u7259\u8bed\", \u5219\u5c01\u88c5\u4e3a\u6574\u65700.\u5bf9\u4e8e\u635f\u5931\u51fd\u6570\u800c\u8a00, \u6982\u7387\u5206\u5e03\u7684 # \u7b2c0\u5217\u5bf9\u5e94\u7684\u662f\"\u897f\u73ed\u7259\u8bed\"\u7684\u635f\u5931\u51fd\u6570. # bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix)) target = autograd.Variable(make_target(label, label_to_ix)) # \u6b65\u9aa4 3\\. Run our forward pass. log_probs = model(bow_vec) # \u6b65\u9aa4 4\\. \u8ba1\u7b97\u635f\u5931, \u68af\u5ea6, \u901a\u8fc7\u8c03\u7528optimizer.step()\u6765\u66f4\u65b0\u7cfb\u6570 # loss = loss_function(log_probs, target) loss.backward() optimizer.step() for instance, label in test_data: bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix)) log_probs = model(bow_vec) print(log_probs) # \u5bf9\u5e94\u4e8e\u897f\u73ed\u7259\u8bed\u4e0a\u5347, \u82f1\u8bed\u4e0b\u964d! print(next(model.parameters())[:, word_to_ix[\"creo\"]]) \u6211\u4eec\u5f97\u5230\u4e86\u6b63\u786e\u7684\u7b54\u6848! \u4f60\u53ef\u4ee5\u770b\u5230, \u7b2c\u4e00\u4e2a\u793a\u4f8b\u4e2d\u897f\u73ed\u7259\u8bed\u7684\u6982\u7387\u8981\u9ad8\u5f97\u591a, \u800c\u6d4b\u8bd5\u6570\u636e\u7684\u7b2c\u4e8c\u4e2a\u82f1\u8bed\u6982\u7387\u5e94\u8be5\u9ad8\u5f97\u591a. \u73b0\u5728\u4f60\u770b\u5230\u4e86\u5982\u4f55\u5236\u4f5c\u4e00\u4e2aPytorch\u7ec4\u4ef6, \u901a\u8fc7\u5b83\u4f20\u9012\u4e00\u4e9b\u6570\u636e\u5e76\u505a\u68af\u5ea6\u66f4\u65b0.\u6211\u4eec\u51c6\u5907\u6df1\u5165\u6316\u6398NLP\u6240\u80fd\u63d0\u4f9b\u7684\u5185\u5bb9. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: deep_learning_tutorial.py Download Jupyter notebook: deep_learning_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"PyTorch\u6df1\u5ea6\u5b66\u4e60"},{"location":"26/#pytorch","text":"","title":"PyTorch\u6df1\u5ea6\u5b66\u4e60"},{"location":"26/#affine-maps-non-linearities-and-objectives","text":"\u6df1\u5ea6\u5b66\u4e60\u4ee5\u5de7\u5999\u7684\u65b9\u5f0f\u5c06non-linearities\u548clinearities\u7ec4\u5408\u5728\u4e00\u8d77.non-linearities\u7684\u5f15\u5165\u5141\u8bb8\u5f3a\u5927\u7684\u6a21\u578b. \u5728\u672c\u8282\u4e2d, \u6211\u4eec\u5c06\u4f7f\u7528\u8fd9\u4e9b\u6838\u5fc3\u7ec4\u4ef6, \u6784\u5efa\u4e00\u4e2aobjective\u51fd\u6570, \u5e76\u4e14\u770b\u770b\u6a21\u578b\u662f\u5982\u4f55\u8bad\u7ec3\u7684.","title":"\u6df1\u5ea6\u5b66\u4e60\u6784\u5efa\u6a21\u5757: Affine maps, non-linearities and objectives"},{"location":"26/#affine-maps","text":"\u6df1\u5ea6\u5b66\u4e60\u7684\u6838\u5fc3\u5de5\u4f5c\u4e4b\u4e00\u662faffine map, \u8fd9\u662f\u4e00\u4e2a\u51fd\u6570 \\(f(x)\\) \u5176\u4e2d \\[f(x) = Ax + b\\] \u5bf9\u4e8e\u77e9\u9635 \\(A\\) \u548c\u5411\u91cf \\(x, b\\) . \u8fd9\u91cc\u5b66\u4e60\u7684\u53c2\u6570\u662f \\(A\\) and \\(b\\) . \u901a\u5e38, \\(b\\) \u88ab\u79f0\u4e3a \u504f\u5dee \u9879. Pytorch\u548c\u5927\u591a\u6570\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e0e\u4f20\u7edf\u7684\u7ebf\u6027\u4ee3\u6570\u6709\u6240\u4e0d\u540c.\u5b83\u6620\u5c04\u8f93\u5165\u7684\u662f\u884c\u800c\u4e0d\u662f\u5217. \u4e5f\u5c31\u662f\u8bf4, \u4e0b\u9762\u7684\u8f93\u51fa\u7684\u7b2c \\(i\\) \u884c\u662f \\(A\\) \u7684\u8f93\u5165\u7684\u7b2c \\(i\\) \u884c\u52a0\u4e0a\u504f\u7f6e\u9879\u7684\u6620\u5c04. \u770b\u4e0b\u9762\u7684\u4f8b\u5b50. # \u4f5c\u8005: Robert Guthrie import torch import torch.autograd as autograd import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) lin = nn.Linear(5, 3) # \u4ece R^5 \u6620\u5c04\u5230 R^3, \u53c2\u6570 A, b # \u6570\u636e\u662f 2x5\\. A \u662f\u4ece 5 \u6620\u5c04\u5230 3... \u6211\u4eec\u80fd\u5728A\u4e0b\u6620\u5c04\u5f97\u5230\u60f3\u8981\u7684\u6570\u636e\u5417? data = autograd.Variable(torch.randn(2, 5)) print(lin(data)) # \u662f\u7684","title":"Affine Maps"},{"location":"26/#non-linearities","text":"\u9996\u5148, \u6ce8\u610f\u4ee5\u4e0b\u5c06\u89e3\u91ca\u4e3a\u4ec0\u4e48\u6211\u4eec\u9996\u5148\u9700\u8981 non-linearities.\u5047\u8bbe\u6211\u4eec\u6709\u4e24\u4e2a affine maps \\(f(x) = Ax + b\\) and \\(g(x) = Cx + d\\) . \u4ec0\u4e48\u662f \\(f(g(x))\\) ? \\[f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)\\] \\(AC\\) \u662f\u4e00\u4e2a\u77e9\u9635, \\(Ad + b\\) \u662f\u4e00\u4e2a\u5411\u91cf, \u6240\u4ee5\u6211\u4eec\u770b\u5230\u7ec4\u5408\u4e24\u4e2aaffine maps\u4f1a\u5f97\u5230\u4e00\u4e2aaffine map \u7531\u6b64\u53ef\u4ee5\u770b\u51fa, \u5982\u679c\u4f60\u60f3\u8ba9\u4f60\u7684\u795e\u7ecf\u7f51\u7edc\u6210\u4e3aaffine \u7ec4\u5408\u7684\u957f\u94fe\u6761, \u90a3\u4e48\u76f8\u6bd4\u4e8e\u505a\u4e00\u4e2a\u7b80\u5355\u7684affine map, \u6b64\u4e3e\u4e0d\u4f1a\u7ed9\u4f60\u7684\u6a21\u578b\u589e\u52a0\u65b0\u7684\u4f5c\u7528. \u5982\u679c\u6211\u4eec\u5728affine\u5c42\u4e4b\u95f4\u5f15\u5165non-linearities, \u5219\u4e0d\u518d\u662f\u8fd9\u79cd\u60c5\u51b5, \u6211\u4eec\u53ef\u4ee5\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u6a21\u578b. \u63a5\u4e0b\u6765\u6709\u4e00\u4e9b\u91cd\u8981\u7684non-linearities. \\(\\tanh(x), \\sigma(x), \\text{ReLU}(x)\\) \u662f\u6700\u5e38\u89c1\u7684. \u4f60\u53ef\u80fd\u60f3\u77e5\u9053: \u201c\u4e3a\u4ec0\u4e48\u8fd9\u4e9b\u51fd\u6570\uff1f\u6211\u53ef\u4ee5\u60f3\u5230\u5f88\u591a\u5176\u4ed6\u7684non-linearities \u51fd\u6570.\u201d \u5176\u539f\u56e0\u662f\u4ed6\u4eec\u7684\u68af\u5ea6\u5bb9\u6613\u8ba1\u7b97, \u5e76\u4e14\u8ba1\u7b97\u68af\u5ea6\u5bf9\u5b66\u4e60\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684. \u4f8b\u5982 \\[\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))\\] \u4e00\u4e2a\u7b80\u5355\u7684\u63d0\u793a: \u867d\u7136\u4f60\u53ef\u80fd\u5df2\u7ecf\u5728\u5165\u95e8AI\u4e2d\u5b66\u4e60\u5230\u4e86\u4e00\u4e9b\u795e\u7ecf\u7f51\u7edc, \u5176\u4e2d \\(\\sigma(x)\\) \u662f\u9ed8\u8ba4\u7684non-linearity, \u4f46\u901a\u5e38\u4eba\u4eec\u5728\u5b9e\u8df5\u4e2d\u4f1a\u907f\u514d\u5b83. \u8fd9\u662f\u56e0\u4e3a\u968f\u7740\u53c2\u6570\u7edd\u5bf9\u503c\u7684\u589e\u957f, \u68af\u5ea6\u4f1a\u5f88\u5feb \u6d88\u5931 . \u5c0f\u68af\u5ea6\u610f\u5473\u7740\u5f88\u96be\u5b66\u4e60. \u5927\u591a\u6570\u4eba\u9ed8\u8ba4tanh\u6216ReLU. # \u5728pytorch\u4e2d, \u5f88\u591a\u7684non-linearities\u662f\u5728torch\u4e2d.\u662f\u529f\u80fd\u6027\u7684 (\u6211\u4eec\u5c06\u5b83\u8bb0\u4e3a F) # \u8fd9\u610f\u5473\u7740 non-linearites \u4e0d\u50cf affine maps \u4e00\u6837\u62e5\u6709\u53c2\u6570.\u4e5f\u610f\u5473\u7740\u5b83\u4eec\u518d\u8bad\u7ec3\u65f6\u6ca1\u6709\u53ef\u4ee5\u66f4\u65b0\u7684\u53c2\u6570. # data = autograd.Variable(torch.randn(2, 2)) print(data) print(F.relu(data))","title":"Non-Linearities(\u975e\u7ebf\u6027)"},{"location":"26/#softmax-and-probabilitiessoftmax","text":"\u51fd\u6570 \\(\\text{Softmax}(x)\\) \u4e5f\u662f\u4e00\u4e2a non-linearity, \u4f46\u5b83\u7684\u7279\u6b8a\u4e4b\u5904\u5728\u4e8e\u5b83\u901a\u5e38\u662f\u7f51\u7edc\u4e2d\u4e00\u6b21\u64cd\u4f5c. \u8fd9\u662f\u56e0\u4e3a\u5b83\u63a5\u53d7\u4e86\u4e00\u4e2a\u5b9e\u6570\u5411\u91cf\u5e76\u8fd4\u56de\u4e00\u4e2a\u6982\u7387\u5206\u5e03.\u5176\u5b9a\u4e49\u5982\u4e0b. \u5b9a\u4e49 \\(x\\) \u662f\u4e00\u4e2a\u5b9e\u6570\u7684\u5411\u91cf(\u6b63\u6570\u6216\u8d1f\u6570\u90fd\u65e0\u6240\u8c13, \u6ca1\u6709\u9650\u5236). \u7136\u540e, \u7b2ci\u4e2a \\(\\text{Softmax}(x)\\) \u7684\u7ec4\u6210\u662f \\[\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\] \u5e94\u8be5\u6e05\u695a\u7684\u662f, \u8f93\u51fa\u662f\u4e00\u4e2a\u6982\u7387\u5206\u5e03: \u6bcf\u4e2a\u5143\u7d20\u90fd\u662f\u975e\u8d1f\u7684, \u5e76\u4e14\u6240\u6709\u5143\u7d20\u7684\u603b\u548c\u90fd\u662f1. \u4f60\u4e5f\u53ef\u4ee5\u628a\u5b83\u770b\u4f5c\u53ea\u662f\u5c06\u4e00\u4e2a\u5143\u7d20\u660e\u786e\u7684\u6307\u6570\u8fd0\u7b97\u7b26\u5e94\u7528\u4e8e\u8f93\u5165, \u4ee5\u4f7f\u6240\u6709\u5185\u5bb9\u90fd\u4e3a\u975e\u8d1f\u503c, \u7136\u540e\u9664\u4ee5\u5f52\u4e00\u5316\u5e38\u6570. # Softmax\u4e5f\u5728 torch.nn.functional data = autograd.Variable(torch.randn(5)) print(data) print(F.softmax(data, dim=0)) print(F.softmax(data, dim=0).sum()) #\u603b\u548c\u4e3a\u4e00\u56e0\u4e3a\u4ed6\u662f\u6982\u7387\u5206\u5e03! print(F.log_softmax(data, dim=0)) # \u4ed6\u4e5f\u662f log_softmax","title":"Softmax and Probabilities(Softmax\u548c\u6982\u7387\u5206\u5e03)"},{"location":"26/#objective-functions","text":"Objective function \u662f\u4e00\u4e2a\u76ee\u6807\u51fd\u6570\uff0c\u4f60\u8bad\u7ec3\u7f51\u7edc\u7684\u76ee\u7684\u662f\u4f7f\u5176\u6700\u5c0f(\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u5b83\u901a\u5e38\u88ab\u79f0\u4e3a \u635f\u5931\u51fd\u6570 \u6216 \u6210\u672c\u51fd\u6570 ). \u9996\u5148\u9009\u62e9\u4e00\u4e2a\u8bad\u7ec3\u5b9e\u4f8b, \u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8fd0\u884c\u5b83, \u8ba1\u7b97\u8f93\u51fa\u7684\u635f\u5931. \u7136\u540e\u5229\u7528\u635f\u5931\u51fd\u6570\u7684\u5bfc\u6570\u6765\u66f4\u65b0\u6a21\u578b\u7684\u53c2\u6570. \u76f4\u89c2\u5730\u8bf4, \u5982\u679c\u4f60\u7684\u6a21\u578b\u5bf9\u7b54\u6848\u5b8c\u5168\u6709\u4fe1\u5fc3, \u4f46\u7b54\u6848\u662f\u9519\u8bef\u7684, \u4f60\u7684\u635f\u5931\u5c31\u4f1a\u5f88\u9ad8. \u5982\u679c\u5b83\u7684\u7b54\u6848\u975e\u5e38\u6709\u4fe1\u5fc3, \u800c\u4e14\u7b54\u6848\u662f\u6b63\u786e\u7684, \u90a3\u4e48\u635f\u5931\u5c31\u4f1a\u5f88\u4f4e. \u5c06\u8bad\u7ec3\u6837\u4f8b\u7684\u635f\u5931\u51fd\u6570\u6700\u5c0f\u5316\u7684\u60f3\u6cd5\u662f, \u4f60\u7684\u7f51\u7edc\u5e0c\u671b\u80fd\u591f\u5f88\u597d\u5730\u4ea7\u751f, \u5e76\u4e14\u5728\u5f00\u53d1\u96c6, \u6d4b\u8bd5\u96c6\u6216\u751f\u4ea7\u73af\u5883\u4e2d\u672a\u77e5\u7684\u793a\u4f8b\u6709\u5c0f\u7684\u635f\u5931. \u4e00\u4e2a\u793a\u4f8b\u635f\u5931\u51fd\u6570\u662f \u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931 , \u8fd9\u662f\u591a\u7c7b\u5206\u7c7b\u7684\u4e00\u4e2a\u975e\u5e38\u666e\u904d\u7684\u76ee\u6807\u51fd\u6570. \u5bf9\u4e8e\u6709\u76d1\u7763\u7684\u591a\u7c7b\u522b\u5206\u7c7b, \u8fd9\u610f\u5473\u7740\u8bad\u7ec3\u7f51\u7edc\u4ee5\u6700\u5c0f\u5316\u6b63\u786e\u8f93\u51fa\u7684\u8d1f\u5bf9\u6570\u6982\u7387(\u6216\u7b49\u540c\u5730, \u6700\u5927\u5316\u6b63\u786e\u8f93\u51fa\u7684\u5bf9\u6570\u6982\u7387).","title":"Objective Functions(\u76ee\u6807\u51fd\u6570)"},{"location":"26/#optimization-and-training","text":"\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u4e00\u4e2a\u5b9e\u4f8b\u7684\u635f\u5931\u51fd\u6570?\u6211\u4eec\u8be5\u600e\u4e48\u505a?\u6211\u4eec\u4e4b\u524d\u770b\u5230autograd.\u53d8\u91cf\u77e5\u9053\u5982\u4f55\u8ba1\u7b97\u4e0e\u8ba1\u7b97\u68af\u5ea6\u6709\u5173\u7684\u4e8b\u7269.\u90a3\u4e48, \u56e0\u4e3a\u6211\u4eec\u7684\u635f\u5931\u662f\u4e00\u4e2a autograd. \u5bf9\u4e8e Variable, \u6211\u4eec\u53ef\u4ee5\u5bf9\u6240\u6709\u7528\u4e8e\u8ba1\u7b97\u7684\u53c2\u6570\u8ba1\u7b97\u68af\u5ea6\uff01\u7136\u540e\u6211\u4eec\u53ef\u4ee5\u6267\u884c\u6807\u51c6\u6e10\u53d8\u66f4\u65b0. \u4ee4 \\(\\theta\\) \u662f\u6211\u4eec\u7684\u53c2\u6570, \\(L(\\theta)\\) \u635f\u5931\u51fd\u6570, \u4ee5\u53ca: \\(\\eta\\) \u662f\u4e00\u4e2a\u6b63\u7684\u7684\u5b66\u4e60\u7387. \u7136\u540e: \\[\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\\] \u6709\u5927\u91cf\u7684\u7b97\u6cd5\u548c\u79ef\u6781\u7684\u7814\u7a76\u53bb\u5c1d\u8bd5\u6bd4vanilla\u68af\u5ea6\u66f4\u65b0\u66f4\u51fa\u8272\u7684\u65b9\u6cd5. \u8bb8\u591a\u4eba\u8bd5\u56fe\u6839\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u6539\u53d8\u5b66\u4e60\u7387. \u9664\u975e\u4f60\u771f\u7684\u611f\u5174\u8da3, \u5426\u5219\u4f60\u4e0d\u5fc5\u62c5\u5fc3\u8fd9\u4e9b\u7b97\u6cd5\u5177\u4f53\u505a\u4ec0\u4e48. Torch\u63d0\u4f9b\u4e86\u8bb8\u591a torch.optim \u5305, \u5b83\u4eec\u90fd\u662f\u5f00\u6e90\u7684.\u4f7f\u7528\u6700\u7b80\u5355\u7684\u68af\u5ea6\u66f4\u65b0\u4e0e\u66f4\u590d\u6742\u7684\u7b97\u6cd5\u6548\u679c\u76f8\u540c.\u5c1d\u8bd5\u4e0d\u540c\u7684\u66f4\u65b0\u7b97\u6cd5\u548c\u66f4\u65b0\u7b97\u6cd5\u7684\u4e0d\u540c\u53c2\u6570(\u5982\u4e0d\u540c\u7684\u521d\u59cb\u5b66\u4e60\u901f\u7387)\u5bf9\u4e8e\u4f18\u5316\u7f51\u7edc\u6027\u80fd\u975e\u5e38\u91cd\u8981. \u901a\u5e38, \u53ea\u9700\u7528Adam\u6216RMSProp\u7b49\u4f18\u5316\u5668\u66ff\u6362vanilla SGD \u5373\u53ef\u663e\u7740\u63d0\u5347\u6027\u80fd.","title":"Optimization and Training(\u4f18\u5316\u548c\u8bad\u7ec3)"},{"location":"26/#creating-network-components-in-pytorchpytorch","text":"\u5728\u6211\u4eec\u5f00\u59cb\u5173\u6ce8NLP\u4e4b\u524d, \u8ba9\u6211\u4eec\u505a\u4e00\u4e2a\u6ce8\u91ca\u7684\u4f8b\u5b50, \u5728Pytorch\u4e2d\u53ea\u4f7f\u7528affine maps\u548cnon-linearities\u6784\u5efa\u7f51\u7edc.\u6211\u4eec\u8fd8\u5c06\u770b\u5230\u5982\u4f55\u4f7f\u7528Pytorch\u5efa\u7acb\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u8ba1\u7b97\u635f\u5931\u51fd\u6570, \u5e76\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u53c2\u6570. \u6240\u6709\u795e\u7ecf\u5143\u90fd\u5e94\u8be5\u4ecenn.Module\u7ee7\u627f\u5e76\u8986\u76d6forward()\u65b9\u6cd5.\u5c31\u6837\u677f\u800c\u8a00\u5c31\u662f\u8fd9\u6837.\u4ecenn.Module\u7ee7\u627f\u80fd\u4e3a\u4f60\u7684\u795e\u7ecf\u5143\u63d0\u4f9b\u529f\u80fd.\u4f8b\u5982, \u5b83\u53ef\u4ee5\u8ddf\u8e2a\u5176\u53ef\u8bad\u7ec3\u7684\u53c2\u6570, \u53ef\u4ee5\u4f7f\u7528.cuda()\u6216.cpu()\u51fd\u6570\u7b49\u5728CPU\u548cGPU\u4e4b\u95f4\u4ea4\u6362, \u7b49\u7b49. \u6211\u4eec\u6765\u7f16\u5199\u4e00\u4e2a\u5e26\u6709\u6ce8\u91ca\u7684\u7f51\u7edc\u793a\u4f8b, \u8be5\u7f51\u7edc\u91c7\u7528\u7a00\u758f\u7684\u8bcd\u888b\u8868\u793a\u6cd5, \u5e76\u8f93\u51fa\u6982\u7387\u5206\u5e03\u5728\u4e24\u4e2a\u6807\u7b7e\u4e0a: \u201c\u82f1\u8bed\u201d\u548c\u201c\u897f\u73ed\u7259\u8bed\u201d.\u4f7f\u7528\u7684\u6a21\u578b\u662f\u903b\u8f91\u56de\u5f52.","title":"Creating Network Components in Pytorch(\u5728Pytorch\u4e2d\u521b\u5efa\u795e\u7ecf\u5143)"},{"location":"26/#example-logistic-regression-bag-of-words-classifier","text":"\u6211\u4eec\u7684\u6a21\u578b\u5c06\u6620\u5c04\u4e00\u4e2a\u7a00\u758f\u7684BOW\u8868\u793a\u6765\u8bb0\u5f55\u6807\u7b7e\u4e0a\u7684\u6982\u7387.\u6211\u4eec\u4e3a\u8bcd\u6c47\u8868\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd\u5206\u914d\u4e00\u4e2a\u7d22\u5f15. \u4f8b\u5982, \u6211\u4eec\u7684\u5b8c\u6574\u7684\u8bcd\u6c47\u8868\u6709\u4e24\u4e2a\u5355\u8bcd: \u201c\u4f60\u597d\u201d \u548c \u201c\u4e16\u754c\u201d, \u8fd9\u4e24\u4e2a\u5355\u8bcd\u7684\u7d22\u5f15\u5206\u522b\u4e3a0\u548c1. \u53e5\u5b50\u4e3a \u201chello hello hello hello\u201d \u7684BoW\u5411\u91cf\u4e3a \\[\\left[ 4, 0 \\right]\\] \u5bf9\u4e8e \u201chello world world hello\u201d , \u5b83\u662f \\[\\left[ 2, 2 \\right]\\] \u7b49\u7b49.\u4e00\u822c\u6765\u8bf4, \u5b83\u662f \\[\\left[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world}) \\right]\\] \u5c06\u8fd9\u4e2aBOW\u5411\u91cf\u8868\u793a\u4e3a \\(x\\) . \u6211\u4eec\u7684\u7f51\u7edc\u8f93\u51fa\u662f: \\[\\log \\text{Softmax}(Ax + b)\\] \u4e5f\u5c31\u662f\u8bf4, \u6211\u4eec\u901a\u8fc7affine map\u4f20\u9012\u8f93\u5165, \u7136\u540e\u8fdb\u884csoftmax. data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"), (\"Give it to me\".split(), \"ENGLISH\"), (\"No creo que sea una buena idea\".split(), \"SPANISH\"), (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")] test_data = [(\"Yo creo que si\".split(), \"SPANISH\"), (\"it is lost on me\".split(), \"ENGLISH\")] # word_to_ix \u5c06\u5728\u8bcd\u6c47\u4e2d\u7684\u5355\u8bcd\u6620\u5c04\u4e3a\u4e00\u4e2a\u7279\u5f81\u6570, # \u8fd9\u4e2a\u7279\u5f81\u6570\u5c31\u662f\u5355\u8bcd\u5728\u8bcd\u888b\u4e2d\u7684\u7d22\u5f15 word_to_ix = {} for sent, _ in data + test_data: for word in sent: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix) print(word_to_ix) VOCAB_SIZE = len(word_to_ix) NUM_LABELS = 2 class BoWClassifier(nn.Module): # \u4ece nn.Module\u7ee7\u627f! def __init__(self, num_labels, vocab_size): # \u5728 nn.Module\u4e2d\u8c03\u7528\u521d\u59cb\u5316\u51fd\u6570. \u4e0d\u8981\u88ab\u8fd9\u4e2a\u56f0\u60d1, # \u8fd9\u4e2a\u505a\u6cd5\u7ecf\u5e38\u5728 nn.Module\u89c1\u5230 super(BoWClassifier, self).__init__() # \u5b9a\u4e49\u4f60\u9700\u8981\u7684\u53d8\u91cf. \u5728\u672c\u4f8b\u4e2d, \u6211\u4eec\u9700\u8981affine mapping\u7684\u7cfb\u6570 A \u548c b. # Torch \u5b9a\u4e49\u4e86\u53ef\u63d0\u4f9b affine map\u7684nn.Linear(). # \u786e\u5b9a\u4f60\u7406\u89e3\u4e86\u4e3a\u4ec0\u4e48\u8f93\u5165\u77e9\u9635\u7684\u7ef4\u5ea6\u662f vocab_size\u800c\u8f93\u51fa\u7684\u662fnum_labels! self.linear = nn.Linear(vocab_size, num_labels) # \u6ce8\u610f! non-linearity log softmax \u6ca1\u6709\u7cfb\u6570! # \u6240\u4ee5\u6211\u4eec\u5728\u8fd9\u5e76\u4e0d\u9700\u8981\u62c5\u5fc3 def forward(self, bow_vec): # \u5c06\u8f93\u5165\u5f15\u5165\u5230\u7ebf\u6027\u795e\u7ecf\u5143\u5c42\u4e2d, \u968f\u540e\u5f15\u5165\u5230log_softmax. # \u5728torch.nn.functional\u4e2d\u6709\u5f88\u591a\u975e\u7ebf\u6027\u548c\u5176\u4ed6\u7684\u51fd\u6570 return F.log_softmax(self.linear(bow_vec), dim=1) def make_bow_vector(sentence, word_to_ix): vec = torch.zeros(len(word_to_ix)) for word in sentence: vec[word_to_ix[word]] += 1 return vec.view(1, -1) def make_target(label, label_to_ix): return torch.LongTensor([label_to_ix[label]]) model = BoWClassifier(NUM_LABELS, VOCAB_SIZE) # model\u77e5\u9053\u5b83\u7684\u7cfb\u6570.\u7b2c\u4e00\u4e2a\u8f93\u51fa\u7684\u662fA, \u7b2c\u4e8c\u4e2a\u662fb. # \u5f53\u4f60\u5728\u6a21\u5757__init__\u51fd\u6570\u4e2d\u6307\u5b9a\u4e00\u4e2a\u795e\u7ecf\u5143\u53bb\u5206\u7c7b\u53d8\u91cf, self.linear = nn.Linear(...)\u88ab\u6267\u884c # \u968f\u540e\u4ecePytorch devs\u901a\u8fc7Python magic, \u4f60\u7684\u6a21\u5757(\u5728\u672c\u4f8b\u4e2d, BoWClassifier) \u5c06\u4f1a\u5b58\u50a8 nn.Linear\u7684\u7cfb\u6570 for param in model.parameters(): print(param) # \u8981\u8fd0\u884c\u8be5\u6a21\u578b, \u8bf7\u4f20\u5165\u4e00\u4e2aBoW vector, \u4f46\u8981\u5c06\u5176\u5c01\u88c5\u5728\u4e00\u4e2aautograd.Variable\u4e2d. sample = data[0] bow_vector = make_bow_vector(sample[0], word_to_ix) log_probs = model(autograd.Variable(bow_vector)) print(log_probs) \u4ee5\u4e0a\u54ea\u4e2a\u503c\u5bf9\u5e94\u4e8e\u201d\u82f1\u8bed\u201d\u7684\u6982\u7387, \u4ee5\u53ca\u54ea\u4e2a\u503c\u662f\u201d\u897f\u73ed\u7259\u8bed\u201d?\u6211\u4eec\u4ece\u6765\u6ca1\u6709\u5b9a\u4e49\u8fc7\u5b83, \u4f46\u5982\u679c\u6211\u4eec\u60f3\u8981\u8bad\u7ec3\u8fd9\u4e2a\u6a21\u578b, \u6211\u4eec\u9700\u8981\u53bb\u5b9a\u4e49. label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1} \u6765\u505a\u8bad\u7ec3\u5427\uff01\u8981\u505a\u5230\u8fd9\u4e00\u70b9, \u6211\u4eec\u901a\u8fc7\u5b9e\u4f8b\u6765\u83b7\u53d6\u6982\u7387, \u8ba1\u7b97\u635f\u5931\u51fd\u6570, \u8ba1\u7b97\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6, \u7136\u540e\u7528\u68af\u5ea6\u6b65\u9aa4\u66f4\u65b0\u53c2\u6570.Torch\u5728nn\u8f6f\u4ef6\u5305\u4e2d\u63d0\u4f9b\u4e86\u635f\u5931\u51fd\u6570.nn.NLLLoss()\u662f\u6211\u4eec\u60f3\u8981\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931.\u5b83\u8fd8\u5b9a\u4e49\u4e86torch.optim\u4e2d\u7684\u4f18\u5316\u51fd\u6570.\u5728\u8fd9\u91cc, \u6211\u4eec\u53ea\u4f7f\u7528SGD. \u8bf7\u6ce8\u610f, NLLLoss \u7684 \u8f93\u5165 \u662f\u4e00\u4e2a\u5bf9\u6570\u6982\u7387\u5411\u91cf\u548c\u4e00\u4e2a\u76ee\u6807\u6807\u7b7e. \u5b83\u4e0d\u4f1a\u4e3a\u6211\u4eec\u8ba1\u7b97\u5bf9\u6570\u6982\u7387. \u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u6211\u4eec\u7f51\u7edc\u7684\u6700\u540e\u4e00\u5c42\u662flog softmax. \u635f\u5931\u51fd\u6570 nn.CrossEntropyLoss() \u4e0e NLLLoss() \u76f8\u540c, \u552f\u4e00\u7684\u4e0d\u540c\u662f\u5b83\u4e3a\u4f60\u53bb\u505a softmax. # \u5728\u6211\u4eec\u8bad\u7ec3\u524d\u8fd0\u884c\u8bad\u7ec3\u96c6, \u53bb\u770b\u770b\u524d\u540e\u7684\u53d8\u5316 for instance, label in test_data: bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix)) log_probs = model(bow_vec) print(log_probs) # \u5728\u77e9\u9635\u4e2d\u8f93\u51fa\"creo\"\u5217 print(next(model.parameters())[:, word_to_ix[\"creo\"]]) loss_function = nn.NLLLoss() optimizer = optim.SGD(model.parameters(), lr=0.1) # \u901a\u5e38\u4f60\u60f3\u8981\u591a\u6b21\u6d4f\u89c8\u8bad\u7ec3\u96c6.100\u6bd4\u8d77\u5b9e\u9645\u6570\u636e\u96c6\u662f\u5f88\u591a\u7684, \u4f46\u5b9e\u9645\u6570\u636e\u96c6\u4f1a\u591a\u4e8e2\u4e2a\u5b9e\u4f8b. # \u901a\u5e38, \u57285\u523030\u4e4b\u95f4\u662f\u5408\u7406\u7684. for epoch in range(100): for instance, label in data: # \u6b65\u9aa4 1\\. \u7262\u8bb0 Pytorch \u4f1a\u79ef\u7d2f\u68af\u5ea6. # \u6211\u4eec\u9700\u8981\u5728\u6bcf\u4e00\u4f8b\u524d\u6e05\u7406\u6389 model.zero_grad() # \u6b65\u9aa4 2\\. \u5236\u4f5c\u6211\u4eec\u7684 BOW \u5411\u91cf \u5e76\u4e14\u6211\u4eec\u5fc5\u987b\u5c06\u76ee\u6807\u5c01\u88c5\u5728\u53d8\u91cf\u4e2d\u5e76\u4e14\u4e3a\u6574\u6570 . # \u4f8b\u5982, \u5982\u679c\u76ee\u6807\u662f\"\u897f\u73ed\u7259\u8bed\", \u5219\u5c01\u88c5\u4e3a\u6574\u65700.\u5bf9\u4e8e\u635f\u5931\u51fd\u6570\u800c\u8a00, \u6982\u7387\u5206\u5e03\u7684 # \u7b2c0\u5217\u5bf9\u5e94\u7684\u662f\"\u897f\u73ed\u7259\u8bed\"\u7684\u635f\u5931\u51fd\u6570. # bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix)) target = autograd.Variable(make_target(label, label_to_ix)) # \u6b65\u9aa4 3\\. Run our forward pass. log_probs = model(bow_vec) # \u6b65\u9aa4 4\\. \u8ba1\u7b97\u635f\u5931, \u68af\u5ea6, \u901a\u8fc7\u8c03\u7528optimizer.step()\u6765\u66f4\u65b0\u7cfb\u6570 # loss = loss_function(log_probs, target) loss.backward() optimizer.step() for instance, label in test_data: bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix)) log_probs = model(bow_vec) print(log_probs) # \u5bf9\u5e94\u4e8e\u897f\u73ed\u7259\u8bed\u4e0a\u5347, \u82f1\u8bed\u4e0b\u964d! print(next(model.parameters())[:, word_to_ix[\"creo\"]]) \u6211\u4eec\u5f97\u5230\u4e86\u6b63\u786e\u7684\u7b54\u6848! \u4f60\u53ef\u4ee5\u770b\u5230, \u7b2c\u4e00\u4e2a\u793a\u4f8b\u4e2d\u897f\u73ed\u7259\u8bed\u7684\u6982\u7387\u8981\u9ad8\u5f97\u591a, \u800c\u6d4b\u8bd5\u6570\u636e\u7684\u7b2c\u4e8c\u4e2a\u82f1\u8bed\u6982\u7387\u5e94\u8be5\u9ad8\u5f97\u591a. \u73b0\u5728\u4f60\u770b\u5230\u4e86\u5982\u4f55\u5236\u4f5c\u4e00\u4e2aPytorch\u7ec4\u4ef6, \u901a\u8fc7\u5b83\u4f20\u9012\u4e00\u4e9b\u6570\u636e\u5e76\u505a\u68af\u5ea6\u66f4\u65b0.\u6211\u4eec\u51c6\u5907\u6df1\u5165\u6316\u6398NLP\u6240\u80fd\u63d0\u4f9b\u7684\u5185\u5bb9. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: deep_learning_tutorial.py Download Jupyter notebook: deep_learning_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"Example: Logistic Regression Bag-of-Words classifier(\u4f8b\u5b50: \u57fa\u4e8e\u8bcd\u888b\u8868\u793a\u6cd5\u7684\u903b\u8f91\u65af\u8482\u56de\u5f52\u5206\u7c7b\u5668)"},{"location":"27/","text":"\u8bcd\u6c47\u5d4c\u5165:\u7f16\u7801\u8bcd\u6c47\u8bed\u4e49 \u5355\u8bcd\u5d4c\u5165\u662f\u771f\u5b9e\u6570\u5b57\u7684\u5bc6\u96c6\u5411\u91cf,\u5728\u4f60\u7684\u8bcd\u6c47\u8868\u4e2d\u6bcf\u4e00\u4e2a\u5355\u8bcd\u90fd\u662f. \u5728NLP\u4e2d, \u901a\u5e38\u60c5\u51b5\u4e0b, \u60a8\u7684\u7279\u6027\u5c31\u662f\u5355\u8bcd!\u4f46\u662f\u4f60\u5e94\u8be5\u600e\u4e48\u5728\u4f60\u7684\u7535\u8111\u4e2d\u8868\u793a\u4e00\u4e2a\u5355\u8bcd?\u53ef\u4ee5\u5b58\u50a8\u5b83\u7684ascii\u5b57\u7b26\u8868\u793a, \u4f46\u90a3\u4ec5\u4ec5\u544a\u8bc9\u4f60\u5355\u8bcd \u662f \u4ec0\u4e48,\u6ca1\u6709\u8bf4\u592a\u591a\u5b83 \u610f\u5473 \u7740\u4ec0\u4e48 (\u4f60\u4e5f\u8bb8\u53ef\u4ee5\u4ece\u5b83\u7684\u8bcd\u7f00\u4e2d\u6d3e\u751f\u51fa\u5b83\u7684\u8bcd\u6027, \u6216\u8005\u4ece\u5b83\u7684\u5927\u5c0f\u5199\u4e2d\u5f97\u5230\u5b83\u7684\u5c5e\u6027,\u4f46\u5e76\u4e0d\u591a.). \u66f4\u91cd\u8981\u7684\u662f, \u5728\u4ec0\u4e48\u610f\u4e49\u4e0a\u4f60\u80fd\u628a\u8fd9\u4e9b\u8868\u8c61\u7ed3\u5408\u8d77\u6765? \u6211\u4eec\u7ecf\u5e38\u9700\u8981\u795e\u7ecf\u7f51\u7edc\u7684\u5bc6\u96c6\u8f93\u51fa, \u8f93\u5165\u4e3a \\(|V|\\) \u7ef4, \u5176\u4e2d \\(V\\) \u662f\u6211\u4eec\u7684\u8bcd\u6c47\u8868, \u4f46\u7ecf\u5e38\u8f93\u51fa\u662f\u66f4\u5c0f\u7ef4\u5ea6\u7684 (\u5982\u679c\u6211\u4eec\u53ea\u9884\u6d4b\u5c11\u91cf\u7684\u6807\u7b7e\u7684\u8bdd). \u6211\u4eec\u5982\u4f55\u4ece\u4e00\u4e2a\u5927\u7684\u7ef4\u5ea6\u7a7a\u95f4\u5f97\u5230\u4e00\u4e2a\u66f4\u5c0f\u7684\u7ef4\u5ea6\u7a7a\u95f4? \u5982\u679c\u6211\u4eec\u4e0d\u7528 ascii \u5b57\u7b26\u8868\u793a, \u800c\u4f7f\u7528one-hot encoding\u5462? \u90a3\u5c31\u662f, \u6211\u4eec\u7528\u5982\u4e0b\u6240\u793a\u8868\u793a \\(w\\) \u5b57\u7b26 \\[\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\\] \u5176\u4e2d 1 \u662f \\(w\\) \u7684\u7279\u5f81\u4f4d\u7f6e.\u5176\u4ed6\u7684\u5355\u8bcd\u4e5f\u662f\u5728\u5176\u4ed6\u4f4d\u7f6e\u6709\u4e00\u4e2a1, \u5728\u53e6\u5916\u7684\u4f4d\u7f6e\u90fd\u662f0. \u9664\u4e86\u5de8\u5927\u7684\u5360\u7528\u7a7a\u95f4\u5916,\u8fd9\u79cd\u8868\u8ff0\u6709\u4e00\u4e2a\u5de8\u5927\u7684\u7f3a\u9677. \u5b83\u4ec5\u4ec5\u7b80\u5355\u7684\u628a\u6240\u6709\u7684\u5355\u8bcd\u90fd\u770b\u4f5c\u72ec\u7acb\u5b9e\u4f53\u8ba4 \u4e3a\u5b83\u4eec\u5f7c\u6b64\u4e4b\u95f4\u6beb\u65e0\u5173\u8054.\u6211\u4eec\u771f\u6b63\u60f3\u8981\u7684\u662f\u5355\u8bcd\u4e4b\u95f4\u6709\u4e00\u4e9b \u76f8\u4f3c .\u4e3a\u4ec0\u4e48? \u8ba9\u6211\u4eec\u6765\u770b\u4e00\u4e0b\u4f8b\u5b50. \u5047\u8bbe\u6211\u4eec\u6b63\u5728\u642d\u5efa\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b. \u5047\u8bbe\u6211\u4eec\u5728\u8bad\u7ec3\u96c6\u4e2d\u770b\u5230\u4e86\u5982\u4e0b\u8bed\u53e5. The mathematician ran to the store. The physicist ran to the store. The mathematician solved the open problem. \u73b0\u5728\u5047\u8bbe\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4e2a\u5728\u8bad\u7ec3\u96c6\u4ece\u672a\u770b\u5230\u8fc7\u7684\u65b0\u53e5\u5b50: The physicist solved the open problem. \u6211\u4eec\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u6b64\u53e5\u53ef\u80fd\u8fd0\u884c\u7684\u4e0d\u9519, \u4f46\u5982\u679c\u6211\u4eec\u80fd\u4f7f\u7528\u4ee5\u4e0b\u4e24\u4e2a\u4e8b\u5b9e,\u60c5\u51b5\u4f1a\u597d\u5f97\u591a\u5417: \u6211\u4eec\u770b\u5230\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u5728\u53e5\u5b50\u4e2d\u6709\u7740\u76f8\u540c\u7684\u4f5c\u7528.\u5b83\u4eec\u4e4b\u95f4\u6709\u4e00\u4e2a\u8bed\u4e49\u5173\u7cfb. \u6211\u4eec\u5df2\u7ecf\u770b\u5230\u6570\u5b66\u5bb6\u5728\u8fd9\u4e2a\u65b0\u7684\u6ca1\u770b\u8fc7\u7684\u7684\u53e5\u5b50\u4e2d\u626e\u6f14\u7740\u540c\u6837\u7684\u89d2\u8272, \u5c31\u50cf\u6211\u4eec\u73b0\u5728\u770b\u5230\u7684\u7269\u7406\u5b66\u5bb6\u4e00\u6837. \u7136\u540e\u6211\u4eec\u5c31\u63a8\u65ad\u7269\u7406\u5b66\u5bb6\u5728\u8fd9\u4e2a\u53e5\u5b50\u91cc\u662f\u5f88\u5408\u9002\u7684?\u8fd9\u5c31\u662f\u6211\u4eec\u6307\u7684\u76f8\u4f3c\u7684\u610f\u601d:\u6211\u4eec\u6307\u7684\u662f \u8bed\u4e49\u76f8\u4f3c\u5ea6 , \u4e0d\u4ec5\u4ec5\u662f\u62fc\u5b57\u4e00\u6837\u7684\u8868\u793a. \u5b83\u662f\u4e00\u79cd\u901a\u8fc7\u8fde\u63a5\u6211\u4eec\u6240\u770b\u5230\u7684\u548c\u6211\u4eec\u6ca1\u6709\u770b\u5230\u7684\u4e1c\u897f\u4e4b\u95f4\u7684\u70b9\u6765\u5bf9\u6297\u8bed\u8a00\u6570\u636e\u7a00\u758f\u6027\u7684\u6280\u672f. \u8fd9\u4e2a\u4f8b\u5b50\u5f53\u7136\u8981\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u57fa\u672c\u7684\u8bed\u8a00\u5047\u8bbe:\u5728\u76f8\u4f3c\u7684\u8bed\u5883\u4e2d\u51fa\u73b0\u7684\u5355\u8bcd\u5728\u8bed\u4e49\u4e0a\u662f\u76f8\u4e92\u5173\u8054\u7684. \u8fd9\u88ab\u53eb\u505a distributional hypothesis . Getting Dense Word Embeddings(\u5bc6\u96c6\u5b57\u5d4c\u5165) \u6211\u4eec\u5982\u4f55\u6765\u89e3\u51b3\u90a3\u4e2a\u95ee\u9898?\u6211\u4eec\u600e\u4e48\u80fd\u5728\u5355\u8bcd\u4e2d\u7f16\u7801\u8bed\u4e49\u76f8\u4f3c\u5462? \u4e5f\u8bb8\u6211\u4eec\u4f1a\u60f3\u51fa\u4e00\u4e9b\u8bed\u4e49\u5c5e\u6027. \u4e3e\u4e2a\u4f8b\u5b50, \u6211\u4eec\u770b\u5230\u4e86, \u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u90fd\u4f1a\u8dd1, \u4e5f\u8bb8\u6211\u4eec\u53ef\u4ee5\u628a\u201d\u80fd\u8dd1\u201d\u8fd9\u4e2a\u8bed\u4e49\u5c5e\u6027\u7ed9\u4e00\u4e2a\u9ad8\u5206. \u8003\u8651\u4e00\u4e0b\u5176\u4ed6\u7684\u5c5e\u6027, \u60f3\u8c61\u4e00\u4e0b, \u4f60\u53ef\u80fd\u4f1a\u5728\u8fd9\u4e9b\u5c5e\u6027\u4e0a\u7ed9\u4e00\u4e9b\u666e\u901a\u7684\u5355\u8bcd\u5f97\u5206. \u5982\u679c\u6bcf\u4e00\u4e2a\u5c5e\u6027\u90fd\u662f\u4e00\u7ef4, \u90a3\u6211\u4eec\u53ef\u4ee5\u7ed9\u4e00\u4e2a\u5411\u91cf\u4ee3\u8868\u4e00\u4e2a\u5355\u8bcd, \u50cf\u8fd9\u6837: \\[ q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run}, \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]\\] \\[ q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run}, \\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]\\] \u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u6765\u5f97\u5230\u8fd9\u4e9b\u5355\u8bcd\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6: \\[\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}\\] \u5c3d\u7ba1\u901a\u5e38\u60c5\u51b5\u4e0b\u9700\u8981\u5f52\u4e00\u5316: \\[ \\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}} {\\| q_\\text{\\physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)\\] \u5176\u4e2d \\(\\phi\\) \u662f\u4e24\u4e2a\u5411\u91cf\u7684\u89d2\u5ea6. \u8fd9\u5c31\u610f\u5473\u7740,\u6781\u7aef\u76f8\u4f3c\u7684\u5355\u8bcd(\u5d4c\u5165\u65b9\u5411\u662f\u540c\u4e00\u4e2a) \u4f1a\u5f97\u5230\u76f8\u4f3c\u5ea6\u4e3a1.\u53cd\u4e4b\u4e3a -1. \u4f60\u53ef\u4ee5\u8ba4\u4e3a\u672c\u7ae0\u521a\u5f00\u59cb\u7684\u7a00\u758fone-hot \u5411\u91cf\u662f\u6211\u4eec\u521a\u5b9a\u4e49\u5411\u91cf\u7684\u7279\u6b8a\u5f62\u5f0f,\u5176\u4e2d\u5355\u8bcd\u7684\u76f8\u4f3c\u5ea6\u4e3a 0, \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u7ed9\u6bcf\u4e00\u4e2a\u5355\u8bcd\u4e00\u4e9b\u72ec\u7279\u7684\u8bed\u4e49\u5c5e\u6027.\u8fd9\u4e9b\u5411\u91cf\u662f \u5bc6\u96c6\u7684 , \u4e5f\u5c31\u662f\u8bf4\u4ed6\u4eec\u662f\u975e\u96f6\u7684. \u4f46\u662f\u8fd9\u4e9b\u65b0\u7684\u5411\u91cf\u662f\u4e00\u79cd\u5de8\u5927\u7684\u75db\u82e6:\u4f60\u53ef\u4ee5\u60f3\u5230\u6570\u5343\u79cd\u4e0d\u540c\u7684\u8bed\u4e49\u5c5e\u6027,\u5b83\u4eec\u53ef\u80fd\u4e0e\u51b3\u5b9a\u76f8\u4f3c\u6027\u6709\u5173, \u800c\u4e14\u7a76\u7adf\u4f60\u600e\u6837\u628a\u5b83\u4eec\u8bbe\u7f6e\u6210\u4e0d\u540c\u7684\u5c5e\u6027? \u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u5fc3\u601d\u60f3\u662f\u6bd4\u8d77\u9700\u8981\u7a0b\u5e8f\u5458\u53bb\u81ea\u5df1\u8bbe\u8ba1\u7279\u5f81, \u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7279\u5f81\u7684\u8868\u793a. \u6240\u4ee5\u4e3a\u4ec0\u4e48\u4e0d\u5728\u6211\u4eec\u7684\u6a21\u578b\u4e2d\u8ba9\u5355\u8bcd\u5d4c\u5165\u5230\u7cfb\u6570\u4e2d,\u7136\u540e\u8ba9\u5b83\u4eec\u5728\u8bad\u7ec3\u4e2d\u66f4\u65b0\u5462? \u8fd9\u5c31\u662f\u6211\u4eec\u8981\u505a\u7684. \u6211\u4eec\u4f1a\u6709\u4e00\u4e9b \u6f5c\u5728\u7684\u8bed\u4e49\u5c5e\u6027 \u7f51\u7edc\u53ef\u4ee5, \u4e25\u8083\u6765\u8bb2, \u5b66\u4e60. \u6ce8\u610f, \u5d4c\u5165\u8bcd\u53ef\u80fd\u65e0\u6cd5\u89e3\u91ca. \u90a3\u5c31\u662f\u5c3d\u7ba1\u5982\u4e0a\u6240\u793a\u6211\u4eec\u624b\u5de5\u5236\u4f5c\u7684\u77e2\u91cf\u56fe,\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u7684\u76f8\u4f3c\u4e4b\u5904\u662f\u4ed6\u4eec\u90fd\u559c\u6b22\u5496\u5561. \u5982\u679c\u6211\u4eec\u5141\u8bb8\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5d4c\u5165, \u5e76\u770b\u5230\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u5728\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u4e2d\u6709\u5f88\u5927\u7684\u4ef7\u503c, \u4f46\u662f\u5b83\u610f\u5473\u7740\u4ec0\u4e48\u5f88\u4e0d\u6e05\u6670. \u5728\u6f5c\u5728\u8bed\u4e49\u6765\u8bb2\u5b83\u4eec\u662f\u76f8\u4f3c\u7684, \u4f46\u662f\u5bf9\u6211\u4eec\u6765\u8bf4\u662f\u65e0\u6cd5\u89e3\u91ca\u7684. \u603b\u7ed3\u4e00\u4e0b, \u5355\u8bcd\u5d4c\u5165\u662f\u4e00\u4e2a\u5355\u8bcd \u8bed\u4e49 \u7684\u8868\u793a,\u8bed\u4e49\u4fe1\u606f\u7684\u6709\u6548\u7f16\u7801\u53ef\u80fd\u4e0e\u624b\u5934\u4efb\u52a1\u76f8\u5173 . \u4f60\u4e5f\u53ef\u4ee5\u5d4c\u5165\u5176\u4ed6\u7684\u4e1c\u897f: \u90e8\u5206\u7684\u8bed\u97f3\u6807\u7b7e, \u89e3\u6790\u6811, \u5176\u4ed6\u4efb\u4f55\u4e1c\u897f! \u7279\u5f81\u5d4c\u5165\u662f\u8fd9\u4e2a\u9886\u57df\u7684\u6838\u5fc3\u601d\u60f3. Word Embeddings in Pytorch\uff08Pytorch\u4e2d\u7684\u5355\u8bcd\u5d4c\u5165\uff09 \u5728\u6211\u4eec\u4e3e\u4f8b\u6216\u7ec3\u4e60\u4e4b\u524d, \u5173\u4e8e\u5982\u4f55\u5728Pytorch\u4e2d\u4f7f\u7528\u5d4c\u5165\u4ee5\u53ca\u5728\u4e00\u822c\u7684\u6df1\u5ea6\u5b66\u4e60\u7f16\u7a0b\u4e2d,\u6709\u4e00\u4e9b\u5feb\u901f \u7684\u8bf4\u660e.\u4e0e\u5236\u4f5cone-hot\u5411\u91cf\u65f6\u6211\u4eec\u5bf9\u6bcf\u4e00\u4e2a\u5355\u8bcd\u5b9a\u4e49\u4e00\u4e2a\u7279\u522b\u7684\u7d22\u5f15\u76f8\u4f3c,\u5355\u8bcd\u5d4c\u5165\u65f6\u540c\u6837\u9700\u8981\u5bf9\u6bcf \u4e00\u4e2a\u5355\u8bcd\u5b9a\u4e49\u4e00\u4e2a\u7279\u522b\u7684\u7d22\u5f15. \u8fd9\u4e9b\u5c06\u662f\u67e5\u627e\u8868\u4e2d\u7684\u952e. \u610f\u601d\u662f,\u5d4c\u5165\u88ab\u50a8\u5b58\u4e3a\u4e00\u4e2a \\(|V| \\times D\\) \u77e9\u9635, \u5176\u4e2d \\(D\\) \u662f\u5d4c\u5165\u7684\u7ef4\u5ea6, \u8fd9\u6837\u7684\u8bcd\u88ab\u8d4b\u4e88\u4e86\u7d22\u5f15 \\(i\\) \u5b83\u7684\u5d4c\u5165\u88ab\u50a8\u5b58\u5728\u77e9\u9635\u7684 \u7b2c \\(i\\) \u884c. \u5728\u6240\u6709\u7684\u4ee3\u7801\u4e2d, \u4ece\u5355\u8bcd\u5230\u7d22\u5f15\u7684\u6620\u5c04\u662f\u4e00\u4e2a\u547d\u540d\u7684\u5b57\u5178 word_to_ix. \u5141\u8bb8\u4f60\u4f7f\u7528\u5d4c\u5165\u7684\u6a21\u5757\u5f0f torch.nn.Embedding,\u8fd9\u9700\u8981\u4e24\u4e2a\u53c2\u6570:\u8bcd\u6c47\u91cf\u548c\u5d4c\u5165\u7684\u7ef4\u5ea6. \u4e3a\u4e86\u7d22\u5f15\u5230\u8fd9\u4e2a\u8868\u4e2d,\u4f60\u9700\u8981\u4f7f\u7528 torch.LongTensor (\u7d22\u5f15\u4e3a\u6574\u6570,\u4e0d\u80fd\u4e3a\u6d6e\u70b9\u6570). # \u4f5c\u8005: Robert Guthrie import torch import torch.autograd as autograd import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) word_to_ix = {\"hello\": 0, \"world\": 1} embeds = nn.Embedding(2, 5) # 2 \u5355\u8bcd, 5 \u7ef4\u5d4c\u5165 lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"]]) hello_embed = embeds(autograd.Variable(lookup_tensor)) print(hello_embed) \u4f8b\u5b50: N-Gram \u8bed\u8a00\u6a21\u578b \u56de\u60f3\u4e00\u4e0b \u5728n-gram\u8bed\u8a00\u6a21\u578b\u4e2d,\u7ed9\u5b9a\u4e00\u7cfb\u5217\u5355\u8bcd \\(w\\) , \u6211\u4eec\u9700\u8981\u8ba1\u7b97 \\[P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\\] \\(w_i\\) \u662f\u53e5\u5b50\u4e2d\u7b2ci\u4e2a\u5355\u8bcd. \u672c\u4f8b\u4e2d, \u6211\u4eec\u5c06\u8ba1\u7b97\u4e00\u4e9b\u8bad\u7ec3\u96c6\u7684\u635f\u5931\u51fd\u6570\u5e76\u4e14\u7528\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u7cfb\u6570. CONTEXT_SIZE = 2 EMBEDDING_DIM = 10 # \u6211\u4eec\u5c06\u4f7f\u7528 Shakespeare Sonnet 2 test_sentence = \"\"\"When forty winters shall besiege thy brow, And dig deep trenches in thy beauty's field, Thy youth's proud livery so gazed on now, Will be a totter'd weed of small worth held: Then being asked, where all thy beauty lies, Where all the treasure of thy lusty days; To say, within thine own deep sunken eyes, Were an all-eating shame, and thriftless praise. How much more praise deserv'd thy beauty's use, If thou couldst answer 'This fair child of mine Shall sum my count, and make my old excuse,' Proving his beauty by succession thine! This were to be new made when thou art old, And see thy blood warm when thou feel'st it cold.\"\"\".split() # \u6211\u4eec\u5e94\u8be5\u5bf9\u8f93\u5165\u8fdb\u884c\u6807\u8bb0,\u4f46\u662f\u6211\u4eec\u5c06\u5ffd\u7565\u5b83 # \u5efa\u9020\u4e00\u7cfb\u5217\u5143\u7ec4. \u6bcf\u4e2a\u5143\u7ec4 ([ word_i-2, word_i-1 ], \u90fd\u662f\u76ee\u6807\u5355\u8bcd) trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2]) for i in range(len(test_sentence) - 2)] # \u8f93\u51fa\u524d 3, \u4e3a\u4e86\u8ba9\u4f60\u770b\u5230\u4ed6\u7684\u5404\u5f0f print(trigrams[:3]) vocab = set(test_sentence) word_to_ix = {word: i for i, word in enumerate(vocab)} class NGramLanguageModeler(nn.Module): def __init__(self, vocab_size, embedding_dim, context_size): super(NGramLanguageModeler, self).__init__() self.embeddings = nn.Embedding(vocab_size, embedding_dim) self.linear1 = nn.Linear(context_size * embedding_dim, 128) self.linear2 = nn.Linear(128, vocab_size) def forward(self, inputs): embeds = self.embeddings(inputs).view((1, -1)) out = F.relu(self.linear1(embeds)) out = self.linear2(out) log_probs = F.log_softmax(out, dim=1) return log_probs losses = [] loss_function = nn.NLLLoss() model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE) optimizer = optim.SGD(model.parameters(), lr=0.001) for epoch in range(10): total_loss = torch.Tensor([0]) for context, target in trigrams: # \u6b65\u9aa4 1\\. \u51c6\u5907\u597d\u8fdb\u5165\u6a21\u578b\u7684\u6570\u636e (\u4f8b\u5982\u5c06\u5355\u8bcd\u8f6c\u6362\u6210\u6574\u6570\u7d22\u5f15,\u5e76\u5c06\u5176\u5c01\u88c5\u5728\u53d8\u91cf\u4e2d) context_idxs = [word_to_ix[w] for w in context] context_var = autograd.Variable(torch.LongTensor(context_idxs)) # \u6b65\u9aa4 2\\. \u56de\u8c03 *\u79ef\u7d2f* \u68af\u5ea6. \u5728\u8fdb\u5165\u4e00\u4e2a\u5b9e\u4f8b\u524d,\u9700\u8981\u5c06\u4e4b\u524d\u7684\u5b9e\u529b\u68af\u5ea6\u7f6e\u96f6 model.zero_grad() # \u6b65\u9aa4 3\\. \u8fd0\u884c\u53cd\u5411\u4f20\u64ad,\u5f97\u5230\u5355\u8bcd\u7684\u6982\u7387\u5206\u5e03 log_probs = model(context_var) # \u6b65\u9aa4 4\\. \u8ba1\u7b97\u635f\u5931\u51fd\u6570. (\u518d\u6b21\u6ce8\u610f, Torch\u9700\u8981\u5c06\u76ee\u6807\u5355\u8bcd\u5c01\u88c5\u5728\u53d8\u91cf\u4e2d) loss = loss_function(log_probs, autograd.Variable( torch.LongTensor([word_to_ix[target]]))) # \u6b65\u9aa4 5\\. \u53cd\u5411\u4f20\u64ad\u5e76\u66f4\u65b0\u68af\u5ea6 loss.backward() optimizer.step() total_loss += loss.data losses.append(total_loss) print(losses) # \u5728\u8bad\u7ec3\u96c6\u4e2d\u6bcf\u6b21\u8fed\u4ee3\u635f\u5931\u90fd\u4f1a\u51cf\u5c0f! Exercise: Computing Word Embeddings: Continuous Bag-of-Words(\u7ec3\u4e60: \u8ba1\u7b97\u5355\u8bcd\u5d4c\u5165: \u8fde\u7eed\u5355\u8bcd\u5305) \u8fde\u7eed\u5355\u8bcd\u5305\u6a21\u578b (CBOW) \u5728NLP\u6df1\u5ea6\u5b66\u4e60\u4e2d\u4f7f\u7528\u7684\u5f88\u9891\u7e41. \u8fd9\u4e2a\u6a21\u578b\u5c1d\u8bd5\u53bb\u9884\u6d4b\u6587\u4e2d\u76ee\u6807\u5355\u8bcd\u7684 \u524d\u540e\u4e00\u4e9b\u5355\u8bcd. \u5b83\u6709\u522b\u4e8e\u8bed\u8a00\u5efa\u6a21, \u56e0\u4e3aCBOW\u4e0d\u662f\u987a\u5e8f\u7684, \u4e5f\u4e0d\u9700\u8981\u662f\u6982\u7387\u6027\u7684.CBOW\u88ab\u7528\u6765\u5feb \u901f\u8bad\u7ec3\u5355\u8bcd\u5d4c\u5165,\u800c\u8fd9\u4e9b\u5d4c\u5165\u88ab\u7528\u6765\u521d\u59cb\u5316\u4e00\u4e9b\u590d\u6742\u6a21\u578b\u7684\u5d4c\u5165.\u901a\u5e38\u60c5\u51b5\u4e0b, \u8fd9\u88ab\u79f0\u4e3a \u9884\u8bad\u7ec3\u5d4c\u5165 . \u5b83\u51e0\u4e4e\u603b\u662f\u80fd\u5e2e\u52a9\u63d0\u5347\u767e\u5206\u4e4b\u51e0\u7684\u6027\u80fd. CBOW\u6a21\u578b\u5982\u4e0b\u6240\u793a.\u7ed9\u5b9a\u4e00\u4e2a\u76ee\u6807\u5355\u8bcd \\(w_i\\) \u548c \\(N\\) \u4ee3\u8868\u5355\u8bcd\u6bcf\u4e00\u904d\u7684\u6ed1\u7a97\u8ddd, \\(w_{i-1}, \\dots, w_{i-N}\\) \u548c \\(w_{i+1}, \\dots, w_{i+N}\\) , \u5c06\u6240\u6709\u4e0a\u4e0b\u6587\u8bcd\u7edf\u79f0\u4e3a \\(C\\) ,CBOW\u8bd5\u56fe\u53bb\u6700\u5c0f\u5316\u5982\u4e0b \\[-\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\] \u5176\u4e2d \\(q_w\\) \u662f\u5355\u8bcd \\(w\\) \u7684\u5d4c\u5165. \u5728Pytorch\u4e2d\u901a\u8fc7\u586b\u5145\u4e0b\u9762\u7684\u7c7b\u6765\u5b9e\u73b0\u8fd9\u4e2a\u6a21\u578b. \u4e00\u4e9b\u5efa\u8bae: \u60f3\u597d\u4f60\u9700\u8981\u5b9a\u4e49\u7684\u7cfb\u6570. \u786e\u4fdd\u4f60\u77e5\u9053\u6bcf\u4e00\u6b65\u64cd\u4f5c\u540e\u7684\u6784\u9020. \u5982\u679c\u60f3\u8981\u91cd\u6784\u8bf7\u4f7f\u7528 .view(). CONTEXT_SIZE = 2 # \u5de6\u53f3\u54042\u4e2a\u5355\u8bcd raw_text = \"\"\"We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\"\"\".split() # \u901a\u8fc7\u4ece `raw_text` \u5f97\u5230\u4e00\u7ec4\u5355\u8bcd, \u8fdb\u884c\u53bb\u91cd\u64cd\u4f5c vocab = set(raw_text) vocab_size = len(vocab) word_to_ix = {word: i for i, word in enumerate(vocab)} data = [] for i in range(2, len(raw_text) - 2): context = [raw_text[i - 2], raw_text[i - 1], raw_text[i + 1], raw_text[i + 2]] target = raw_text[i] data.append((context, target)) print(data[:5]) class CBOW(nn.Module): def __init__(self): pass def forward(self, inputs): pass # \u521b\u5efa\u6a21\u578b\u5e76\u4e14\u8bad\u7ec3. \u8fd9\u91cc\u6709\u4e00\u4e9b\u51fd\u6570\u53ef\u4ee5\u5728\u4f7f\u7528\u6a21\u578b\u4e4b\u524d\u5e2e\u52a9\u4f60\u51c6\u5907\u6570\u636e def make_context_vector(context, word_to_ix): idxs = [word_to_ix[w] for w in context] tensor = torch.LongTensor(idxs) return autograd.Variable(tensor) make_context_vector(data[0][0], word_to_ix) # \u4f8b\u5b50 Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: word_embeddings_tutorial.py Download Jupyter notebook: word_embeddings_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u8bcd\u6c47\u5d4c\u5165:\u7f16\u7801\u8bcd\u6c47\u8bed\u4e49"},{"location":"27/#_1","text":"\u5355\u8bcd\u5d4c\u5165\u662f\u771f\u5b9e\u6570\u5b57\u7684\u5bc6\u96c6\u5411\u91cf,\u5728\u4f60\u7684\u8bcd\u6c47\u8868\u4e2d\u6bcf\u4e00\u4e2a\u5355\u8bcd\u90fd\u662f. \u5728NLP\u4e2d, \u901a\u5e38\u60c5\u51b5\u4e0b, \u60a8\u7684\u7279\u6027\u5c31\u662f\u5355\u8bcd!\u4f46\u662f\u4f60\u5e94\u8be5\u600e\u4e48\u5728\u4f60\u7684\u7535\u8111\u4e2d\u8868\u793a\u4e00\u4e2a\u5355\u8bcd?\u53ef\u4ee5\u5b58\u50a8\u5b83\u7684ascii\u5b57\u7b26\u8868\u793a, \u4f46\u90a3\u4ec5\u4ec5\u544a\u8bc9\u4f60\u5355\u8bcd \u662f \u4ec0\u4e48,\u6ca1\u6709\u8bf4\u592a\u591a\u5b83 \u610f\u5473 \u7740\u4ec0\u4e48 (\u4f60\u4e5f\u8bb8\u53ef\u4ee5\u4ece\u5b83\u7684\u8bcd\u7f00\u4e2d\u6d3e\u751f\u51fa\u5b83\u7684\u8bcd\u6027, \u6216\u8005\u4ece\u5b83\u7684\u5927\u5c0f\u5199\u4e2d\u5f97\u5230\u5b83\u7684\u5c5e\u6027,\u4f46\u5e76\u4e0d\u591a.). \u66f4\u91cd\u8981\u7684\u662f, \u5728\u4ec0\u4e48\u610f\u4e49\u4e0a\u4f60\u80fd\u628a\u8fd9\u4e9b\u8868\u8c61\u7ed3\u5408\u8d77\u6765? \u6211\u4eec\u7ecf\u5e38\u9700\u8981\u795e\u7ecf\u7f51\u7edc\u7684\u5bc6\u96c6\u8f93\u51fa, \u8f93\u5165\u4e3a \\(|V|\\) \u7ef4, \u5176\u4e2d \\(V\\) \u662f\u6211\u4eec\u7684\u8bcd\u6c47\u8868, \u4f46\u7ecf\u5e38\u8f93\u51fa\u662f\u66f4\u5c0f\u7ef4\u5ea6\u7684 (\u5982\u679c\u6211\u4eec\u53ea\u9884\u6d4b\u5c11\u91cf\u7684\u6807\u7b7e\u7684\u8bdd). \u6211\u4eec\u5982\u4f55\u4ece\u4e00\u4e2a\u5927\u7684\u7ef4\u5ea6\u7a7a\u95f4\u5f97\u5230\u4e00\u4e2a\u66f4\u5c0f\u7684\u7ef4\u5ea6\u7a7a\u95f4? \u5982\u679c\u6211\u4eec\u4e0d\u7528 ascii \u5b57\u7b26\u8868\u793a, \u800c\u4f7f\u7528one-hot encoding\u5462? \u90a3\u5c31\u662f, \u6211\u4eec\u7528\u5982\u4e0b\u6240\u793a\u8868\u793a \\(w\\) \u5b57\u7b26 \\[\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\\] \u5176\u4e2d 1 \u662f \\(w\\) \u7684\u7279\u5f81\u4f4d\u7f6e.\u5176\u4ed6\u7684\u5355\u8bcd\u4e5f\u662f\u5728\u5176\u4ed6\u4f4d\u7f6e\u6709\u4e00\u4e2a1, \u5728\u53e6\u5916\u7684\u4f4d\u7f6e\u90fd\u662f0. \u9664\u4e86\u5de8\u5927\u7684\u5360\u7528\u7a7a\u95f4\u5916,\u8fd9\u79cd\u8868\u8ff0\u6709\u4e00\u4e2a\u5de8\u5927\u7684\u7f3a\u9677. \u5b83\u4ec5\u4ec5\u7b80\u5355\u7684\u628a\u6240\u6709\u7684\u5355\u8bcd\u90fd\u770b\u4f5c\u72ec\u7acb\u5b9e\u4f53\u8ba4 \u4e3a\u5b83\u4eec\u5f7c\u6b64\u4e4b\u95f4\u6beb\u65e0\u5173\u8054.\u6211\u4eec\u771f\u6b63\u60f3\u8981\u7684\u662f\u5355\u8bcd\u4e4b\u95f4\u6709\u4e00\u4e9b \u76f8\u4f3c .\u4e3a\u4ec0\u4e48? \u8ba9\u6211\u4eec\u6765\u770b\u4e00\u4e0b\u4f8b\u5b50. \u5047\u8bbe\u6211\u4eec\u6b63\u5728\u642d\u5efa\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b. \u5047\u8bbe\u6211\u4eec\u5728\u8bad\u7ec3\u96c6\u4e2d\u770b\u5230\u4e86\u5982\u4e0b\u8bed\u53e5. The mathematician ran to the store. The physicist ran to the store. The mathematician solved the open problem. \u73b0\u5728\u5047\u8bbe\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4e2a\u5728\u8bad\u7ec3\u96c6\u4ece\u672a\u770b\u5230\u8fc7\u7684\u65b0\u53e5\u5b50: The physicist solved the open problem. \u6211\u4eec\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u6b64\u53e5\u53ef\u80fd\u8fd0\u884c\u7684\u4e0d\u9519, \u4f46\u5982\u679c\u6211\u4eec\u80fd\u4f7f\u7528\u4ee5\u4e0b\u4e24\u4e2a\u4e8b\u5b9e,\u60c5\u51b5\u4f1a\u597d\u5f97\u591a\u5417: \u6211\u4eec\u770b\u5230\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u5728\u53e5\u5b50\u4e2d\u6709\u7740\u76f8\u540c\u7684\u4f5c\u7528.\u5b83\u4eec\u4e4b\u95f4\u6709\u4e00\u4e2a\u8bed\u4e49\u5173\u7cfb. \u6211\u4eec\u5df2\u7ecf\u770b\u5230\u6570\u5b66\u5bb6\u5728\u8fd9\u4e2a\u65b0\u7684\u6ca1\u770b\u8fc7\u7684\u7684\u53e5\u5b50\u4e2d\u626e\u6f14\u7740\u540c\u6837\u7684\u89d2\u8272, \u5c31\u50cf\u6211\u4eec\u73b0\u5728\u770b\u5230\u7684\u7269\u7406\u5b66\u5bb6\u4e00\u6837. \u7136\u540e\u6211\u4eec\u5c31\u63a8\u65ad\u7269\u7406\u5b66\u5bb6\u5728\u8fd9\u4e2a\u53e5\u5b50\u91cc\u662f\u5f88\u5408\u9002\u7684?\u8fd9\u5c31\u662f\u6211\u4eec\u6307\u7684\u76f8\u4f3c\u7684\u610f\u601d:\u6211\u4eec\u6307\u7684\u662f \u8bed\u4e49\u76f8\u4f3c\u5ea6 , \u4e0d\u4ec5\u4ec5\u662f\u62fc\u5b57\u4e00\u6837\u7684\u8868\u793a. \u5b83\u662f\u4e00\u79cd\u901a\u8fc7\u8fde\u63a5\u6211\u4eec\u6240\u770b\u5230\u7684\u548c\u6211\u4eec\u6ca1\u6709\u770b\u5230\u7684\u4e1c\u897f\u4e4b\u95f4\u7684\u70b9\u6765\u5bf9\u6297\u8bed\u8a00\u6570\u636e\u7a00\u758f\u6027\u7684\u6280\u672f. \u8fd9\u4e2a\u4f8b\u5b50\u5f53\u7136\u8981\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u57fa\u672c\u7684\u8bed\u8a00\u5047\u8bbe:\u5728\u76f8\u4f3c\u7684\u8bed\u5883\u4e2d\u51fa\u73b0\u7684\u5355\u8bcd\u5728\u8bed\u4e49\u4e0a\u662f\u76f8\u4e92\u5173\u8054\u7684. \u8fd9\u88ab\u53eb\u505a distributional hypothesis .","title":"\u8bcd\u6c47\u5d4c\u5165:\u7f16\u7801\u8bcd\u6c47\u8bed\u4e49"},{"location":"27/#getting-dense-word-embeddings","text":"\u6211\u4eec\u5982\u4f55\u6765\u89e3\u51b3\u90a3\u4e2a\u95ee\u9898?\u6211\u4eec\u600e\u4e48\u80fd\u5728\u5355\u8bcd\u4e2d\u7f16\u7801\u8bed\u4e49\u76f8\u4f3c\u5462? \u4e5f\u8bb8\u6211\u4eec\u4f1a\u60f3\u51fa\u4e00\u4e9b\u8bed\u4e49\u5c5e\u6027. \u4e3e\u4e2a\u4f8b\u5b50, \u6211\u4eec\u770b\u5230\u4e86, \u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u90fd\u4f1a\u8dd1, \u4e5f\u8bb8\u6211\u4eec\u53ef\u4ee5\u628a\u201d\u80fd\u8dd1\u201d\u8fd9\u4e2a\u8bed\u4e49\u5c5e\u6027\u7ed9\u4e00\u4e2a\u9ad8\u5206. \u8003\u8651\u4e00\u4e0b\u5176\u4ed6\u7684\u5c5e\u6027, \u60f3\u8c61\u4e00\u4e0b, \u4f60\u53ef\u80fd\u4f1a\u5728\u8fd9\u4e9b\u5c5e\u6027\u4e0a\u7ed9\u4e00\u4e9b\u666e\u901a\u7684\u5355\u8bcd\u5f97\u5206. \u5982\u679c\u6bcf\u4e00\u4e2a\u5c5e\u6027\u90fd\u662f\u4e00\u7ef4, \u90a3\u6211\u4eec\u53ef\u4ee5\u7ed9\u4e00\u4e2a\u5411\u91cf\u4ee3\u8868\u4e00\u4e2a\u5355\u8bcd, \u50cf\u8fd9\u6837: \\[ q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run}, \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]\\] \\[ q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run}, \\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]\\] \u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u6765\u5f97\u5230\u8fd9\u4e9b\u5355\u8bcd\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6: \\[\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}\\] \u5c3d\u7ba1\u901a\u5e38\u60c5\u51b5\u4e0b\u9700\u8981\u5f52\u4e00\u5316: \\[ \\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}} {\\| q_\\text{\\physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)\\] \u5176\u4e2d \\(\\phi\\) \u662f\u4e24\u4e2a\u5411\u91cf\u7684\u89d2\u5ea6. \u8fd9\u5c31\u610f\u5473\u7740,\u6781\u7aef\u76f8\u4f3c\u7684\u5355\u8bcd(\u5d4c\u5165\u65b9\u5411\u662f\u540c\u4e00\u4e2a) \u4f1a\u5f97\u5230\u76f8\u4f3c\u5ea6\u4e3a1.\u53cd\u4e4b\u4e3a -1. \u4f60\u53ef\u4ee5\u8ba4\u4e3a\u672c\u7ae0\u521a\u5f00\u59cb\u7684\u7a00\u758fone-hot \u5411\u91cf\u662f\u6211\u4eec\u521a\u5b9a\u4e49\u5411\u91cf\u7684\u7279\u6b8a\u5f62\u5f0f,\u5176\u4e2d\u5355\u8bcd\u7684\u76f8\u4f3c\u5ea6\u4e3a 0, \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u7ed9\u6bcf\u4e00\u4e2a\u5355\u8bcd\u4e00\u4e9b\u72ec\u7279\u7684\u8bed\u4e49\u5c5e\u6027.\u8fd9\u4e9b\u5411\u91cf\u662f \u5bc6\u96c6\u7684 , \u4e5f\u5c31\u662f\u8bf4\u4ed6\u4eec\u662f\u975e\u96f6\u7684. \u4f46\u662f\u8fd9\u4e9b\u65b0\u7684\u5411\u91cf\u662f\u4e00\u79cd\u5de8\u5927\u7684\u75db\u82e6:\u4f60\u53ef\u4ee5\u60f3\u5230\u6570\u5343\u79cd\u4e0d\u540c\u7684\u8bed\u4e49\u5c5e\u6027,\u5b83\u4eec\u53ef\u80fd\u4e0e\u51b3\u5b9a\u76f8\u4f3c\u6027\u6709\u5173, \u800c\u4e14\u7a76\u7adf\u4f60\u600e\u6837\u628a\u5b83\u4eec\u8bbe\u7f6e\u6210\u4e0d\u540c\u7684\u5c5e\u6027? \u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u5fc3\u601d\u60f3\u662f\u6bd4\u8d77\u9700\u8981\u7a0b\u5e8f\u5458\u53bb\u81ea\u5df1\u8bbe\u8ba1\u7279\u5f81, \u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7279\u5f81\u7684\u8868\u793a. \u6240\u4ee5\u4e3a\u4ec0\u4e48\u4e0d\u5728\u6211\u4eec\u7684\u6a21\u578b\u4e2d\u8ba9\u5355\u8bcd\u5d4c\u5165\u5230\u7cfb\u6570\u4e2d,\u7136\u540e\u8ba9\u5b83\u4eec\u5728\u8bad\u7ec3\u4e2d\u66f4\u65b0\u5462? \u8fd9\u5c31\u662f\u6211\u4eec\u8981\u505a\u7684. \u6211\u4eec\u4f1a\u6709\u4e00\u4e9b \u6f5c\u5728\u7684\u8bed\u4e49\u5c5e\u6027 \u7f51\u7edc\u53ef\u4ee5, \u4e25\u8083\u6765\u8bb2, \u5b66\u4e60. \u6ce8\u610f, \u5d4c\u5165\u8bcd\u53ef\u80fd\u65e0\u6cd5\u89e3\u91ca. \u90a3\u5c31\u662f\u5c3d\u7ba1\u5982\u4e0a\u6240\u793a\u6211\u4eec\u624b\u5de5\u5236\u4f5c\u7684\u77e2\u91cf\u56fe,\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u7684\u76f8\u4f3c\u4e4b\u5904\u662f\u4ed6\u4eec\u90fd\u559c\u6b22\u5496\u5561. \u5982\u679c\u6211\u4eec\u5141\u8bb8\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5d4c\u5165, \u5e76\u770b\u5230\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u5728\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u4e2d\u6709\u5f88\u5927\u7684\u4ef7\u503c, \u4f46\u662f\u5b83\u610f\u5473\u7740\u4ec0\u4e48\u5f88\u4e0d\u6e05\u6670. \u5728\u6f5c\u5728\u8bed\u4e49\u6765\u8bb2\u5b83\u4eec\u662f\u76f8\u4f3c\u7684, \u4f46\u662f\u5bf9\u6211\u4eec\u6765\u8bf4\u662f\u65e0\u6cd5\u89e3\u91ca\u7684. \u603b\u7ed3\u4e00\u4e0b, \u5355\u8bcd\u5d4c\u5165\u662f\u4e00\u4e2a\u5355\u8bcd \u8bed\u4e49 \u7684\u8868\u793a,\u8bed\u4e49\u4fe1\u606f\u7684\u6709\u6548\u7f16\u7801\u53ef\u80fd\u4e0e\u624b\u5934\u4efb\u52a1\u76f8\u5173 . \u4f60\u4e5f\u53ef\u4ee5\u5d4c\u5165\u5176\u4ed6\u7684\u4e1c\u897f: \u90e8\u5206\u7684\u8bed\u97f3\u6807\u7b7e, \u89e3\u6790\u6811, \u5176\u4ed6\u4efb\u4f55\u4e1c\u897f! \u7279\u5f81\u5d4c\u5165\u662f\u8fd9\u4e2a\u9886\u57df\u7684\u6838\u5fc3\u601d\u60f3.","title":"Getting Dense Word Embeddings(\u5bc6\u96c6\u5b57\u5d4c\u5165)"},{"location":"27/#word-embeddings-in-pytorchpytorch","text":"\u5728\u6211\u4eec\u4e3e\u4f8b\u6216\u7ec3\u4e60\u4e4b\u524d, \u5173\u4e8e\u5982\u4f55\u5728Pytorch\u4e2d\u4f7f\u7528\u5d4c\u5165\u4ee5\u53ca\u5728\u4e00\u822c\u7684\u6df1\u5ea6\u5b66\u4e60\u7f16\u7a0b\u4e2d,\u6709\u4e00\u4e9b\u5feb\u901f \u7684\u8bf4\u660e.\u4e0e\u5236\u4f5cone-hot\u5411\u91cf\u65f6\u6211\u4eec\u5bf9\u6bcf\u4e00\u4e2a\u5355\u8bcd\u5b9a\u4e49\u4e00\u4e2a\u7279\u522b\u7684\u7d22\u5f15\u76f8\u4f3c,\u5355\u8bcd\u5d4c\u5165\u65f6\u540c\u6837\u9700\u8981\u5bf9\u6bcf \u4e00\u4e2a\u5355\u8bcd\u5b9a\u4e49\u4e00\u4e2a\u7279\u522b\u7684\u7d22\u5f15. \u8fd9\u4e9b\u5c06\u662f\u67e5\u627e\u8868\u4e2d\u7684\u952e. \u610f\u601d\u662f,\u5d4c\u5165\u88ab\u50a8\u5b58\u4e3a\u4e00\u4e2a \\(|V| \\times D\\) \u77e9\u9635, \u5176\u4e2d \\(D\\) \u662f\u5d4c\u5165\u7684\u7ef4\u5ea6, \u8fd9\u6837\u7684\u8bcd\u88ab\u8d4b\u4e88\u4e86\u7d22\u5f15 \\(i\\) \u5b83\u7684\u5d4c\u5165\u88ab\u50a8\u5b58\u5728\u77e9\u9635\u7684 \u7b2c \\(i\\) \u884c. \u5728\u6240\u6709\u7684\u4ee3\u7801\u4e2d, \u4ece\u5355\u8bcd\u5230\u7d22\u5f15\u7684\u6620\u5c04\u662f\u4e00\u4e2a\u547d\u540d\u7684\u5b57\u5178 word_to_ix. \u5141\u8bb8\u4f60\u4f7f\u7528\u5d4c\u5165\u7684\u6a21\u5757\u5f0f torch.nn.Embedding,\u8fd9\u9700\u8981\u4e24\u4e2a\u53c2\u6570:\u8bcd\u6c47\u91cf\u548c\u5d4c\u5165\u7684\u7ef4\u5ea6. \u4e3a\u4e86\u7d22\u5f15\u5230\u8fd9\u4e2a\u8868\u4e2d,\u4f60\u9700\u8981\u4f7f\u7528 torch.LongTensor (\u7d22\u5f15\u4e3a\u6574\u6570,\u4e0d\u80fd\u4e3a\u6d6e\u70b9\u6570). # \u4f5c\u8005: Robert Guthrie import torch import torch.autograd as autograd import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) word_to_ix = {\"hello\": 0, \"world\": 1} embeds = nn.Embedding(2, 5) # 2 \u5355\u8bcd, 5 \u7ef4\u5d4c\u5165 lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"]]) hello_embed = embeds(autograd.Variable(lookup_tensor)) print(hello_embed)","title":"Word Embeddings in Pytorch\uff08Pytorch\u4e2d\u7684\u5355\u8bcd\u5d4c\u5165\uff09"},{"location":"27/#n-gram","text":"\u56de\u60f3\u4e00\u4e0b \u5728n-gram\u8bed\u8a00\u6a21\u578b\u4e2d,\u7ed9\u5b9a\u4e00\u7cfb\u5217\u5355\u8bcd \\(w\\) , \u6211\u4eec\u9700\u8981\u8ba1\u7b97 \\[P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\\] \\(w_i\\) \u662f\u53e5\u5b50\u4e2d\u7b2ci\u4e2a\u5355\u8bcd. \u672c\u4f8b\u4e2d, \u6211\u4eec\u5c06\u8ba1\u7b97\u4e00\u4e9b\u8bad\u7ec3\u96c6\u7684\u635f\u5931\u51fd\u6570\u5e76\u4e14\u7528\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u7cfb\u6570. CONTEXT_SIZE = 2 EMBEDDING_DIM = 10 # \u6211\u4eec\u5c06\u4f7f\u7528 Shakespeare Sonnet 2 test_sentence = \"\"\"When forty winters shall besiege thy brow, And dig deep trenches in thy beauty's field, Thy youth's proud livery so gazed on now, Will be a totter'd weed of small worth held: Then being asked, where all thy beauty lies, Where all the treasure of thy lusty days; To say, within thine own deep sunken eyes, Were an all-eating shame, and thriftless praise. How much more praise deserv'd thy beauty's use, If thou couldst answer 'This fair child of mine Shall sum my count, and make my old excuse,' Proving his beauty by succession thine! This were to be new made when thou art old, And see thy blood warm when thou feel'st it cold.\"\"\".split() # \u6211\u4eec\u5e94\u8be5\u5bf9\u8f93\u5165\u8fdb\u884c\u6807\u8bb0,\u4f46\u662f\u6211\u4eec\u5c06\u5ffd\u7565\u5b83 # \u5efa\u9020\u4e00\u7cfb\u5217\u5143\u7ec4. \u6bcf\u4e2a\u5143\u7ec4 ([ word_i-2, word_i-1 ], \u90fd\u662f\u76ee\u6807\u5355\u8bcd) trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2]) for i in range(len(test_sentence) - 2)] # \u8f93\u51fa\u524d 3, \u4e3a\u4e86\u8ba9\u4f60\u770b\u5230\u4ed6\u7684\u5404\u5f0f print(trigrams[:3]) vocab = set(test_sentence) word_to_ix = {word: i for i, word in enumerate(vocab)} class NGramLanguageModeler(nn.Module): def __init__(self, vocab_size, embedding_dim, context_size): super(NGramLanguageModeler, self).__init__() self.embeddings = nn.Embedding(vocab_size, embedding_dim) self.linear1 = nn.Linear(context_size * embedding_dim, 128) self.linear2 = nn.Linear(128, vocab_size) def forward(self, inputs): embeds = self.embeddings(inputs).view((1, -1)) out = F.relu(self.linear1(embeds)) out = self.linear2(out) log_probs = F.log_softmax(out, dim=1) return log_probs losses = [] loss_function = nn.NLLLoss() model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE) optimizer = optim.SGD(model.parameters(), lr=0.001) for epoch in range(10): total_loss = torch.Tensor([0]) for context, target in trigrams: # \u6b65\u9aa4 1\\. \u51c6\u5907\u597d\u8fdb\u5165\u6a21\u578b\u7684\u6570\u636e (\u4f8b\u5982\u5c06\u5355\u8bcd\u8f6c\u6362\u6210\u6574\u6570\u7d22\u5f15,\u5e76\u5c06\u5176\u5c01\u88c5\u5728\u53d8\u91cf\u4e2d) context_idxs = [word_to_ix[w] for w in context] context_var = autograd.Variable(torch.LongTensor(context_idxs)) # \u6b65\u9aa4 2\\. \u56de\u8c03 *\u79ef\u7d2f* \u68af\u5ea6. \u5728\u8fdb\u5165\u4e00\u4e2a\u5b9e\u4f8b\u524d,\u9700\u8981\u5c06\u4e4b\u524d\u7684\u5b9e\u529b\u68af\u5ea6\u7f6e\u96f6 model.zero_grad() # \u6b65\u9aa4 3\\. \u8fd0\u884c\u53cd\u5411\u4f20\u64ad,\u5f97\u5230\u5355\u8bcd\u7684\u6982\u7387\u5206\u5e03 log_probs = model(context_var) # \u6b65\u9aa4 4\\. \u8ba1\u7b97\u635f\u5931\u51fd\u6570. (\u518d\u6b21\u6ce8\u610f, Torch\u9700\u8981\u5c06\u76ee\u6807\u5355\u8bcd\u5c01\u88c5\u5728\u53d8\u91cf\u4e2d) loss = loss_function(log_probs, autograd.Variable( torch.LongTensor([word_to_ix[target]]))) # \u6b65\u9aa4 5\\. \u53cd\u5411\u4f20\u64ad\u5e76\u66f4\u65b0\u68af\u5ea6 loss.backward() optimizer.step() total_loss += loss.data losses.append(total_loss) print(losses) # \u5728\u8bad\u7ec3\u96c6\u4e2d\u6bcf\u6b21\u8fed\u4ee3\u635f\u5931\u90fd\u4f1a\u51cf\u5c0f!","title":"\u4f8b\u5b50: N-Gram \u8bed\u8a00\u6a21\u578b"},{"location":"27/#exercise-computing-word-embeddings-continuous-bag-of-words","text":"\u8fde\u7eed\u5355\u8bcd\u5305\u6a21\u578b (CBOW) \u5728NLP\u6df1\u5ea6\u5b66\u4e60\u4e2d\u4f7f\u7528\u7684\u5f88\u9891\u7e41. \u8fd9\u4e2a\u6a21\u578b\u5c1d\u8bd5\u53bb\u9884\u6d4b\u6587\u4e2d\u76ee\u6807\u5355\u8bcd\u7684 \u524d\u540e\u4e00\u4e9b\u5355\u8bcd. \u5b83\u6709\u522b\u4e8e\u8bed\u8a00\u5efa\u6a21, \u56e0\u4e3aCBOW\u4e0d\u662f\u987a\u5e8f\u7684, \u4e5f\u4e0d\u9700\u8981\u662f\u6982\u7387\u6027\u7684.CBOW\u88ab\u7528\u6765\u5feb \u901f\u8bad\u7ec3\u5355\u8bcd\u5d4c\u5165,\u800c\u8fd9\u4e9b\u5d4c\u5165\u88ab\u7528\u6765\u521d\u59cb\u5316\u4e00\u4e9b\u590d\u6742\u6a21\u578b\u7684\u5d4c\u5165.\u901a\u5e38\u60c5\u51b5\u4e0b, \u8fd9\u88ab\u79f0\u4e3a \u9884\u8bad\u7ec3\u5d4c\u5165 . \u5b83\u51e0\u4e4e\u603b\u662f\u80fd\u5e2e\u52a9\u63d0\u5347\u767e\u5206\u4e4b\u51e0\u7684\u6027\u80fd. CBOW\u6a21\u578b\u5982\u4e0b\u6240\u793a.\u7ed9\u5b9a\u4e00\u4e2a\u76ee\u6807\u5355\u8bcd \\(w_i\\) \u548c \\(N\\) \u4ee3\u8868\u5355\u8bcd\u6bcf\u4e00\u904d\u7684\u6ed1\u7a97\u8ddd, \\(w_{i-1}, \\dots, w_{i-N}\\) \u548c \\(w_{i+1}, \\dots, w_{i+N}\\) , \u5c06\u6240\u6709\u4e0a\u4e0b\u6587\u8bcd\u7edf\u79f0\u4e3a \\(C\\) ,CBOW\u8bd5\u56fe\u53bb\u6700\u5c0f\u5316\u5982\u4e0b \\[-\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\] \u5176\u4e2d \\(q_w\\) \u662f\u5355\u8bcd \\(w\\) \u7684\u5d4c\u5165. \u5728Pytorch\u4e2d\u901a\u8fc7\u586b\u5145\u4e0b\u9762\u7684\u7c7b\u6765\u5b9e\u73b0\u8fd9\u4e2a\u6a21\u578b. \u4e00\u4e9b\u5efa\u8bae: \u60f3\u597d\u4f60\u9700\u8981\u5b9a\u4e49\u7684\u7cfb\u6570. \u786e\u4fdd\u4f60\u77e5\u9053\u6bcf\u4e00\u6b65\u64cd\u4f5c\u540e\u7684\u6784\u9020. \u5982\u679c\u60f3\u8981\u91cd\u6784\u8bf7\u4f7f\u7528 .view(). CONTEXT_SIZE = 2 # \u5de6\u53f3\u54042\u4e2a\u5355\u8bcd raw_text = \"\"\"We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\"\"\".split() # \u901a\u8fc7\u4ece `raw_text` \u5f97\u5230\u4e00\u7ec4\u5355\u8bcd, \u8fdb\u884c\u53bb\u91cd\u64cd\u4f5c vocab = set(raw_text) vocab_size = len(vocab) word_to_ix = {word: i for i, word in enumerate(vocab)} data = [] for i in range(2, len(raw_text) - 2): context = [raw_text[i - 2], raw_text[i - 1], raw_text[i + 1], raw_text[i + 2]] target = raw_text[i] data.append((context, target)) print(data[:5]) class CBOW(nn.Module): def __init__(self): pass def forward(self, inputs): pass # \u521b\u5efa\u6a21\u578b\u5e76\u4e14\u8bad\u7ec3. \u8fd9\u91cc\u6709\u4e00\u4e9b\u51fd\u6570\u53ef\u4ee5\u5728\u4f7f\u7528\u6a21\u578b\u4e4b\u524d\u5e2e\u52a9\u4f60\u51c6\u5907\u6570\u636e def make_context_vector(context, word_to_ix): idxs = [word_to_ix[w] for w in context] tensor = torch.LongTensor(idxs) return autograd.Variable(tensor) make_context_vector(data[0][0], word_to_ix) # \u4f8b\u5b50 Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: word_embeddings_tutorial.py Download Jupyter notebook: word_embeddings_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"Exercise: Computing Word Embeddings: Continuous Bag-of-Words(\u7ec3\u4e60: \u8ba1\u7b97\u5355\u8bcd\u5d4c\u5165: \u8fde\u7eed\u5355\u8bcd\u5305)"},{"location":"28/","text":"\u5e8f\u5217\u6a21\u578b\u548c LSTM \u7f51\u7edc\uff08\u957f\u77ed\u8bb0\u5fc6\u7f51\u7edc\uff09 \u4e4b\u524d\u6211\u4eec\u5df2\u7ecf\u5b66\u8fc7\u4e86\u8bb8\u591a\u7684\u524d\u9988\u7f51\u7edc. \u6240\u8c13\u524d\u9988\u7f51\u7edc, \u5c31\u662f\u7f51\u7edc\u4e2d\u4e0d\u4f1a\u4fdd\u5b58\u72b6\u6001. \u7136\u800c\u6709\u65f6 \u8fd9\u5e76\u4e0d\u662f\u6211\u4eec\u60f3\u8981\u7684\u6548\u679c. \u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP, Natural Language Processing) \u4e2d, \u5e8f\u5217\u6a21\u578b\u662f\u4e00\u4e2a\u6838\u5fc3\u7684\u6982\u5ff5. \u6240\u8c13\u5e8f\u5217\u6a21\u578b, \u5373\u8f93\u5165\u4f9d\u8d56\u4e8e\u65f6\u95f4\u4fe1\u606f\u7684\u6a21\u578b. \u4e00\u4e2a\u5178\u578b \u7684\u5e8f\u5217\u6a21\u578b\u662f\u9690\u9a6c\u5c14\u79d1\u592b\u6a21\u578b (HMM, Hidden Markov Model). \u53e6\u4e00\u4e2a\u5e8f\u5217\u6a21\u578b\u7684\u4f8b\u5b50 \u662f\u6761\u4ef6\u968f\u673a\u573a (CRF, Conditional Random Field). \u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u662f\u6307\u53ef\u4ee5\u4fdd\u5b58\u67d0\u79cd\u72b6\u6001\u7684\u795e\u7ecf\u7f51\u7edc. \u6bd4\u5982\u8bf4, \u7f51\u7edc\u4e0a\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u53ef\u4ee5\u4f5c\u4e3a\u4e0b\u4e2a \u65f6\u523b\u7684\u8f93\u5165, \u8fd9\u6837\u4fe1\u606f\u5c31\u53ef\u4ee5\u901a\u8fc7\u5e8f\u5217\u5728\u7f51\u7edc\u4e2d\u4e00\u76f4\u5f80\u540e\u4f20\u9012. \u5bf9\u4e8eLSTM (Long-Short Term Memory) \u6765\u8bf4, \u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u6709\u4e00\u4e2a\u76f8\u5e94\u7684\u9690\u72b6\u6001 \\(h_t\\) , \u8be5\u9690\u72b6\u6001 \u539f\u5219\u4e0a\u53ef\u4ee5\u5305\u542b\u5e8f\u5217\u5f53\u524d\u7ed3\u70b9\u4e4b\u524d\u7684\u4efb\u4e00\u8282\u70b9\u7684\u4fe1\u606f. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u9690\u85cf\u72b6\u6001\u6765\u9884\u6d4b\u8bed\u8a00\u6a21\u578b \u4e2d\u7684\u5355\u8bcd, \u8bcd\u6027\u6807\u7b7e\u4ee5\u53ca\u5176\u4ed6\u5404\u79cd\u5404\u6837\u7684\u4e1c\u897f. Pytorch \u4e2d\u7684 LSTM \u5f00\u59cb\u4f8b\u5b50\u4e4b\u524d,\u6709\u51e0\u4e2a\u70b9\u8bf4\u660e\u4e00\u4e0b. Pytorch \u4e2d, LSTM \u7684\u6240\u6709\u7684\u5f62\u5f0f\u56fa\u5b9a\u4e3a3D \u7684 tensor. \u6bcf\u4e2a\u7ef4\u5ea6\u6709\u56fa\u5b9a\u7684\u8bed\u4e49\u542b\u4e49, \u4e0d\u80fd\u4e71\u6389. \u5176\u4e2d\u7b2c\u4e00\u7ef4\u662f\u5e8f\u5217\u672c\u8eab, \u7b2c\u4e8c\u7ef4\u4ee5 mini-batch \u5f62\u5f0f \u6765\u7d22\u5f15\u5b9e\u4f8b, \u800c\u7b2c\u4e09\u7ef4\u5219\u7d22\u5f15\u8f93\u5165\u7684\u5143\u7d20. \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u8ba8\u8bba\u8fc7 mini-batch, \u6240\u4ee5\u5728\u8fd9\u91cc\u6211\u4eec \u5047\u8bbe\u7b2c\u4e8c\u7ef4\u7684\u7ef4\u5ea6\u603b\u662f1. \u5982\u679c\u6211\u4eec\u60f3\u5728\u53e5\u5b50 \u201cThe cow jumped\u201d \u4e0a\u8fd0\u884c\u4e00\u4e2a\u5e8f\u5217\u6a21\u578b, \u6a21\u578b \u7684\u8f93\u5165\u7c7b\u4f3c\u8fd9\u6837: \\[\\begin{split}\\begin{bmatrix} \\overbrace{q_\\text{The}}^\\text{row vector} \\\\ q_\\text{cow} \\\\ q_\\text{jumped} \\end{bmatrix}\\end{split}\\] \u9664\u4e86\u6709\u4e00\u4e2a\u989d\u5916\u7684\u5927\u5c0f\u4e3a1\u7684\u7b2c\u4e8c\u7ef4\u5ea6. \u6b64\u5916, \u4f60\u8fd8\u53ef\u4ee5\u5411\u7f51\u7edc\u9010\u4e2a\u8f93\u5165\u5e8f\u5217, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u7b2c\u4e00\u4e2a\u8f74\u7684\u5927\u5c0f\u4e5f\u662f1. \u6765\u770b\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50. # \u4f5c\u8005: Robert Guthrie import torch import torch.autograd as autograd import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) lstm = nn.LSTM(3, 3) # \u8f93\u5165\u7ef4\u5ea6\u662f3, \u8f93\u51fa\u7ef4\u5ea6\u4e5f\u662f3 inputs = [autograd.Variable(torch.randn((1, 3))) for _ in range(5)] # \u6784\u9020\u4e00\u4e2a\u957f\u5ea6\u4e3a5\u7684\u5e8f\u5217 # \u521d\u59cb\u5316\u9690\u85cf\u72b6\u6001 hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn((1, 1, 3)))) for i in inputs: # \u5c06\u5e8f\u5217\u7684\u5143\u7d20\u9010\u4e2a\u8f93\u5165\u5230LSTM # \u7ecf\u8fc7\u6bcf\u6b65\u64cd\u4f5c,hidden \u7684\u503c\u5305\u542b\u4e86\u9690\u85cf\u72b6\u6001\u7684\u4fe1\u606f out, hidden = lstm(i.view(1, 1, -1), hidden) # \u53e6\u5916, \u6211\u4eec\u8fd8\u53ef\u4ee5\u4e00\u6b21\u5bf9\u6574\u4e2a\u5e8f\u5217\u8fdb\u884c\u8bad\u7ec3. LSTM \u8fd4\u56de\u7684\u7b2c\u4e00\u4e2a\u503c\u8868\u793a\u6240\u6709\u65f6\u523b\u7684\u9690\u72b6\u6001\u503c, # \u7b2c\u4e8c\u4e2a\u503c\u8868\u793a\u6700\u8fd1\u7684\u9690\u72b6\u6001\u503c (\u56e0\u6b64\u4e0b\u9762\u7684 \"out\"\u7684\u6700\u540e\u4e00\u4e2a\u503c\u548c \"hidden\" \u7684\u503c\u662f\u4e00\u6837\u7684). # \u4e4b\u6240\u4ee5\u8fd9\u6837\u8bbe\u8ba1, \u662f\u4e3a\u4e86\u901a\u8fc7 \"out\" \u7684\u503c\u6765\u83b7\u53d6\u6240\u6709\u7684\u9690\u72b6\u6001\u503c, \u800c\u7528 \"hidden\" \u7684\u503c\u6765 # \u8fdb\u884c\u5e8f\u5217\u7684\u53cd\u5411\u4f20\u64ad\u8fd0\u7b97, \u5177\u4f53\u65b9\u5f0f\u5c31\u662f\u5c06\u5b83\u4f5c\u4e3a\u53c2\u6570\u4f20\u5165\u540e\u9762\u7684 LSTM \u7f51\u7edc. # \u589e\u52a0\u989d\u5916\u7684\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6 inputs = torch.cat(inputs).view(len(inputs), 1, -1) hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable( torch.randn((1, 1, 3)))) # \u6e05\u7a7a\u8f93\u51fa\u9690\u72b6\u6001 out, hidden = lstm(inputs, hidden) print(out) print(hidden) \u4f8b\u5b50: \u7528 LSTM \u6765\u8fdb\u884c\u8bcd\u6027\u6807\u6ce8 \u5728\u8fd9\u90e8\u5206, \u6211\u4eec\u5c06\u4f1a\u4f7f\u7528\u4e00\u4e2a LSTM \u7f51\u7edc\u6765\u8fdb\u884c\u8bcd\u6027\u6807\u6ce8. \u5728\u8fd9\u91cc\u6211\u4eec\u4e0d\u4f1a\u7528\u5230\u7ef4\u7279\u6bd4\u7b97\u6cd5, \u524d\u5411\u540e\u5411\u7b97\u6cd5\u6216\u8005\u4efb\u4f55\u7c7b\u4f3c\u7684\u7b97\u6cd5, \u800c\u662f\u5c06\u8fd9\u90e8\u5206\u5185\u5bb9\u4f5c\u4e3a\u4e00\u4e2a (\u6709\u6311\u6218) \u7684\u7ec3\u4e60\u7559\u7ed9\u8bfb\u8005, \u5e0c\u671b\u8bfb\u8005\u5728\u4e86\u89e3\u4e86\u8fd9\u90e8\u5206\u7684\u5185\u5bb9\u540e\u80fd\u591f\u5b9e\u73b0\u5982\u4f55\u5c06\u7ef4\u7279\u6bd4\u7b97\u6cd5\u5e94\u7528\u5230 LSTM \u7f51\u7edc\u4e2d\u6765. \u6574\u4e2a\u6a21\u578b\u7684\u53c2\u6570\u5b9a\u4e49\u5982\u4e0b: \u8f93\u5165\u7684\u53e5\u5b50\u5b9a\u4e49\u4e3a \\(w_1, \\dots, w_M\\) , \u5176\u4e2d\u52a8\u8bcd\u5b9a\u4e49 \u4e3a \\(w_1, \\dots, w_M\\) , \u6807\u7b7e\u96c6\u5408\u5b9a\u4e49\u4e3a \\(T\\) , \u5355\u8bcd \\(w_i\\) \u7684\u5b9e\u9645 \u6807\u7b7e\u4e3a \\(y_i\\) . \u5b9a\u4e49\u5355\u8bcd \\(w_i\\) \u7684\u9884\u6d4b\u6807\u7b7e\u4e3a \\(\\hat{y}_i\\) . \u8fd9\u662f\u4e00\u4e2a\u7ed3\u6784\u9884\u6d4b\u6a21\u578b, \u6211\u4eec\u7684\u8f93\u51fa\u662f\u4e00\u4e2a\u5e8f\u5217 \\(\\hat{y}_1, \\dots, \\hat{y}_M\\) , \u5176\u4e2d \\(\\hat{y}_i \\in T\\) . \u5728\u8fdb\u884c\u9884\u6d4b\u65f6, \u9700\u5c06\u53e5\u5b50\u6bcf\u4e2a\u8bcd\u8f93\u5165\u5230\u4e00\u4e2a LSTM \u7f51\u7edc\u4e2d. \u5c06\u65f6\u523b \\(i\\) \u7684\u9690\u72b6\u6001\u6807\u8bb0 \u4e3a \\(h_i\\) . \u540c\u6837\u5730, \u5bf9\u6bcf\u4e2a\u6807\u7b7e\u8d4b\u4e00\u4e2a\u72ec\u4e00\u65e0\u4e8c\u7684\u7d22\u5f15 (\u7c7b\u4f3c word embeddings \u90e8\u5206 word_to_ix \u7684\u8bbe\u7f6e). \u7136\u540e\u5c31\u5f97\u5230\u4e86 \\(\\hat{y}_i\\) \u7684\u9884\u6d4b\u89c4\u5219: \\[\\hat{y}_i = \\text{argmax}_j \\ (\\log \\text{Softmax}(Ah_i + b))_j\\] \u5373\u5148\u5bf9\u9690\u72b6\u6001\u8fdb\u884c\u4e00\u4e2a\u4eff\u5c04\u53d8\u6362, \u7136\u540e\u8ba1\u7b97\u4e00\u4e2a\u5bf9\u6570 softmax, \u6700\u540e\u5f97\u5230\u7684\u9884\u6d4b\u6807\u7b7e\u5373\u4e3a\u5bf9\u6570 softmax \u4e2d\u6700\u5927\u7684\u503c\u5bf9\u5e94\u7684\u6807\u7b7e. \u6ce8\u610f, \u8fd9\u4e5f\u610f\u5473\u7740 \\(A\\) \u7a7a\u95f4\u7684\u7ef4\u5ea6\u662f \\(|T|\\) . \u51c6\u5907\u6570\u636e: def prepare_sequence(seq, to_ix): idxs = [to_ix[w] for w in seq] tensor = torch.LongTensor(idxs) return autograd.Variable(tensor) training_data = [ (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]), (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]) ] word_to_ix = {} for sent, tags in training_data: for word in sent: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix) print(word_to_ix) tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2} # \u5b9e\u9645\u4e2d\u901a\u5e38\u4f7f\u7528\u66f4\u5927\u7684\u7ef4\u5ea6\u598232\u7ef4, 64\u7ef4. # \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u5c0f\u7684\u7ef4\u5ea6, \u4e3a\u4e86\u65b9\u4fbf\u67e5\u770b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6743\u91cd\u7684\u53d8\u5316. EMBEDDING_DIM = 6 HIDDEN_DIM = 6 \u6784\u9020\u6a21\u578b: class LSTMTagger(nn.Module): def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size): super(LSTMTagger, self).__init__() self.hidden_dim = hidden_dim self.word_embeddings = nn.Embedding(vocab_size, embedding_dim) # LSTM \u4ee5 word_embeddings \u4f5c\u4e3a\u8f93\u5165, \u8f93\u51fa\u7ef4\u5ea6\u4e3a hidden_dim \u7684\u9690\u72b6\u6001\u503c self.lstm = nn.LSTM(embedding_dim, hidden_dim) # \u7ebf\u6027\u5c42\u5c06\u9690\u72b6\u6001\u7a7a\u95f4\u6620\u5c04\u5230\u6807\u6ce8\u7a7a\u95f4 self.hidden2tag = nn.Linear(hidden_dim, tagset_size) self.hidden = self.init_hidden() def init_hidden(self): # \u5f00\u59cb\u65f6\u523b, \u6ca1\u6709\u9690\u72b6\u6001 # \u5173\u4e8e\u7ef4\u5ea6\u8bbe\u7f6e\u7684\u8be6\u60c5,\u8bf7\u53c2\u8003 Pytorch \u6587\u6863 # \u5404\u4e2a\u7ef4\u5ea6\u7684\u542b\u4e49\u662f (num_layers, minibatch_size, hidden_dim) return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)), autograd.Variable(torch.zeros(1, 1, self.hidden_dim))) def forward(self, sentence): embeds = self.word_embeddings(sentence) lstm_out, self.hidden = self.lstm( embeds.view(len(sentence), 1, -1), self.hidden) tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1)) tag_scores = F.log_softmax(tag_space, dim=1) return tag_scores \u8bad\u7ec3\u6a21\u578b: model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix)) loss_function = nn.NLLLoss() optimizer = optim.SGD(model.parameters(), lr=0.1) # \u67e5\u770b\u4e0b\u8bad\u7ec3\u524d\u5f97\u5206\u7684\u503c # \u6ce8\u610f: \u8f93\u51fa\u7684 i,j \u5143\u7d20\u7684\u503c\u8868\u793a\u5355\u8bcd i \u7684 j \u6807\u7b7e\u7684\u5f97\u5206 inputs = prepare_sequence(training_data[0][0], word_to_ix) tag_scores = model(inputs) print(tag_scores) for epoch in range(300): # \u518d\u6b21\u8bf4\u660e\u4e0b, \u5b9e\u9645\u60c5\u51b5\u4e0b\u4f60\u4e0d\u4f1a\u8bad\u7ec3300\u4e2a\u5468\u671f, \u6b64\u4f8b\u4e2d\u6211\u4eec\u53ea\u662f\u6784\u9020\u4e86\u4e00\u4e9b\u5047\u6570\u636e for sentence, tags in training_data: # Step 1\\. \u8bf7\u8bb0\u4f4f Pytorch \u4f1a\u7d2f\u52a0\u68af\u5ea6 # \u6bcf\u6b21\u8bad\u7ec3\u524d\u9700\u8981\u6e05\u7a7a\u68af\u5ea6\u503c model.zero_grad() # \u6b64\u5916\u8fd8\u9700\u8981\u6e05\u7a7a LSTM \u7684\u9690\u72b6\u6001 # \u5c06\u5176\u4ece\u4e0a\u4e2a\u5b9e\u4f8b\u7684\u5386\u53f2\u4e2d\u5206\u79bb\u51fa\u6765 model.hidden = model.init_hidden() # Step 2\\. \u51c6\u5907\u7f51\u7edc\u8f93\u5165, \u5c06\u5176\u53d8\u4e3a\u8bcd\u7d22\u5f15\u7684 Variables \u7c7b\u578b\u6570\u636e sentence_in = prepare_sequence(sentence, word_to_ix) targets = prepare_sequence(tags, tag_to_ix) # Step 3\\. \u524d\u5411\u4f20\u64ad tag_scores = model(sentence_in) # Step 4\\. \u8ba1\u7b97\u635f\u5931\u548c\u68af\u5ea6\u503c, \u901a\u8fc7\u8c03\u7528 optimizer.step() \u6765\u66f4\u65b0\u68af\u5ea6 loss = loss_function(tag_scores, targets) loss.backward() optimizer.step() # \u67e5\u770b\u8bad\u7ec3\u540e\u5f97\u5206\u7684\u503c inputs = prepare_sequence(training_data[0][0], word_to_ix) tag_scores = model(inputs) # \u53e5\u5b50\u662f \"the dog ate the apple\", i,j \u8868\u793a\u5bf9\u4e8e\u5355\u8bcd i, \u6807\u7b7e j \u7684\u5f97\u5206. # \u6211\u4eec\u91c7\u7528\u5f97\u5206\u6700\u9ad8\u7684\u6807\u7b7e\u4f5c\u4e3a\u9884\u6d4b\u7684\u6807\u7b7e. \u4ece\u4e0b\u9762\u7684\u8f93\u51fa\u6211\u4eec\u53ef\u4ee5\u770b\u5230, \u9884\u6d4b\u5f97 # \u5230\u7684\u7ed3\u679c\u662f0 1 2 0 1\\. \u56e0\u4e3a \u7d22\u5f15\u662f\u4ece0\u5f00\u59cb\u7684, \u56e0\u6b64\u7b2c\u4e00\u4e2a\u503c0\u8868\u793a\u7b2c\u4e00\u884c\u7684 # \u6700\u5927\u503c, \u7b2c\u4e8c\u4e2a\u503c1\u8868\u793a\u7b2c\u4e8c\u884c\u7684\u6700\u5927\u503c, \u4ee5\u6b64\u7c7b\u63a8. \u6240\u4ee5\u6700\u540e\u7684\u7ed3\u679c\u662f DET # NOUN VERB DET NOUN, \u6574\u4e2a\u5e8f\u5217\u90fd\u662f\u6b63\u786e\u7684! print(tag_scores) \u7ec3\u4e60: \u4f7f\u7528\u5b57\u7b26\u7ea7\u7279\u5f81\u6765\u589e\u5f3a LSTM \u8bcd\u6027\u6807\u6ce8\u5668 \u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d, \u6bcf\u4e2a\u8bcd\u90fd\u6709\u4e00\u4e2a\u8bcd\u5d4c\u5165, \u4f5c\u4e3a\u5e8f\u5217\u6a21\u578b\u7684\u8f93\u5165. \u63a5\u4e0b\u6765\u8ba9\u6211\u4eec\u4f7f\u7528\u6bcf\u4e2a\u7684\u5355\u8bcd\u7684 \u5b57\u7b26\u7ea7\u522b\u7684\u8868\u8fbe\u6765\u589e\u5f3a\u8bcd\u5d4c\u5165. \u6211\u4eec\u671f\u671b\u8fd9\u4e2a\u64cd\u4f5c\u5bf9\u7ed3\u679c\u80fd\u6709\u663e\u8457\u63d0\u5347, \u56e0\u4e3a\u50cf\u8bcd\u7f00\u8fd9\u6837\u7684\u5b57\u7b26\u7ea7 \u4fe1\u606f\u5bf9\u4e8e\u8bcd\u6027\u6709\u5f88\u5927\u7684\u5f71\u54cd. \u6bd4\u5982\u8bf4, \u50cf\u5305\u542b\u8bcd\u7f00 -ly \u7684\u5355\u8bcd\u57fa\u672c\u4e0a\u90fd\u662f\u88ab\u6807\u6ce8\u4e3a\u526f\u8bcd. \u5177\u4f53\u64cd\u4f5c\u5982\u4e0b. \u7528 \\(c_w\\) \u6765\u8868\u793a\u5355\u8bcd \\(w\\) \u7684\u5b57\u7b26\u7ea7\u8868\u8fbe, \u540c\u4e4b\u524d\u4e00\u6837, \u6211\u4eec\u4f7f \u7528 \\(x_w\\) \u6765\u8868\u793a\u8bcd\u5d4c\u5165. \u5e8f\u5217\u6a21\u578b\u7684\u8f93\u5165\u5c31\u53d8\u6210\u4e86 \\(x_w\\) \u548c \\(c_w\\) \u7684\u62fc\u63a5. \u56e0\u6b64, \u5982\u679c \\(x_w\\) \u7684\u7ef4\u5ea6\u662f5, \\(c_w\\) \u7684\u7ef4\u5ea6\u662f3, \u90a3\u4e48\u6211\u4eec\u7684 LSTM \u7f51\u7edc\u7684\u8f93\u5165\u7ef4\u5ea6\u5927\u5c0f\u5c31\u662f8. \u4e3a\u4e86\u5f97\u5230\u5b57\u7b26\u7ea7\u522b\u7684\u8868\u8fbe, \u5c06\u5355\u8bcd\u7684\u6bcf\u4e2a\u5b57\u7b26\u8f93\u5165\u4e00\u4e2a LSTM \u7f51\u7edc, \u800c \\(c_w\\) \u5219\u4e3a\u8fd9\u4e2a LSTM \u7f51\u7edc\u6700\u540e\u7684\u9690\u72b6\u6001. \u4e00\u4e9b\u63d0\u793a: \u65b0\u6a21\u578b\u4e2d\u9700\u8981\u4e24\u4e2a LSTM, \u4e00\u4e2a\u8ddf\u4e4b\u524d\u4e00\u6837, \u7528\u6765\u8f93\u51fa\u8bcd\u6027\u6807\u6ce8\u7684\u5f97\u5206, \u53e6\u5916\u4e00\u4e2a\u65b0\u589e\u52a0\u7684\u7528\u6765 \u83b7\u53d6\u6bcf\u4e2a\u5355\u8bcd\u7684\u5b57\u7b26\u7ea7\u522b\u8868\u8fbe. \u4e3a\u4e86\u5728\u5b57\u7b26\u7ea7\u522b\u4e0a\u8fd0\u884c\u5e8f\u5217\u6a21\u578b, \u4f60\u9700\u8981\u7528\u5d4c\u5165\u7684\u5b57\u7b26\u6765\u4f5c\u4e3a\u5b57\u7b26 LSTM \u7684\u8f93\u5165. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: sequence_models_tutorial.py Download Jupyter notebook: sequence_models_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u5e8f\u5217\u6a21\u578b\u548c LSTM \u7f51\u7edc\uff08\u957f\u77ed\u8bb0\u5fc6\u7f51\u7edc\uff09"},{"location":"28/#lstm","text":"\u4e4b\u524d\u6211\u4eec\u5df2\u7ecf\u5b66\u8fc7\u4e86\u8bb8\u591a\u7684\u524d\u9988\u7f51\u7edc. \u6240\u8c13\u524d\u9988\u7f51\u7edc, \u5c31\u662f\u7f51\u7edc\u4e2d\u4e0d\u4f1a\u4fdd\u5b58\u72b6\u6001. \u7136\u800c\u6709\u65f6 \u8fd9\u5e76\u4e0d\u662f\u6211\u4eec\u60f3\u8981\u7684\u6548\u679c. \u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP, Natural Language Processing) \u4e2d, \u5e8f\u5217\u6a21\u578b\u662f\u4e00\u4e2a\u6838\u5fc3\u7684\u6982\u5ff5. \u6240\u8c13\u5e8f\u5217\u6a21\u578b, \u5373\u8f93\u5165\u4f9d\u8d56\u4e8e\u65f6\u95f4\u4fe1\u606f\u7684\u6a21\u578b. \u4e00\u4e2a\u5178\u578b \u7684\u5e8f\u5217\u6a21\u578b\u662f\u9690\u9a6c\u5c14\u79d1\u592b\u6a21\u578b (HMM, Hidden Markov Model). \u53e6\u4e00\u4e2a\u5e8f\u5217\u6a21\u578b\u7684\u4f8b\u5b50 \u662f\u6761\u4ef6\u968f\u673a\u573a (CRF, Conditional Random Field). \u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u662f\u6307\u53ef\u4ee5\u4fdd\u5b58\u67d0\u79cd\u72b6\u6001\u7684\u795e\u7ecf\u7f51\u7edc. \u6bd4\u5982\u8bf4, \u7f51\u7edc\u4e0a\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u53ef\u4ee5\u4f5c\u4e3a\u4e0b\u4e2a \u65f6\u523b\u7684\u8f93\u5165, \u8fd9\u6837\u4fe1\u606f\u5c31\u53ef\u4ee5\u901a\u8fc7\u5e8f\u5217\u5728\u7f51\u7edc\u4e2d\u4e00\u76f4\u5f80\u540e\u4f20\u9012. \u5bf9\u4e8eLSTM (Long-Short Term Memory) \u6765\u8bf4, \u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u6709\u4e00\u4e2a\u76f8\u5e94\u7684\u9690\u72b6\u6001 \\(h_t\\) , \u8be5\u9690\u72b6\u6001 \u539f\u5219\u4e0a\u53ef\u4ee5\u5305\u542b\u5e8f\u5217\u5f53\u524d\u7ed3\u70b9\u4e4b\u524d\u7684\u4efb\u4e00\u8282\u70b9\u7684\u4fe1\u606f. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u9690\u85cf\u72b6\u6001\u6765\u9884\u6d4b\u8bed\u8a00\u6a21\u578b \u4e2d\u7684\u5355\u8bcd, \u8bcd\u6027\u6807\u7b7e\u4ee5\u53ca\u5176\u4ed6\u5404\u79cd\u5404\u6837\u7684\u4e1c\u897f.","title":"\u5e8f\u5217\u6a21\u578b\u548c LSTM \u7f51\u7edc\uff08\u957f\u77ed\u8bb0\u5fc6\u7f51\u7edc\uff09"},{"location":"28/#pytorch-lstm","text":"\u5f00\u59cb\u4f8b\u5b50\u4e4b\u524d,\u6709\u51e0\u4e2a\u70b9\u8bf4\u660e\u4e00\u4e0b. Pytorch \u4e2d, LSTM \u7684\u6240\u6709\u7684\u5f62\u5f0f\u56fa\u5b9a\u4e3a3D \u7684 tensor. \u6bcf\u4e2a\u7ef4\u5ea6\u6709\u56fa\u5b9a\u7684\u8bed\u4e49\u542b\u4e49, \u4e0d\u80fd\u4e71\u6389. \u5176\u4e2d\u7b2c\u4e00\u7ef4\u662f\u5e8f\u5217\u672c\u8eab, \u7b2c\u4e8c\u7ef4\u4ee5 mini-batch \u5f62\u5f0f \u6765\u7d22\u5f15\u5b9e\u4f8b, \u800c\u7b2c\u4e09\u7ef4\u5219\u7d22\u5f15\u8f93\u5165\u7684\u5143\u7d20. \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u8ba8\u8bba\u8fc7 mini-batch, \u6240\u4ee5\u5728\u8fd9\u91cc\u6211\u4eec \u5047\u8bbe\u7b2c\u4e8c\u7ef4\u7684\u7ef4\u5ea6\u603b\u662f1. \u5982\u679c\u6211\u4eec\u60f3\u5728\u53e5\u5b50 \u201cThe cow jumped\u201d \u4e0a\u8fd0\u884c\u4e00\u4e2a\u5e8f\u5217\u6a21\u578b, \u6a21\u578b \u7684\u8f93\u5165\u7c7b\u4f3c\u8fd9\u6837: \\[\\begin{split}\\begin{bmatrix} \\overbrace{q_\\text{The}}^\\text{row vector} \\\\ q_\\text{cow} \\\\ q_\\text{jumped} \\end{bmatrix}\\end{split}\\] \u9664\u4e86\u6709\u4e00\u4e2a\u989d\u5916\u7684\u5927\u5c0f\u4e3a1\u7684\u7b2c\u4e8c\u7ef4\u5ea6. \u6b64\u5916, \u4f60\u8fd8\u53ef\u4ee5\u5411\u7f51\u7edc\u9010\u4e2a\u8f93\u5165\u5e8f\u5217, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u7b2c\u4e00\u4e2a\u8f74\u7684\u5927\u5c0f\u4e5f\u662f1. \u6765\u770b\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50. # \u4f5c\u8005: Robert Guthrie import torch import torch.autograd as autograd import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) lstm = nn.LSTM(3, 3) # \u8f93\u5165\u7ef4\u5ea6\u662f3, \u8f93\u51fa\u7ef4\u5ea6\u4e5f\u662f3 inputs = [autograd.Variable(torch.randn((1, 3))) for _ in range(5)] # \u6784\u9020\u4e00\u4e2a\u957f\u5ea6\u4e3a5\u7684\u5e8f\u5217 # \u521d\u59cb\u5316\u9690\u85cf\u72b6\u6001 hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn((1, 1, 3)))) for i in inputs: # \u5c06\u5e8f\u5217\u7684\u5143\u7d20\u9010\u4e2a\u8f93\u5165\u5230LSTM # \u7ecf\u8fc7\u6bcf\u6b65\u64cd\u4f5c,hidden \u7684\u503c\u5305\u542b\u4e86\u9690\u85cf\u72b6\u6001\u7684\u4fe1\u606f out, hidden = lstm(i.view(1, 1, -1), hidden) # \u53e6\u5916, \u6211\u4eec\u8fd8\u53ef\u4ee5\u4e00\u6b21\u5bf9\u6574\u4e2a\u5e8f\u5217\u8fdb\u884c\u8bad\u7ec3. LSTM \u8fd4\u56de\u7684\u7b2c\u4e00\u4e2a\u503c\u8868\u793a\u6240\u6709\u65f6\u523b\u7684\u9690\u72b6\u6001\u503c, # \u7b2c\u4e8c\u4e2a\u503c\u8868\u793a\u6700\u8fd1\u7684\u9690\u72b6\u6001\u503c (\u56e0\u6b64\u4e0b\u9762\u7684 \"out\"\u7684\u6700\u540e\u4e00\u4e2a\u503c\u548c \"hidden\" \u7684\u503c\u662f\u4e00\u6837\u7684). # \u4e4b\u6240\u4ee5\u8fd9\u6837\u8bbe\u8ba1, \u662f\u4e3a\u4e86\u901a\u8fc7 \"out\" \u7684\u503c\u6765\u83b7\u53d6\u6240\u6709\u7684\u9690\u72b6\u6001\u503c, \u800c\u7528 \"hidden\" \u7684\u503c\u6765 # \u8fdb\u884c\u5e8f\u5217\u7684\u53cd\u5411\u4f20\u64ad\u8fd0\u7b97, \u5177\u4f53\u65b9\u5f0f\u5c31\u662f\u5c06\u5b83\u4f5c\u4e3a\u53c2\u6570\u4f20\u5165\u540e\u9762\u7684 LSTM \u7f51\u7edc. # \u589e\u52a0\u989d\u5916\u7684\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6 inputs = torch.cat(inputs).view(len(inputs), 1, -1) hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable( torch.randn((1, 1, 3)))) # \u6e05\u7a7a\u8f93\u51fa\u9690\u72b6\u6001 out, hidden = lstm(inputs, hidden) print(out) print(hidden)","title":"Pytorch \u4e2d\u7684 LSTM"},{"location":"28/#lstm_1","text":"\u5728\u8fd9\u90e8\u5206, \u6211\u4eec\u5c06\u4f1a\u4f7f\u7528\u4e00\u4e2a LSTM \u7f51\u7edc\u6765\u8fdb\u884c\u8bcd\u6027\u6807\u6ce8. \u5728\u8fd9\u91cc\u6211\u4eec\u4e0d\u4f1a\u7528\u5230\u7ef4\u7279\u6bd4\u7b97\u6cd5, \u524d\u5411\u540e\u5411\u7b97\u6cd5\u6216\u8005\u4efb\u4f55\u7c7b\u4f3c\u7684\u7b97\u6cd5, \u800c\u662f\u5c06\u8fd9\u90e8\u5206\u5185\u5bb9\u4f5c\u4e3a\u4e00\u4e2a (\u6709\u6311\u6218) \u7684\u7ec3\u4e60\u7559\u7ed9\u8bfb\u8005, \u5e0c\u671b\u8bfb\u8005\u5728\u4e86\u89e3\u4e86\u8fd9\u90e8\u5206\u7684\u5185\u5bb9\u540e\u80fd\u591f\u5b9e\u73b0\u5982\u4f55\u5c06\u7ef4\u7279\u6bd4\u7b97\u6cd5\u5e94\u7528\u5230 LSTM \u7f51\u7edc\u4e2d\u6765. \u6574\u4e2a\u6a21\u578b\u7684\u53c2\u6570\u5b9a\u4e49\u5982\u4e0b: \u8f93\u5165\u7684\u53e5\u5b50\u5b9a\u4e49\u4e3a \\(w_1, \\dots, w_M\\) , \u5176\u4e2d\u52a8\u8bcd\u5b9a\u4e49 \u4e3a \\(w_1, \\dots, w_M\\) , \u6807\u7b7e\u96c6\u5408\u5b9a\u4e49\u4e3a \\(T\\) , \u5355\u8bcd \\(w_i\\) \u7684\u5b9e\u9645 \u6807\u7b7e\u4e3a \\(y_i\\) . \u5b9a\u4e49\u5355\u8bcd \\(w_i\\) \u7684\u9884\u6d4b\u6807\u7b7e\u4e3a \\(\\hat{y}_i\\) . \u8fd9\u662f\u4e00\u4e2a\u7ed3\u6784\u9884\u6d4b\u6a21\u578b, \u6211\u4eec\u7684\u8f93\u51fa\u662f\u4e00\u4e2a\u5e8f\u5217 \\(\\hat{y}_1, \\dots, \\hat{y}_M\\) , \u5176\u4e2d \\(\\hat{y}_i \\in T\\) . \u5728\u8fdb\u884c\u9884\u6d4b\u65f6, \u9700\u5c06\u53e5\u5b50\u6bcf\u4e2a\u8bcd\u8f93\u5165\u5230\u4e00\u4e2a LSTM \u7f51\u7edc\u4e2d. \u5c06\u65f6\u523b \\(i\\) \u7684\u9690\u72b6\u6001\u6807\u8bb0 \u4e3a \\(h_i\\) . \u540c\u6837\u5730, \u5bf9\u6bcf\u4e2a\u6807\u7b7e\u8d4b\u4e00\u4e2a\u72ec\u4e00\u65e0\u4e8c\u7684\u7d22\u5f15 (\u7c7b\u4f3c word embeddings \u90e8\u5206 word_to_ix \u7684\u8bbe\u7f6e). \u7136\u540e\u5c31\u5f97\u5230\u4e86 \\(\\hat{y}_i\\) \u7684\u9884\u6d4b\u89c4\u5219: \\[\\hat{y}_i = \\text{argmax}_j \\ (\\log \\text{Softmax}(Ah_i + b))_j\\] \u5373\u5148\u5bf9\u9690\u72b6\u6001\u8fdb\u884c\u4e00\u4e2a\u4eff\u5c04\u53d8\u6362, \u7136\u540e\u8ba1\u7b97\u4e00\u4e2a\u5bf9\u6570 softmax, \u6700\u540e\u5f97\u5230\u7684\u9884\u6d4b\u6807\u7b7e\u5373\u4e3a\u5bf9\u6570 softmax \u4e2d\u6700\u5927\u7684\u503c\u5bf9\u5e94\u7684\u6807\u7b7e. \u6ce8\u610f, \u8fd9\u4e5f\u610f\u5473\u7740 \\(A\\) \u7a7a\u95f4\u7684\u7ef4\u5ea6\u662f \\(|T|\\) . \u51c6\u5907\u6570\u636e: def prepare_sequence(seq, to_ix): idxs = [to_ix[w] for w in seq] tensor = torch.LongTensor(idxs) return autograd.Variable(tensor) training_data = [ (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]), (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]) ] word_to_ix = {} for sent, tags in training_data: for word in sent: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix) print(word_to_ix) tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2} # \u5b9e\u9645\u4e2d\u901a\u5e38\u4f7f\u7528\u66f4\u5927\u7684\u7ef4\u5ea6\u598232\u7ef4, 64\u7ef4. # \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u5c0f\u7684\u7ef4\u5ea6, \u4e3a\u4e86\u65b9\u4fbf\u67e5\u770b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6743\u91cd\u7684\u53d8\u5316. EMBEDDING_DIM = 6 HIDDEN_DIM = 6 \u6784\u9020\u6a21\u578b: class LSTMTagger(nn.Module): def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size): super(LSTMTagger, self).__init__() self.hidden_dim = hidden_dim self.word_embeddings = nn.Embedding(vocab_size, embedding_dim) # LSTM \u4ee5 word_embeddings \u4f5c\u4e3a\u8f93\u5165, \u8f93\u51fa\u7ef4\u5ea6\u4e3a hidden_dim \u7684\u9690\u72b6\u6001\u503c self.lstm = nn.LSTM(embedding_dim, hidden_dim) # \u7ebf\u6027\u5c42\u5c06\u9690\u72b6\u6001\u7a7a\u95f4\u6620\u5c04\u5230\u6807\u6ce8\u7a7a\u95f4 self.hidden2tag = nn.Linear(hidden_dim, tagset_size) self.hidden = self.init_hidden() def init_hidden(self): # \u5f00\u59cb\u65f6\u523b, \u6ca1\u6709\u9690\u72b6\u6001 # \u5173\u4e8e\u7ef4\u5ea6\u8bbe\u7f6e\u7684\u8be6\u60c5,\u8bf7\u53c2\u8003 Pytorch \u6587\u6863 # \u5404\u4e2a\u7ef4\u5ea6\u7684\u542b\u4e49\u662f (num_layers, minibatch_size, hidden_dim) return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)), autograd.Variable(torch.zeros(1, 1, self.hidden_dim))) def forward(self, sentence): embeds = self.word_embeddings(sentence) lstm_out, self.hidden = self.lstm( embeds.view(len(sentence), 1, -1), self.hidden) tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1)) tag_scores = F.log_softmax(tag_space, dim=1) return tag_scores \u8bad\u7ec3\u6a21\u578b: model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix)) loss_function = nn.NLLLoss() optimizer = optim.SGD(model.parameters(), lr=0.1) # \u67e5\u770b\u4e0b\u8bad\u7ec3\u524d\u5f97\u5206\u7684\u503c # \u6ce8\u610f: \u8f93\u51fa\u7684 i,j \u5143\u7d20\u7684\u503c\u8868\u793a\u5355\u8bcd i \u7684 j \u6807\u7b7e\u7684\u5f97\u5206 inputs = prepare_sequence(training_data[0][0], word_to_ix) tag_scores = model(inputs) print(tag_scores) for epoch in range(300): # \u518d\u6b21\u8bf4\u660e\u4e0b, \u5b9e\u9645\u60c5\u51b5\u4e0b\u4f60\u4e0d\u4f1a\u8bad\u7ec3300\u4e2a\u5468\u671f, \u6b64\u4f8b\u4e2d\u6211\u4eec\u53ea\u662f\u6784\u9020\u4e86\u4e00\u4e9b\u5047\u6570\u636e for sentence, tags in training_data: # Step 1\\. \u8bf7\u8bb0\u4f4f Pytorch \u4f1a\u7d2f\u52a0\u68af\u5ea6 # \u6bcf\u6b21\u8bad\u7ec3\u524d\u9700\u8981\u6e05\u7a7a\u68af\u5ea6\u503c model.zero_grad() # \u6b64\u5916\u8fd8\u9700\u8981\u6e05\u7a7a LSTM \u7684\u9690\u72b6\u6001 # \u5c06\u5176\u4ece\u4e0a\u4e2a\u5b9e\u4f8b\u7684\u5386\u53f2\u4e2d\u5206\u79bb\u51fa\u6765 model.hidden = model.init_hidden() # Step 2\\. \u51c6\u5907\u7f51\u7edc\u8f93\u5165, \u5c06\u5176\u53d8\u4e3a\u8bcd\u7d22\u5f15\u7684 Variables \u7c7b\u578b\u6570\u636e sentence_in = prepare_sequence(sentence, word_to_ix) targets = prepare_sequence(tags, tag_to_ix) # Step 3\\. \u524d\u5411\u4f20\u64ad tag_scores = model(sentence_in) # Step 4\\. \u8ba1\u7b97\u635f\u5931\u548c\u68af\u5ea6\u503c, \u901a\u8fc7\u8c03\u7528 optimizer.step() \u6765\u66f4\u65b0\u68af\u5ea6 loss = loss_function(tag_scores, targets) loss.backward() optimizer.step() # \u67e5\u770b\u8bad\u7ec3\u540e\u5f97\u5206\u7684\u503c inputs = prepare_sequence(training_data[0][0], word_to_ix) tag_scores = model(inputs) # \u53e5\u5b50\u662f \"the dog ate the apple\", i,j \u8868\u793a\u5bf9\u4e8e\u5355\u8bcd i, \u6807\u7b7e j \u7684\u5f97\u5206. # \u6211\u4eec\u91c7\u7528\u5f97\u5206\u6700\u9ad8\u7684\u6807\u7b7e\u4f5c\u4e3a\u9884\u6d4b\u7684\u6807\u7b7e. \u4ece\u4e0b\u9762\u7684\u8f93\u51fa\u6211\u4eec\u53ef\u4ee5\u770b\u5230, \u9884\u6d4b\u5f97 # \u5230\u7684\u7ed3\u679c\u662f0 1 2 0 1\\. \u56e0\u4e3a \u7d22\u5f15\u662f\u4ece0\u5f00\u59cb\u7684, \u56e0\u6b64\u7b2c\u4e00\u4e2a\u503c0\u8868\u793a\u7b2c\u4e00\u884c\u7684 # \u6700\u5927\u503c, \u7b2c\u4e8c\u4e2a\u503c1\u8868\u793a\u7b2c\u4e8c\u884c\u7684\u6700\u5927\u503c, \u4ee5\u6b64\u7c7b\u63a8. \u6240\u4ee5\u6700\u540e\u7684\u7ed3\u679c\u662f DET # NOUN VERB DET NOUN, \u6574\u4e2a\u5e8f\u5217\u90fd\u662f\u6b63\u786e\u7684! print(tag_scores)","title":"\u4f8b\u5b50: \u7528 LSTM \u6765\u8fdb\u884c\u8bcd\u6027\u6807\u6ce8"},{"location":"28/#lstm_2","text":"\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d, \u6bcf\u4e2a\u8bcd\u90fd\u6709\u4e00\u4e2a\u8bcd\u5d4c\u5165, \u4f5c\u4e3a\u5e8f\u5217\u6a21\u578b\u7684\u8f93\u5165. \u63a5\u4e0b\u6765\u8ba9\u6211\u4eec\u4f7f\u7528\u6bcf\u4e2a\u7684\u5355\u8bcd\u7684 \u5b57\u7b26\u7ea7\u522b\u7684\u8868\u8fbe\u6765\u589e\u5f3a\u8bcd\u5d4c\u5165. \u6211\u4eec\u671f\u671b\u8fd9\u4e2a\u64cd\u4f5c\u5bf9\u7ed3\u679c\u80fd\u6709\u663e\u8457\u63d0\u5347, \u56e0\u4e3a\u50cf\u8bcd\u7f00\u8fd9\u6837\u7684\u5b57\u7b26\u7ea7 \u4fe1\u606f\u5bf9\u4e8e\u8bcd\u6027\u6709\u5f88\u5927\u7684\u5f71\u54cd. \u6bd4\u5982\u8bf4, \u50cf\u5305\u542b\u8bcd\u7f00 -ly \u7684\u5355\u8bcd\u57fa\u672c\u4e0a\u90fd\u662f\u88ab\u6807\u6ce8\u4e3a\u526f\u8bcd. \u5177\u4f53\u64cd\u4f5c\u5982\u4e0b. \u7528 \\(c_w\\) \u6765\u8868\u793a\u5355\u8bcd \\(w\\) \u7684\u5b57\u7b26\u7ea7\u8868\u8fbe, \u540c\u4e4b\u524d\u4e00\u6837, \u6211\u4eec\u4f7f \u7528 \\(x_w\\) \u6765\u8868\u793a\u8bcd\u5d4c\u5165. \u5e8f\u5217\u6a21\u578b\u7684\u8f93\u5165\u5c31\u53d8\u6210\u4e86 \\(x_w\\) \u548c \\(c_w\\) \u7684\u62fc\u63a5. \u56e0\u6b64, \u5982\u679c \\(x_w\\) \u7684\u7ef4\u5ea6\u662f5, \\(c_w\\) \u7684\u7ef4\u5ea6\u662f3, \u90a3\u4e48\u6211\u4eec\u7684 LSTM \u7f51\u7edc\u7684\u8f93\u5165\u7ef4\u5ea6\u5927\u5c0f\u5c31\u662f8. \u4e3a\u4e86\u5f97\u5230\u5b57\u7b26\u7ea7\u522b\u7684\u8868\u8fbe, \u5c06\u5355\u8bcd\u7684\u6bcf\u4e2a\u5b57\u7b26\u8f93\u5165\u4e00\u4e2a LSTM \u7f51\u7edc, \u800c \\(c_w\\) \u5219\u4e3a\u8fd9\u4e2a LSTM \u7f51\u7edc\u6700\u540e\u7684\u9690\u72b6\u6001. \u4e00\u4e9b\u63d0\u793a: \u65b0\u6a21\u578b\u4e2d\u9700\u8981\u4e24\u4e2a LSTM, \u4e00\u4e2a\u8ddf\u4e4b\u524d\u4e00\u6837, \u7528\u6765\u8f93\u51fa\u8bcd\u6027\u6807\u6ce8\u7684\u5f97\u5206, \u53e6\u5916\u4e00\u4e2a\u65b0\u589e\u52a0\u7684\u7528\u6765 \u83b7\u53d6\u6bcf\u4e2a\u5355\u8bcd\u7684\u5b57\u7b26\u7ea7\u522b\u8868\u8fbe. \u4e3a\u4e86\u5728\u5b57\u7b26\u7ea7\u522b\u4e0a\u8fd0\u884c\u5e8f\u5217\u6a21\u578b, \u4f60\u9700\u8981\u7528\u5d4c\u5165\u7684\u5b57\u7b26\u6765\u4f5c\u4e3a\u5b57\u7b26 LSTM \u7684\u8f93\u5165. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: sequence_models_tutorial.py Download Jupyter notebook: sequence_models_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u7ec3\u4e60: \u4f7f\u7528\u5b57\u7b26\u7ea7\u7279\u5f81\u6765\u589e\u5f3a LSTM \u8bcd\u6027\u6807\u6ce8\u5668"},{"location":"29/","text":"\u9ad8\u7ea7\u6559\u7a0b: \u4f5c\u51fa\u52a8\u6001\u51b3\u7b56\u548c Bi-LSTM CRF \u52a8\u6001 VS \u9759\u6001\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u96c6 Pytorch \u662f\u4e00\u4e2a \u52a8\u6001 \u795e\u7ecf\u7f51\u7edc\u5de5\u5177\u5305. \u53e6\u4e00\u4e2a\u52a8\u6001\u5de5\u5177\u5305\u7684\u4f8b\u5b50\u662f Dynet (\u6211\u4e4b\u6240\u4ee5\u63d0\u8fd9\u4e2a\u662f\u56e0\u4e3a\u4f7f\u7528 Pytorch \u548c Dynet \u662f\u5341\u5206\u7c7b\u4f3c\u7684. \u5982\u679c\u4f60\u770b\u8fc7 Dynet \u4e2d\u7684\u4f8b\u5b50, \u90a3\u4e48\u5b83\u5c06\u6709\u53ef\u80fd\u5bf9\u4f60\u5728 Pytorch \u4e0b\u5b9e\u73b0\u5b83\u6709\u5e2e\u52a9). \u4e0e\u52a8\u6001\u76f8\u53cd\u7684\u662f \u9759\u6001 \u5de5\u5177\u5305, \u5305\u62ec\u4e86 Theano, Keras, TensorFlow \u7b49\u7b49. \u4e0b\u9762\u662f\u8fd9\u4e24\u8005\u6838\u5fc3\u7684\u4e00\u4e9b\u533a\u522b: \u5728\u4e00\u4e2a\u9759\u6001\u5de5\u5177\u5305\u4e2d, \u4f60\u4e00\u6b21\u6027\u5b9a\u4e49\u597d\u4e00\u4e2a\u8ba1\u7b97\u56fe, \u63a5\u7740\u7f16\u8bd1\u5b83, \u7136\u540e\u628a\u6570\u636e\u6d41\u8f93\u5b9e\u4f8b\u9001\u8fdb\u53bb. \u5728\u4e00\u4e2a\u52a8\u6001\u5de5\u5177\u5305\u4e2d, \u4f60 \u4e3a\u6bcf\u4e00\u4e2a\u5b9e\u4f8b \u5b9a\u4e49\u4e00\u4e2a\u8ba1\u7b97\u56fe, \u5b83\u5b8c\u5168\u4e0d\u9700\u8981\u88ab\u7f16\u8bd1\u5e76\u4e14\u662f\u5728\u8fd0\u884c\u4e2d\u5b9e\u65f6\u6267\u884c\u7684. \u82e5\u6ca1\u6709\u4e30\u5bcc\u7684\u7ecf\u9a8c, \u4f60\u5f88\u96be\u4f53\u4f1a\u51fa\u5176\u4e2d\u7684\u5dee\u522b. \u4e3e\u4e00\u4e2a\u4f8b\u5b50, \u5047\u8bbe\u6211\u4eec\u60f3\u8981\u6784\u5efa\u4e00\u4e2a\u6df1\u5ea6\u53e5\u6cd5\u5206\u6790\u5668. \u90a3\u4e48\u6211\u4eec\u7684\u6a21\u578b\u9700\u8981\u4e0b\u5217\u7684\u4e00\u4e9b\u6b65\u9aa4: \u6211\u4eec\u4ece\u4e0b\u5f80\u4e0a\u6784\u5efa\u6811 \u6807\u6ce8\u6839\u8282\u70b9(\u53e5\u5b50\u4e2d\u7684\u8bcd\u8bed) \u4ece\u90a3\u513f\u5f00\u59cb, \u4f7f\u7528\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u548c\u8bcd\u5411\u91cf\u6765\u627e\u5230\u7ec4\u6210\u53e5\u6cd5\u7684\u4e0d\u540c\u7ec4\u5408. \u4e00\u65e6\u5f53\u4f60\u5f62\u6210\u4e86\u4e00\u4e2a\u65b0\u7684\u53e5\u6cd5, \u4f7f\u7528\u67d0\u79cd\u65b9\u5f0f\u5f97\u5230\u53e5\u6cd5\u7684\u5d4c\u5165\u8868\u793a (embedding). \u5728\u8fd9\u4e2a\u4f8b\u5b50\u91cc, \u6211\u4eec\u7684\u7f51\u7edc\u67b6\u6784\u5c06\u4f1a \u5b8c\u5168\u7684\u4f9d\u8d56\u4e8e\u8f93\u5165\u7684\u53e5\u5b50. \u6765\u770b\u8fd9\u4e2a\u53e5\u5b50: \u201c\u7eff\u8272\u732b\u6293\u4e86\u5899\u201d, \u5728\u8fd9\u4e2a\u6a21\u578b\u7684\u67d0\u4e00\u8282\u70b9, \u6211\u4eec\u60f3\u8981\u628a\u8303\u56f4 \\((i,j,r) = (1, 3, \\text{NP})\\) \u5408\u5e76\u8d77\u6765(\u5373, \u4e00\u4e2a NP \u53e5\u6cd5\u8303\u56f4\u8de8\u8d8a\u8bcd1\u5230\u8bcd3, \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\u662f\u201d\u7eff\u8272\u732b\u201d). \u7136\u800c, \u53e6\u4e00\u4e2a\u53e5\u5b50\u53ef\u80fd\u662f\u201d\u67d0\u5904, \u90a3\u4e2a\u5927\u80a5\u732b\u6293\u4e86\u5899.\u201d \u5728\u8fd9\u4e2a\u53e5\u5b50\u4e2d, \u6211\u4eec\u60f3\u8981\u5728\u67d0\u70b9\u5f62\u6210\u53e5\u6cd5 \\((2, 4, NP)\\) . \u6211\u4eec\u60f3\u8981\u5f62\u6210\u7684\u53e5\u6cd5\u5c06\u4f1a\u4f9d\u8d56\u4e8e\u8fd9\u4e2a\u5b9e\u4f8b. \u5982\u679c\u4ec5\u4ec5\u7f16\u8bd1\u8fd9\u4e2a\u8ba1\u7b97\u56fe\u4e00\u6b21, \u5c31\u50cf\u5728\u9759\u6001\u5de5\u5177\u5305\u4e2d\u90a3\u6837, \u90a3\u4e48\u6211\u4eec\u7ed9\u8fd9\u4e2a\u903b\u8f91\u7f16\u7a0b\u5c06\u4f1a\u53d8\u5f97\u5341\u5206\u56f0\u96be\u6216\u8005\u6839\u672c\u4e0d\u53ef\u80fd. \u7136\u800c, \u5728\u4e00\u4e2a\u52a8\u6001\u5de5\u5177\u5305\u4e2d, \u5e76\u4e0d\u4ec5\u4ec5\u53ea\u6709\u4e00\u4e2a\u9884\u5b9a\u4e49\u7684\u8ba1\u7b97\u56fe. \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5b9e\u4f8b, \u90fd\u80fd\u591f\u6709\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u56fe, \u6240\u4ee5\u4e0a\u9762\u7684\u95ee\u9898\u5c31\u4e0d\u590d\u5b58\u5728\u4e86. \u52a8\u6001\u5de5\u5177\u5305\u4e5f\u5177\u6709\u66f4\u5bb9\u6613\u8c03\u8bd5\u548c\u66f4\u63a5\u8fd1\u6240\u4f7f\u7528\u7684\u7f16\u7a0b\u8bed\u8a00\u7684\u7279\u70b9(\u6211\u7684\u610f\u601d\u662f Pytorch \u548c Dynet \u770b\u4e0a\u53bb \u6bd4 Keras \u548c Theano \u66f4\u50cf Python). Bi-LSTM CRF (\u6761\u4ef6\u968f\u673a\u573a) \u8ba8\u8bba \u5728\u8fd9\u4e00\u90e8\u5206, \u6211\u4eec\u5c06\u4f1a\u770b\u5230\u4e00\u4e2a\u5b8c\u6574\u4e14\u590d\u6742\u7684 Bi-LSTM CRF (\u6761\u4ef6\u968f\u673a\u573a)\u7528\u6765\u547d\u540d\u5b9e\u4f53\u8bc6\u522b (NER) \u7684\u4f8b\u5b50. \u4e0a\u9762\u7684 LSTM \u6807\u6ce8\u5de5\u5177\u901a\u5e38\u60c5\u51b5\u4e0b\u5bf9\u8bcd\u6027\u6807\u6ce8\u5df2\u7ecf\u8db3\u591f\u7528\u4e86, \u4f46\u4e00\u4e2a\u5e8f\u5217\u6a21\u578b\u6bd4\u5982 CRF \u5bf9\u4e8e\u5728 NER \u4e0b\u53d6\u5f97 \u5f3a\u52b2\u7684\u8868\u73b0\u662f\u81f3\u5173\u91cd\u8981\u7684. \u5047\u8bbe\u719f\u6089 CRF. \u5c3d\u7ba1\u8fd9\u4e2a\u540d\u5b57\u542c\u4e0a\u53bb\u5413\u4eba, \u4f46\u6240\u6709\u7684\u6a21\u578b\u53ea\u662f\u4e00\u4e2a\u7531 LSTM \u63d0\u4f9b \u7279\u5f81\u7684 CRF. \u4f46\u8fd9\u662f\u4e00\u4e2a\u9ad8\u7ea7\u7684\u6a21\u578b, \u8fdc\u6bd4\u8fd9\u4e2a\u6559\u7a0b\u4e2d\u7684\u5176\u5b83\u65e9\u671f\u7684\u6a21\u578b\u66f4\u52a0\u590d\u6742. \u5982\u679c\u4f60\u8981\u8df3\u8fc7\u8fd9\u4e00\u90e8\u5206, \u6ca1\u6709\u5173\u7cfb. \u60f3\u8981\u786e\u5b9a\u4f60\u662f\u5426\u51c6\u5907\u597d, \u90a3\u770b\u770b\u4f60\u662f\u4e0d\u662f\u80fd\u591f: \u590d\u73b0\u6807\u7b7e k \u7684\u7b2c i \u6b65\u7ef4\u7279\u6bd4\u53d8\u91cf\u7684\u7b97\u6cd5. \u4fee\u6539\u4e0a\u8ff0\u5faa\u73af\u6765\u8ba1\u7b97\u6b63\u5411\u53d8\u91cf. \u518d\u4e00\u6b21\u4fee\u6539\u4e0a\u8ff0\u590d\u73b0\u6765\u5728\u5bf9\u6570\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u6b63\u5411\u53d8\u91cf. (\u63d0\u793a: \u5bf9\u6570-\u6c42\u548c-\u6307\u6570) \u5982\u679c\u4f60\u80fd\u591f\u5b8c\u6210\u4ee5\u4e0a\u4e09\u4ef6\u4e8b, \u90a3\u4e48\u4f60\u5c31\u4e0d\u96be\u7406\u89e3\u4e0b\u9762\u7684\u4ee3\u7801\u4e86. \u56de\u60f3\u4e00\u4e0b, CRF \u8ba1\u7b97\u7684\u662f\u4e00\u4e2a\u6761\u4ef6\u6982\u7387. \u8ba9 \\(y\\) \u4f5c\u4e3a\u4e00\u4e2a\u6807\u6ce8\u5e8f\u5217, \\(x\\) \u4f5c\u4e3a\u67d0\u4e2a\u8bcd\u7684\u8f93\u5165\u5e8f\u5217. \u63a5\u4e0b\u6765\u6211\u4eec\u8ba1\u7b97: \\[P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})}\\] \u4e0a\u9762\u7684\u5206\u6570 Score \u662f\u7531\u5b9a\u4e49\u4e00\u4e9b\u5bf9\u6570\u52bf\u80fd \\(\\log \\psi_i(x,y)\\) \u800c\u51b3\u5b9a\u7684. \u8fdb\u800c \\[\\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)\\] \u8981\u4f7f\u5206\u5272\u51fd\u6570\u6613\u4e8e\u638c\u63a7, \u52bf\u80fd\u5fc5\u987b\u53ea\u80fd\u96c6\u4e2d\u4e8e\u5c40\u90e8\u7684\u7279\u5f81. \u5728 Bi-LSTM CRF \u4e2d, \u6211\u4eec\u5b9a\u4e49\u4e24\u79cd\u52bf\u80fd (potential): \u91ca\u653e (emission) \u548c\u8fc7\u6e21 (transition). \u7d22\u5f15 \\(i\\) \u5904\u5b57\u7684\u91ca\u653e\u52bf\u80fd\u6765\u81ea\u4e8e \\(i\\) \u65f6\u95f4\u5904\u7684 Bi-LSTM \u7684\u9690\u85cf\u72b6\u6001. \u8fc7\u6e21\u52bf\u80fd\u7684\u5206\u6570\u50a8\u5b58\u5728 \\(|T|x|T|\\) \u77e9\u9635 \\(\\textbf{P}\\) , \u5176\u4e2d \\(T\\) \u662f\u6807\u6ce8\u96c6\u5408. \u5728\u6211\u7684\u5b9e\u73b0\u4e2d, \\(\\textbf{P}_{j,k}\\) \u662f\u4ece\u6807\u6ce8 \\(k\\) \u8fc7\u6e21\u5230 \u6807\u6ce8 \\(j\\) \u7684\u5f97\u5206. \u56e0\u6b64: \\[\\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\\] \\[= \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}\\] \u5728\u4e0a\u9762\u7b2c\u4e8c\u4e2a\u8868\u8fbe\u5f0f\u4e2d, \u6211\u4eec\u8ba4\u4e3a\u6807\u7b7e\u88ab\u5206\u914d\u4e86\u72ec\u4e00\u65e0\u4e8c\u7684\u975e\u8d1f\u7d22\u5f15. \u5982\u679c\u4e0a\u9762\u7684\u8ba8\u8bba\u592a\u7b80\u77ed\u4e86, \u4f60\u8fd8\u53ef\u4ee5\u770b\u770b \u8fd9\u4e2a \u7531 Michael Collins \u5199\u7684\u5173\u4e8e CRFs \u7684\u6587\u7ae0. \u5177\u4f53\u5b9e\u73b0\u7b14\u8bb0 \u4e0b\u9762\u7684\u4f8b\u5b50\u5b9e\u73b0\u4e86\u5728\u5bf9\u6570\u7a7a\u95f4\u4e2d\u7684\u524d\u5411\u7b97\u6cd5\u6765\u8ba1\u7b97\u51fa\u5206\u5272\u51fd\u6570\u548c\u7ef4\u7279\u6bd4\u7b97\u6cd5\u6765\u8fdb\u884c\u8bd1\u7801. \u53cd\u5411\u4f20\u64ad\u5c06\u4f1a\u4e3a\u6211\u4eec\u81ea\u52a8\u8ba1\u7b97\u51fa\u68af\u5ea6. \u6211\u4eec\u4e0d\u9700\u8981\u624b\u52a8\u53bb\u5b9e\u73b0\u8fd9\u4e2a. \u8fd9\u4e2a\u4ee3\u7801\u4e2d\u7684\u5b9e\u73b0\u5e76\u6ca1\u6709\u4f18\u5316\u8fc7. \u5982\u679c\u4f60\u7406\u89e3\u4e0b\u9762\u7684\u8fc7\u7a0b, \u4e5f\u8bb8\u4f60\u4f1a\u89c9\u5f97\u4e0b\u9762\u7684\u4ee3\u7801\u4e2d, \u524d\u5411\u7b97\u6cd5\u4e2d \u7684\u8fed\u4ee3\u4e0b\u4e00\u6b21\u6807\u6ce8\u53ef\u4ee5\u5728\u4e00\u6b21\u5927\u7684\u8fd0\u7b97\u4e2d\u5b8c\u6210. \u867d\u7136\u6709\u7b80\u5316\u7684\u4f59\u5730, \u4f46\u6211\u60f3\u7684\u662f\u8ba9\u4ee3\u7801\u53ef\u8bfb\u6027\u66f4\u597d. \u5982\u679c\u4f60\u60f3\u8fdb\u884c\u76f8\u5173\u7684\u4fee\u6539, \u4e5f\u8bb8\u4f60\u53ef\u4ee5\u5728\u4e00\u4e9b\u771f\u5b9e\u7684\u4efb\u52a1\u4e2d\u4f7f\u7528\u8fd9\u4e2a\u6807\u6ce8\u5668. # \u4f5c\u8005: Robert Guthrie import torch import torch.autograd as autograd import torch.nn as nn import torch.optim as optim torch.manual_seed(1) \u4e00\u4e9b\u5e2e\u52a9\u51fd\u6570, \u4f7f\u4ee3\u7801\u53ef\u8bfb\u6027\u66f4\u597d def to_scalar(var): # \u8fd4\u56de python \u6d6e\u70b9\u6570 (float) return var.view(-1).data.tolist()[0] def argmax(vec): # \u4ee5 python \u6574\u6570\u7684\u5f62\u5f0f\u8fd4\u56de argmax _, idx = torch.max(vec, 1) return to_scalar(idx) def prepare_sequence(seq, to_ix): idxs = [to_ix[w] for w in seq] tensor = torch.LongTensor(idxs) return autograd.Variable(tensor) # \u4f7f\u7528\u6570\u503c\u4e0a\u7a33\u5b9a\u7684\u65b9\u6cd5\u4e3a\u524d\u5411\u7b97\u6cd5\u8ba1\u7b97\u6307\u6570\u548c\u7684\u5bf9\u6570 def log_sum_exp(vec): max_score = vec[0, argmax(vec)] max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1]) return max_score + \\ torch.log(torch.sum(torch.exp(vec - max_score_broadcast))) \u521b\u5efa\u6a21\u578b class BiLSTM_CRF(nn.Module): def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim): super(BiLSTM_CRF, self).__init__() self.embedding_dim = embedding_dim self.hidden_dim = hidden_dim self.vocab_size = vocab_size self.tag_to_ix = tag_to_ix self.tagset_size = len(tag_to_ix) self.word_embeds = nn.Embedding(vocab_size, embedding_dim) self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True) # \u5c06LSTM\u7684\u8f93\u51fa\u6620\u5c04\u5230\u6807\u8bb0\u7a7a\u95f4 self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size) # \u8fc7\u6e21\u53c2\u6570\u77e9\u9635. \u6761\u76ee i,j \u662f # *\u4ece* j *\u5230* i \u7684\u8fc7\u6e21\u7684\u5206\u6570 self.transitions = nn.Parameter( torch.randn(self.tagset_size, self.tagset_size)) # \u8fd9\u4e24\u53e5\u58f0\u660e\u5f3a\u5236\u7ea6\u675f\u4e86\u6211\u4eec\u4e0d\u80fd # \u5411\u5f00\u59cb\u6807\u8bb0\u6807\u6ce8\u4f20\u9012\u548c\u4ece\u7ed3\u675f\u6807\u6ce8\u4f20\u9012 self.transitions.data[tag_to_ix[START_TAG], :] = -10000 self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000 self.hidden = self.init_hidden() def init_hidden(self): return (autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)), autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2))) def _forward_alg(self, feats): # \u6267\u884c\u524d\u5411\u7b97\u6cd5\u6765\u8ba1\u7b97\u5206\u5272\u51fd\u6570 init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.) # START_TAG \u5305\u542b\u6240\u6709\u7684\u5206\u6570 init_alphas[0][self.tag_to_ix[START_TAG]] = 0. # \u5c06\u5176\u5305\u5728\u4e00\u4e2a\u53d8\u91cf\u7c7b\u578b\u4e2d\u7ee7\u800c\u5f97\u5230\u81ea\u52a8\u7684\u53cd\u5411\u4f20\u64ad forward_var = autograd.Variable(init_alphas) # \u5728\u53e5\u5b50\u4e2d\u8fed\u4ee3 for feat in feats: alphas_t = [] # \u5728\u8fd9\u4e2a\u65f6\u95f4\u6b65\u7684\u524d\u5411\u53d8\u91cf for next_tag in range(self.tagset_size): # \u5bf9 emission \u5f97\u5206\u6267\u884c\u5e7f\u64ad\u673a\u5236: \u5b83\u603b\u662f\u76f8\u540c\u7684, # \u4e0d\u8bba\u524d\u4e00\u4e2a\u6807\u6ce8\u5982\u4f55 emit_score = feat[next_tag].view( 1, -1).expand(1, self.tagset_size) # trans_score \u7b2c i \u4e2a\u6761\u76ee\u662f # \u4ecei\u8fc7\u6e21\u5230 next_tag \u7684\u5206\u6570 trans_score = self.transitions[next_tag].view(1, -1) # next_tag_var \u7b2c i \u4e2a\u6761\u76ee\u662f\u5728\u6211\u4eec\u6267\u884c \u5bf9\u6570-\u6c42\u548c-\u6307\u6570 \u524d # \u8fb9\u7f18\u7684\u503c (i -> next_tag) next_tag_var = forward_var + trans_score + emit_score # \u8fd9\u4e2a\u6807\u6ce8\u7684\u524d\u5411\u53d8\u91cf\u662f # \u5bf9\u6240\u6709\u7684\u5206\u6570\u6267\u884c \u5bf9\u6570-\u6c42\u548c-\u6307\u6570 alphas_t.append(log_sum_exp(next_tag_var)) forward_var = torch.cat(alphas_t).view(1, -1) terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]] alpha = log_sum_exp(terminal_var) return alpha def _get_lstm_features(self, sentence): self.hidden = self.init_hidden() embeds = self.word_embeds(sentence).view(len(sentence), 1, -1) lstm_out, self.hidden = self.lstm(embeds, self.hidden) lstm_out = lstm_out.view(len(sentence), self.hidden_dim) lstm_feats = self.hidden2tag(lstm_out) return lstm_feats def _score_sentence(self, feats, tags): # \u7ed9\u51fa\u6807\u8bb0\u5e8f\u5217\u7684\u5206\u6570 score = autograd.Variable(torch.Tensor([0])) tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags]) for i, feat in enumerate(feats): score = score + \\ self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]] score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]] return score def _viterbi_decode(self, feats): backpointers = [] # \u5728\u5bf9\u6570\u7a7a\u95f4\u4e2d\u521d\u59cb\u5316\u7ef4\u7279\u6bd4\u53d8\u91cf init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.) init_vvars[0][self.tag_to_ix[START_TAG]] = 0 # \u5728\u7b2c i \u6b65\u7684 forward_var \u5b58\u653e\u7b2c i-1 \u6b65\u7684\u7ef4\u7279\u6bd4\u53d8\u91cf forward_var = autograd.Variable(init_vvars) for feat in feats: bptrs_t = [] # \u5b58\u653e\u8fd9\u4e00\u6b65\u7684\u540e\u6307\u9488 viterbivars_t = [] # \u5b58\u653e\u8fd9\u4e00\u6b65\u7684\u7ef4\u7279\u6bd4\u53d8\u91cf for next_tag in range(self.tagset_size): # next_tag_var[i] \u5b58\u653e\u5148\u524d\u4e00\u6b65\u6807\u6ce8i\u7684 # \u7ef4\u7279\u6bd4\u53d8\u91cf, \u52a0\u4e0a\u4e86\u4ece\u6807\u6ce8 i \u5230 next_tag \u7684\u8fc7\u6e21 # \u7684\u5206\u6570 # \u6211\u4eec\u5728\u8fd9\u91cc\u5e76\u6ca1\u6709\u5c06 emission \u5206\u6570\u5305\u542b\u8fdb\u6765, \u56e0\u4e3a # \u6700\u5927\u503c\u5e76\u4e0d\u4f9d\u8d56\u4e8e\u5b83\u4eec(\u6211\u4eec\u5728\u4e0b\u9762\u5bf9\u5b83\u4eec\u8fdb\u884c\u7684\u662f\u76f8\u52a0) next_tag_var = forward_var + self.transitions[next_tag] best_tag_id = argmax(next_tag_var) bptrs_t.append(best_tag_id) viterbivars_t.append(next_tag_var[0][best_tag_id]) # \u73b0\u5728\u5c06\u6240\u6709 emission \u5f97\u5206\u76f8\u52a0, \u5c06 forward_var # \u8d4b\u503c\u5230\u6211\u4eec\u521a\u521a\u8ba1\u7b97\u51fa\u6765\u7684\u7ef4\u7279\u6bd4\u53d8\u91cf\u96c6\u5408 forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1) backpointers.append(bptrs_t) # \u8fc7\u6e21\u5230 STOP_TAG terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]] best_tag_id = argmax(terminal_var) path_score = terminal_var[0][best_tag_id] # \u8ddf\u7740\u540e\u6307\u9488\u53bb\u89e3\u7801\u6700\u4f73\u8def\u5f84 best_path = [best_tag_id] for bptrs_t in reversed(backpointers): best_tag_id = bptrs_t[best_tag_id] best_path.append(best_tag_id) # \u5f39\u51fa\u5f00\u59cb\u7684\u6807\u7b7e (\u6211\u4eec\u5e76\u4e0d\u5e0c\u671b\u628a\u8fd9\u4e2a\u8fd4\u56de\u5230\u8c03\u7528\u51fd\u6570) start = best_path.pop() assert start == self.tag_to_ix[START_TAG] # \u5065\u5168\u6027\u68c0\u67e5 best_path.reverse() return path_score, best_path def neg_log_likelihood(self, sentence, tags): feats = self._get_lstm_features(sentence) forward_score = self._forward_alg(feats) gold_score = self._score_sentence(feats, tags) return forward_score - gold_score def forward(self, sentence): # \u4e0d\u8981\u628a\u8fd9\u548c\u4e0a\u9762\u7684 _forward_alg \u6df7\u6dc6 # \u5f97\u5230 BiLSTM \u8f93\u51fa\u5206\u6570 lstm_feats = self._get_lstm_features(sentence) # \u7ed9\u5b9a\u7279\u5f81, \u627e\u5230\u6700\u597d\u7684\u8def\u5f84 score, tag_seq = self._viterbi_decode(lstm_feats) return score, tag_seq \u8fd0\u884c\u8bad\u7ec3 START_TAG = \"<START>\" STOP_TAG = \"<STOP>\" EMBEDDING_DIM = 5 HIDDEN_DIM = 4 # \u5236\u9020\u4e00\u4e9b\u8bad\u7ec3\u6570\u636e training_data = [( \"the wall street journal reported today that apple corporation made money\".split(), \"B I I I O O O B I O O\".split() ), ( \"georgia tech is a university in georgia\".split(), \"B I O O O O B\".split() )] word_to_ix = {} for sentence, tags in training_data: for word in sentence: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix) tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4} model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4) # \u5728\u8bad\u7ec3\u4e4b\u524d\u68c0\u67e5\u9884\u6d4b\u7ed3\u679c precheck_sent = prepare_sequence(training_data[0][0], word_to_ix) precheck_tags = torch.LongTensor([tag_to_ix[t] for t in training_data[0][1]]) print(model(precheck_sent)) # \u786e\u8ba4\u4ece\u4e4b\u524d\u7684 LSTM \u90e8\u5206\u7684 prepare_sequence \u88ab\u52a0\u8f7d\u4e86 for epoch in range( 300): # \u53c8\u4e00\u6b21, \u6b63\u5e38\u60c5\u51b5\u4e0b\u4f60\u4e0d\u4f1a\u8bad\u7ec3300\u4e2a epoch, \u8fd9\u53ea\u662f\u793a\u4f8b\u6570\u636e for sentence, tags in training_data: # \u7b2c\u4e00\u6b65: \u9700\u8981\u8bb0\u4f4f\u7684\u662fPytorch\u4f1a\u7d2f\u79ef\u68af\u5ea6 # \u6211\u4eec\u9700\u8981\u5728\u6bcf\u6b21\u5b9e\u4f8b\u4e4b\u524d\u628a\u5b83\u4eec\u6e05\u9664 model.zero_grad() # \u7b2c\u4e8c\u6b65: \u4e3a\u6211\u4eec\u7684\u7f51\u7edc\u51c6\u5907\u597d\u8f93\u5165, \u5373 # \u628a\u5b83\u4eec\u8f6c\u53d8\u6210\u5355\u8bcd\u7d22\u5f15\u53d8\u91cf (Variables) sentence_in = prepare_sequence(sentence, word_to_ix) targets = torch.LongTensor([tag_to_ix[t] for t in tags]) # \u7b2c\u4e09\u6b65: \u8fd0\u884c\u524d\u5411\u4f20\u9012. neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets) # \u7b2c\u56db\u6b65: \u8ba1\u7b97\u635f\u5931, \u68af\u5ea6\u4ee5\u53ca # \u4f7f\u7528 optimizer.step() \u6765\u66f4\u65b0\u53c2\u6570 neg_log_likelihood.backward() optimizer.step() # \u5728\u8bad\u7ec3\u4e4b\u540e\u68c0\u67e5\u9884\u6d4b\u7ed3\u679c precheck_sent = prepare_sequence(training_data[0][0], word_to_ix) print(model(precheck_sent)) # \u6211\u4eec\u5b8c\u6210\u4e86! \u7ec3\u4e60: \u4e3a\u533a\u522b\u6027\u6807\u6ce8\u5b9a\u4e49\u4e00\u4e2a\u65b0\u7684\u635f\u5931\u51fd\u6570 \u5728\u89e3\u7801\u7684\u65f6\u5019, \u6211\u4eec\u4e0d\u4e00\u5b9a\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u8ba1\u7b97\u56fe, \u56e0\u4e3a\u6211\u4eec\u5e76\u4e0d\u4ece\u7ef4\u7279\u6bd4\u8def\u5f84\u5206\u6570\u4e2d\u505a\u53cd\u5411\u4f20\u64ad. \u4e0d\u7ba1\u600e\u6837, \u65e2\u7136\u6211\u4eec\u6709\u4e86\u5b83, \u5c1d\u8bd5\u8bad\u7ec3\u8fd9\u4e2a\u6807\u6ce8\u5668, \u4f7f\u5176\u635f\u5931\u51fd\u6570\u662f\u7ef4\u7279\u6bd4\u8def\u5f84\u5206\u6570\u548c\u9ec4\u91d1\u6807\u51c6\u5206\u6570\u4e4b\u5dee. \u9700\u8981\u5f04\u6e05\u695a\u7684\u662f, \u8fd9\u4e2a\u51fd\u6570\u5728\u9884\u6d4b\u6807\u6ce8\u5e8f\u5217\u662f\u6b63\u786e\u7684\u65f6\u5019\u5e94\u5f53\u5927\u4e8e\u7b49\u4e8e0. \u8fd9\u672c\u8d28\u4e0a\u662f \u7ed3\u6784\u5316\u611f\u77e5\u673a . \u8fd9\u4e2a\u6539\u52a8\u5e94\u5f53\u662f\u5f88\u7b80\u77ed\u7684, \u56e0\u4e3a Viterbi \u548c score_sentence \u662f\u5df2\u7ecf\u5b9e\u73b0\u597d\u4e86\u7684. \u8fd9\u662f \u4f9d\u8d56\u4e8e\u8bad\u7ec3\u5b9e\u4f8b\u7684 \u8ba1\u7b97\u56fe\u7684\u5f62\u72b6\u7684\u4e00\u4e2a\u4f8b\u5b50. \u4f46\u6211\u4eec\u5e76\u6ca1\u6709\u5c1d\u8bd5\u8fc7\u5728\u4e00\u4e2a\u9759\u6001\u5de5\u5177\u5305\u4e0a\u5b9e\u73b0\u8fc7, \u6211\u60f3\u8c61\u4e2d\u8fd9\u662f\u53ef\u884c\u7684\u4f46\u5e76\u4e0d\u662f\u5f88\u663e\u800c\u6613\u89c1. \u627e\u4e00\u4e9b\u771f\u5b9e\u6570\u636e\u505a\u4e00\u4e0b\u6bd4\u8f83\u5427! Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: advanced_tutorial.py Download Jupyter notebook: advanced_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u9ad8\u7ea7\u6559\u7a0b: \u4f5c\u51fa\u52a8\u6001\u51b3\u7b56\u548c Bi-LSTM CRF"},{"location":"29/#bi-lstm-crf","text":"","title":"\u9ad8\u7ea7\u6559\u7a0b: \u4f5c\u51fa\u52a8\u6001\u51b3\u7b56\u548c Bi-LSTM CRF"},{"location":"29/#vs","text":"Pytorch \u662f\u4e00\u4e2a \u52a8\u6001 \u795e\u7ecf\u7f51\u7edc\u5de5\u5177\u5305. \u53e6\u4e00\u4e2a\u52a8\u6001\u5de5\u5177\u5305\u7684\u4f8b\u5b50\u662f Dynet (\u6211\u4e4b\u6240\u4ee5\u63d0\u8fd9\u4e2a\u662f\u56e0\u4e3a\u4f7f\u7528 Pytorch \u548c Dynet \u662f\u5341\u5206\u7c7b\u4f3c\u7684. \u5982\u679c\u4f60\u770b\u8fc7 Dynet \u4e2d\u7684\u4f8b\u5b50, \u90a3\u4e48\u5b83\u5c06\u6709\u53ef\u80fd\u5bf9\u4f60\u5728 Pytorch \u4e0b\u5b9e\u73b0\u5b83\u6709\u5e2e\u52a9). \u4e0e\u52a8\u6001\u76f8\u53cd\u7684\u662f \u9759\u6001 \u5de5\u5177\u5305, \u5305\u62ec\u4e86 Theano, Keras, TensorFlow \u7b49\u7b49. \u4e0b\u9762\u662f\u8fd9\u4e24\u8005\u6838\u5fc3\u7684\u4e00\u4e9b\u533a\u522b: \u5728\u4e00\u4e2a\u9759\u6001\u5de5\u5177\u5305\u4e2d, \u4f60\u4e00\u6b21\u6027\u5b9a\u4e49\u597d\u4e00\u4e2a\u8ba1\u7b97\u56fe, \u63a5\u7740\u7f16\u8bd1\u5b83, \u7136\u540e\u628a\u6570\u636e\u6d41\u8f93\u5b9e\u4f8b\u9001\u8fdb\u53bb. \u5728\u4e00\u4e2a\u52a8\u6001\u5de5\u5177\u5305\u4e2d, \u4f60 \u4e3a\u6bcf\u4e00\u4e2a\u5b9e\u4f8b \u5b9a\u4e49\u4e00\u4e2a\u8ba1\u7b97\u56fe, \u5b83\u5b8c\u5168\u4e0d\u9700\u8981\u88ab\u7f16\u8bd1\u5e76\u4e14\u662f\u5728\u8fd0\u884c\u4e2d\u5b9e\u65f6\u6267\u884c\u7684. \u82e5\u6ca1\u6709\u4e30\u5bcc\u7684\u7ecf\u9a8c, \u4f60\u5f88\u96be\u4f53\u4f1a\u51fa\u5176\u4e2d\u7684\u5dee\u522b. \u4e3e\u4e00\u4e2a\u4f8b\u5b50, \u5047\u8bbe\u6211\u4eec\u60f3\u8981\u6784\u5efa\u4e00\u4e2a\u6df1\u5ea6\u53e5\u6cd5\u5206\u6790\u5668. \u90a3\u4e48\u6211\u4eec\u7684\u6a21\u578b\u9700\u8981\u4e0b\u5217\u7684\u4e00\u4e9b\u6b65\u9aa4: \u6211\u4eec\u4ece\u4e0b\u5f80\u4e0a\u6784\u5efa\u6811 \u6807\u6ce8\u6839\u8282\u70b9(\u53e5\u5b50\u4e2d\u7684\u8bcd\u8bed) \u4ece\u90a3\u513f\u5f00\u59cb, \u4f7f\u7528\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u548c\u8bcd\u5411\u91cf\u6765\u627e\u5230\u7ec4\u6210\u53e5\u6cd5\u7684\u4e0d\u540c\u7ec4\u5408. \u4e00\u65e6\u5f53\u4f60\u5f62\u6210\u4e86\u4e00\u4e2a\u65b0\u7684\u53e5\u6cd5, \u4f7f\u7528\u67d0\u79cd\u65b9\u5f0f\u5f97\u5230\u53e5\u6cd5\u7684\u5d4c\u5165\u8868\u793a (embedding). \u5728\u8fd9\u4e2a\u4f8b\u5b50\u91cc, \u6211\u4eec\u7684\u7f51\u7edc\u67b6\u6784\u5c06\u4f1a \u5b8c\u5168\u7684\u4f9d\u8d56\u4e8e\u8f93\u5165\u7684\u53e5\u5b50. \u6765\u770b\u8fd9\u4e2a\u53e5\u5b50: \u201c\u7eff\u8272\u732b\u6293\u4e86\u5899\u201d, \u5728\u8fd9\u4e2a\u6a21\u578b\u7684\u67d0\u4e00\u8282\u70b9, \u6211\u4eec\u60f3\u8981\u628a\u8303\u56f4 \\((i,j,r) = (1, 3, \\text{NP})\\) \u5408\u5e76\u8d77\u6765(\u5373, \u4e00\u4e2a NP \u53e5\u6cd5\u8303\u56f4\u8de8\u8d8a\u8bcd1\u5230\u8bcd3, \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\u662f\u201d\u7eff\u8272\u732b\u201d). \u7136\u800c, \u53e6\u4e00\u4e2a\u53e5\u5b50\u53ef\u80fd\u662f\u201d\u67d0\u5904, \u90a3\u4e2a\u5927\u80a5\u732b\u6293\u4e86\u5899.\u201d \u5728\u8fd9\u4e2a\u53e5\u5b50\u4e2d, \u6211\u4eec\u60f3\u8981\u5728\u67d0\u70b9\u5f62\u6210\u53e5\u6cd5 \\((2, 4, NP)\\) . \u6211\u4eec\u60f3\u8981\u5f62\u6210\u7684\u53e5\u6cd5\u5c06\u4f1a\u4f9d\u8d56\u4e8e\u8fd9\u4e2a\u5b9e\u4f8b. \u5982\u679c\u4ec5\u4ec5\u7f16\u8bd1\u8fd9\u4e2a\u8ba1\u7b97\u56fe\u4e00\u6b21, \u5c31\u50cf\u5728\u9759\u6001\u5de5\u5177\u5305\u4e2d\u90a3\u6837, \u90a3\u4e48\u6211\u4eec\u7ed9\u8fd9\u4e2a\u903b\u8f91\u7f16\u7a0b\u5c06\u4f1a\u53d8\u5f97\u5341\u5206\u56f0\u96be\u6216\u8005\u6839\u672c\u4e0d\u53ef\u80fd. \u7136\u800c, \u5728\u4e00\u4e2a\u52a8\u6001\u5de5\u5177\u5305\u4e2d, \u5e76\u4e0d\u4ec5\u4ec5\u53ea\u6709\u4e00\u4e2a\u9884\u5b9a\u4e49\u7684\u8ba1\u7b97\u56fe. \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5b9e\u4f8b, \u90fd\u80fd\u591f\u6709\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u56fe, \u6240\u4ee5\u4e0a\u9762\u7684\u95ee\u9898\u5c31\u4e0d\u590d\u5b58\u5728\u4e86. \u52a8\u6001\u5de5\u5177\u5305\u4e5f\u5177\u6709\u66f4\u5bb9\u6613\u8c03\u8bd5\u548c\u66f4\u63a5\u8fd1\u6240\u4f7f\u7528\u7684\u7f16\u7a0b\u8bed\u8a00\u7684\u7279\u70b9(\u6211\u7684\u610f\u601d\u662f Pytorch \u548c Dynet \u770b\u4e0a\u53bb \u6bd4 Keras \u548c Theano \u66f4\u50cf Python).","title":"\u52a8\u6001 VS \u9759\u6001\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u96c6"},{"location":"29/#bi-lstm-crf_1","text":"\u5728\u8fd9\u4e00\u90e8\u5206, \u6211\u4eec\u5c06\u4f1a\u770b\u5230\u4e00\u4e2a\u5b8c\u6574\u4e14\u590d\u6742\u7684 Bi-LSTM CRF (\u6761\u4ef6\u968f\u673a\u573a)\u7528\u6765\u547d\u540d\u5b9e\u4f53\u8bc6\u522b (NER) \u7684\u4f8b\u5b50. \u4e0a\u9762\u7684 LSTM \u6807\u6ce8\u5de5\u5177\u901a\u5e38\u60c5\u51b5\u4e0b\u5bf9\u8bcd\u6027\u6807\u6ce8\u5df2\u7ecf\u8db3\u591f\u7528\u4e86, \u4f46\u4e00\u4e2a\u5e8f\u5217\u6a21\u578b\u6bd4\u5982 CRF \u5bf9\u4e8e\u5728 NER \u4e0b\u53d6\u5f97 \u5f3a\u52b2\u7684\u8868\u73b0\u662f\u81f3\u5173\u91cd\u8981\u7684. \u5047\u8bbe\u719f\u6089 CRF. \u5c3d\u7ba1\u8fd9\u4e2a\u540d\u5b57\u542c\u4e0a\u53bb\u5413\u4eba, \u4f46\u6240\u6709\u7684\u6a21\u578b\u53ea\u662f\u4e00\u4e2a\u7531 LSTM \u63d0\u4f9b \u7279\u5f81\u7684 CRF. \u4f46\u8fd9\u662f\u4e00\u4e2a\u9ad8\u7ea7\u7684\u6a21\u578b, \u8fdc\u6bd4\u8fd9\u4e2a\u6559\u7a0b\u4e2d\u7684\u5176\u5b83\u65e9\u671f\u7684\u6a21\u578b\u66f4\u52a0\u590d\u6742. \u5982\u679c\u4f60\u8981\u8df3\u8fc7\u8fd9\u4e00\u90e8\u5206, \u6ca1\u6709\u5173\u7cfb. \u60f3\u8981\u786e\u5b9a\u4f60\u662f\u5426\u51c6\u5907\u597d, \u90a3\u770b\u770b\u4f60\u662f\u4e0d\u662f\u80fd\u591f: \u590d\u73b0\u6807\u7b7e k \u7684\u7b2c i \u6b65\u7ef4\u7279\u6bd4\u53d8\u91cf\u7684\u7b97\u6cd5. \u4fee\u6539\u4e0a\u8ff0\u5faa\u73af\u6765\u8ba1\u7b97\u6b63\u5411\u53d8\u91cf. \u518d\u4e00\u6b21\u4fee\u6539\u4e0a\u8ff0\u590d\u73b0\u6765\u5728\u5bf9\u6570\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u6b63\u5411\u53d8\u91cf. (\u63d0\u793a: \u5bf9\u6570-\u6c42\u548c-\u6307\u6570) \u5982\u679c\u4f60\u80fd\u591f\u5b8c\u6210\u4ee5\u4e0a\u4e09\u4ef6\u4e8b, \u90a3\u4e48\u4f60\u5c31\u4e0d\u96be\u7406\u89e3\u4e0b\u9762\u7684\u4ee3\u7801\u4e86. \u56de\u60f3\u4e00\u4e0b, CRF \u8ba1\u7b97\u7684\u662f\u4e00\u4e2a\u6761\u4ef6\u6982\u7387. \u8ba9 \\(y\\) \u4f5c\u4e3a\u4e00\u4e2a\u6807\u6ce8\u5e8f\u5217, \\(x\\) \u4f5c\u4e3a\u67d0\u4e2a\u8bcd\u7684\u8f93\u5165\u5e8f\u5217. \u63a5\u4e0b\u6765\u6211\u4eec\u8ba1\u7b97: \\[P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})}\\] \u4e0a\u9762\u7684\u5206\u6570 Score \u662f\u7531\u5b9a\u4e49\u4e00\u4e9b\u5bf9\u6570\u52bf\u80fd \\(\\log \\psi_i(x,y)\\) \u800c\u51b3\u5b9a\u7684. \u8fdb\u800c \\[\\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)\\] \u8981\u4f7f\u5206\u5272\u51fd\u6570\u6613\u4e8e\u638c\u63a7, \u52bf\u80fd\u5fc5\u987b\u53ea\u80fd\u96c6\u4e2d\u4e8e\u5c40\u90e8\u7684\u7279\u5f81. \u5728 Bi-LSTM CRF \u4e2d, \u6211\u4eec\u5b9a\u4e49\u4e24\u79cd\u52bf\u80fd (potential): \u91ca\u653e (emission) \u548c\u8fc7\u6e21 (transition). \u7d22\u5f15 \\(i\\) \u5904\u5b57\u7684\u91ca\u653e\u52bf\u80fd\u6765\u81ea\u4e8e \\(i\\) \u65f6\u95f4\u5904\u7684 Bi-LSTM \u7684\u9690\u85cf\u72b6\u6001. \u8fc7\u6e21\u52bf\u80fd\u7684\u5206\u6570\u50a8\u5b58\u5728 \\(|T|x|T|\\) \u77e9\u9635 \\(\\textbf{P}\\) , \u5176\u4e2d \\(T\\) \u662f\u6807\u6ce8\u96c6\u5408. \u5728\u6211\u7684\u5b9e\u73b0\u4e2d, \\(\\textbf{P}_{j,k}\\) \u662f\u4ece\u6807\u6ce8 \\(k\\) \u8fc7\u6e21\u5230 \u6807\u6ce8 \\(j\\) \u7684\u5f97\u5206. \u56e0\u6b64: \\[\\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\\] \\[= \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}\\] \u5728\u4e0a\u9762\u7b2c\u4e8c\u4e2a\u8868\u8fbe\u5f0f\u4e2d, \u6211\u4eec\u8ba4\u4e3a\u6807\u7b7e\u88ab\u5206\u914d\u4e86\u72ec\u4e00\u65e0\u4e8c\u7684\u975e\u8d1f\u7d22\u5f15. \u5982\u679c\u4e0a\u9762\u7684\u8ba8\u8bba\u592a\u7b80\u77ed\u4e86, \u4f60\u8fd8\u53ef\u4ee5\u770b\u770b \u8fd9\u4e2a \u7531 Michael Collins \u5199\u7684\u5173\u4e8e CRFs \u7684\u6587\u7ae0.","title":"Bi-LSTM CRF (\u6761\u4ef6\u968f\u673a\u573a) \u8ba8\u8bba"},{"location":"29/#_1","text":"\u4e0b\u9762\u7684\u4f8b\u5b50\u5b9e\u73b0\u4e86\u5728\u5bf9\u6570\u7a7a\u95f4\u4e2d\u7684\u524d\u5411\u7b97\u6cd5\u6765\u8ba1\u7b97\u51fa\u5206\u5272\u51fd\u6570\u548c\u7ef4\u7279\u6bd4\u7b97\u6cd5\u6765\u8fdb\u884c\u8bd1\u7801. \u53cd\u5411\u4f20\u64ad\u5c06\u4f1a\u4e3a\u6211\u4eec\u81ea\u52a8\u8ba1\u7b97\u51fa\u68af\u5ea6. \u6211\u4eec\u4e0d\u9700\u8981\u624b\u52a8\u53bb\u5b9e\u73b0\u8fd9\u4e2a. \u8fd9\u4e2a\u4ee3\u7801\u4e2d\u7684\u5b9e\u73b0\u5e76\u6ca1\u6709\u4f18\u5316\u8fc7. \u5982\u679c\u4f60\u7406\u89e3\u4e0b\u9762\u7684\u8fc7\u7a0b, \u4e5f\u8bb8\u4f60\u4f1a\u89c9\u5f97\u4e0b\u9762\u7684\u4ee3\u7801\u4e2d, \u524d\u5411\u7b97\u6cd5\u4e2d \u7684\u8fed\u4ee3\u4e0b\u4e00\u6b21\u6807\u6ce8\u53ef\u4ee5\u5728\u4e00\u6b21\u5927\u7684\u8fd0\u7b97\u4e2d\u5b8c\u6210. \u867d\u7136\u6709\u7b80\u5316\u7684\u4f59\u5730, \u4f46\u6211\u60f3\u7684\u662f\u8ba9\u4ee3\u7801\u53ef\u8bfb\u6027\u66f4\u597d. \u5982\u679c\u4f60\u60f3\u8fdb\u884c\u76f8\u5173\u7684\u4fee\u6539, \u4e5f\u8bb8\u4f60\u53ef\u4ee5\u5728\u4e00\u4e9b\u771f\u5b9e\u7684\u4efb\u52a1\u4e2d\u4f7f\u7528\u8fd9\u4e2a\u6807\u6ce8\u5668. # \u4f5c\u8005: Robert Guthrie import torch import torch.autograd as autograd import torch.nn as nn import torch.optim as optim torch.manual_seed(1) \u4e00\u4e9b\u5e2e\u52a9\u51fd\u6570, \u4f7f\u4ee3\u7801\u53ef\u8bfb\u6027\u66f4\u597d def to_scalar(var): # \u8fd4\u56de python \u6d6e\u70b9\u6570 (float) return var.view(-1).data.tolist()[0] def argmax(vec): # \u4ee5 python \u6574\u6570\u7684\u5f62\u5f0f\u8fd4\u56de argmax _, idx = torch.max(vec, 1) return to_scalar(idx) def prepare_sequence(seq, to_ix): idxs = [to_ix[w] for w in seq] tensor = torch.LongTensor(idxs) return autograd.Variable(tensor) # \u4f7f\u7528\u6570\u503c\u4e0a\u7a33\u5b9a\u7684\u65b9\u6cd5\u4e3a\u524d\u5411\u7b97\u6cd5\u8ba1\u7b97\u6307\u6570\u548c\u7684\u5bf9\u6570 def log_sum_exp(vec): max_score = vec[0, argmax(vec)] max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1]) return max_score + \\ torch.log(torch.sum(torch.exp(vec - max_score_broadcast))) \u521b\u5efa\u6a21\u578b class BiLSTM_CRF(nn.Module): def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim): super(BiLSTM_CRF, self).__init__() self.embedding_dim = embedding_dim self.hidden_dim = hidden_dim self.vocab_size = vocab_size self.tag_to_ix = tag_to_ix self.tagset_size = len(tag_to_ix) self.word_embeds = nn.Embedding(vocab_size, embedding_dim) self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True) # \u5c06LSTM\u7684\u8f93\u51fa\u6620\u5c04\u5230\u6807\u8bb0\u7a7a\u95f4 self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size) # \u8fc7\u6e21\u53c2\u6570\u77e9\u9635. \u6761\u76ee i,j \u662f # *\u4ece* j *\u5230* i \u7684\u8fc7\u6e21\u7684\u5206\u6570 self.transitions = nn.Parameter( torch.randn(self.tagset_size, self.tagset_size)) # \u8fd9\u4e24\u53e5\u58f0\u660e\u5f3a\u5236\u7ea6\u675f\u4e86\u6211\u4eec\u4e0d\u80fd # \u5411\u5f00\u59cb\u6807\u8bb0\u6807\u6ce8\u4f20\u9012\u548c\u4ece\u7ed3\u675f\u6807\u6ce8\u4f20\u9012 self.transitions.data[tag_to_ix[START_TAG], :] = -10000 self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000 self.hidden = self.init_hidden() def init_hidden(self): return (autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)), autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2))) def _forward_alg(self, feats): # \u6267\u884c\u524d\u5411\u7b97\u6cd5\u6765\u8ba1\u7b97\u5206\u5272\u51fd\u6570 init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.) # START_TAG \u5305\u542b\u6240\u6709\u7684\u5206\u6570 init_alphas[0][self.tag_to_ix[START_TAG]] = 0. # \u5c06\u5176\u5305\u5728\u4e00\u4e2a\u53d8\u91cf\u7c7b\u578b\u4e2d\u7ee7\u800c\u5f97\u5230\u81ea\u52a8\u7684\u53cd\u5411\u4f20\u64ad forward_var = autograd.Variable(init_alphas) # \u5728\u53e5\u5b50\u4e2d\u8fed\u4ee3 for feat in feats: alphas_t = [] # \u5728\u8fd9\u4e2a\u65f6\u95f4\u6b65\u7684\u524d\u5411\u53d8\u91cf for next_tag in range(self.tagset_size): # \u5bf9 emission \u5f97\u5206\u6267\u884c\u5e7f\u64ad\u673a\u5236: \u5b83\u603b\u662f\u76f8\u540c\u7684, # \u4e0d\u8bba\u524d\u4e00\u4e2a\u6807\u6ce8\u5982\u4f55 emit_score = feat[next_tag].view( 1, -1).expand(1, self.tagset_size) # trans_score \u7b2c i \u4e2a\u6761\u76ee\u662f # \u4ecei\u8fc7\u6e21\u5230 next_tag \u7684\u5206\u6570 trans_score = self.transitions[next_tag].view(1, -1) # next_tag_var \u7b2c i \u4e2a\u6761\u76ee\u662f\u5728\u6211\u4eec\u6267\u884c \u5bf9\u6570-\u6c42\u548c-\u6307\u6570 \u524d # \u8fb9\u7f18\u7684\u503c (i -> next_tag) next_tag_var = forward_var + trans_score + emit_score # \u8fd9\u4e2a\u6807\u6ce8\u7684\u524d\u5411\u53d8\u91cf\u662f # \u5bf9\u6240\u6709\u7684\u5206\u6570\u6267\u884c \u5bf9\u6570-\u6c42\u548c-\u6307\u6570 alphas_t.append(log_sum_exp(next_tag_var)) forward_var = torch.cat(alphas_t).view(1, -1) terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]] alpha = log_sum_exp(terminal_var) return alpha def _get_lstm_features(self, sentence): self.hidden = self.init_hidden() embeds = self.word_embeds(sentence).view(len(sentence), 1, -1) lstm_out, self.hidden = self.lstm(embeds, self.hidden) lstm_out = lstm_out.view(len(sentence), self.hidden_dim) lstm_feats = self.hidden2tag(lstm_out) return lstm_feats def _score_sentence(self, feats, tags): # \u7ed9\u51fa\u6807\u8bb0\u5e8f\u5217\u7684\u5206\u6570 score = autograd.Variable(torch.Tensor([0])) tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags]) for i, feat in enumerate(feats): score = score + \\ self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]] score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]] return score def _viterbi_decode(self, feats): backpointers = [] # \u5728\u5bf9\u6570\u7a7a\u95f4\u4e2d\u521d\u59cb\u5316\u7ef4\u7279\u6bd4\u53d8\u91cf init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.) init_vvars[0][self.tag_to_ix[START_TAG]] = 0 # \u5728\u7b2c i \u6b65\u7684 forward_var \u5b58\u653e\u7b2c i-1 \u6b65\u7684\u7ef4\u7279\u6bd4\u53d8\u91cf forward_var = autograd.Variable(init_vvars) for feat in feats: bptrs_t = [] # \u5b58\u653e\u8fd9\u4e00\u6b65\u7684\u540e\u6307\u9488 viterbivars_t = [] # \u5b58\u653e\u8fd9\u4e00\u6b65\u7684\u7ef4\u7279\u6bd4\u53d8\u91cf for next_tag in range(self.tagset_size): # next_tag_var[i] \u5b58\u653e\u5148\u524d\u4e00\u6b65\u6807\u6ce8i\u7684 # \u7ef4\u7279\u6bd4\u53d8\u91cf, \u52a0\u4e0a\u4e86\u4ece\u6807\u6ce8 i \u5230 next_tag \u7684\u8fc7\u6e21 # \u7684\u5206\u6570 # \u6211\u4eec\u5728\u8fd9\u91cc\u5e76\u6ca1\u6709\u5c06 emission \u5206\u6570\u5305\u542b\u8fdb\u6765, \u56e0\u4e3a # \u6700\u5927\u503c\u5e76\u4e0d\u4f9d\u8d56\u4e8e\u5b83\u4eec(\u6211\u4eec\u5728\u4e0b\u9762\u5bf9\u5b83\u4eec\u8fdb\u884c\u7684\u662f\u76f8\u52a0) next_tag_var = forward_var + self.transitions[next_tag] best_tag_id = argmax(next_tag_var) bptrs_t.append(best_tag_id) viterbivars_t.append(next_tag_var[0][best_tag_id]) # \u73b0\u5728\u5c06\u6240\u6709 emission \u5f97\u5206\u76f8\u52a0, \u5c06 forward_var # \u8d4b\u503c\u5230\u6211\u4eec\u521a\u521a\u8ba1\u7b97\u51fa\u6765\u7684\u7ef4\u7279\u6bd4\u53d8\u91cf\u96c6\u5408 forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1) backpointers.append(bptrs_t) # \u8fc7\u6e21\u5230 STOP_TAG terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]] best_tag_id = argmax(terminal_var) path_score = terminal_var[0][best_tag_id] # \u8ddf\u7740\u540e\u6307\u9488\u53bb\u89e3\u7801\u6700\u4f73\u8def\u5f84 best_path = [best_tag_id] for bptrs_t in reversed(backpointers): best_tag_id = bptrs_t[best_tag_id] best_path.append(best_tag_id) # \u5f39\u51fa\u5f00\u59cb\u7684\u6807\u7b7e (\u6211\u4eec\u5e76\u4e0d\u5e0c\u671b\u628a\u8fd9\u4e2a\u8fd4\u56de\u5230\u8c03\u7528\u51fd\u6570) start = best_path.pop() assert start == self.tag_to_ix[START_TAG] # \u5065\u5168\u6027\u68c0\u67e5 best_path.reverse() return path_score, best_path def neg_log_likelihood(self, sentence, tags): feats = self._get_lstm_features(sentence) forward_score = self._forward_alg(feats) gold_score = self._score_sentence(feats, tags) return forward_score - gold_score def forward(self, sentence): # \u4e0d\u8981\u628a\u8fd9\u548c\u4e0a\u9762\u7684 _forward_alg \u6df7\u6dc6 # \u5f97\u5230 BiLSTM \u8f93\u51fa\u5206\u6570 lstm_feats = self._get_lstm_features(sentence) # \u7ed9\u5b9a\u7279\u5f81, \u627e\u5230\u6700\u597d\u7684\u8def\u5f84 score, tag_seq = self._viterbi_decode(lstm_feats) return score, tag_seq \u8fd0\u884c\u8bad\u7ec3 START_TAG = \"<START>\" STOP_TAG = \"<STOP>\" EMBEDDING_DIM = 5 HIDDEN_DIM = 4 # \u5236\u9020\u4e00\u4e9b\u8bad\u7ec3\u6570\u636e training_data = [( \"the wall street journal reported today that apple corporation made money\".split(), \"B I I I O O O B I O O\".split() ), ( \"georgia tech is a university in georgia\".split(), \"B I O O O O B\".split() )] word_to_ix = {} for sentence, tags in training_data: for word in sentence: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix) tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4} model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4) # \u5728\u8bad\u7ec3\u4e4b\u524d\u68c0\u67e5\u9884\u6d4b\u7ed3\u679c precheck_sent = prepare_sequence(training_data[0][0], word_to_ix) precheck_tags = torch.LongTensor([tag_to_ix[t] for t in training_data[0][1]]) print(model(precheck_sent)) # \u786e\u8ba4\u4ece\u4e4b\u524d\u7684 LSTM \u90e8\u5206\u7684 prepare_sequence \u88ab\u52a0\u8f7d\u4e86 for epoch in range( 300): # \u53c8\u4e00\u6b21, \u6b63\u5e38\u60c5\u51b5\u4e0b\u4f60\u4e0d\u4f1a\u8bad\u7ec3300\u4e2a epoch, \u8fd9\u53ea\u662f\u793a\u4f8b\u6570\u636e for sentence, tags in training_data: # \u7b2c\u4e00\u6b65: \u9700\u8981\u8bb0\u4f4f\u7684\u662fPytorch\u4f1a\u7d2f\u79ef\u68af\u5ea6 # \u6211\u4eec\u9700\u8981\u5728\u6bcf\u6b21\u5b9e\u4f8b\u4e4b\u524d\u628a\u5b83\u4eec\u6e05\u9664 model.zero_grad() # \u7b2c\u4e8c\u6b65: \u4e3a\u6211\u4eec\u7684\u7f51\u7edc\u51c6\u5907\u597d\u8f93\u5165, \u5373 # \u628a\u5b83\u4eec\u8f6c\u53d8\u6210\u5355\u8bcd\u7d22\u5f15\u53d8\u91cf (Variables) sentence_in = prepare_sequence(sentence, word_to_ix) targets = torch.LongTensor([tag_to_ix[t] for t in tags]) # \u7b2c\u4e09\u6b65: \u8fd0\u884c\u524d\u5411\u4f20\u9012. neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets) # \u7b2c\u56db\u6b65: \u8ba1\u7b97\u635f\u5931, \u68af\u5ea6\u4ee5\u53ca # \u4f7f\u7528 optimizer.step() \u6765\u66f4\u65b0\u53c2\u6570 neg_log_likelihood.backward() optimizer.step() # \u5728\u8bad\u7ec3\u4e4b\u540e\u68c0\u67e5\u9884\u6d4b\u7ed3\u679c precheck_sent = prepare_sequence(training_data[0][0], word_to_ix) print(model(precheck_sent)) # \u6211\u4eec\u5b8c\u6210\u4e86!","title":"\u5177\u4f53\u5b9e\u73b0\u7b14\u8bb0"},{"location":"29/#_2","text":"\u5728\u89e3\u7801\u7684\u65f6\u5019, \u6211\u4eec\u4e0d\u4e00\u5b9a\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u8ba1\u7b97\u56fe, \u56e0\u4e3a\u6211\u4eec\u5e76\u4e0d\u4ece\u7ef4\u7279\u6bd4\u8def\u5f84\u5206\u6570\u4e2d\u505a\u53cd\u5411\u4f20\u64ad. \u4e0d\u7ba1\u600e\u6837, \u65e2\u7136\u6211\u4eec\u6709\u4e86\u5b83, \u5c1d\u8bd5\u8bad\u7ec3\u8fd9\u4e2a\u6807\u6ce8\u5668, \u4f7f\u5176\u635f\u5931\u51fd\u6570\u662f\u7ef4\u7279\u6bd4\u8def\u5f84\u5206\u6570\u548c\u9ec4\u91d1\u6807\u51c6\u5206\u6570\u4e4b\u5dee. \u9700\u8981\u5f04\u6e05\u695a\u7684\u662f, \u8fd9\u4e2a\u51fd\u6570\u5728\u9884\u6d4b\u6807\u6ce8\u5e8f\u5217\u662f\u6b63\u786e\u7684\u65f6\u5019\u5e94\u5f53\u5927\u4e8e\u7b49\u4e8e0. \u8fd9\u672c\u8d28\u4e0a\u662f \u7ed3\u6784\u5316\u611f\u77e5\u673a . \u8fd9\u4e2a\u6539\u52a8\u5e94\u5f53\u662f\u5f88\u7b80\u77ed\u7684, \u56e0\u4e3a Viterbi \u548c score_sentence \u662f\u5df2\u7ecf\u5b9e\u73b0\u597d\u4e86\u7684. \u8fd9\u662f \u4f9d\u8d56\u4e8e\u8bad\u7ec3\u5b9e\u4f8b\u7684 \u8ba1\u7b97\u56fe\u7684\u5f62\u72b6\u7684\u4e00\u4e2a\u4f8b\u5b50. \u4f46\u6211\u4eec\u5e76\u6ca1\u6709\u5c1d\u8bd5\u8fc7\u5728\u4e00\u4e2a\u9759\u6001\u5de5\u5177\u5305\u4e0a\u5b9e\u73b0\u8fc7, \u6211\u60f3\u8c61\u4e2d\u8fd9\u662f\u53ef\u884c\u7684\u4f46\u5e76\u4e0d\u662f\u5f88\u663e\u800c\u6613\u89c1. \u627e\u4e00\u4e9b\u771f\u5b9e\u6570\u636e\u505a\u4e00\u4e0b\u6bd4\u8f83\u5427! Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: advanced_tutorial.py Download Jupyter notebook: advanced_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u7ec3\u4e60: \u4e3a\u533a\u522b\u6027\u6807\u6ce8\u5b9a\u4e49\u4e00\u4e2a\u65b0\u7684\u635f\u5931\u51fd\u6570"},{"location":"3/","text":"\u81ea\u52a8\u6c42\u5bfc: \u81ea\u52a8\u5fae\u5206 PyTorch \u4e2d\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u6838\u5fc3\u662f autograd \u81ea\u52a8\u6c42\u5bfc\u5305. \u6211\u4eec\u5148\u6765\u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0b, \u7136\u540e\u6211\u4eec\u4f1a\u53bb\u8bad\u7ec3\u6211\u4eec\u7684\u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc. autograd \u81ea\u52a8\u6c42\u5bfc\u5305\u9488\u5bf9\u5f20\u91cf\u4e0a\u7684\u6240\u6709\u64cd\u4f5c\u90fd\u63d0\u4f9b\u4e86\u81ea\u52a8\u5fae\u5206\u64cd\u4f5c. \u8fd9\u662f\u4e00\u4e2a\u9010\u4e2a\u8fd0\u884c\u7684\u6846\u67b6, \u8fd9\u610f\u5473\u7740\u60a8\u7684\u53cd\u5411\u4f20\u64ad\u662f\u7531\u60a8\u7684\u4ee3\u7801\u5982\u4f55\u8fd0\u884c\u6765\u5b9a\u4e49\u7684, \u6bcf\u4e2a\u5355\u4e00\u7684\u8fed\u4ee3\u90fd\u53ef\u4ee5\u4e0d\u4e00\u6837. \u8ba9\u6211\u4eec\u7528\u4e00\u4e9b\u66f4\u7b80\u5355\u7684\u672f\u8bed\u4e0e\u4f8b\u5b50\u6765\u4e86\u89e3\u8fd9\u4e9b\u5957\u8def. Variable\uff08\u53d8\u91cf\uff09 autograd.Variable \u662f\u5305\u7684\u6838\u5fc3\u7c7b. \u5b83\u5305\u88c5\u4e86\u5f20\u91cf, \u5e76\u4e14\u652f\u6301\u51e0\u4e4e\u6240\u6709\u7684\u64cd\u4f5c. \u4e00\u65e6\u4f60\u5b8c\u6210\u4e86\u4f60\u7684\u8ba1\u7b97, \u4f60\u5c31\u53ef\u4ee5\u8c03\u7528 .backward() \u65b9\u6cd5, \u7136\u540e\u6240\u6709\u7684\u68af\u5ea6\u8ba1\u7b97\u4f1a\u81ea\u52a8\u8fdb\u884c. \u4f60\u8fd8\u53ef\u4ee5\u901a\u8fc7 .data \u5c5e\u6027\u6765\u8bbf\u95ee\u539f\u59cb\u7684\u5f20\u91cf, \u800c\u5173\u4e8e\u8be5 variable\uff08\u53d8\u91cf\uff09\u7684\u68af\u5ea6\u4f1a\u88ab\u7d2f\u8ba1\u5230 .grad \u4e0a\u53bb. Variable \u8fd8\u6709\u4e00\u4e2a\u9488\u5bf9\u81ea\u52a8\u6c42\u5bfc\u5b9e\u73b0\u6765\u8bf4\u975e\u5e38\u91cd\u8981\u7684\u7c7b - Function . Variable \u548c Function \u662f\u76f8\u4e92\u8054\u7cfb\u7684, \u5e76\u4e14\u5b83\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u975e\u5faa\u73af\u7684\u56fe, \u7f16\u7801\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u8ba1\u7b97\u5386\u53f2\u4fe1\u606f. \u6bcf\u4e00\u4e2a variable\uff08\u53d8\u91cf\uff09\u90fd\u6709\u4e00\u4e2a .grad_fn \u5c5e\u6027, \u5b83\u5f15\u7528\u4e86\u4e00\u4e2a\u5df2\u7ecf\u521b\u5efa\u4e86 Variable \u7684 Function \uff08\u9664\u4e86\u7528\u6237\u521b\u5efa\u7684 Variable `` \u4e4b\u5916 - \u5b83\u4eec\u7684 ``grad_fn is None \uff09. \u5982\u679c\u4f60\u60f3\u8ba1\u7b97\u5bfc\u6570, \u4f60\u53ef\u4ee5\u5728 Variable \u4e0a\u8c03\u7528 .backward() \u65b9\u6cd5. \u5982\u679c Variable \u662f\u6807\u91cf\u7684\u5f62\u5f0f\uff08\u4f8b\u5982, \u5b83\u5305\u542b\u4e00\u4e2a\u5143\u7d20\u6570\u636e\uff09, \u4f60\u4e0d\u5fc5\u6307\u5b9a\u4efb\u4f55\u53c2\u6570\u7ed9 backward() , \u4f46\u662f, \u5982\u679c\u5b83\u6709\u66f4\u591a\u7684\u5143\u7d20. \u4f60\u9700\u8981\u53bb\u6307\u5b9a\u4e00\u4e2a grad_output \u53c2\u6570, \u8be5\u53c2\u6570\u662f\u4e00\u4e2a\u5339\u914d shape\uff08\u5f62\u72b6\uff09\u7684\u5f20\u91cf. import torch from torch.autograd import Variable \u521b\u5efa variable\uff08\u53d8\u91cf\uff09: x = Variable(torch.ones(2, 2), requires_grad = True) print(x) variable\uff08\u53d8\u91cf\uff09\u7684\u64cd\u4f5c: y = x + 2 print(y) y \u7531\u64cd\u4f5c\u521b\u5efa,\u6240\u4ee5\u5b83\u6709 grad_fn \u5c5e\u6027. print(y.grad_fn) y \u7684\u66f4\u591a\u64cd\u4f5c z = y * y * 3 out = z.mean() print(z, out) \u68af\u5ea6 \u6211\u4eec\u73b0\u5728\u5f00\u59cb\u4e86\u89e3\u53cd\u5411\u4f20\u64ad, out.backward() \u4e0e out.backward(torch.Tensor([1.0])) \u8fd9\u6837\u7684\u65b9\u5f0f\u4e00\u6837 out.backward() \u4f46\u56e0 d(out)/dx \u7684\u68af\u5ea6 print(x.grad) \u4f60\u5e94\u8be5\u5f97\u5230\u4e00\u4e2a 4.5 \u7684\u77e9\u9635. \u8ba9\u6211\u4eec\u63a8\u5bfc\u51fa out Variable \u201c \\(o\\) \u201d. \u6211\u4eec\u6709 \\(o = \\frac{1}{4}\\sum_i z_i\\) , \\(z_i = 3(x_i+2)^2\\) \u548c \\(z_i\\bigr\\rvert_{x_i=1} = 27\\) . \u56e0\u6b64, \\(\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)\\) , \u6240\u4ee5 \\(\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5\\) . \u4f60\u53ef\u4ee5\u4f7f\u7528\u81ea\u52a8\u6c42\u5bfc\u6765\u505a\u5f88\u591a\u6709\u8da3\u7684\u4e8b\u60c5 x = torch.randn(3) x = Variable(x, requires_grad = True) y = x * 2 while y.data.norm() < 1000: y = y * 2 print(y) gradients = torch.FloatTensor([0.1, 1.0, 0.0001]) y.backward(gradients) print(x.grad) \u7a0d\u5019\u9605\u8bfb: Variable \u548c Function \u7684\u6587\u6863\u8bf7\u53c2\u9605 http://pytorch.apachecn.org/cn/docs/0.3.0/autograd.html Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: autograd_tutorial.py Download Jupyter notebook: autograd_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u81ea\u52a8\u6c42\u5bfc: \u81ea\u52a8\u5fae\u5206"},{"location":"3/#_1","text":"PyTorch \u4e2d\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u6838\u5fc3\u662f autograd \u81ea\u52a8\u6c42\u5bfc\u5305. \u6211\u4eec\u5148\u6765\u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0b, \u7136\u540e\u6211\u4eec\u4f1a\u53bb\u8bad\u7ec3\u6211\u4eec\u7684\u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc. autograd \u81ea\u52a8\u6c42\u5bfc\u5305\u9488\u5bf9\u5f20\u91cf\u4e0a\u7684\u6240\u6709\u64cd\u4f5c\u90fd\u63d0\u4f9b\u4e86\u81ea\u52a8\u5fae\u5206\u64cd\u4f5c. \u8fd9\u662f\u4e00\u4e2a\u9010\u4e2a\u8fd0\u884c\u7684\u6846\u67b6, \u8fd9\u610f\u5473\u7740\u60a8\u7684\u53cd\u5411\u4f20\u64ad\u662f\u7531\u60a8\u7684\u4ee3\u7801\u5982\u4f55\u8fd0\u884c\u6765\u5b9a\u4e49\u7684, \u6bcf\u4e2a\u5355\u4e00\u7684\u8fed\u4ee3\u90fd\u53ef\u4ee5\u4e0d\u4e00\u6837. \u8ba9\u6211\u4eec\u7528\u4e00\u4e9b\u66f4\u7b80\u5355\u7684\u672f\u8bed\u4e0e\u4f8b\u5b50\u6765\u4e86\u89e3\u8fd9\u4e9b\u5957\u8def.","title":"\u81ea\u52a8\u6c42\u5bfc: \u81ea\u52a8\u5fae\u5206"},{"location":"3/#variable","text":"autograd.Variable \u662f\u5305\u7684\u6838\u5fc3\u7c7b. \u5b83\u5305\u88c5\u4e86\u5f20\u91cf, \u5e76\u4e14\u652f\u6301\u51e0\u4e4e\u6240\u6709\u7684\u64cd\u4f5c. \u4e00\u65e6\u4f60\u5b8c\u6210\u4e86\u4f60\u7684\u8ba1\u7b97, \u4f60\u5c31\u53ef\u4ee5\u8c03\u7528 .backward() \u65b9\u6cd5, \u7136\u540e\u6240\u6709\u7684\u68af\u5ea6\u8ba1\u7b97\u4f1a\u81ea\u52a8\u8fdb\u884c. \u4f60\u8fd8\u53ef\u4ee5\u901a\u8fc7 .data \u5c5e\u6027\u6765\u8bbf\u95ee\u539f\u59cb\u7684\u5f20\u91cf, \u800c\u5173\u4e8e\u8be5 variable\uff08\u53d8\u91cf\uff09\u7684\u68af\u5ea6\u4f1a\u88ab\u7d2f\u8ba1\u5230 .grad \u4e0a\u53bb. Variable \u8fd8\u6709\u4e00\u4e2a\u9488\u5bf9\u81ea\u52a8\u6c42\u5bfc\u5b9e\u73b0\u6765\u8bf4\u975e\u5e38\u91cd\u8981\u7684\u7c7b - Function . Variable \u548c Function \u662f\u76f8\u4e92\u8054\u7cfb\u7684, \u5e76\u4e14\u5b83\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u975e\u5faa\u73af\u7684\u56fe, \u7f16\u7801\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u8ba1\u7b97\u5386\u53f2\u4fe1\u606f. \u6bcf\u4e00\u4e2a variable\uff08\u53d8\u91cf\uff09\u90fd\u6709\u4e00\u4e2a .grad_fn \u5c5e\u6027, \u5b83\u5f15\u7528\u4e86\u4e00\u4e2a\u5df2\u7ecf\u521b\u5efa\u4e86 Variable \u7684 Function \uff08\u9664\u4e86\u7528\u6237\u521b\u5efa\u7684 Variable `` \u4e4b\u5916 - \u5b83\u4eec\u7684 ``grad_fn is None \uff09. \u5982\u679c\u4f60\u60f3\u8ba1\u7b97\u5bfc\u6570, \u4f60\u53ef\u4ee5\u5728 Variable \u4e0a\u8c03\u7528 .backward() \u65b9\u6cd5. \u5982\u679c Variable \u662f\u6807\u91cf\u7684\u5f62\u5f0f\uff08\u4f8b\u5982, \u5b83\u5305\u542b\u4e00\u4e2a\u5143\u7d20\u6570\u636e\uff09, \u4f60\u4e0d\u5fc5\u6307\u5b9a\u4efb\u4f55\u53c2\u6570\u7ed9 backward() , \u4f46\u662f, \u5982\u679c\u5b83\u6709\u66f4\u591a\u7684\u5143\u7d20. \u4f60\u9700\u8981\u53bb\u6307\u5b9a\u4e00\u4e2a grad_output \u53c2\u6570, \u8be5\u53c2\u6570\u662f\u4e00\u4e2a\u5339\u914d shape\uff08\u5f62\u72b6\uff09\u7684\u5f20\u91cf. import torch from torch.autograd import Variable \u521b\u5efa variable\uff08\u53d8\u91cf\uff09: x = Variable(torch.ones(2, 2), requires_grad = True) print(x) variable\uff08\u53d8\u91cf\uff09\u7684\u64cd\u4f5c: y = x + 2 print(y) y \u7531\u64cd\u4f5c\u521b\u5efa,\u6240\u4ee5\u5b83\u6709 grad_fn \u5c5e\u6027. print(y.grad_fn) y \u7684\u66f4\u591a\u64cd\u4f5c z = y * y * 3 out = z.mean() print(z, out)","title":"Variable\uff08\u53d8\u91cf\uff09"},{"location":"3/#_2","text":"\u6211\u4eec\u73b0\u5728\u5f00\u59cb\u4e86\u89e3\u53cd\u5411\u4f20\u64ad, out.backward() \u4e0e out.backward(torch.Tensor([1.0])) \u8fd9\u6837\u7684\u65b9\u5f0f\u4e00\u6837 out.backward() \u4f46\u56e0 d(out)/dx \u7684\u68af\u5ea6 print(x.grad) \u4f60\u5e94\u8be5\u5f97\u5230\u4e00\u4e2a 4.5 \u7684\u77e9\u9635. \u8ba9\u6211\u4eec\u63a8\u5bfc\u51fa out Variable \u201c \\(o\\) \u201d. \u6211\u4eec\u6709 \\(o = \\frac{1}{4}\\sum_i z_i\\) , \\(z_i = 3(x_i+2)^2\\) \u548c \\(z_i\\bigr\\rvert_{x_i=1} = 27\\) . \u56e0\u6b64, \\(\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)\\) , \u6240\u4ee5 \\(\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5\\) . \u4f60\u53ef\u4ee5\u4f7f\u7528\u81ea\u52a8\u6c42\u5bfc\u6765\u505a\u5f88\u591a\u6709\u8da3\u7684\u4e8b\u60c5 x = torch.randn(3) x = Variable(x, requires_grad = True) y = x * 2 while y.data.norm() < 1000: y = y * 2 print(y) gradients = torch.FloatTensor([0.1, 1.0, 0.0001]) y.backward(gradients) print(x.grad) \u7a0d\u5019\u9605\u8bfb: Variable \u548c Function \u7684\u6587\u6863\u8bf7\u53c2\u9605 http://pytorch.apachecn.org/cn/docs/0.3.0/autograd.html Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: autograd_tutorial.py Download Jupyter notebook: autograd_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u68af\u5ea6"},{"location":"30/","text":"\u4e2d\u7ea7\u6559\u7a0b","title":"\u4e2d\u7ea7\u6559\u7a0b"},{"location":"30/#_1","text":"","title":"\u4e2d\u7ea7\u6559\u7a0b"},{"location":"31/","text":"\u7528\u5b57\u7b26\u7ea7RNN\u5206\u7c7b\u540d\u79f0 \u4f5c\u8005 : Sean Robertson \u6211\u4eec\u5c06\u5efa\u7acb\u548c\u8bad\u7ec3\u4e00\u4e2a\u57fa\u672c\u7684\u5b57\u7b26\u7ea7RNN\u8fdb\u884c\u5206\u7c7b\u5355\u8bcd. \u5b57\u7b26\u7ea7\u522b\u7684RNN\u5c06\u5355\u8bcd\u8bfb\u4e3a\u4e00\u7cfb\u5217\u5b57\u7b26 - \u5728\u6bcf\u4e2a\u6b65\u9aa4\u8f93\u51fa\u4e00\u4e2a\u9884\u6d4b\u548c\u201c\u9690\u85cf\u72b6\u6001\u201d, \u5c06\u5148\u524d\u7684\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u4e0b\u4e00\u6b65\u7684\u8f93\u5165. \u6211\u4eec\u91c7\u53d6\u6700\u540e\u7684\u9884\u6d4b\u4f5c\u4e3a\u8f93\u51fa,\u5373\u8be5\u5355\u8bcd\u5c5e\u4e8e\u54ea\u4e00\u7c7b. \u5177\u4f53\u6765\u8bf4,\u6211\u4eec\u5c06\u752818\u79cd\u8bed\u8a00\u7684\u51e0\u5343\u4e2a\u59d3\u6c0f\u4f5c\u4e3a\u8bad\u7ec3\u96c6\u5e76\u6839\u636e\u62fc\u5199\u9884\u6d4b\u540d\u79f0\u6765\u81ea\u54ea\u79cd\u8bed\u8a00: $ python predict.py Hinton (-0.47) Scottish (-1.52) English (-3.57) Irish $ python predict.py Schmidhuber (-0.19) German (-2.48) Czech (-2.68) Dutch \u63a8\u8350\u9605\u8bfb: \u5047\u8bbe\u4f60\u81f3\u5c11\u5df2\u7ecf\u5b89\u88c5\u4e86PyTorch,\u77e5\u9053Python\u548c\u4e86\u89e3\u5f20\u91cf: http://pytorch.org/ \u5b89\u88c5\u6b65\u9aa4 PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b \u5927\u4f53\u4e86\u89e3PyTorch \u8ddf\u7740\u4f8b\u5b50\u5b66\u4e60 PyTorch \u6df1\u5165\u6982\u62ec PyTorch for former Torch users \u5047\u8bbe\u4f60\u662f\u524dLua Torch\u7528\u6237 \u4e86\u89e3RNN\u53ca\u5176\u5de5\u4f5c\u65b9\u5f0f\u4e5f\u5f88\u6709\u7528: \u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u5408\u7406\u6709\u6548\u6027 \u5c55\u793a\u4e86\u4e00\u5806\u771f\u5b9e\u751f\u6d3b\u7684\u4f8b\u5b50 \u7406\u89e3LSTM\u7f51\u7edc \u662f\u5173\u4e8eLSTM\u7684\u5177\u4f53\u5185\u5bb9,\u4f46\u4e5f\u5305\u542b\u6709\u5173RNN\u7684\u4e00\u822c\u4fe1\u606f \u51c6\u5907\u6570\u636e \u5728 data/names \u76ee\u5f55\u4e2d\u5305\u542b18\u4e2a\u540d\u4e3aas\u7684\u6587\u672c\u6587\u4ef6 \u201c[Language].txt\u201d . \u6bcf\u4e2a\u6587\u4ef6\u90fd\u5305\u542b\u4e00\u5806\u540d\u79f0,\u6bcf\u4e2a\u540d\u79f0\u4e00\u884c\u5927\u591a\u662f\u7f57\u9a6c\u5316\uff08\u4f46\u6211\u4eec\u4ecd\u7136\u9700\u8981\u4eceUnicode\u8f6c\u6362\u4e3aASCII\uff09. \u6211\u4eec\u6700\u7ec8\u4f1a\u5f97\u5230\u6bcf\u79cd\u8bed\u8a00\u7684\u540d\u79f0\u5217\u8868\u5b57\u5178 {language: [names ...]} \u901a\u7528\u53d8\u91cf\u201c\u7c7b\u522b\u201d\u548c\u201c\u884c\u201d \uff08\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\u7528\u4e8e\u8bed\u8a00\u548c\u540d\u79f0\uff09\u7528\u4e8e\u4ee5\u540e\u7684\u6269\u5c55\u6027. from __future__ import unicode_literals, print_function, division from io import open import glob def findFiles(path): return glob.glob(path) print(findFiles('data/names/*.txt')) import unicodedata import string all_letters = string.ascii_letters + \" .,;'\" n_letters = len(all_letters) # \u5c06 Unicode \u5b57\u7b26\u4e32\u8f6c\u6362\u4e3a\u7eaf ASCII \u7f16\u7801, \u8fd9\u91cc\u611f\u8c22 http://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters ) print(unicodeToAscii('\u015alus\u00e0rski')) # \u6784\u5efacategory_lines\u5b57\u5178, \u6bcf\u79cd\u8bed\u8a00\u7684\u540d\u79f0\u5217\u8868 category_lines = {} all_categories = [] # \u8bfb\u53d6\u4e00\u4e2a\u6587\u4ef6\u5e76\u5206\u6210\u51e0\u884c def readLines(filename): lines = open(filename, encoding='utf-8').read().strip().split('\\n') return [unicodeToAscii(line) for line in lines] for filename in findFiles('data/names/*.txt'): category = filename.split('/')[-1].split('.')[0] all_categories.append(category) lines = readLines(filename) category_lines[category] = lines n_categories = len(all_categories) \u73b0\u5728\u6211\u4eec\u6709 category_lines , \u8fd9\u662f\u4e00\u4e2a\u6620\u5c04\u6bcf\u4e2a\u7c7b\u522b\u7684\u5b57\u5178 (\u8bed\u8a00) \u5230\u884c\u5217\u8868 (\u540d\u79f0). \u6211\u4eec\u4e5f\u8ddf\u8e2a all_categories (\u53ea\u662f\u4e00\u4e2a\u8bed\u8a00\u5217\u8868) \u548c n_categories \u4e3a\u4ee5\u540e\u505a\u53c2\u8003. print(category_lines['Italian'][:5]) \u5c06\u540d\u5b57\u8f6c\u5316\u4e3a\u5f20\u91cf \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u7ec4\u7ec7\u4e86\u6240\u6709\u7684\u540d\u5b57,\u6211\u4eec\u9700\u8981\u5c06\u5b83\u4eec\u53d8\u6210\u5f20\u91cf\u4ee5\u4fbf\u4f7f\u7528\u5b83\u4eec. \u4e3a\u4e86\u8868\u793a\u5355\u4e2a\u5b57\u6bcd,\u6211\u4eec\u4f7f\u7528\u5927\u5c0f\u4e3a &lt;1 x n_letters&gt; \u7684\u201d\u5355\u70ed\u77e2\u91cf\u201d. \u9664\u4e86\u5f53\u524d\u5b57\u6bcd\u7684\u7d22\u5f15\u5904\u76841\u4ee5\u5916,\u5355\u70ed\u77e2\u91cf\u5269\u4f59\u586b\u51450, e.g. \"b\" = &lt;0 1 0 0 0 ...&gt; . \u4e3a\u4e86\u8bf4\u51fa\u4e00\u4e2a\u8bcd,\u6211\u4eec\u5c06\u5176\u4e2d\u7684\u4e00\u90e8\u5206\u52a0\u5165\u5230\u4e8c\u7ef4\u77e9\u9635\u4e2d &lt;line_length x 1 x n_letters&gt; . \u989d\u5916\u76841\u7ef4\u5ea6\u662f\u56e0\u4e3aPyTorch\u5047\u5b9a\u6240\u6709\u5185\u5bb9\u90fd\u662f\u6279\u91cf\u7684 - \u6211\u4eec\u5728\u8fd9\u91cc\u53ea\u4f7f\u75281\u7684\u6279\u91cf\u5927\u5c0f. import torch # \u4eceall_letters\u4e2d\u67e5\u627e\u5b57\u6bcd\u7d22\u5f15, e.g. \"a\" = 0 def letterToIndex(letter): return all_letters.find(letter) # \u53ea\u662f\u4e3a\u4e86\u6f14\u793a, \u628a\u4e00\u4e2a\u5b57\u6bcd\u53d8\u6210\u4e00\u4e2a <1 x n_letters> \u5f20\u91cf def letterToTensor(letter): tensor = torch.zeros(1, n_letters) tensor[0][letterToIndex(letter)] = 1 return tensor # \u628a\u4e00\u884c\u53d8\u6210\u4e00\u4e2a <line_length x 1 x n_letters>, # \u6216\u4e00\u6279\u5355\u70ed\u5b57\u7b26\u5411\u91cf def lineToTensor(line): tensor = torch.zeros(len(line), 1, n_letters) for li, letter in enumerate(line): tensor[li][0][letterToIndex(letter)] = 1 return tensor print(letterToTensor('J')) print(lineToTensor('Jones').size()) \u521b\u5efa\u7f51\u7edc \u5728autograd\u4e4b\u524d, \u5728Torch\u4e2d\u521b\u5efa\u4e00\u4e2a\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6d89\u53ca\u5230\u514b\u9686\u51e0\u4e2a\u6b65\u9aa4\u4e00\u4e2a\u56fe\u5c42\u7684\u53c2\u6570. \u56fe\u5c42\u4fdd\u6301\u9690\u85cf\u72b6\u6001\u548c\u6e10\u53d8, \u73b0\u5728\u5b8c\u5168\u7531\u56fe\u5f62\u672c\u8eab\u5904\u7406. \u8fd9\u610f\u5473\u7740\u60a8\u53ef\u4ee5\u4ee5\u975e\u5e38\u201c\u7eaf\u7cb9\u201d\u7684\u65b9\u5f0f\u5b9e\u73b0RNN, \u4f5c\u4e3a\u5e38\u89c4\u7684\u524d\u9988\u5c42. \u8fd9\u4e2aRNN\u6a21\u5757 (\u5927\u90e8\u5206\u90fd\u662f\u590d\u5236 the PyTorch for Torch users tutorial ) \u53ea\u67092\u4e2a\u7ebf\u6027\u5c42\u53ef\u4ee5\u5728\u8f93\u5165\u548c\u9690\u85cf\u72b6\u6001\u4e0b\u8fd0\u884c, \u5728\u8f93\u51fa\u4e4b\u540e\u6709\u4e00\u4e2aLogSoftmax\u5c42. import torch.nn as nn from torch.autograd import Variable class RNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(input_size + hidden_size, hidden_size) self.i2o = nn.Linear(input_size + hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): combined = torch.cat((input, hidden), 1) hidden = self.i2h(combined) output = self.i2o(combined) output = self.softmax(output) return output, hidden def initHidden(self): return Variable(torch.zeros(1, self.hidden_size)) n_hidden = 128 rnn = RNN(n_letters, n_hidden, n_categories) \u4e3a\u4e86\u8fd0\u884c\u8fd9\u4e2a\u7f51\u7edc\u7684\u4e00\u4e2a\u6b65\u9aa4, \u6211\u4eec\u9700\u8981\u4f20\u9012\u4e00\u4e2a\u8f93\u5165 (\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\u662f\u5f53\u524d\u5b57\u6bcd\u7684\u5f20\u91cf) \u548c\u4e00\u4e2a\u5148\u524d\u7684\u9690\u85cf\u72b6\u6001 (\u6211\u4eec\u9996\u5148\u521d\u59cb\u5316\u4e3a\u96f6) . \u6211\u4eec\u5c06\u8fd4\u56de\u8f93\u51fa (\u6bcf\u79cd\u8bed\u8a00\u7684\u6982\u7387) \u548c\u4e0b\u4e00\u4e2a\u9690\u85cf\u72b6\u6001 (\u6211\u4eec\u4e3a\u4e0b\u4e00\u6b65\u4fdd\u7559). \u8bf7\u8bb0\u4f4f, PyTorch\u6a21\u5757\u5bf9\u53d8\u91cf\u8fdb\u884c\u64cd\u4f5c, \u800c\u4e0d\u662f\u76f4\u63a5\u5bf9\u5f20\u91cf\u8fdb\u884c\u64cd\u4f5c. input = Variable(letterToTensor('A')) hidden = Variable(torch.zeros(1, n_hidden)) output, next_hidden = rnn(input, hidden) \u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\u6211\u4eec\u4e0d\u5e0c\u671b\u4e3a\u6bcf\u4e00\u6b65\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf, \u6240\u4ee5\u6211\u4eec\u4f7f\u7528 lineToTensor \u800c\u4e0d\u662f letterToTensor \u5e76\u4f7f\u7528\u5207\u7247. \u8fd9\u53ef\u4ee5\u901a\u8fc7\u9884\u5148\u8ba1\u7b97\u6279\u6b21\u7684\u5f20\u91cf\u8fdb\u4e00\u6b65\u4f18\u5316. input = Variable(lineToTensor('Albert')) hidden = Variable(torch.zeros(1, n_hidden)) output, next_hidden = rnn(input[0], hidden) print(output) \u6b63\u5982\u4f60\u6240\u770b\u5230\u7684\u8f93\u51fa\u662f\u4e00\u4e2a &lt;1 x n_categories&gt; \u5f20\u91cf, \u6bcf\u4e2a\u9879\u76ee\u90fd\u662f\u8be5\u7c7b\u522b\u7684\u53ef\u80fd\u6027 (\u8d8a\u9ad8\u8d8a\u6709\u53ef\u80fd). \u8bad\u7ec3 \u51c6\u5907\u8bad\u7ec3 \u5728\u8bad\u7ec3\u4e4b\u524d,\u6211\u4eec\u5e94\u8be5\u505a\u4e00\u4e9b\u8f85\u52a9\u529f\u80fd. \u9996\u5148\u662f\u89e3\u91ca\u7f51\u7edc\u7684\u8f93\u51fa, \u6211\u4eec\u77e5\u9053\u8fd9\u662f\u6bcf\u4e2a\u7c7b\u522b\u7684\u53ef\u80fd\u6027. \u6211\u4e48\u53ef\u4ee5\u4f7f\u7528 Tensor.topk \u5f97\u5230\u6700\u5927\u4ef7\u503c\u7684\u6307\u6570: def categoryFromOutput(output): top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data category_i = top_i[0][0] return all_categories[category_i], category_i print(categoryFromOutput(output)) \u6211\u4eec\u4e5f\u5e0c\u671b\u80fd\u591f\u5feb\u901f\u83b7\u5f97\u8bad\u7ec3\u793a\u4f8b (\u540d\u79f0\u53ca\u5176\u8bed\u8a00): import random def randomChoice(l): return l[random.randint(0, len(l) - 1)] def randomTrainingExample(): category = randomChoice(all_categories) line = randomChoice(category_lines[category]) category_tensor = Variable(torch.LongTensor([all_categories.index(category)])) line_tensor = Variable(lineToTensor(line)) return category, line, category_tensor, line_tensor for i in range(10): category, line, category_tensor, line_tensor = randomTrainingExample() print('category =', category, '/ line =', line) \u8bad\u7ec3\u7f51\u7edc \u73b0\u5728\u8bad\u7ec3\u8fd9\u4e2a\u7f51\u7edc\u6240\u9700\u8981\u7684\u5c31\u662f\u5411\u5927\u5bb6\u5c55\u793a\u4e00\u4e9b\u4f8b\u5b50, \u8ba9\u5b83\u731c\u6d4b, \u5e76\u544a\u8bc9\u5b83\u662f\u5426\u662f\u9519\u8bef\u7684. \u5bf9\u4e8e\u635f\u5931\u51fd\u6570 nn.NLLLoss \u662f\u9002\u5f53\u7684, \u56e0\u4e3aRNN\u7684\u6700\u540e\u4e00\u5c42\u662f nn.LogSoftmax . criterion = nn.NLLLoss() \u6bcf\u4e2a\u8bad\u7ec3\u5faa\u73af\u90fd\u4f1a: \u521b\u5efa\u8f93\u5165\u548c\u76ee\u6807\u5f20\u91cf \u521b\u5efa\u4e00\u4e2a\u5f52\u96f6\u7684\u521d\u59cb\u9690\u85cf\u72b6\u6001 \u8bfb\u5165\u6bcf\u4e2a\u5b57\u6bcd \u4e3a\u4e0b\u4e00\u4e2a\u5b57\u6bcd\u4fdd\u6301\u9690\u85cf\u72b6\u6001 \u6bd4\u8f83\u6700\u7ec8\u8f93\u51fa\u4e0e\u76ee\u6807 \u53cd\u5411\u4f20\u64ad \u8fd4\u56de\u8f93\u51fa\u548c\u635f\u5931 learning_rate = 0.005 # \u5982\u679c\u8bbe\u7f6e\u5f97\u592a\u9ad8, \u53ef\u80fd\u4f1a\u7206\u70b8. \u5982\u679c\u592a\u4f4e, \u53ef\u80fd\u65e0\u6cd5\u5b66\u4e60. def train(category_tensor, line_tensor): hidden = rnn.initHidden() rnn.zero_grad() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) loss = criterion(output, category_tensor) loss.backward() # \u5c06\u53c2\u6570\u68af\u5ea6\u6dfb\u52a0\u5230\u5b83\u4eec\u7684\u503c,\u518d\u4e58\u4ee5\u5b66\u4e60\u901f\u7387 for p in rnn.parameters(): p.data.add_(-learning_rate, p.grad.data) return output, loss.data[0] \u73b0\u5728\u6211\u4eec\u53ea\u9700\u8981\u8fd0\u884c\u4e00\u4e9b\u4f8b\u5b50. \u7531\u4e8e train \u51fd\u6570\u8fd4\u56de\u8f93\u51fa\u548c\u635f\u5931,\u6211\u4eec\u53ef\u4ee5\u6253\u5370\u5b83\u7684\u731c\u6d4b,\u5e76\u8bb0\u5f55\u7ed8\u56fe\u7684\u635f\u5931 \u65e2\u7136\u67091000\u4e2a\u4f8b\u5b50, \u6211\u4eec\u53ea\u6253\u5370\u6bcf\u4e2a print_every \u7684\u4f8b\u5b50, \u5e76\u53d6\u5e73\u5747\u7684\u635f\u5931. import time import math n_iters = 100000 print_every = 5000 plot_every = 1000 # \u8ddf\u8e2a\u7ed8\u56fe\u7684\u635f\u5931 current_loss = 0 all_losses = [] def timeSince(since): now = time.time() s = now - since m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) start = time.time() for iter in range(1, n_iters + 1): category, line, category_tensor, line_tensor = randomTrainingExample() output, loss = train(category_tensor, line_tensor) current_loss += loss # \u6253\u5370\u5faa\u73af\u6570,\u635f\u5931,\u540d\u79f0\u548c\u731c\u6d4b if iter % print_every == 0: guess, guess_i = categoryFromOutput(output) correct = '\u2713' if guess == category else '\u2717 (%s)' % category print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct)) # \u5c06\u5f53\u524d\u635f\u5931\u5e73\u5747\u503c\u6dfb\u52a0\u5230\u635f\u5931\u6e05\u5355 if iter % plot_every == 0: all_losses.append(current_loss / plot_every) current_loss = 0 \u7ed8\u5236\u7ed3\u679c \u4ece all_losses \u7ed8\u5236\u5386\u53f2\u635f\u5931\u663e\u793a\u7f51\u7edc\u5b66\u4e60: import matplotlib.pyplot as plt import matplotlib.ticker as ticker plt.figure() plt.plot(all_losses) \u8bc4\u4f30\u7ed3\u679c \u8981\u67e5\u770b\u7f51\u7edc\u5728\u4e0d\u540c\u7c7b\u522b\u4e2d\u7684\u8868\u73b0\u5982\u4f55, \u6211\u4eec\u5c06\u521b\u5efa\u4e00\u4e2a\u6df7\u6dc6\u77e9\u9635, \u4e3a\u6bcf\u4e2a\u5b9e\u9645\u8bed\u8a00 (\u884c) \u6307\u793a\u7f51\u7edc\u731c\u6d4b\u54ea\u79cd\u8bed\u8a00 (\u5217). \u4e3a\u4e86\u8ba1\u7b97\u6df7\u6dc6\u77e9\u9635,\u4e00\u5806\u6837\u672c\u901a\u8fc7\u7f51\u7edc\u8fd0\u884c evaluate() , \u8fd9\u548c train() \u51cf\u53bb\u53cd\u5411\u4f20\u64ad\u662f\u4e00\u6837\u7684. # \u5728\u6df7\u6dc6\u77e9\u9635\u4e2d\u8ddf\u8e2a\u6b63\u786e\u7684\u731c\u6d4b confusion = torch.zeros(n_categories, n_categories) n_confusion = 10000 # \u53ea\u8981\u8fd4\u56de\u7ed9\u5b9a\u4e00\u884c\u7684\u8f93\u51fa\u5373\u53ef def evaluate(line_tensor): hidden = rnn.initHidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) return output # \u901a\u8fc7\u4e00\u5806\u793a\u4f8b\u5e76\u8bb0\u5f55\u54ea\u4e9b\u662f\u6b63\u786e\u7684\u731c\u6d4b for i in range(n_confusion): category, line, category_tensor, line_tensor = randomTrainingExample() output = evaluate(line_tensor) guess, guess_i = categoryFromOutput(output) category_i = all_categories.index(category) confusion[category_i][guess_i] += 1 # \u901a\u8fc7\u5c06\u6bcf\u4e00\u884c\u9664\u4ee5\u5176\u603b\u548c\u6765\u6807\u51c6\u5316 for i in range(n_categories): confusion[i] = confusion[i] / confusion[i].sum() # \u8bbe\u7f6e\u7ed8\u56fe fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(confusion.numpy()) fig.colorbar(cax) # \u8bbe\u7f6e\u8f74 ax.set_xticklabels([''] + all_categories, rotation=90) ax.set_yticklabels([''] + all_categories) # Force label at every tick ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) # sphinx_gallery_thumbnail_number = 2 plt.show() \u60a8\u53ef\u4ee5\u4ece\u4e3b\u8f74\u4e0a\u9009\u53d6\u663e\u793a\u9519\u8bef\u731c\u6d4b\u54ea\u4e9b\u8bed\u8a00\u7684\u4eae\u70b9, e.g. Chinese for Korean, and Spanish for Italian. \u5b83\u4f3c\u4e4e\u4e0e\u5e0c\u814a\u8bed\u5f88\u597d,\u82f1\u8bed\u5f88\u5dee (\u53ef\u80fd\u662f\u56e0\u4e3a\u4e0e\u5176\u4ed6\u8bed\u8a00\u91cd\u53e0). \u5728\u7528\u6237\u8f93\u5165\u4e0a\u8fd0\u884c def predict(input_line, n_predictions=3): print('\\n> %s' % input_line) output = evaluate(Variable(lineToTensor(input_line))) # \u83b7\u53d6\u524dN\u4e2a\u7c7b\u522b topv, topi = output.data.topk(n_predictions, 1, True) predictions = [] for i in range(n_predictions): value = topv[0][i] category_index = topi[0][i] print('(%.2f) %s' % (value, all_categories[category_index])) predictions.append([value, all_categories[category_index]]) predict('Dovesky') predict('Jackson') predict('Satoshi') \u811a\u672c\u7684\u6700\u7ec8\u7248\u672c in the Practical PyTorch repo \u5c06\u4e0a\u9762\u7684\u4ee3\u7801\u5206\u6210\u51e0\u4e2a\u6587\u4ef6: data.py (\u52a0\u8f7d\u6587\u4ef6) model.py (\u5b9a\u4e49RNN) train.py (\u8fd0\u884c\u8bad\u7ec3) predict.py (\u7528\u547d\u4ee4\u884c\u53c2\u6570\u8fd0\u884c predict() ) server.py (\u4f7f\u7528bottle.py\u5c06\u9884\u6d4b\u7528\u4f5cJSON API) \u8fd0\u884c train.py \u6765\u8bad\u7ec3\u548c\u4fdd\u5b58\u7f51\u7edc. \u8fd0\u884c\u5177\u6709\u540d\u79f0\u7684 predict.py \u6765\u67e5\u770b\u9884\u6d4b: $ python predict.py Hazaki (-0.42) Japanese (-1.39) Polish (-3.51) Czech \u8fd0\u884c server.py \u548c\u67e5\u770b http://localhost:5533/Yourname \u83b7\u53d6\u9884\u6d4b\u7684JSON\u8f93\u51fa. \u7ec3\u4e60 \u5c1d\u8bd5\u4f7f\u7528\u4e0d\u540c\u7684\u6570\u636e\u96c6 \u7ebf\u6761 -> \u7c7b\u522b, \u4f8b\u5982: \u4efb\u4f55\u5355\u8bcd -> \u8bed\u8a00 \u59d3 -> \u6027\u522b \u89d2\u8272\u540d\u5b57 -> \u4f5c\u5bb6 \u9875\u9762\u6807\u9898 -> \u535a\u5ba2\u6216subreddit \u901a\u8fc7\u66f4\u5927\u548c/\u6216\u66f4\u597d\u7684\u5f62\u72b6\u7f51\u7edc\u83b7\u5f97\u66f4\u597d\u7684\u7ed3\u679c \u6dfb\u52a0\u66f4\u591a\u7ebf\u6027\u56fe\u5c42 \u8bd5\u8bd5 nn.LSTM \u548c nn.GRU \u56fe\u5c42 \u5c06\u591a\u4e2a\u8fd9\u4e9bRNN\u7ec4\u5408\u4e3a\u66f4\u9ad8\u7ea7\u522b\u7684\u7f51\u7edc Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: char_rnn_classification_tutorial.py Download Jupyter notebook: char_rnn_classification_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u7528\u5b57\u7b26\u7ea7RNN\u5206\u7c7b\u540d\u79f0"},{"location":"31/#rnn","text":"\u4f5c\u8005 : Sean Robertson \u6211\u4eec\u5c06\u5efa\u7acb\u548c\u8bad\u7ec3\u4e00\u4e2a\u57fa\u672c\u7684\u5b57\u7b26\u7ea7RNN\u8fdb\u884c\u5206\u7c7b\u5355\u8bcd. \u5b57\u7b26\u7ea7\u522b\u7684RNN\u5c06\u5355\u8bcd\u8bfb\u4e3a\u4e00\u7cfb\u5217\u5b57\u7b26 - \u5728\u6bcf\u4e2a\u6b65\u9aa4\u8f93\u51fa\u4e00\u4e2a\u9884\u6d4b\u548c\u201c\u9690\u85cf\u72b6\u6001\u201d, \u5c06\u5148\u524d\u7684\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u4e0b\u4e00\u6b65\u7684\u8f93\u5165. \u6211\u4eec\u91c7\u53d6\u6700\u540e\u7684\u9884\u6d4b\u4f5c\u4e3a\u8f93\u51fa,\u5373\u8be5\u5355\u8bcd\u5c5e\u4e8e\u54ea\u4e00\u7c7b. \u5177\u4f53\u6765\u8bf4,\u6211\u4eec\u5c06\u752818\u79cd\u8bed\u8a00\u7684\u51e0\u5343\u4e2a\u59d3\u6c0f\u4f5c\u4e3a\u8bad\u7ec3\u96c6\u5e76\u6839\u636e\u62fc\u5199\u9884\u6d4b\u540d\u79f0\u6765\u81ea\u54ea\u79cd\u8bed\u8a00: $ python predict.py Hinton (-0.47) Scottish (-1.52) English (-3.57) Irish $ python predict.py Schmidhuber (-0.19) German (-2.48) Czech (-2.68) Dutch \u63a8\u8350\u9605\u8bfb: \u5047\u8bbe\u4f60\u81f3\u5c11\u5df2\u7ecf\u5b89\u88c5\u4e86PyTorch,\u77e5\u9053Python\u548c\u4e86\u89e3\u5f20\u91cf: http://pytorch.org/ \u5b89\u88c5\u6b65\u9aa4 PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b \u5927\u4f53\u4e86\u89e3PyTorch \u8ddf\u7740\u4f8b\u5b50\u5b66\u4e60 PyTorch \u6df1\u5165\u6982\u62ec PyTorch for former Torch users \u5047\u8bbe\u4f60\u662f\u524dLua Torch\u7528\u6237 \u4e86\u89e3RNN\u53ca\u5176\u5de5\u4f5c\u65b9\u5f0f\u4e5f\u5f88\u6709\u7528: \u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u5408\u7406\u6709\u6548\u6027 \u5c55\u793a\u4e86\u4e00\u5806\u771f\u5b9e\u751f\u6d3b\u7684\u4f8b\u5b50 \u7406\u89e3LSTM\u7f51\u7edc \u662f\u5173\u4e8eLSTM\u7684\u5177\u4f53\u5185\u5bb9,\u4f46\u4e5f\u5305\u542b\u6709\u5173RNN\u7684\u4e00\u822c\u4fe1\u606f","title":"\u7528\u5b57\u7b26\u7ea7RNN\u5206\u7c7b\u540d\u79f0"},{"location":"31/#_1","text":"\u5728 data/names \u76ee\u5f55\u4e2d\u5305\u542b18\u4e2a\u540d\u4e3aas\u7684\u6587\u672c\u6587\u4ef6 \u201c[Language].txt\u201d . \u6bcf\u4e2a\u6587\u4ef6\u90fd\u5305\u542b\u4e00\u5806\u540d\u79f0,\u6bcf\u4e2a\u540d\u79f0\u4e00\u884c\u5927\u591a\u662f\u7f57\u9a6c\u5316\uff08\u4f46\u6211\u4eec\u4ecd\u7136\u9700\u8981\u4eceUnicode\u8f6c\u6362\u4e3aASCII\uff09. \u6211\u4eec\u6700\u7ec8\u4f1a\u5f97\u5230\u6bcf\u79cd\u8bed\u8a00\u7684\u540d\u79f0\u5217\u8868\u5b57\u5178 {language: [names ...]} \u901a\u7528\u53d8\u91cf\u201c\u7c7b\u522b\u201d\u548c\u201c\u884c\u201d \uff08\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\u7528\u4e8e\u8bed\u8a00\u548c\u540d\u79f0\uff09\u7528\u4e8e\u4ee5\u540e\u7684\u6269\u5c55\u6027. from __future__ import unicode_literals, print_function, division from io import open import glob def findFiles(path): return glob.glob(path) print(findFiles('data/names/*.txt')) import unicodedata import string all_letters = string.ascii_letters + \" .,;'\" n_letters = len(all_letters) # \u5c06 Unicode \u5b57\u7b26\u4e32\u8f6c\u6362\u4e3a\u7eaf ASCII \u7f16\u7801, \u8fd9\u91cc\u611f\u8c22 http://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters ) print(unicodeToAscii('\u015alus\u00e0rski')) # \u6784\u5efacategory_lines\u5b57\u5178, \u6bcf\u79cd\u8bed\u8a00\u7684\u540d\u79f0\u5217\u8868 category_lines = {} all_categories = [] # \u8bfb\u53d6\u4e00\u4e2a\u6587\u4ef6\u5e76\u5206\u6210\u51e0\u884c def readLines(filename): lines = open(filename, encoding='utf-8').read().strip().split('\\n') return [unicodeToAscii(line) for line in lines] for filename in findFiles('data/names/*.txt'): category = filename.split('/')[-1].split('.')[0] all_categories.append(category) lines = readLines(filename) category_lines[category] = lines n_categories = len(all_categories) \u73b0\u5728\u6211\u4eec\u6709 category_lines , \u8fd9\u662f\u4e00\u4e2a\u6620\u5c04\u6bcf\u4e2a\u7c7b\u522b\u7684\u5b57\u5178 (\u8bed\u8a00) \u5230\u884c\u5217\u8868 (\u540d\u79f0). \u6211\u4eec\u4e5f\u8ddf\u8e2a all_categories (\u53ea\u662f\u4e00\u4e2a\u8bed\u8a00\u5217\u8868) \u548c n_categories \u4e3a\u4ee5\u540e\u505a\u53c2\u8003. print(category_lines['Italian'][:5])","title":"\u51c6\u5907\u6570\u636e"},{"location":"31/#_2","text":"\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u7ec4\u7ec7\u4e86\u6240\u6709\u7684\u540d\u5b57,\u6211\u4eec\u9700\u8981\u5c06\u5b83\u4eec\u53d8\u6210\u5f20\u91cf\u4ee5\u4fbf\u4f7f\u7528\u5b83\u4eec. \u4e3a\u4e86\u8868\u793a\u5355\u4e2a\u5b57\u6bcd,\u6211\u4eec\u4f7f\u7528\u5927\u5c0f\u4e3a &lt;1 x n_letters&gt; \u7684\u201d\u5355\u70ed\u77e2\u91cf\u201d. \u9664\u4e86\u5f53\u524d\u5b57\u6bcd\u7684\u7d22\u5f15\u5904\u76841\u4ee5\u5916,\u5355\u70ed\u77e2\u91cf\u5269\u4f59\u586b\u51450, e.g. \"b\" = &lt;0 1 0 0 0 ...&gt; . \u4e3a\u4e86\u8bf4\u51fa\u4e00\u4e2a\u8bcd,\u6211\u4eec\u5c06\u5176\u4e2d\u7684\u4e00\u90e8\u5206\u52a0\u5165\u5230\u4e8c\u7ef4\u77e9\u9635\u4e2d &lt;line_length x 1 x n_letters&gt; . \u989d\u5916\u76841\u7ef4\u5ea6\u662f\u56e0\u4e3aPyTorch\u5047\u5b9a\u6240\u6709\u5185\u5bb9\u90fd\u662f\u6279\u91cf\u7684 - \u6211\u4eec\u5728\u8fd9\u91cc\u53ea\u4f7f\u75281\u7684\u6279\u91cf\u5927\u5c0f. import torch # \u4eceall_letters\u4e2d\u67e5\u627e\u5b57\u6bcd\u7d22\u5f15, e.g. \"a\" = 0 def letterToIndex(letter): return all_letters.find(letter) # \u53ea\u662f\u4e3a\u4e86\u6f14\u793a, \u628a\u4e00\u4e2a\u5b57\u6bcd\u53d8\u6210\u4e00\u4e2a <1 x n_letters> \u5f20\u91cf def letterToTensor(letter): tensor = torch.zeros(1, n_letters) tensor[0][letterToIndex(letter)] = 1 return tensor # \u628a\u4e00\u884c\u53d8\u6210\u4e00\u4e2a <line_length x 1 x n_letters>, # \u6216\u4e00\u6279\u5355\u70ed\u5b57\u7b26\u5411\u91cf def lineToTensor(line): tensor = torch.zeros(len(line), 1, n_letters) for li, letter in enumerate(line): tensor[li][0][letterToIndex(letter)] = 1 return tensor print(letterToTensor('J')) print(lineToTensor('Jones').size())","title":"\u5c06\u540d\u5b57\u8f6c\u5316\u4e3a\u5f20\u91cf"},{"location":"31/#_3","text":"\u5728autograd\u4e4b\u524d, \u5728Torch\u4e2d\u521b\u5efa\u4e00\u4e2a\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6d89\u53ca\u5230\u514b\u9686\u51e0\u4e2a\u6b65\u9aa4\u4e00\u4e2a\u56fe\u5c42\u7684\u53c2\u6570. \u56fe\u5c42\u4fdd\u6301\u9690\u85cf\u72b6\u6001\u548c\u6e10\u53d8, \u73b0\u5728\u5b8c\u5168\u7531\u56fe\u5f62\u672c\u8eab\u5904\u7406. \u8fd9\u610f\u5473\u7740\u60a8\u53ef\u4ee5\u4ee5\u975e\u5e38\u201c\u7eaf\u7cb9\u201d\u7684\u65b9\u5f0f\u5b9e\u73b0RNN, \u4f5c\u4e3a\u5e38\u89c4\u7684\u524d\u9988\u5c42. \u8fd9\u4e2aRNN\u6a21\u5757 (\u5927\u90e8\u5206\u90fd\u662f\u590d\u5236 the PyTorch for Torch users tutorial ) \u53ea\u67092\u4e2a\u7ebf\u6027\u5c42\u53ef\u4ee5\u5728\u8f93\u5165\u548c\u9690\u85cf\u72b6\u6001\u4e0b\u8fd0\u884c, \u5728\u8f93\u51fa\u4e4b\u540e\u6709\u4e00\u4e2aLogSoftmax\u5c42. import torch.nn as nn from torch.autograd import Variable class RNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(input_size + hidden_size, hidden_size) self.i2o = nn.Linear(input_size + hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): combined = torch.cat((input, hidden), 1) hidden = self.i2h(combined) output = self.i2o(combined) output = self.softmax(output) return output, hidden def initHidden(self): return Variable(torch.zeros(1, self.hidden_size)) n_hidden = 128 rnn = RNN(n_letters, n_hidden, n_categories) \u4e3a\u4e86\u8fd0\u884c\u8fd9\u4e2a\u7f51\u7edc\u7684\u4e00\u4e2a\u6b65\u9aa4, \u6211\u4eec\u9700\u8981\u4f20\u9012\u4e00\u4e2a\u8f93\u5165 (\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\u662f\u5f53\u524d\u5b57\u6bcd\u7684\u5f20\u91cf) \u548c\u4e00\u4e2a\u5148\u524d\u7684\u9690\u85cf\u72b6\u6001 (\u6211\u4eec\u9996\u5148\u521d\u59cb\u5316\u4e3a\u96f6) . \u6211\u4eec\u5c06\u8fd4\u56de\u8f93\u51fa (\u6bcf\u79cd\u8bed\u8a00\u7684\u6982\u7387) \u548c\u4e0b\u4e00\u4e2a\u9690\u85cf\u72b6\u6001 (\u6211\u4eec\u4e3a\u4e0b\u4e00\u6b65\u4fdd\u7559). \u8bf7\u8bb0\u4f4f, PyTorch\u6a21\u5757\u5bf9\u53d8\u91cf\u8fdb\u884c\u64cd\u4f5c, \u800c\u4e0d\u662f\u76f4\u63a5\u5bf9\u5f20\u91cf\u8fdb\u884c\u64cd\u4f5c. input = Variable(letterToTensor('A')) hidden = Variable(torch.zeros(1, n_hidden)) output, next_hidden = rnn(input, hidden) \u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\u6211\u4eec\u4e0d\u5e0c\u671b\u4e3a\u6bcf\u4e00\u6b65\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf, \u6240\u4ee5\u6211\u4eec\u4f7f\u7528 lineToTensor \u800c\u4e0d\u662f letterToTensor \u5e76\u4f7f\u7528\u5207\u7247. \u8fd9\u53ef\u4ee5\u901a\u8fc7\u9884\u5148\u8ba1\u7b97\u6279\u6b21\u7684\u5f20\u91cf\u8fdb\u4e00\u6b65\u4f18\u5316. input = Variable(lineToTensor('Albert')) hidden = Variable(torch.zeros(1, n_hidden)) output, next_hidden = rnn(input[0], hidden) print(output) \u6b63\u5982\u4f60\u6240\u770b\u5230\u7684\u8f93\u51fa\u662f\u4e00\u4e2a &lt;1 x n_categories&gt; \u5f20\u91cf, \u6bcf\u4e2a\u9879\u76ee\u90fd\u662f\u8be5\u7c7b\u522b\u7684\u53ef\u80fd\u6027 (\u8d8a\u9ad8\u8d8a\u6709\u53ef\u80fd).","title":"\u521b\u5efa\u7f51\u7edc"},{"location":"31/#_4","text":"","title":"\u8bad\u7ec3"},{"location":"31/#_5","text":"\u5728\u8bad\u7ec3\u4e4b\u524d,\u6211\u4eec\u5e94\u8be5\u505a\u4e00\u4e9b\u8f85\u52a9\u529f\u80fd. \u9996\u5148\u662f\u89e3\u91ca\u7f51\u7edc\u7684\u8f93\u51fa, \u6211\u4eec\u77e5\u9053\u8fd9\u662f\u6bcf\u4e2a\u7c7b\u522b\u7684\u53ef\u80fd\u6027. \u6211\u4e48\u53ef\u4ee5\u4f7f\u7528 Tensor.topk \u5f97\u5230\u6700\u5927\u4ef7\u503c\u7684\u6307\u6570: def categoryFromOutput(output): top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data category_i = top_i[0][0] return all_categories[category_i], category_i print(categoryFromOutput(output)) \u6211\u4eec\u4e5f\u5e0c\u671b\u80fd\u591f\u5feb\u901f\u83b7\u5f97\u8bad\u7ec3\u793a\u4f8b (\u540d\u79f0\u53ca\u5176\u8bed\u8a00): import random def randomChoice(l): return l[random.randint(0, len(l) - 1)] def randomTrainingExample(): category = randomChoice(all_categories) line = randomChoice(category_lines[category]) category_tensor = Variable(torch.LongTensor([all_categories.index(category)])) line_tensor = Variable(lineToTensor(line)) return category, line, category_tensor, line_tensor for i in range(10): category, line, category_tensor, line_tensor = randomTrainingExample() print('category =', category, '/ line =', line)","title":"\u51c6\u5907\u8bad\u7ec3"},{"location":"31/#_6","text":"\u73b0\u5728\u8bad\u7ec3\u8fd9\u4e2a\u7f51\u7edc\u6240\u9700\u8981\u7684\u5c31\u662f\u5411\u5927\u5bb6\u5c55\u793a\u4e00\u4e9b\u4f8b\u5b50, \u8ba9\u5b83\u731c\u6d4b, \u5e76\u544a\u8bc9\u5b83\u662f\u5426\u662f\u9519\u8bef\u7684. \u5bf9\u4e8e\u635f\u5931\u51fd\u6570 nn.NLLLoss \u662f\u9002\u5f53\u7684, \u56e0\u4e3aRNN\u7684\u6700\u540e\u4e00\u5c42\u662f nn.LogSoftmax . criterion = nn.NLLLoss() \u6bcf\u4e2a\u8bad\u7ec3\u5faa\u73af\u90fd\u4f1a: \u521b\u5efa\u8f93\u5165\u548c\u76ee\u6807\u5f20\u91cf \u521b\u5efa\u4e00\u4e2a\u5f52\u96f6\u7684\u521d\u59cb\u9690\u85cf\u72b6\u6001 \u8bfb\u5165\u6bcf\u4e2a\u5b57\u6bcd \u4e3a\u4e0b\u4e00\u4e2a\u5b57\u6bcd\u4fdd\u6301\u9690\u85cf\u72b6\u6001 \u6bd4\u8f83\u6700\u7ec8\u8f93\u51fa\u4e0e\u76ee\u6807 \u53cd\u5411\u4f20\u64ad \u8fd4\u56de\u8f93\u51fa\u548c\u635f\u5931 learning_rate = 0.005 # \u5982\u679c\u8bbe\u7f6e\u5f97\u592a\u9ad8, \u53ef\u80fd\u4f1a\u7206\u70b8. \u5982\u679c\u592a\u4f4e, \u53ef\u80fd\u65e0\u6cd5\u5b66\u4e60. def train(category_tensor, line_tensor): hidden = rnn.initHidden() rnn.zero_grad() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) loss = criterion(output, category_tensor) loss.backward() # \u5c06\u53c2\u6570\u68af\u5ea6\u6dfb\u52a0\u5230\u5b83\u4eec\u7684\u503c,\u518d\u4e58\u4ee5\u5b66\u4e60\u901f\u7387 for p in rnn.parameters(): p.data.add_(-learning_rate, p.grad.data) return output, loss.data[0] \u73b0\u5728\u6211\u4eec\u53ea\u9700\u8981\u8fd0\u884c\u4e00\u4e9b\u4f8b\u5b50. \u7531\u4e8e train \u51fd\u6570\u8fd4\u56de\u8f93\u51fa\u548c\u635f\u5931,\u6211\u4eec\u53ef\u4ee5\u6253\u5370\u5b83\u7684\u731c\u6d4b,\u5e76\u8bb0\u5f55\u7ed8\u56fe\u7684\u635f\u5931 \u65e2\u7136\u67091000\u4e2a\u4f8b\u5b50, \u6211\u4eec\u53ea\u6253\u5370\u6bcf\u4e2a print_every \u7684\u4f8b\u5b50, \u5e76\u53d6\u5e73\u5747\u7684\u635f\u5931. import time import math n_iters = 100000 print_every = 5000 plot_every = 1000 # \u8ddf\u8e2a\u7ed8\u56fe\u7684\u635f\u5931 current_loss = 0 all_losses = [] def timeSince(since): now = time.time() s = now - since m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) start = time.time() for iter in range(1, n_iters + 1): category, line, category_tensor, line_tensor = randomTrainingExample() output, loss = train(category_tensor, line_tensor) current_loss += loss # \u6253\u5370\u5faa\u73af\u6570,\u635f\u5931,\u540d\u79f0\u548c\u731c\u6d4b if iter % print_every == 0: guess, guess_i = categoryFromOutput(output) correct = '\u2713' if guess == category else '\u2717 (%s)' % category print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct)) # \u5c06\u5f53\u524d\u635f\u5931\u5e73\u5747\u503c\u6dfb\u52a0\u5230\u635f\u5931\u6e05\u5355 if iter % plot_every == 0: all_losses.append(current_loss / plot_every) current_loss = 0","title":"\u8bad\u7ec3\u7f51\u7edc"},{"location":"31/#_7","text":"\u4ece all_losses \u7ed8\u5236\u5386\u53f2\u635f\u5931\u663e\u793a\u7f51\u7edc\u5b66\u4e60: import matplotlib.pyplot as plt import matplotlib.ticker as ticker plt.figure() plt.plot(all_losses)","title":"\u7ed8\u5236\u7ed3\u679c"},{"location":"31/#_8","text":"\u8981\u67e5\u770b\u7f51\u7edc\u5728\u4e0d\u540c\u7c7b\u522b\u4e2d\u7684\u8868\u73b0\u5982\u4f55, \u6211\u4eec\u5c06\u521b\u5efa\u4e00\u4e2a\u6df7\u6dc6\u77e9\u9635, \u4e3a\u6bcf\u4e2a\u5b9e\u9645\u8bed\u8a00 (\u884c) \u6307\u793a\u7f51\u7edc\u731c\u6d4b\u54ea\u79cd\u8bed\u8a00 (\u5217). \u4e3a\u4e86\u8ba1\u7b97\u6df7\u6dc6\u77e9\u9635,\u4e00\u5806\u6837\u672c\u901a\u8fc7\u7f51\u7edc\u8fd0\u884c evaluate() , \u8fd9\u548c train() \u51cf\u53bb\u53cd\u5411\u4f20\u64ad\u662f\u4e00\u6837\u7684. # \u5728\u6df7\u6dc6\u77e9\u9635\u4e2d\u8ddf\u8e2a\u6b63\u786e\u7684\u731c\u6d4b confusion = torch.zeros(n_categories, n_categories) n_confusion = 10000 # \u53ea\u8981\u8fd4\u56de\u7ed9\u5b9a\u4e00\u884c\u7684\u8f93\u51fa\u5373\u53ef def evaluate(line_tensor): hidden = rnn.initHidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) return output # \u901a\u8fc7\u4e00\u5806\u793a\u4f8b\u5e76\u8bb0\u5f55\u54ea\u4e9b\u662f\u6b63\u786e\u7684\u731c\u6d4b for i in range(n_confusion): category, line, category_tensor, line_tensor = randomTrainingExample() output = evaluate(line_tensor) guess, guess_i = categoryFromOutput(output) category_i = all_categories.index(category) confusion[category_i][guess_i] += 1 # \u901a\u8fc7\u5c06\u6bcf\u4e00\u884c\u9664\u4ee5\u5176\u603b\u548c\u6765\u6807\u51c6\u5316 for i in range(n_categories): confusion[i] = confusion[i] / confusion[i].sum() # \u8bbe\u7f6e\u7ed8\u56fe fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(confusion.numpy()) fig.colorbar(cax) # \u8bbe\u7f6e\u8f74 ax.set_xticklabels([''] + all_categories, rotation=90) ax.set_yticklabels([''] + all_categories) # Force label at every tick ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) # sphinx_gallery_thumbnail_number = 2 plt.show() \u60a8\u53ef\u4ee5\u4ece\u4e3b\u8f74\u4e0a\u9009\u53d6\u663e\u793a\u9519\u8bef\u731c\u6d4b\u54ea\u4e9b\u8bed\u8a00\u7684\u4eae\u70b9, e.g. Chinese for Korean, and Spanish for Italian. \u5b83\u4f3c\u4e4e\u4e0e\u5e0c\u814a\u8bed\u5f88\u597d,\u82f1\u8bed\u5f88\u5dee (\u53ef\u80fd\u662f\u56e0\u4e3a\u4e0e\u5176\u4ed6\u8bed\u8a00\u91cd\u53e0).","title":"\u8bc4\u4f30\u7ed3\u679c"},{"location":"31/#_9","text":"def predict(input_line, n_predictions=3): print('\\n> %s' % input_line) output = evaluate(Variable(lineToTensor(input_line))) # \u83b7\u53d6\u524dN\u4e2a\u7c7b\u522b topv, topi = output.data.topk(n_predictions, 1, True) predictions = [] for i in range(n_predictions): value = topv[0][i] category_index = topi[0][i] print('(%.2f) %s' % (value, all_categories[category_index])) predictions.append([value, all_categories[category_index]]) predict('Dovesky') predict('Jackson') predict('Satoshi') \u811a\u672c\u7684\u6700\u7ec8\u7248\u672c in the Practical PyTorch repo \u5c06\u4e0a\u9762\u7684\u4ee3\u7801\u5206\u6210\u51e0\u4e2a\u6587\u4ef6: data.py (\u52a0\u8f7d\u6587\u4ef6) model.py (\u5b9a\u4e49RNN) train.py (\u8fd0\u884c\u8bad\u7ec3) predict.py (\u7528\u547d\u4ee4\u884c\u53c2\u6570\u8fd0\u884c predict() ) server.py (\u4f7f\u7528bottle.py\u5c06\u9884\u6d4b\u7528\u4f5cJSON API) \u8fd0\u884c train.py \u6765\u8bad\u7ec3\u548c\u4fdd\u5b58\u7f51\u7edc. \u8fd0\u884c\u5177\u6709\u540d\u79f0\u7684 predict.py \u6765\u67e5\u770b\u9884\u6d4b: $ python predict.py Hazaki (-0.42) Japanese (-1.39) Polish (-3.51) Czech \u8fd0\u884c server.py \u548c\u67e5\u770b http://localhost:5533/Yourname \u83b7\u53d6\u9884\u6d4b\u7684JSON\u8f93\u51fa.","title":"\u5728\u7528\u6237\u8f93\u5165\u4e0a\u8fd0\u884c"},{"location":"31/#_10","text":"\u5c1d\u8bd5\u4f7f\u7528\u4e0d\u540c\u7684\u6570\u636e\u96c6 \u7ebf\u6761 -> \u7c7b\u522b, \u4f8b\u5982: \u4efb\u4f55\u5355\u8bcd -> \u8bed\u8a00 \u59d3 -> \u6027\u522b \u89d2\u8272\u540d\u5b57 -> \u4f5c\u5bb6 \u9875\u9762\u6807\u9898 -> \u535a\u5ba2\u6216subreddit \u901a\u8fc7\u66f4\u5927\u548c/\u6216\u66f4\u597d\u7684\u5f62\u72b6\u7f51\u7edc\u83b7\u5f97\u66f4\u597d\u7684\u7ed3\u679c \u6dfb\u52a0\u66f4\u591a\u7ebf\u6027\u56fe\u5c42 \u8bd5\u8bd5 nn.LSTM \u548c nn.GRU \u56fe\u5c42 \u5c06\u591a\u4e2a\u8fd9\u4e9bRNN\u7ec4\u5408\u4e3a\u66f4\u9ad8\u7ea7\u522b\u7684\u7f51\u7edc Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: char_rnn_classification_tutorial.py Download Jupyter notebook: char_rnn_classification_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u7ec3\u4e60"},{"location":"32/","text":"\u57fa\u4e0e\u5b57\u7b26\u7ea7RNN\uff08Char-RNN\uff09\u7684\u4eba\u540d\u751f\u6210 \u4f5c\u8005 : Sean Robertson \u5728 \u4e0a\u4e00\u4e2a\u6559\u7a0b \u91cc\u6211\u4eec\u4f7f\u7528RNN\u628a\u540d\u5b57\u5206\u7c7b\u5230\u5b83\u6240\u5c5e\u7684\u8bed\u8a00\u4e2d, \u8fd9\u6b21\u6211\u4eec\u6539\u53d8\u4e00\u4e0b\u6765\u5b66\u4e60\u4ece\u8bed\u8a00\u4e2d\u751f\u6210\u540d\u5b57. > python sample.py Russian RUS Rovakov Uantov Shavakov > python sample.py German GER Gerren Ereng Rosher > python sample.py Spanish SPA Salla Parer Allan > python sample.py Chinese CHI Chan Hang Iun \u6211\u4eec\u4ecd\u7136\u624b\u5de5\u642d\u5efa\u4e00\u4e2a\u5305\u542b\u51e0\u4e2a\u7ebf\u6027\u5c42\u7684\u5c0f\u7684RNN. \u8fd9\u6b21\u7684\u6700\u5927\u7684\u4e0d\u540c\u662f\u8f93\u5165\u4e00\u4e2a\u7c7b\u522b, \u6bcf\u6b21\u8f93\u51fa\u4e00\u4e2a\u5b57\u6bcd, \u800c\u4e0d\u662f\u8bfb\u5165\u6240\u6709\u540d\u5b57\u7684\u5b57\u6bcd\u6765\u9884\u6d4b\u4e00\u4e2a\u7c7b\u522b. \u5faa\u73af\u7684\u9884\u6d4b\u6bcf\u4e00\u4e2a\u5b57\u6bcd\u6765\u6784\u6210\u8bed\u8a00\uff08\u4e5f\u53ef\u4ee5\u7528\u6587 \u5b57\u6216\u8005\u5176\u4ed6\u66f4\u9ad8\u7ea7\u7684\u7ed3\u6784\u5b8c\u6210\uff09, \u901a\u5e38\u88ab\u79f0\u4e3a\u201c\u8bed\u8a00\u6a21\u578b\u201d. ** \u63a8\u8350\u9605\u8bfb: ** \u5047\u8bbe\u4f60\u81f3\u5c11\u5b89\u88c5\u4e86PyTorch, \u719f\u6089Python, \u7406\u89e3Tensors: http://pytorch.org/ : \u5b89\u88c5\u8bf4\u660e PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b \u83b7\u53d6\u4e00\u822c\u7684 PyTorch \u5165\u95e8 \u8ddf\u7740\u4f8b\u5b50\u5b66\u4e60 PyTorch \u5e7f\u6cdb\u4e14\u6df1\u5165\u7684\u6982\u8ff0 PyTorch for former Torch users \u5982\u679c\u66fe\u7ecf\u662f Lua Torch \u7684\u7528\u6237 \u4e0b\u9762\u8fd9\u4e9b\u5bf9\u4e86\u89e3 RNNs \u548c\u5176\u5de5\u4f5c\u539f\u7406\u4e5f\u662f\u5f88\u6709\u7528\u7684: The Unreasonable Effectiveness of Recurrent Neural Networks \u5c55\u793a\u4e86\u4e00\u7cfb\u5217\u771f\u5b9e\u751f\u6d3b\u4e2d\u7684\u4f8b\u5b50 Understanding LSTM Networks \u662f\u4e00\u7bc7\u7279\u522b\u5173\u4e8eLSTMs\u7684\u6587\u7ae0, \u4f46\u662f\u5bf9\u4e8e\u4e00\u822c\u7684RNNs\u4e5f\u5f88\u6709\u76ca\u7684 \u8fd8\u5efa\u8bae\u4e0a\u4e00\u4e2a\u6559\u7a0b: \u7528\u5b57\u7b26\u7ea7RNN\u5206\u7c7b\u540d\u79f0 \u6570\u636e\u51c6\u5907 Note \u4ece \u8fd9\u91cc \u4e0b\u8f7d\u6570\u636e, \u5e76\u89e3\u538b\u5230\u5f53\u524d\u76ee\u5f55. \u66f4\u591a\u7684\u7ec6\u8282\u53c2\u8003\u4e0a\u4e00\u4e2a\u6559\u7a0b, \u603b\u4e4b, \u6570\u636e\u542b\u6709\u4e00\u6279\u7eaf\u6587\u672c\u6587\u4ef6: data/names/[Language].txt \u6bcf\u4e00\u884c\u4e00\u4e2a\u4eba\u540d. \u5c06\u884c\u5206\u5272\u6210\u6570\u7ec4, \u5e76\u628a Unicode \u8f6c\u6362\u6210 ASCII \u7f16\u7801, \u6700\u540e\u653e\u8fdb\u4e00\u4e2a\u5b57\u5178\u91cc {language: [names ...]} . from __future__ import unicode_literals, print_function, division from io import open import glob import unicodedata import string all_letters = string.ascii_letters + \" .,;'-\" n_letters = len(all_letters) + 1 # \u6dfb\u52a0 EOS \u6807\u8bb0 def findFiles(path): return glob.glob(path) # \u5c06 Unicode \u5b57\u7b26\u4e32\u8f6c\u6362\u4e3a\u7eaf ASCII \u7f16\u7801, \u611f\u8c22 http://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters ) # \u8bfb\u53d6\u6587\u4ef6\u5e76\u5206\u5272\u6210\u884c def readLines(filename): lines = open(filename, encoding='utf-8').read().strip().split('\\n') return [unicodeToAscii(line) for line in lines] # \u6784\u5efa\u6620\u5c04\u5b57\u5178 category_lines , \u6bcf\u4e2a\u7c7b\u522b\u662f\u7531\u5f88\u591a\u4e2a\u884c\u7ec4\u6210\u7684list category_lines = {} all_categories = [] for filename in findFiles('data/names/*.txt'): category = filename.split('/')[-1].split('.')[0] all_categories.append(category) lines = readLines(filename) category_lines[category] = lines n_categories = len(all_categories) print('# categories:', n_categories, all_categories) print(unicodeToAscii(\"O'N\u00e9\u00e0l\")) \u521b\u5efa\u7f51\u7edc \u8fd9\u4e2a\u7f51\u7edc\u6269\u5c55\u4e86 \u4e0a\u4e00\u4e2a\u6559\u7a0b\u7684RNN , \u4e3a\u7c7b\u522b\u5f20\u91cf\u6dfb\u52a0\u4e86\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570, \u5e76\u548c\u5176\u4ed6\u7684\u53c2\u6570\u4e32\u8054\u5728\u4e00\u8d77. \u7c7b\u522b\u5f20\u91cf \u548c\u5b57\u6bcd\u7684\u8f93\u5165\u4e00\u6837\u662f one-hot \u5411\u91cf. \u6211\u4eec\u5c06\u8f93\u51fa\u89e3\u91ca\u6210\u4e3a\u4e0b\u4e00\u4e2a\u5b57\u6bcd\u7684\u6982\u7387, \u91c7\u6837\u7684\u65f6\u5019, \u6700\u6709\u53ef\u80fd\u7684\u8f93\u51fa\u88ab\u5f53\u505a\u4e0b\u4e00\u4e2a\u8f93\u5165. \u4e3a\u4e86\u8ba9\u7f51\u7edc\u66f4\u52a0\u6709\u6548\u5de5\u4f5c, \u6211\u6dfb\u52a0\u4e86\u7b2c\u4e8c\u4e2a\u7ebf\u6027\u5c42 o2o \uff08\u5728\u5408\u5e76\u4e86\u9690\u85cf\u5c42\u548c\u8f93\u51fa\u5c42\u7684\u540e\u9762\uff09. \u8fd8\u6709\u4e00\u4e2a Dropout \u5c42, \u4f7f\u8f93\u5165\u7684\u90e8\u5206\u503c\u4ee5\u7ed9\u5b9a\u7684\u6982\u7387\u503c\u968f\u673a\u7684\u53d8\u6210 0 \uff08\u8fd9\u91cc\u6982\u7387\u53d60.1\uff09, \u8fd9\u6837\u505a\u901a\u5e38\u662f\u4e3a\u4e86\u6a21\u7cca\u8f93\u5165\u4ee5\u9632\u6b62\u8fc7\u62df\u5408. \u8fd9\u91cc\u6211\u4eec\u5728\u7f51\u7edc\u7684\u6700\u672b\u7aef\u4f7f\u7528\u5b83, \u4ece\u800c\u6545\u610f\u6dfb\u52a0\u4e00\u4e9b\u6df7\u4e71\u548c\u589e\u52a0\u91c7\u6837\u7684\u591a\u6837\u5316. import torch import torch.nn as nn from torch.autograd import Variable class RNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size) self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size) self.o2o = nn.Linear(hidden_size + output_size, output_size) self.dropout = nn.Dropout(0.1) self.softmax = nn.LogSoftmax(dim=1) def forward(self, category, input, hidden): input_combined = torch.cat((category, input, hidden), 1) hidden = self.i2h(input_combined) output = self.i2o(input_combined) output_combined = torch.cat((hidden, output), 1) output = self.o2o(output_combined) output = self.dropout(output) output = self.softmax(output) return output, hidden def initHidden(self): return Variable(torch.zeros(1, self.hidden_size)) \u8bad\u7ec3 \u8bad\u7ec3\u524d\u7684\u51c6\u5907 \u9996\u5148, \u5229\u7528\u8f85\u52a9\u51fd\u6570\u4ea7\u751f\u968f\u673a\u7684\uff08category, line\uff09\u5bf9: import random # \u4ecelist\u4e2d\u968f\u673a\u9009\u53d6\u9879 def randomChoice(l): return l[random.randint(0, len(l) - 1)] # \u83b7\u53d6\u968f\u673a\u7684\u7c7b\u522b\u548c\u8be5\u7c7b\u522b\u4e2d\u968f\u673a\u7684\u884c def randomTrainingPair(): category = randomChoice(all_categories) line = randomChoice(category_lines[category]) return category, line \u5bf9\u6bcf\u4e00\u4e2a\u65f6\u95f4\u70b9\uff08\u4e5f\u5c31\u662f\u8bf4\u5728\u8bad\u7ec3\u96c6\u4e2d\u8bcd\u7684\u6bcf\u4e2a\u5b57\u6bcd\uff09\u7f51\u7edc\u7684\u8f93\u5165\u662f (\u7c7b\u522b, \u5f53\u524d\u5b57\u6bcd, \u9690\u85cf\u5c42\u72b6\u6001) , \u8f93\u51fa\u662f (\u4e0b\u4e00\u4e2a\u5b57\u6bcd, \u4e0b\u4e00\u4e2a\u9690\u85cf\u5c42\u72b6\u6001) . \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u8bad\u7ec3\u96c6, \u6211\u4eec\u9700\u8981\u7684\u662f\u7c7b\u522b\u3001\u8f93\u5165\u7684\u5b57\u6bcd\u96c6\u3001\u8f93\u51fa/\u76ee\u6807\u5b57\u6bcd\u96c6. \u56e0\u4e3a\u5728\u6bcf\u4e00\u6b65, \u6211\u4eec\u4ece\u5f53\u524d\u7684\u5b57\u6bcd\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5b57\u6bcd, \u8fd9\u6837\u7684\u5b57\u6bcd\u5bf9\u662f\u5728\u539f\u6709\u884c\u4e2d\u8fde\u7eed\u5b57\u6bcd\u7684\u96c6\u5408, \u4f8b\u5982, \u5bf9\u4e8e \"ABCD&lt;EOS&gt;\" \u5c06\u4f1a\u4ea7\u751f (\u201cA\u201d, \u201cB\u201d), (\u201cB\u201d, \u201cC\u201d), (\u201cC\u201d, \u201cD\u201d), (\u201cD\u201d, \u201cEOS\u201d). \u7c7b\u522b\u5f20\u91cf\u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a &lt;1 x n_categories&gt; \u7684 one-hot tensor \u5f20\u91cf, \u5728\u8bad\u7ec3\u7684\u6bcf\u4e00\u4e2a\u65f6\u95f4\u70b9\u628a\u5b83\u5582\u7ed9\u7f51\u7edc \u2014\u2014 \u8fd9\u662f\u4e00\u4e2a\u8bbe\u8ba1\u7684\u9009\u62e9, \u5b83\u53ef\u4ee5\u88ab\u5f53\u4f5c\u4e3a\u521d\u59cb\u9690\u85cf\u72b6\u6216\u5176\u4ed6\u7b56\u7565\u7684\u4e00\u90e8\u5206. # \u7c7b\u522b\u7684 one-hot \u5411\u91cf def categoryTensor(category): li = all_categories.index(category) tensor = torch.zeros(1, n_categories) tensor[0][li] = 1 return tensor # \u8f93\u5165\u4e32\u4ece\u7b2c\u4e00\u4e2a\u5b57\u6bcd\u5230\u6700\u540e\u4e00\u4e2a\u5b57\u6bcd\uff08\u4e0d\u5305\u62ec EOS \uff09\u7684 one-hot \u77e9\u9635 def inputTensor(line): tensor = torch.zeros(len(line), 1, n_letters) for li in range(len(line)): letter = line[li] tensor[li][0][all_letters.find(letter)] = 1 return tensor # \u76ee\u6807\u7684\u7b2c\u4e8c\u4e2a\u5b57\u6bcd\u5230\u7ed3\u5c3e\uff08EOS\uff09\u7684 LongTensor def targetTensor(line): letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))] letter_indexes.append(n_letters - 1) # EOS return torch.LongTensor(letter_indexes) \u4e3a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u7684\u4fbf\u5229, \u6dfb\u52a0\u4e00\u4e2a randomTrainingExample \u51fd\u6570, \u83b7\u53d6\u968f\u673a\u7684 (category, line) \u5bf9, \u5e76\u628a\u4ed6\u4eec\u8f6c\u6362\u6210\u9700\u8981\u7684 (category, input, target) \u5f20\u91cf. # \u4ece\u968f\u673a\u7684\uff08category, line\uff09\u5bf9\u4e2d\u751f\u6210 category, input, and target \u5f20\u91cf def randomTrainingExample(): category, line = randomTrainingPair() category_tensor = Variable(categoryTensor(category)) input_line_tensor = Variable(inputTensor(line)) target_line_tensor = Variable(targetTensor(line)) return category_tensor, input_line_tensor, target_line_tensor \u7f51\u7edc\u7684\u8bad\u7ec3 \u4e0e\u5206\u7c7b\u76f8\u6bd4, \u5206\u7c7b\u53ea\u7528\u5230\u4e86\u6700\u540e\u7684\u8f93\u51fa, \u800c\u8fd9\u91cc\u6bcf\u4e2a\u6b65\u90fd\u4f1a\u4ea7\u751f\u4e00\u4e2a\u9884\u6d4b, \u6240\u4ee5\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u6bcf\u4e00\u6b65\u7684\u635f\u5931. \u81ea\u52a8\u6c42\u5bfc\uff08autograd\uff09\u7684\u9b54\u529b\u5c31\u5728\u4e8e, \u5b83\u5141\u8bb8\u5c06\u6bcf\u4e00\u6b65\u7684\u635f\u5931\u7b80\u5355\u7684\u52a0\u548c, \u5e76\u5728\u6700\u540e\u8c03\u7528 backward criterion = nn.NLLLoss() learning_rate = 0.0005 def train(category_tensor, input_line_tensor, target_line_tensor): hidden = rnn.initHidden() rnn.zero_grad() loss = 0 for i in range(input_line_tensor.size()[0]): output, hidden = rnn(category_tensor, input_line_tensor[i], hidden) loss += criterion(output, target_line_tensor[i]) loss.backward() for p in rnn.parameters(): p.data.add_(-learning_rate, p.grad.data) return output, loss.data[0] / input_line_tensor.size()[0] \u4e3a\u4e86\u8ddf\u8e2a\u8bad\u7ec3\u82b1\u8d39\u4e86\u591a\u957f\u65f6\u95f4, \u8fd9\u91cc\u6dfb\u52a0\u4e00\u4e2a timeSince(timestamp) \u51fd\u6570, \u8fd4\u56de\u4e00\u4e2a\u4eba\u4eec\u6613\u8bfb\u7684\u5b57\u7b26\u4e32: import time import math def timeSince(since): now = time.time() s = now - since m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) \u8bad\u7ec3\u548c\u5f80\u5e38\u4e00\u6837, \u4e0d\u505c\u7684\u8c03\u7528 train \u5e76\u7b49\u5f85\u4e00\u4f1a, \u6253\u5370\u5f53\u524d\u65f6\u95f4, \u6bcf\u9694 print_every \u4e2a\u4f8b\u5b50\u6253\u5370 loss, \u5c06\u6bcf plot_every \u4e2a\u4f8b\u5b50\u7684\u5e73\u5747\u635f\u5931\u4fdd\u5b58\u5728 all_losses \u4e2d\u4ee5\u4fbf\u540e\u9762\u753b\u56fe. rnn = RNN(n_letters, 128, n_letters) n_iters = 100000 print_every = 5000 plot_every = 500 all_losses = [] total_loss = 0 # \u6bcf plot_every \u6b21\u8fed\u4ee3\u9700\u8981\u91cd\u7f6e start = time.time() for iter in range(1, n_iters + 1): output, loss = train(*randomTrainingExample()) total_loss += loss if iter % print_every == 0: print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss)) if iter % plot_every == 0: all_losses.append(total_loss / plot_every) total_loss = 0 \u7ed8\u5236\u635f\u5931 \u4ece all_losses \u4e2d\u7ed8\u5236\u5386\u53f2\u635f\u5931, \u4ee5\u5c55\u73b0\u7f51\u7edc\u7684\u5b66\u4e60\u8fc7\u7a0b import matplotlib.pyplot as plt import matplotlib.ticker as ticker plt.figure() plt.plot(all_losses) \u7f51\u7edc\u91c7\u6837 \u4e3a\u4e86\u91c7\u6837, \u6211\u4eec\u7ed9\u7f51\u7edc\u4e00\u4e2a\u5b57\u6bcd\u5e76\u95ee\u4e0b\u4e00\u4e2a\u5b57\u6bcd\u662f\u4ec0\u4e48, \u91cd\u590d\u8fd9\u4e2a\u8fc7\u7a0b\u76f4\u5230 EOS \u6807\u8bb0. \u521b\u5efa\u8f93\u5165\u7c7b\u522b\u3001\u8d77\u59cb\u5b57\u6bcd\u548c\u9690\u85cf\u5c42\u72b6\u6001\u7684\u5f20\u91cf \u521b\u5efa\u4e00\u4e2a\u5e26\u6709\u8d77\u59cb\u5b57\u6bcd\u7684 output_name \u4e32 \u76f4\u5230\u6700\u5927\u7684\u8f93\u51fa\u957f\u5ea6, \u5f53\u524d\u5b57\u6bcd\u5582\u7ed9\u7f51\u7edc \u4ece\u6700\u9ad8\u7684\u8f93\u51fa\u83b7\u53d6\u4e0b\u4e00\u4e2a\u5b57\u6bcd\u548c\u4e0b\u4e00\u4e2a\u9690\u85cf\u5c42\u72b6\u6001 \u5982\u679c\u8f93\u51fa\u5b57\u6bcd\u662f EOS, \u7b97\u6cd5\u7ed3\u675f \u5982\u679c\u8f93\u51fa\u662f\u5e38\u89c4\u5b57\u6bcd, \u5c06\u5176\u52a0\u5165\u5230 output_name \u5e76\u7ee7\u7eed \u8fd4\u56de\u6700\u7ec8\u7684\u540d\u5b57 Note \u4e0e\u7ed9\u5b9a\u8d77\u59cb\u5b57\u6bcd\u4e0d\u540c\u7684\u662f, \u6709\u5176\u4ed6\u7684\u7b56\u7565\u662f\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u5305\u542b\u4e00\u4e2a\u201c\u4e32\u8d77\u59cb\u201d\u6807\u8bb0, \u8ba9\u7f51\u7edc\u9009\u62e9\u5c5e\u4e8e\u81ea\u5df1\u7684\u8d77\u59cb\u5b57\u6bcd. max_length = 20 # \u4ece\u7c7b\u522b\u548c\u8d77\u59cb\u5b57\u6bcd\u91c7\u6837 def sample(category, start_letter='A'): category_tensor = Variable(categoryTensor(category)) input = Variable(inputTensor(start_letter)) hidden = rnn.initHidden() output_name = start_letter for i in range(max_length): output, hidden = rnn(category_tensor, input[0], hidden) topv, topi = output.data.topk(1) topi = topi[0][0] if topi == n_letters - 1: break else: letter = all_letters[topi] output_name += letter input = Variable(inputTensor(letter)) return output_name # \u7ed9\u5b9a\u4e00\u4e2a\u7c7b\u522b\u548c\u591a\u4e2a\u8d77\u59cb\u5b57\u6bcd \u83b7\u53d6\u4e2a\u91c7\u6837\u7ed3\u679c def samples(category, start_letters='ABC'): for start_letter in start_letters: print(sample(category, start_letter)) samples('Russian', 'RUS') samples('German', 'GER') samples('Spanish', 'SPA') samples('Chinese', 'CHI') \u7ec3\u4e60 \u5c1d\u8bd5\u4f7f\u7528\u4e0d\u540c \u7c7b\u522b->\u884c \u6570\u636e\u96c6, \u4f8b\u5982: \u5c0f\u8bf4\u7cfb\u5217 -> \u89d2\u8272\u540d\u5b57 \u8bcd\u6027 -> \u8bcd\u8bed \u56fd\u5bb6 -> \u57ce\u5e02 \u4f7f\u7528\u201c\u4e32\u8d77\u59cb\u201d\u6807\u8bb0, \u4f7f\u91c7\u6837\u7684\u65f6\u5019\u4e0d\u7528\u7ed9\u5b9a\u8d77\u59cb\u5b57\u6bcd \u4f7f\u7528\u66f4\u5927\u548c/\u6216\u66f4\u597d\u7684\u7f51\u7edc\u7ed3\u6784\u83b7\u53d6\u66f4\u597d\u7684\u7ed3\u679c \u5c1d\u8bd5\u4e00\u4e0b nn.LSTM \u548c nn.GRU \u5c42 \u5c06\u8fd9\u4e9b RNNs \u7ec4\u5408\u6210\u66f4\u9ad8\u7ea7\u7684\u7f51\u7edc Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: char_rnn_generation_tutorial.py Download Jupyter notebook: char_rnn_generation_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u57fa\u4e0e\u5b57\u7b26\u7ea7RNN\uff08Char-RNN\uff09\u7684\u4eba\u540d\u751f\u6210"},{"location":"32/#rnnchar-rnn","text":"\u4f5c\u8005 : Sean Robertson \u5728 \u4e0a\u4e00\u4e2a\u6559\u7a0b \u91cc\u6211\u4eec\u4f7f\u7528RNN\u628a\u540d\u5b57\u5206\u7c7b\u5230\u5b83\u6240\u5c5e\u7684\u8bed\u8a00\u4e2d, \u8fd9\u6b21\u6211\u4eec\u6539\u53d8\u4e00\u4e0b\u6765\u5b66\u4e60\u4ece\u8bed\u8a00\u4e2d\u751f\u6210\u540d\u5b57. > python sample.py Russian RUS Rovakov Uantov Shavakov > python sample.py German GER Gerren Ereng Rosher > python sample.py Spanish SPA Salla Parer Allan > python sample.py Chinese CHI Chan Hang Iun \u6211\u4eec\u4ecd\u7136\u624b\u5de5\u642d\u5efa\u4e00\u4e2a\u5305\u542b\u51e0\u4e2a\u7ebf\u6027\u5c42\u7684\u5c0f\u7684RNN. \u8fd9\u6b21\u7684\u6700\u5927\u7684\u4e0d\u540c\u662f\u8f93\u5165\u4e00\u4e2a\u7c7b\u522b, \u6bcf\u6b21\u8f93\u51fa\u4e00\u4e2a\u5b57\u6bcd, \u800c\u4e0d\u662f\u8bfb\u5165\u6240\u6709\u540d\u5b57\u7684\u5b57\u6bcd\u6765\u9884\u6d4b\u4e00\u4e2a\u7c7b\u522b. \u5faa\u73af\u7684\u9884\u6d4b\u6bcf\u4e00\u4e2a\u5b57\u6bcd\u6765\u6784\u6210\u8bed\u8a00\uff08\u4e5f\u53ef\u4ee5\u7528\u6587 \u5b57\u6216\u8005\u5176\u4ed6\u66f4\u9ad8\u7ea7\u7684\u7ed3\u6784\u5b8c\u6210\uff09, \u901a\u5e38\u88ab\u79f0\u4e3a\u201c\u8bed\u8a00\u6a21\u578b\u201d. ** \u63a8\u8350\u9605\u8bfb: ** \u5047\u8bbe\u4f60\u81f3\u5c11\u5b89\u88c5\u4e86PyTorch, \u719f\u6089Python, \u7406\u89e3Tensors: http://pytorch.org/ : \u5b89\u88c5\u8bf4\u660e PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b \u83b7\u53d6\u4e00\u822c\u7684 PyTorch \u5165\u95e8 \u8ddf\u7740\u4f8b\u5b50\u5b66\u4e60 PyTorch \u5e7f\u6cdb\u4e14\u6df1\u5165\u7684\u6982\u8ff0 PyTorch for former Torch users \u5982\u679c\u66fe\u7ecf\u662f Lua Torch \u7684\u7528\u6237 \u4e0b\u9762\u8fd9\u4e9b\u5bf9\u4e86\u89e3 RNNs \u548c\u5176\u5de5\u4f5c\u539f\u7406\u4e5f\u662f\u5f88\u6709\u7528\u7684: The Unreasonable Effectiveness of Recurrent Neural Networks \u5c55\u793a\u4e86\u4e00\u7cfb\u5217\u771f\u5b9e\u751f\u6d3b\u4e2d\u7684\u4f8b\u5b50 Understanding LSTM Networks \u662f\u4e00\u7bc7\u7279\u522b\u5173\u4e8eLSTMs\u7684\u6587\u7ae0, \u4f46\u662f\u5bf9\u4e8e\u4e00\u822c\u7684RNNs\u4e5f\u5f88\u6709\u76ca\u7684 \u8fd8\u5efa\u8bae\u4e0a\u4e00\u4e2a\u6559\u7a0b: \u7528\u5b57\u7b26\u7ea7RNN\u5206\u7c7b\u540d\u79f0","title":"\u57fa\u4e0e\u5b57\u7b26\u7ea7RNN\uff08Char-RNN\uff09\u7684\u4eba\u540d\u751f\u6210"},{"location":"32/#_1","text":"Note \u4ece \u8fd9\u91cc \u4e0b\u8f7d\u6570\u636e, \u5e76\u89e3\u538b\u5230\u5f53\u524d\u76ee\u5f55. \u66f4\u591a\u7684\u7ec6\u8282\u53c2\u8003\u4e0a\u4e00\u4e2a\u6559\u7a0b, \u603b\u4e4b, \u6570\u636e\u542b\u6709\u4e00\u6279\u7eaf\u6587\u672c\u6587\u4ef6: data/names/[Language].txt \u6bcf\u4e00\u884c\u4e00\u4e2a\u4eba\u540d. \u5c06\u884c\u5206\u5272\u6210\u6570\u7ec4, \u5e76\u628a Unicode \u8f6c\u6362\u6210 ASCII \u7f16\u7801, \u6700\u540e\u653e\u8fdb\u4e00\u4e2a\u5b57\u5178\u91cc {language: [names ...]} . from __future__ import unicode_literals, print_function, division from io import open import glob import unicodedata import string all_letters = string.ascii_letters + \" .,;'-\" n_letters = len(all_letters) + 1 # \u6dfb\u52a0 EOS \u6807\u8bb0 def findFiles(path): return glob.glob(path) # \u5c06 Unicode \u5b57\u7b26\u4e32\u8f6c\u6362\u4e3a\u7eaf ASCII \u7f16\u7801, \u611f\u8c22 http://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters ) # \u8bfb\u53d6\u6587\u4ef6\u5e76\u5206\u5272\u6210\u884c def readLines(filename): lines = open(filename, encoding='utf-8').read().strip().split('\\n') return [unicodeToAscii(line) for line in lines] # \u6784\u5efa\u6620\u5c04\u5b57\u5178 category_lines , \u6bcf\u4e2a\u7c7b\u522b\u662f\u7531\u5f88\u591a\u4e2a\u884c\u7ec4\u6210\u7684list category_lines = {} all_categories = [] for filename in findFiles('data/names/*.txt'): category = filename.split('/')[-1].split('.')[0] all_categories.append(category) lines = readLines(filename) category_lines[category] = lines n_categories = len(all_categories) print('# categories:', n_categories, all_categories) print(unicodeToAscii(\"O'N\u00e9\u00e0l\"))","title":"\u6570\u636e\u51c6\u5907"},{"location":"32/#_2","text":"\u8fd9\u4e2a\u7f51\u7edc\u6269\u5c55\u4e86 \u4e0a\u4e00\u4e2a\u6559\u7a0b\u7684RNN , \u4e3a\u7c7b\u522b\u5f20\u91cf\u6dfb\u52a0\u4e86\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570, \u5e76\u548c\u5176\u4ed6\u7684\u53c2\u6570\u4e32\u8054\u5728\u4e00\u8d77. \u7c7b\u522b\u5f20\u91cf \u548c\u5b57\u6bcd\u7684\u8f93\u5165\u4e00\u6837\u662f one-hot \u5411\u91cf. \u6211\u4eec\u5c06\u8f93\u51fa\u89e3\u91ca\u6210\u4e3a\u4e0b\u4e00\u4e2a\u5b57\u6bcd\u7684\u6982\u7387, \u91c7\u6837\u7684\u65f6\u5019, \u6700\u6709\u53ef\u80fd\u7684\u8f93\u51fa\u88ab\u5f53\u505a\u4e0b\u4e00\u4e2a\u8f93\u5165. \u4e3a\u4e86\u8ba9\u7f51\u7edc\u66f4\u52a0\u6709\u6548\u5de5\u4f5c, \u6211\u6dfb\u52a0\u4e86\u7b2c\u4e8c\u4e2a\u7ebf\u6027\u5c42 o2o \uff08\u5728\u5408\u5e76\u4e86\u9690\u85cf\u5c42\u548c\u8f93\u51fa\u5c42\u7684\u540e\u9762\uff09. \u8fd8\u6709\u4e00\u4e2a Dropout \u5c42, \u4f7f\u8f93\u5165\u7684\u90e8\u5206\u503c\u4ee5\u7ed9\u5b9a\u7684\u6982\u7387\u503c\u968f\u673a\u7684\u53d8\u6210 0 \uff08\u8fd9\u91cc\u6982\u7387\u53d60.1\uff09, \u8fd9\u6837\u505a\u901a\u5e38\u662f\u4e3a\u4e86\u6a21\u7cca\u8f93\u5165\u4ee5\u9632\u6b62\u8fc7\u62df\u5408. \u8fd9\u91cc\u6211\u4eec\u5728\u7f51\u7edc\u7684\u6700\u672b\u7aef\u4f7f\u7528\u5b83, \u4ece\u800c\u6545\u610f\u6dfb\u52a0\u4e00\u4e9b\u6df7\u4e71\u548c\u589e\u52a0\u91c7\u6837\u7684\u591a\u6837\u5316. import torch import torch.nn as nn from torch.autograd import Variable class RNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size) self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size) self.o2o = nn.Linear(hidden_size + output_size, output_size) self.dropout = nn.Dropout(0.1) self.softmax = nn.LogSoftmax(dim=1) def forward(self, category, input, hidden): input_combined = torch.cat((category, input, hidden), 1) hidden = self.i2h(input_combined) output = self.i2o(input_combined) output_combined = torch.cat((hidden, output), 1) output = self.o2o(output_combined) output = self.dropout(output) output = self.softmax(output) return output, hidden def initHidden(self): return Variable(torch.zeros(1, self.hidden_size))","title":"\u521b\u5efa\u7f51\u7edc"},{"location":"32/#_3","text":"","title":"\u8bad\u7ec3"},{"location":"32/#_4","text":"\u9996\u5148, \u5229\u7528\u8f85\u52a9\u51fd\u6570\u4ea7\u751f\u968f\u673a\u7684\uff08category, line\uff09\u5bf9: import random # \u4ecelist\u4e2d\u968f\u673a\u9009\u53d6\u9879 def randomChoice(l): return l[random.randint(0, len(l) - 1)] # \u83b7\u53d6\u968f\u673a\u7684\u7c7b\u522b\u548c\u8be5\u7c7b\u522b\u4e2d\u968f\u673a\u7684\u884c def randomTrainingPair(): category = randomChoice(all_categories) line = randomChoice(category_lines[category]) return category, line \u5bf9\u6bcf\u4e00\u4e2a\u65f6\u95f4\u70b9\uff08\u4e5f\u5c31\u662f\u8bf4\u5728\u8bad\u7ec3\u96c6\u4e2d\u8bcd\u7684\u6bcf\u4e2a\u5b57\u6bcd\uff09\u7f51\u7edc\u7684\u8f93\u5165\u662f (\u7c7b\u522b, \u5f53\u524d\u5b57\u6bcd, \u9690\u85cf\u5c42\u72b6\u6001) , \u8f93\u51fa\u662f (\u4e0b\u4e00\u4e2a\u5b57\u6bcd, \u4e0b\u4e00\u4e2a\u9690\u85cf\u5c42\u72b6\u6001) . \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u8bad\u7ec3\u96c6, \u6211\u4eec\u9700\u8981\u7684\u662f\u7c7b\u522b\u3001\u8f93\u5165\u7684\u5b57\u6bcd\u96c6\u3001\u8f93\u51fa/\u76ee\u6807\u5b57\u6bcd\u96c6. \u56e0\u4e3a\u5728\u6bcf\u4e00\u6b65, \u6211\u4eec\u4ece\u5f53\u524d\u7684\u5b57\u6bcd\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5b57\u6bcd, \u8fd9\u6837\u7684\u5b57\u6bcd\u5bf9\u662f\u5728\u539f\u6709\u884c\u4e2d\u8fde\u7eed\u5b57\u6bcd\u7684\u96c6\u5408, \u4f8b\u5982, \u5bf9\u4e8e \"ABCD&lt;EOS&gt;\" \u5c06\u4f1a\u4ea7\u751f (\u201cA\u201d, \u201cB\u201d), (\u201cB\u201d, \u201cC\u201d), (\u201cC\u201d, \u201cD\u201d), (\u201cD\u201d, \u201cEOS\u201d). \u7c7b\u522b\u5f20\u91cf\u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a &lt;1 x n_categories&gt; \u7684 one-hot tensor \u5f20\u91cf, \u5728\u8bad\u7ec3\u7684\u6bcf\u4e00\u4e2a\u65f6\u95f4\u70b9\u628a\u5b83\u5582\u7ed9\u7f51\u7edc \u2014\u2014 \u8fd9\u662f\u4e00\u4e2a\u8bbe\u8ba1\u7684\u9009\u62e9, \u5b83\u53ef\u4ee5\u88ab\u5f53\u4f5c\u4e3a\u521d\u59cb\u9690\u85cf\u72b6\u6216\u5176\u4ed6\u7b56\u7565\u7684\u4e00\u90e8\u5206. # \u7c7b\u522b\u7684 one-hot \u5411\u91cf def categoryTensor(category): li = all_categories.index(category) tensor = torch.zeros(1, n_categories) tensor[0][li] = 1 return tensor # \u8f93\u5165\u4e32\u4ece\u7b2c\u4e00\u4e2a\u5b57\u6bcd\u5230\u6700\u540e\u4e00\u4e2a\u5b57\u6bcd\uff08\u4e0d\u5305\u62ec EOS \uff09\u7684 one-hot \u77e9\u9635 def inputTensor(line): tensor = torch.zeros(len(line), 1, n_letters) for li in range(len(line)): letter = line[li] tensor[li][0][all_letters.find(letter)] = 1 return tensor # \u76ee\u6807\u7684\u7b2c\u4e8c\u4e2a\u5b57\u6bcd\u5230\u7ed3\u5c3e\uff08EOS\uff09\u7684 LongTensor def targetTensor(line): letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))] letter_indexes.append(n_letters - 1) # EOS return torch.LongTensor(letter_indexes) \u4e3a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u7684\u4fbf\u5229, \u6dfb\u52a0\u4e00\u4e2a randomTrainingExample \u51fd\u6570, \u83b7\u53d6\u968f\u673a\u7684 (category, line) \u5bf9, \u5e76\u628a\u4ed6\u4eec\u8f6c\u6362\u6210\u9700\u8981\u7684 (category, input, target) \u5f20\u91cf. # \u4ece\u968f\u673a\u7684\uff08category, line\uff09\u5bf9\u4e2d\u751f\u6210 category, input, and target \u5f20\u91cf def randomTrainingExample(): category, line = randomTrainingPair() category_tensor = Variable(categoryTensor(category)) input_line_tensor = Variable(inputTensor(line)) target_line_tensor = Variable(targetTensor(line)) return category_tensor, input_line_tensor, target_line_tensor","title":"\u8bad\u7ec3\u524d\u7684\u51c6\u5907"},{"location":"32/#_5","text":"\u4e0e\u5206\u7c7b\u76f8\u6bd4, \u5206\u7c7b\u53ea\u7528\u5230\u4e86\u6700\u540e\u7684\u8f93\u51fa, \u800c\u8fd9\u91cc\u6bcf\u4e2a\u6b65\u90fd\u4f1a\u4ea7\u751f\u4e00\u4e2a\u9884\u6d4b, \u6240\u4ee5\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u6bcf\u4e00\u6b65\u7684\u635f\u5931. \u81ea\u52a8\u6c42\u5bfc\uff08autograd\uff09\u7684\u9b54\u529b\u5c31\u5728\u4e8e, \u5b83\u5141\u8bb8\u5c06\u6bcf\u4e00\u6b65\u7684\u635f\u5931\u7b80\u5355\u7684\u52a0\u548c, \u5e76\u5728\u6700\u540e\u8c03\u7528 backward criterion = nn.NLLLoss() learning_rate = 0.0005 def train(category_tensor, input_line_tensor, target_line_tensor): hidden = rnn.initHidden() rnn.zero_grad() loss = 0 for i in range(input_line_tensor.size()[0]): output, hidden = rnn(category_tensor, input_line_tensor[i], hidden) loss += criterion(output, target_line_tensor[i]) loss.backward() for p in rnn.parameters(): p.data.add_(-learning_rate, p.grad.data) return output, loss.data[0] / input_line_tensor.size()[0] \u4e3a\u4e86\u8ddf\u8e2a\u8bad\u7ec3\u82b1\u8d39\u4e86\u591a\u957f\u65f6\u95f4, \u8fd9\u91cc\u6dfb\u52a0\u4e00\u4e2a timeSince(timestamp) \u51fd\u6570, \u8fd4\u56de\u4e00\u4e2a\u4eba\u4eec\u6613\u8bfb\u7684\u5b57\u7b26\u4e32: import time import math def timeSince(since): now = time.time() s = now - since m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) \u8bad\u7ec3\u548c\u5f80\u5e38\u4e00\u6837, \u4e0d\u505c\u7684\u8c03\u7528 train \u5e76\u7b49\u5f85\u4e00\u4f1a, \u6253\u5370\u5f53\u524d\u65f6\u95f4, \u6bcf\u9694 print_every \u4e2a\u4f8b\u5b50\u6253\u5370 loss, \u5c06\u6bcf plot_every \u4e2a\u4f8b\u5b50\u7684\u5e73\u5747\u635f\u5931\u4fdd\u5b58\u5728 all_losses \u4e2d\u4ee5\u4fbf\u540e\u9762\u753b\u56fe. rnn = RNN(n_letters, 128, n_letters) n_iters = 100000 print_every = 5000 plot_every = 500 all_losses = [] total_loss = 0 # \u6bcf plot_every \u6b21\u8fed\u4ee3\u9700\u8981\u91cd\u7f6e start = time.time() for iter in range(1, n_iters + 1): output, loss = train(*randomTrainingExample()) total_loss += loss if iter % print_every == 0: print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss)) if iter % plot_every == 0: all_losses.append(total_loss / plot_every) total_loss = 0","title":"\u7f51\u7edc\u7684\u8bad\u7ec3"},{"location":"32/#_6","text":"\u4ece all_losses \u4e2d\u7ed8\u5236\u5386\u53f2\u635f\u5931, \u4ee5\u5c55\u73b0\u7f51\u7edc\u7684\u5b66\u4e60\u8fc7\u7a0b import matplotlib.pyplot as plt import matplotlib.ticker as ticker plt.figure() plt.plot(all_losses)","title":"\u7ed8\u5236\u635f\u5931"},{"location":"32/#_7","text":"\u4e3a\u4e86\u91c7\u6837, \u6211\u4eec\u7ed9\u7f51\u7edc\u4e00\u4e2a\u5b57\u6bcd\u5e76\u95ee\u4e0b\u4e00\u4e2a\u5b57\u6bcd\u662f\u4ec0\u4e48, \u91cd\u590d\u8fd9\u4e2a\u8fc7\u7a0b\u76f4\u5230 EOS \u6807\u8bb0. \u521b\u5efa\u8f93\u5165\u7c7b\u522b\u3001\u8d77\u59cb\u5b57\u6bcd\u548c\u9690\u85cf\u5c42\u72b6\u6001\u7684\u5f20\u91cf \u521b\u5efa\u4e00\u4e2a\u5e26\u6709\u8d77\u59cb\u5b57\u6bcd\u7684 output_name \u4e32 \u76f4\u5230\u6700\u5927\u7684\u8f93\u51fa\u957f\u5ea6, \u5f53\u524d\u5b57\u6bcd\u5582\u7ed9\u7f51\u7edc \u4ece\u6700\u9ad8\u7684\u8f93\u51fa\u83b7\u53d6\u4e0b\u4e00\u4e2a\u5b57\u6bcd\u548c\u4e0b\u4e00\u4e2a\u9690\u85cf\u5c42\u72b6\u6001 \u5982\u679c\u8f93\u51fa\u5b57\u6bcd\u662f EOS, \u7b97\u6cd5\u7ed3\u675f \u5982\u679c\u8f93\u51fa\u662f\u5e38\u89c4\u5b57\u6bcd, \u5c06\u5176\u52a0\u5165\u5230 output_name \u5e76\u7ee7\u7eed \u8fd4\u56de\u6700\u7ec8\u7684\u540d\u5b57 Note \u4e0e\u7ed9\u5b9a\u8d77\u59cb\u5b57\u6bcd\u4e0d\u540c\u7684\u662f, \u6709\u5176\u4ed6\u7684\u7b56\u7565\u662f\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u5305\u542b\u4e00\u4e2a\u201c\u4e32\u8d77\u59cb\u201d\u6807\u8bb0, \u8ba9\u7f51\u7edc\u9009\u62e9\u5c5e\u4e8e\u81ea\u5df1\u7684\u8d77\u59cb\u5b57\u6bcd. max_length = 20 # \u4ece\u7c7b\u522b\u548c\u8d77\u59cb\u5b57\u6bcd\u91c7\u6837 def sample(category, start_letter='A'): category_tensor = Variable(categoryTensor(category)) input = Variable(inputTensor(start_letter)) hidden = rnn.initHidden() output_name = start_letter for i in range(max_length): output, hidden = rnn(category_tensor, input[0], hidden) topv, topi = output.data.topk(1) topi = topi[0][0] if topi == n_letters - 1: break else: letter = all_letters[topi] output_name += letter input = Variable(inputTensor(letter)) return output_name # \u7ed9\u5b9a\u4e00\u4e2a\u7c7b\u522b\u548c\u591a\u4e2a\u8d77\u59cb\u5b57\u6bcd \u83b7\u53d6\u4e2a\u91c7\u6837\u7ed3\u679c def samples(category, start_letters='ABC'): for start_letter in start_letters: print(sample(category, start_letter)) samples('Russian', 'RUS') samples('German', 'GER') samples('Spanish', 'SPA') samples('Chinese', 'CHI')","title":"\u7f51\u7edc\u91c7\u6837"},{"location":"32/#_8","text":"\u5c1d\u8bd5\u4f7f\u7528\u4e0d\u540c \u7c7b\u522b->\u884c \u6570\u636e\u96c6, \u4f8b\u5982: \u5c0f\u8bf4\u7cfb\u5217 -> \u89d2\u8272\u540d\u5b57 \u8bcd\u6027 -> \u8bcd\u8bed \u56fd\u5bb6 -> \u57ce\u5e02 \u4f7f\u7528\u201c\u4e32\u8d77\u59cb\u201d\u6807\u8bb0, \u4f7f\u91c7\u6837\u7684\u65f6\u5019\u4e0d\u7528\u7ed9\u5b9a\u8d77\u59cb\u5b57\u6bcd \u4f7f\u7528\u66f4\u5927\u548c/\u6216\u66f4\u597d\u7684\u7f51\u7edc\u7ed3\u6784\u83b7\u53d6\u66f4\u597d\u7684\u7ed3\u679c \u5c1d\u8bd5\u4e00\u4e0b nn.LSTM \u548c nn.GRU \u5c42 \u5c06\u8fd9\u4e9b RNNs \u7ec4\u5408\u6210\u66f4\u9ad8\u7ea7\u7684\u7f51\u7edc Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: char_rnn_generation_tutorial.py Download Jupyter notebook: char_rnn_generation_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u7ec3\u4e60"},{"location":"33/","text":"\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684seq2seq\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u7ffb\u8bd1 \u4f5c\u8005 : Sean Robertson \u8fd9\u4e2a\u6559\u7a0b\u4e3b\u8981\u8bb2\u89e3\u7528\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u5c06\u6cd5\u8bed\u7ffb\u8bd1\u6210\u82f1\u8bed. [KEY: > input, = target, < output] > il est en train de peindre un tableau . = he is painting a picture . < he is painting a picture . > pourquoi ne pas essayer ce vin delicieux ? = why not try that delicious wine ? < why not try that delicious wine ? > elle n est pas poete mais romanciere . = she is not a poet but a novelist . < she not not a poet but a novelist . > vous etes trop maigre . = you re too skinny . < you re all alone . \u2026 \u53d6\u5f97\u4e0d\u540c\u9636\u6bb5\u7684\u6210\u529f. \u8fd9\u662f\u901a\u8fc7 seq2seq\u7f51\u7edc &lt;[http://arxiv.org/abs/1409.3215](http://arxiv.org/abs/1409.3215)&gt; __\u5b9e\u73b0\u7684\u7b80\u5355\u5374\u5f3a\u5927\u7684\u60f3\u6cd5, \u901a\u8fc7\u4e24\u4e2a\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u4e00\u8d77\u5de5\u4f5c\u5b9e\u73b0\u5c06\u4e00\u4e2a\u5e8f\u5217\u8f6c\u6362\u4e3a\u53e6\u4e00\u4e2a.\u4e00\u4e2a\u7f16\u7801\u5668\u7f51\u7edc\u5c06\u8f93\u5165\u5e8f\u5217\u538b \u7f29\u6210\u5411\u91cf,\u89e3\u7801\u5668\u7f51\u7edc\u5c06\u8be5\u77e2\u91cf\u5c55\u5f00\u4e3a\u65b0\u7684\u5e8f\u5217. \u4e3a\u4e86\u6539\u8fdb\u8fd9\u4e2a\u6a21\u578b,\u6211\u4eec\u5c06\u4f7f\u7528\u4e00\u79cd \u6ce8\u610f\u529b\u673a\u5236&lt;[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)&gt; __, \u5b83\u53ef\u4ee5\u8ba9\u89e3\u7801\u5668\u5b66\u4e60\u5c06\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u8f93\u5165\u5e8f\u5217\u7684\u7279\u5b9a\u8303\u56f4\u4e0a. \u63a8\u8350\u9605\u8bfb: \u6211\u4eec\u5047\u8bbe\u4f60\u81f3\u5c11\u5df2\u7ecf\u5b89\u88c5\u4e86PyTorch,\u4e86\u89e3Python,\u5e76\u4e14\u4e86\u89e3\u5f20\u91cf: http://pytorch.org/ PyTorch\u5b89\u88c5\u8bf4\u660e PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b \u5f00\u59cb\u4f7f\u7528PyTorch \u8ddf\u7740\u4f8b\u5b50\u5b66\u4e60 PyTorch \u8fdb\u884c\u5e7f\u6cdb\u800c\u6df1\u5165\u7684\u4e86\u89e3 PyTorch for former Torch users \u5982\u679c\u4f60\u662f\u524dLua Torch\u7528\u6237 \u8fd9\u4e9b\u5185\u5bb9\u4e5f\u6709\u5229\u4e8e\u4e86\u89e3seq2seq\u7f51\u7edc\u548c\u5176\u5de5\u4f5c\u673a\u5236: \u7528RNN\u7f16\u7801\u5668 - \u89e3\u7801\u5668\u6765\u5b66\u4e60\u7528\u4e8e\u7edf\u8ba1\u673a\u5668\u7ffb\u8bd1\u7684\u77ed\u8bed\u8868\u793a \u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884cseq2seq\u5b66\u4e60 \u795e\u7ecf\u7f51\u7edc\u673a\u5668\u7ffb\u8bd1\u8054\u5408\u5b66\u4e60\u5bf9\u9f50\u548c\u7ffb\u8bd1 \u795e\u7ecf\u4f1a\u8bdd\u6a21\u578b \u4f60\u8fd8\u53ef\u4ee5\u627e\u5230\u4ee5\u524d\u7684\u6559\u7a0b\u5173\u4e8eCharacter-Level RNN\u540d\u79f0\u5206\u7c7b \u7528\u5b57\u7b26\u7ea7RNN\u5206\u7c7b\u540d\u79f0 \u548c\u751f\u6210\u540d\u79f0 \u57fa\u4e0e\u5b57\u7b26\u7ea7RNN\uff08Char-RNN\uff09\u7684\u4eba\u540d\u751f\u6210 \u8fd9\u4e9b\u6982\u5ff5\u4e0e\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u975e\u5e38\u76f8\u4f3c. \u66f4\u591a\u5185\u5bb9\u8bf7\u9605\u8bfb\u4ecb\u7ecd\u8fd9\u4e9b\u4e3b\u9898\u7684\u8bba\u6587: \u7528RNN\u7f16\u7801\u5668 - \u89e3\u7801\u5668\u6765\u5b66\u4e60\u7528\u4e8e\u7edf\u8ba1\u673a\u5668\u7ffb\u8bd1\u7684\u77ed\u8bed\u8868\u793a \u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884cseq2seq\u5b66\u4e60 \u795e\u7ecf\u7f51\u7edc\u673a\u5668\u7ffb\u8bd1\u8054\u5408\u5b66\u4e60\u5bf9\u9f50\u548c\u7ffb\u8bd1 \u795e\u7ecf\u4f1a\u8bdd\u6a21\u578b \u8981\u6c42 from __future__ import unicode_literals, print_function, division from io import open import unicodedata import string import re import random import torch import torch.nn as nn from torch.autograd import Variable from torch import optim import torch.nn.functional as F use_cuda = torch.cuda.is_available() \u52a0\u8f7d\u6570\u636e\u6587\u4ef6 \u8fd9\u4e2a\u9879\u76ee\u7684\u6570\u636e\u662f\u4e00\u7ec4\u6570\u4ee5\u5343\u8ba1\u7684\u82f1\u8bed\u5230\u6cd5\u8bed\u7684\u7ffb\u8bd1\u5bf9. [ ](#id2)\u8fd9\u4e2a\u95ee\u9898\u5728 Open Data Stack Exchange\u4e0a <[http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages](http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages)> __ \u6307\u5bfc\u6211\u4eec\u4f7f\u7528\u5f00\u653e\u7684\u7ffb\u8bd1\u7f51\u7ad9 http://tatoeba.org/ \u53ef\u4e0b\u8f7d\u5730\u5740\u4e3a http://tatoeba.org/eng/downloads - \u66f4\u597d\u7684\u662f, \u6709\u4eba\u505a\u4e86\u989d\u5916\u7684\u5de5\u4f5c,\u5207\u5206\u8bed\u8a00\u5bf9\u5230\u5355\u4e2a\u6587\u672c\u6587\u4ef6\u4e2d: http://www.manythings.org/anki/ \u82f1\u6587\u5230\u6cd5\u6587\u5bf9\u592a\u5927\u800c\u4e0d\u80fd\u5305\u542b\u5728repo\u4e2d,\u56e0\u6b64\u5f00\u59cb\u524d\u8bf7\u4e0b\u8f7d data/eng-fra.txt . \u8be5\u6587\u4ef6\u662f\u4e00\u4e2a\u5236\u8868\u7b26\u5206\u9694\u7684\u7ffb\u8bd1\u5bf9\u5217\u8868: : I am cold. Je suis froid. Note \u4e0b\u8f7d\u6570\u636e\u6587\u4ef6\u5728 \u8fd9\u91cc \u5e76\u89e3\u538b\u5230\u6b63\u786e\u7684\u8def\u5f84. \u4e0echaracter-level RNN\u6559\u7a0b\u4e2d\u4f7f\u7528\u7684\u5b57\u7b26\u7f16\u7801\u7c7b\u4f3c,\u6211\u4eec\u5c06\u7528\u8bed\u8a00\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd \u4f5c\u4e3a\u72ec\u70ed\u5411\u91cf,\u6216\u8005\u9664\u4e86\u5355\u4e2a\u5355\u8bcd\u4e4b\u5916(\u5728\u5355\u8bcd\u7684\u7d22\u5f15\u5904)\u7684\u5927\u7684\u96f6\u5411\u91cf. \u76f8\u8f83\u4e8e\u53ef\u80fd \u5b58\u5728\u4e8e\u4e00\u79cd\u8bed\u8a00\u4e2d\u4ec5\u6709\u5341\u4e2a\u5b57\u7b26\u76f8\u6bd4,\u591a\u6570\u90fd\u662f\u6709\u5927\u91cf\u7684\u5b57,\u56e0\u6b64\u7f16\u7801\u5411\u91cf\u5f88\u5927. \u7136\u800c,\u6211\u4eec\u4f1a\u6b3a\u9a97\u6027\u7684\u505a\u4e00\u4e9b\u6570\u636e\u4fee\u526a,\u4fdd\u8bc1\u6bcf\u79cd\u8bed\u8a00\u53ea\u4f7f\u7528\u51e0\u5343\u5b57. \u6211\u4eec\u9700\u8981\u6bcf\u4e2a\u5355\u8bcd\u5bf9\u5e94\u552f\u4e00\u7684\u7d22\u5f15\u4f5c\u4e3a\u7a0d\u540e\u7684\u7f51\u7edc\u8f93\u5165\u548c\u76ee\u6807.\u4e3a\u4e86\u8ffd\u8e2a\u8fd9\u4e9b\u7d22\u5f15\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u5e2e\u52a9\u7c7b Lang \u7c7b\u4e2d\u6709 \u8bcd \u2192 \u7d22\u5f15 ( word2index ) \u548c \u7d22\u5f15 \u2192 \u8bcd ( index2word ) \u7684\u5b57\u5178, \u4ee5\u53ca\u6bcf\u4e2a\u8bcd word2count \u7528\u6765\u66ff\u6362\u7a00\u758f\u8bcd\u6c47. SOS_token = 0 EOS_token = 1 class Lang: def __init__(self, name): self.name = name self.word2index = {} self.word2count = {} self.index2word = {0: \"SOS\", 1: \"EOS\"} self.n_words = 2 # Count SOS and EOS def addSentence(self, sentence): for word in sentence.split(' '): self.addWord(word) def addWord(self, word): if word not in self.word2index: self.word2index[word] = self.n_words self.word2count[word] = 1 self.index2word[self.n_words] = word self.n_words += 1 else: self.word2count[word] += 1 \u8fd9\u4e9b\u6587\u4ef6\u5168\u90e8\u91c7\u7528Unicode\u7f16\u7801,\u4e3a\u4e86\u7b80\u5316\u6211\u4eec\u5c06Unicode\u5b57\u7b26\u8f6c\u6362\u4e3aASCII, \u4f7f\u6240\u6709\u5185\u5bb9\u5c0f\u5199,\u5e76\u4fee\u526a\u5927\u90e8\u5206\u6807\u70b9\u7b26\u53f7. # \u611f\u8c22\u60a8\u5c06Unicode\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3a\u7eafASCII # http://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' ) # \u5c0f\u5199,\u4fee\u526a\u548c\u5220\u9664\u975e\u5b57\u6bcd\u5b57\u7b26 def normalizeString(s): s = unicodeToAscii(s.lower().strip()) s = re.sub(r\"([.!?])\", r\" \\1\", s) s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) return s \u8981\u8bfb\u53d6\u6570\u636e\u6587\u4ef6,\u6211\u4eec\u5c06\u628a\u6587\u4ef6\u5206\u6210\u884c,\u7136\u540e\u5c06\u884c\u6210\u5bf9\u5206\u5f00. \u8fd9\u4e9b\u6587\u4ef6\u90fd\u662f\u82f1\u6587\u2192\u5176\u4ed6\u8bed\u8a00,\u6240\u4ee5\u5982\u679c\u6211\u4eec\u60f3\u4ece\u5176\u4ed6\u8bed\u8a00\u7ffb\u8bd1\u2192\u82f1\u6587,\u6211\u4eec\u6dfb\u52a0\u4e86 \u7ffb\u8f6c\u6807\u5fd7 [ ](#id5)reverse \u6765\u7ffb\u8f6c\u8bcd\u8bed\u5bf9. def readLangs(lang1, lang2, reverse=False): print(\"Reading lines...\") # \u8bfb\u53d6\u6587\u4ef6\u5e76\u6309\u884c\u5206\u5f00 lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\ read().strip().split('\\n') # \u5c06\u6bcf\u4e00\u884c\u5206\u6210\u4e24\u5217\u5e76\u8fdb\u884c\u6807\u51c6\u5316 pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines] # \u7ffb\u8f6c\u5bf9,Lang\u5b9e\u4f8b\u5316 if reverse: pairs = [list(reversed(p)) for p in pairs] input_lang = Lang(lang2) output_lang = Lang(lang1) else: input_lang = Lang(lang1) output_lang = Lang(lang2) return input_lang, output_lang, pairs \u7531\u4e8e\u6709\u5f88\u591a\u4f8b\u53e5,\u6211\u4eec\u5e0c\u671b\u5feb\u901f\u8bad\u7ec3,\u6211\u4eec\u4f1a\u5c06\u6570\u636e\u96c6\u88c1\u526a\u4e3a\u76f8\u5bf9\u7b80\u77ed\u7684\u53e5\u5b50. \u8fd9\u91cc\u7684\u5355\u8bcd\u7684\u6700\u5927\u957f\u5ea6\u662f10\u8bcd(\u5305\u62ec\u7ed3\u675f\u6807\u70b9\u7b26\u53f7),\u6211\u4eec\u6b63\u5728\u8fc7\u6ee4\u5230\u7ffb\u8bd1 \u6210\u201dI am\u201d\u6216\u201dHe is\u201d\u7b49\u5f62\u5f0f\u7684\u53e5\u5b50.(\u8003\u8651\u5230\u5148\u524d\u66ff\u6362\u4e86\u6487\u53f7). MAX_LENGTH = 10 eng_prefixes = ( \"i am \", \"i m \", \"he is\", \"he s \", \"she is\", \"she s\", \"you are\", \"you re \", \"we are\", \"we re \", \"they are\", \"they re \" ) def filterPair(p): return len(p[0].split(' ')) < MAX_LENGTH and \\ len(p[1].split(' ')) < MAX_LENGTH and \\ p[1].startswith(eng_prefixes) def filterPairs(pairs): return [pair for pair in pairs if filterPair(pair)] \u5b8c\u6574\u7684\u51c6\u5907\u6570\u636e\u7684\u8fc7\u7a0b: \u52a0\u8f7d\u6587\u672c\u6587\u4ef6\u5207\u5206\u6210\u884c,\u5e76\u5207\u5206\u6210\u5355\u8bcd\u5bf9: \u6587\u672c\u5f52\u4e00\u5316, \u6309\u7167\u957f\u5ea6\u548c\u5185\u5bb9\u8fc7\u6ee4 \u4ece\u6210\u5bf9\u7684\u53e5\u5b50\u4e2d\u5236\u4f5c\u5355\u8bcd\u5217\u8868 def prepareData(lang1, lang2, reverse=False): input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse) print(\"Read %s sentence pairs\" % len(pairs)) pairs = filterPairs(pairs) print(\"Trimmed to %s sentence pairs\" % len(pairs)) print(\"Counting words...\") for pair in pairs: input_lang.addSentence(pair[0]) output_lang.addSentence(pair[1]) print(\"Counted words:\") print(input_lang.name, input_lang.n_words) print(output_lang.name, output_lang.n_words) return input_lang, output_lang, pairs input_lang, output_lang, pairs = prepareData('eng', 'fra', True) print(random.choice(pairs)) Seq2Seq\u6a21\u578b \u9012\u5f52\u795e\u7ecf\u7f51\u7edc(RNN),\u662f\u4e00\u4e2a\u6309\u7167\u4e00\u4e2a\u5e8f\u5217\u8fdb\u884c\u64cd\u4f5c\u7684\u7f51\u8def,\u5e76 \u5c06\u5176\u81ea\u5df1\u7684\u8f93\u51fa\u7528\u4f5c\u540e\u7eed\u6b65\u9aa4\u7684\u8f93\u5165. \u4e00\u4e2a \u5e8f\u5217\u5230\u5e8f\u5217\u7f51\u7edc , \u6216 seq2seq \u7f51\u7edc, \u6216 \u7f16\u7801\u89e3\u7801\u5668\u7f51\u7edc , \u662f\u7531\u4e24\u4e2a\u79f0\u4e3a\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684RNN\u7ec4\u6210\u7684\u6a21\u578b. \u7f16\u7801\u5668\u8bfb\u53d6\u8f93\u5165\u5e8f\u5217\u5e76\u8f93\u51fa\u5355\u4e2a\u5411\u91cf, \u89e3\u7801\u5668\u8bfb\u53d6\u8be5\u5411\u91cf\u4ee5\u4ea7\u751f\u8f93\u51fa\u5e8f\u5217. \u4e0e\u5355\u4e2aRNN\u7684\u5e8f\u5217\u9884\u6d4b\u4e0d\u540c,\u6bcf\u4e2a\u8f93\u5165\u5bf9\u5e94\u4e00\u4e2a\u8f93\u51fa, seq2seq\u6a21\u578b\u5c06\u6211\u4eec\u4ece\u5e8f\u5217\u957f\u5ea6\u548c\u987a\u5e8f\u4e2d\u89e3\u653e\u51fa\u6765, \u8fd9\u4f7f\u5f97\u5b83\u6210\u4e3a\u4e24\u79cd\u8bed\u8a00\u4e4b\u95f4\u7ffb\u8bd1\u7684\u7406\u60f3\u9009\u62e9. \u8003\u8651\u8fd9\u53e5\u8bdd \u201cJe ne suis pas le chat noir\u201d \u2192 \u201cI am not the black cat\u201d. \u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u5927\u90e8\u5206\u5355\u8bcd\u5728\u8f93\u51fa\u53e5\u5b50\u4e2d\u6709\u76f4\u63a5\u7ffb\u8bd1, \u4f46\u987a\u5e8f\u7565\u6709\u4e0d\u540c,\u4f8b\u5982: \u201cchat noir\u201d \u548c \u201cblack cat\u201d. \u7531\u4e8e \u201cne/pas\u201d\u7ed3\u6784, \u5176\u4e2d\u53e6\u4e00\u4e2a\u5355\u8bcd\u5728\u8f93\u5165\u7684\u53e5\u5b50\u4e2d. \u76f4\u63a5\u4ece\u8f93\u5165\u8bcd\u7684\u5e8f\u5217\u4e2d\u76f4\u63a5\u751f\u6210\u6b63\u786e\u7684\u7ffb\u8bd1\u662f\u5f88\u56f0\u96be\u7684. \u4f7f\u7528seq2seq\u6a21\u578b,\u7f16\u7801\u5668\u4f1a\u521b\u5efa\u4e00\u4e2a\u5355\u72ec\u7684\u5411\u91cf, \u5728\u7406\u60f3\u60c5\u51b5\u4e0b,\u5b83\u5c06\u8f93\u5165\u5e8f\u5217\u7684\u201d\u542b\u4e49\u201d\u7f16\u7801\u4e3a\u5355\u4e2a\u5411\u91cf - \u53e5\u5b50\u7684N\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e00\u4e2a\u70b9. \u7f16\u7801\u5668 seq2seq\u7f51\u7edc\u7684\u7f16\u7801\u5668\u662f\u4e00\u4e2aRNN,\u5b83\u4e3a\u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd\u8f93\u51fa\u4e00\u4e9b\u503c. \u5bf9\u4e8e\u6bcf\u4e2a\u8f93\u5165\u5b57,\u7f16\u7801\u5668\u8f93\u51fa\u4e00\u4e2a\u5411\u91cf\u548c\u4e00\u4e2a\u9690\u85cf\u72b6\u6001,\u5e76\u5c06\u9690\u85cf\u72b6\u6001\u7528\u4e8e\u4e0b\u4e00\u4e2a\u8f93\u5165\u5b57. class EncoderRNN(nn.Module): def __init__(self, input_size, hidden_size): super(EncoderRNN, self).__init__() self.hidden_size = hidden_size self.embedding = nn.Embedding(input_size, hidden_size) self.gru = nn.GRU(hidden_size, hidden_size) def forward(self, input, hidden): embedded = self.embedding(input).view(1, 1, -1) output = embedded output, hidden = self.gru(output, hidden) return output, hidden def initHidden(self): result = Variable(torch.zeros(1, 1, self.hidden_size)) if use_cuda: return result.cuda() else: return result \u89e3\u7801\u5668 \u89e3\u7801\u5668\u662f\u53e6\u4e00\u4e2aRNN,\u5b83\u63a5\u6536\u7f16\u7801\u5668\u8f93\u51fa\u5411\u91cf\u5e76\u8f93\u51fa\u4e00\u4e2a\u5355\u8bcd\u5e8f\u5217\u6765\u521b\u5efa\u7ffb\u8bd1. \u7b80\u5355\u7684\u89e3\u7801\u5668 \u5728\u6700\u7b80\u5355\u7684seq2seq\u89e3\u7801\u5668\u4e2d,\u6211\u4eec\u53ea\u4f7f\u7528\u7f16\u7801\u5668\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa. \u8fd9\u4e2a\u6700\u540e\u7684\u8f93\u51fa\u6709\u65f6\u79f0\u4e3a\u4e0a\u4e0b\u6587\u5411\u91cf,\u56e0\u4e3a\u5b83\u4ece\u6574\u4e2a\u5e8f\u5217\u7f16\u7801\u4e0a\u4e0b\u6587. \u8be5\u4e0a\u4e0b\u6587\u5411\u91cf\u88ab\u7528\u4f5c\u89e3\u7801\u5668\u7684\u521d\u59cb\u9690\u85cf\u72b6\u6001. \u5728\u89e3\u7801\u7684\u6bcf\u4e00\u6b65,\u89e3\u7801\u5668\u90fd\u88ab\u8d4b\u4e88\u4e00\u4e2a\u8f93\u5165\u6307\u4ee4\u548c\u9690\u85cf\u72b6\u6001. \u521d\u59cb\u8f93\u5165\u6307\u4ee4\u5b57\u7b26\u4e32\u5f00\u59cb\u7684 &lt;SOS&gt; \u6307\u4ee4,\u7b2c\u4e00\u4e2a\u9690\u85cf\u72b6\u6001\u662f\u4e0a\u4e0b\u6587\u5411\u91cf(\u7f16\u7801\u5668\u7684\u6700\u540e\u9690\u85cf\u72b6\u6001). class DecoderRNN(nn.Module): def __init__(self, hidden_size, output_size): super(DecoderRNN, self).__init__() self.hidden_size = hidden_size self.embedding = nn.Embedding(output_size, hidden_size) self.gru = nn.GRU(hidden_size, hidden_size) self.out = nn.Linear(hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): output = self.embedding(input).view(1, 1, -1) output = F.relu(output) output, hidden = self.gru(output, hidden) output = self.softmax(self.out(output[0])) return output, hidden def initHidden(self): result = Variable(torch.zeros(1, 1, self.hidden_size)) if use_cuda: return result.cuda() else: return result \u6211\u4eec\u9f13\u52b1\u4f60\u8bad\u7ec3\u548c\u89c2\u5bdf\u8fd9\u4e2a\u6a21\u578b\u7684\u7ed3\u679c,\u4f46\u4e3a\u4e86\u8282\u7701\u7a7a\u95f4,\u6211\u4eec\u5c06\u76f4\u63a5\u8fdb\u6b63\u9898\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236. \u6ce8\u610f\u529b\u89e3\u7801\u5668 \u5982\u679c\u4ec5\u5728\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e4b\u95f4\u4f20\u9012\u4e0a\u4e0b\u6587\u5411\u91cf,\u5219\u8be5\u5355\u4e2a\u5411\u91cf\u627f\u62c5\u7f16\u7801\u6574\u4e2a\u53e5\u5b50\u7684\u8d1f\u62c5. \u6ce8\u610f\u529b\u5141\u8bb8\u89e3\u7801\u5668\u7f51\u7edc\u9488\u5bf9\u89e3\u7801\u5668\u81ea\u8eab\u8f93\u51fa\u7684\u6bcf\u4e00\u6b65\u201d\u805a\u7126\u201d\u7f16\u7801\u5668\u8f93\u51fa\u7684\u4e0d\u540c\u90e8\u5206. \u9996\u5148\u6211\u4eec\u8ba1\u7b97\u4e00\u7ec4\u6ce8\u610f\u529b\u6743\u91cd. \u8fd9\u4e9b\u5c06\u88ab\u4e58\u4ee5\u7f16\u7801\u5668\u8f93\u51fa\u77e2\u91cf\u83b7\u5f97\u52a0\u6743\u7684\u7ec4\u5408. \u7ed3\u679c(\u5728\u4ee3\u7801\u4e2d\u4e3a attn_applied ) \u5e94\u8be5\u5305\u542b\u5173\u4e8e\u8f93\u5165\u5e8f\u5217\u7684\u7279\u5b9a\u90e8\u5206\u7684\u4fe1\u606f, \u4ece\u800c\u5e2e\u52a9\u89e3\u7801\u5668\u9009\u62e9\u6b63\u786e\u7684\u8f93\u51fa\u5355\u8bcd. \u4f7f\u7528\u89e3\u7801\u5668\u7684\u8f93\u5165\u548c\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u8f93\u5165,\u5229\u7528\u53e6\u4e00\u4e2a\u524d\u9988\u5c42 [ ](#id12)attn \u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd, \u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u6709\u5404\u79cd\u5927\u5c0f\u7684\u53e5\u5b50,\u4e3a\u4e86\u5b9e\u9645\u521b\u5efa\u548c\u8bad\u7ec3\u6b64\u5c42, \u6211\u4eec\u5fc5\u987b\u9009\u62e9\u6700\u5927\u957f\u5ea6\u7684\u53e5\u5b50(\u8f93\u5165\u957f\u5ea6,\u7528\u4e8e\u7f16\u7801\u5668\u8f93\u51fa),\u4ee5\u9002\u7528\u4e8e\u6b64\u5c42. \u6700\u5927\u957f\u5ea6\u7684\u53e5\u5b50\u5c06\u4f7f\u7528\u6240\u6709\u6ce8\u610f\u529b\u6743\u91cd,\u800c\u8f83\u77ed\u7684\u53e5\u5b50\u53ea\u4f7f\u7528\u524d\u51e0\u4e2a. class AttnDecoderRNN(nn.Module): def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH): super(AttnDecoderRNN, self).__init__() self.hidden_size = hidden_size self.output_size = output_size self.dropout_p = dropout_p self.max_length = max_length self.embedding = nn.Embedding(self.output_size, self.hidden_size) self.attn = nn.Linear(self.hidden_size * 2, self.max_length) self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size) self.dropout = nn.Dropout(self.dropout_p) self.gru = nn.GRU(self.hidden_size, self.hidden_size) self.out = nn.Linear(self.hidden_size, self.output_size) def forward(self, input, hidden, encoder_outputs): embedded = self.embedding(input).view(1, 1, -1) embedded = self.dropout(embedded) attn_weights = F.softmax( self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1) attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)) output = torch.cat((embedded[0], attn_applied[0]), 1) output = self.attn_combine(output).unsqueeze(0) output = F.relu(output) output, hidden = self.gru(output, hidden) output = F.log_softmax(self.out(output[0]), dim=1) return output, hidden, attn_weights def initHidden(self): result = Variable(torch.zeros(1, 1, self.hidden_size)) if use_cuda: return result.cuda() else: return result Note \u8fd8\u6709\u5176\u4ed6\u5f62\u5f0f\u7684\u6ce8\u610f\u529b\u901a\u8fc7\u4f7f\u7528\u76f8\u5bf9\u4f4d\u7f6e\u65b9\u6cd5\u6765\u89e3\u51b3\u957f\u5ea6\u9650\u5236. \u9605\u8bfb\u5173\u4e8e \u201clocal attention\u201d \u5728 \u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7684\u6709\u6548\u9014\u5f84 .\u4e3a\u4e86\u8bad\u7ec3,\u5bf9\u4e8e\u6bcf\u4e00\u5bf9\u6211\u4eec\u5c06\u9700\u8981\u8f93\u5165\u7684\u5f20\u91cf(\u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u8bcd\u7684\u7d22\u5f15)\u548c \u76ee\u6807\u5f20\u91cf(\u76ee\u6807\u8bed\u53e5\u4e2d\u7684\u8bcd\u7684\u7d22\u5f15). \u5728\u521b\u5efa\u8fd9\u4e9b\u5411\u91cf\u65f6,\u6211\u4eec\u4f1a\u5c06EOS\u6807\u8bb0\u6dfb\u52a0\u5230\u4e24\u4e2a\u5e8f\u5217\u4e2d. def indexesFromSentence(lang, sentence): return [lang.word2index[word] for word in sentence.split(' ')] def variableFromSentence(lang, sentence): indexes = indexesFromSentence(lang, sentence) indexes.append(EOS_token) result = Variable(torch.LongTensor(indexes).view(-1, 1)) if use_cuda: return result.cuda() else: return result def variablesFromPair(pair): input_variable = variableFromSentence(input_lang, pair[0]) target_variable = variableFromSentence(output_lang, pair[1]) return (input_variable, target_variable) \u8bad\u7ec3\u6a21\u578b \u4e3a\u4e86\u8bad\u7ec3\u6211\u4eec\u901a\u8fc7\u7f16\u7801\u5668\u8fd0\u884c\u8f93\u5165\u53e5\u5b50,\u5e76\u8ddf\u8e2a\u6bcf\u4e2a\u8f93\u51fa\u548c\u6700\u65b0\u7684\u9690\u85cf\u72b6\u6001. \u7136\u540e\u89e3\u7801\u5668\u88ab\u8d4b\u4e88 &lt;SOS&gt; \u6307\u4ee4\u4f5c\u4e3a\u5176\u7b2c\u4e00\u4e2a\u8f93\u5165, \u5e76\u5c06\u7f16\u7801\u5668\u7684\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u5176\u7b2c\u4e00\u4e2a\u9690\u85cf\u72b6\u6001. \u201cTeacher forcing\u201d \u662f\u5c06\u5b9e\u9645\u76ee\u6807\u8f93\u51fa\u7528\u4f5c\u6bcf\u4e2a\u4e0b\u4e00\u4e2a\u8f93\u5165\u7684\u6982\u5ff5,\u800c\u4e0d\u662f\u5c06\u89e3\u7801\u5668\u7684 \u731c\u6d4b\u7528\u4f5c\u4e0b\u4e00\u4e2a\u8f93\u5165.\u4f7f\u7528\u6559\u5e08\u5f3a\u8feb\u4f1a\u4f7f\u5176\u66f4\u5feb\u5730\u6536\u655b,\u4f46\u662f \u5f53\u8bad\u7ec3\u597d\u7684\u7f51\u7edc\u88ab\u5229\u7528\u65f6,\u5b83\u53ef\u80fd\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u6027. . \u4f60\u53ef\u4ee5\u89c2\u5bdf\u6559\u5e08\u5f3a\u8feb\u7f51\u7edc\u7684\u8f93\u51fa,\u8fd9\u4e9b\u7f51\u7edc\u662f\u7528\u8fde\u8d2f\u7684\u8bed\u6cd5\u9605\u8bfb\u7684,\u4f46\u5374\u8fdc\u79bb\u4e86\u6b63\u786e\u7684\u7ffb\u8bd1 - \u76f4\u89c2\u5730\u6765\u770b\u5b83\u5df2\u7ecf\u5b66\u4f1a\u4e86\u4ee3\u8868\u8f93\u51fa\u8bed\u6cd5,\u5e76\u4e14\u4e00\u65e6\u8001\u5e08\u544a\u8bc9\u5b83\u524d\u51e0\u4e2a\u5355\u8bcd,\u5c31\u53ef\u4ee5\u201d\u62fe\u53d6\u201d\u5b83\u7684\u610f\u601d, \u4f46\u5b83\u6ca1\u6709\u9002\u5f53\u5730\u5b66\u4f1a\u5982\u4f55\u4ece\u7ffb\u8bd1\u4e2d\u521b\u5efa\u53e5\u5b50. \u7531\u4e8ePyTorch\u7684autograd\u7ed9\u6211\u4eec\u7684\u81ea\u7531,\u6211\u4eec\u53ef\u4ee5\u968f\u610f\u9009\u62e9\u4f7f\u7528\u8001\u5e08\u5f3a\u5236\u6216\u4e0d\u4f7f\u7528\u7b80\u5355\u7684if\u8bed\u53e5. \u6253\u5f00 teacher_forcing_ratio \u66f4\u591a\u7684\u4f7f\u7528\u5b83. teacher_forcing_ratio = 0.5 def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH): encoder_hidden = encoder.initHidden() encoder_optimizer.zero_grad() decoder_optimizer.zero_grad() input_length = input_variable.size()[0] target_length = target_variable.size()[0] encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size)) encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs loss = 0 for ei in range(input_length): encoder_output, encoder_hidden = encoder( input_variable[ei], encoder_hidden) encoder_outputs[ei] = encoder_output[0][0] decoder_input = Variable(torch.LongTensor([[SOS_token]])) decoder_input = decoder_input.cuda() if use_cuda else decoder_input decoder_hidden = encoder_hidden use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False if use_teacher_forcing: # \u6559\u5e08\u5f3a\u5236: \u5c06\u76ee\u6807\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u8f93\u5165 for di in range(target_length): decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs) loss += criterion(decoder_output, target_variable[di]) decoder_input = target_variable[di] # Teacher forcing else: # \u6ca1\u6709\u6559\u5e08\u5f3a\u8feb: \u4f7f\u7528\u81ea\u5df1\u7684\u9884\u6d4b\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u8f93\u5165 for di in range(target_length): decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs) topv, topi = decoder_output.data.topk(1) ni = topi[0][0] decoder_input = Variable(torch.LongTensor([[ni]])) decoder_input = decoder_input.cuda() if use_cuda else decoder_input loss += criterion(decoder_output, target_variable[di]) if ni == EOS_token: break loss.backward() encoder_optimizer.step() decoder_optimizer.step() return loss.data[0] / target_length \u6839\u636e\u5f53\u524d\u65f6\u95f4\u548c\u8fdb\u5ea6\u767e\u5206\u6bd4,\u8fd9\u662f\u4e00\u4e2a\u5e2e\u52a9\u529f\u80fd,\u7528\u4e8e\u6253\u5370\u7ecf\u8fc7\u7684\u65f6\u95f4\u548c\u4f30\u8ba1\u7684\u5269\u4f59\u65f6\u95f4. import time import math def asMinutes(s): m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) def timeSince(since, percent): now = time.time() s = now - since es = s / (percent) rs = es - s return '%s (- %s)' % (asMinutes(s), asMinutes(rs)) \u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u5982\u4e0b\u6240\u793a: \u542f\u52a8\u4e00\u4e2a\u8ba1\u65f6\u5668 \u521d\u59cb\u5316\u4f18\u5316\u5668\u548c\u6807\u51c6 \u521b\u5efa\u4e00\u7ec4\u8bad\u7ec3\u5bf9 \u4e3a\u7ed8\u56fe\u5efa\u7a7a\u635f\u5931\u6570\u7ec4 \u7136\u540e\u6211\u4eec\u591a\u6b21\u8c03\u7528 train ,\u5076\u5c14\u6253\u5370\u8fdb\u5ea6(\u6837\u672c\u7684\u767e\u5206\u6bd4,\u5230\u76ee\u524d\u4e3a\u6b62\u7684\u65f6\u95f4,\u4f30\u8ba1\u7684\u65f6\u95f4)\u548c\u5e73\u5747\u635f\u5931. def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01): start = time.time() plot_losses = [] print_loss_total = 0 # Reset every print_every plot_loss_total = 0 # Reset every plot_every encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate) training_pairs = [variablesFromPair(random.choice(pairs)) for i in range(n_iters)] criterion = nn.NLLLoss() for iter in range(1, n_iters + 1): training_pair = training_pairs[iter - 1] input_variable = training_pair[0] target_variable = training_pair[1] loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) print_loss_total += loss plot_loss_total += loss if iter % print_every == 0: print_loss_avg = print_loss_total / print_every print_loss_total = 0 print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg)) if iter % plot_every == 0: plot_loss_avg = plot_loss_total / plot_every plot_losses.append(plot_loss_avg) plot_loss_total = 0 showPlot(plot_losses) \u7ed8\u5236\u7ed3\u679c \u4f7f\u7528matplotlib\u5b8c\u6210\u7ed8\u56fe, \u4f7f\u7528\u8bad\u7ec3\u65f6\u4fdd\u5b58\u7684\u635f\u5931\u503c plot_losses \u6570\u7ec4. import matplotlib.pyplot as plt import matplotlib.ticker as ticker import numpy as np def showPlot(points): plt.figure() fig, ax = plt.subplots() # \u8fd9\u4e2a\u5b9a\u4f4d\u5668\u4f1a\u5b9a\u671f\u53d1\u51fa\u63d0\u793a\u4fe1\u606f loc = ticker.MultipleLocator(base=0.2) ax.yaxis.set_major_locator(loc) plt.plot(points) \u8bc4\u4f30 \u8bc4\u4f30\u4e0e\u8bad\u7ec3\u5927\u90e8\u5206\u76f8\u540c,\u4f46\u6ca1\u6709\u76ee\u6807,\u56e0\u6b64\u6211\u4eec\u53ea\u662f\u5c06\u89e3\u7801\u5668\u7684\u6bcf\u4e00\u6b65\u9884\u6d4b\u53cd\u9988\u7ed9\u5b83\u81ea\u8eab. \u6bcf\u5f53\u5b83\u9884\u6d4b\u5230\u4e00\u4e2a\u5355\u8bcd\u65f6,\u6211\u4eec\u5c31\u4f1a\u5c06\u5b83\u6dfb\u52a0\u5230\u8f93\u51fa\u5b57\u7b26\u4e32\u4e2d,\u5e76\u4e14\u5982\u679c\u5b83\u9884\u6d4b\u5230\u6211\u4eec\u5728\u90a3\u91cc\u505c\u6b62\u7684EOS\u6307\u4ee4. \u6211\u4eec\u8fd8\u5b58\u50a8\u89e3\u7801\u5668\u7684\u6ce8\u610f\u529b\u8f93\u51fa\u4ee5\u4f9b\u7a0d\u540e\u663e\u793a. def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH): input_variable = variableFromSentence(input_lang, sentence) input_length = input_variable.size()[0] encoder_hidden = encoder.initHidden() encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size)) encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs for ei in range(input_length): encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden) encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0] decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS decoder_input = decoder_input.cuda() if use_cuda else decoder_input decoder_hidden = encoder_hidden decoded_words = [] decoder_attentions = torch.zeros(max_length, max_length) for di in range(max_length): decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs) decoder_attentions[di] = decoder_attention.data topv, topi = decoder_output.data.topk(1) ni = topi[0][0] if ni == EOS_token: decoded_words.append('<EOS>') break else: decoded_words.append(output_lang.index2word[ni]) decoder_input = Variable(torch.LongTensor([[ni]])) decoder_input = decoder_input.cuda() if use_cuda else decoder_input return decoded_words, decoder_attentions[:di + 1] \u6211\u4eec\u53ef\u4ee5\u4ece\u8bad\u7ec3\u96c6\u4e2d\u8bc4\u4f30\u968f\u673a\u7684\u53e5\u5b50\u5e76\u6253\u5370\u51fa\u8f93\u5165,\u76ee\u6807\u548c\u8f93\u51fa\u4ee5\u4f5c\u51fa\u4e00\u4e9b\u4e3b\u89c2\u8d28\u91cf\u5224\u65ad: def evaluateRandomly(encoder, decoder, n=10): for i in range(n): pair = random.choice(pairs) print('>', pair[0]) print('=', pair[1]) output_words, attentions = evaluate(encoder, decoder, pair[0]) output_sentence = ' '.join(output_words) print('<', output_sentence) print('') \u8bad\u7ec3\u548c\u8bc4\u4f30 \u6709\u4e86\u6240\u6709\u8fd9\u4e9b\u8f85\u52a9\u529f\u80fd(\u5b83\u770b\u8d77\u6765\u50cf\u662f\u989d\u5916\u7684\u5de5\u4f5c,\u4f46\u5b83\u4f7f\u8fd0\u884c\u591a\u4e2a\u5b9e\u9a8c\u66f4\u5bb9\u6613), \u6211\u4eec\u5c31\u7acb\u9a6c\u53ef\u4ee5\u521d\u59cb\u5316\u7f51\u7edc\u5e76\u5f00\u59cb\u57f9\u8bad. \u8bf7\u8bb0\u4f4f\u8f93\u5165\u53e5\u5b50\u88ab\u4e25\u91cd\u8fc7\u6ee4, \u5bf9\u4e8e\u8fd9\u4e2a\u5c0f\u6570\u636e\u96c6,\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5305\u542b256\u4e2a\u9690\u85cf\u8282\u70b9 \u548c\u5355\u4e2aGRU\u5c42\u7684\u76f8\u5bf9\u8f83\u5c0f\u7684\u7f51\u7edc.\u5728MacBook CPU\u4e0a\u7ea640\u5206\u949f\u540e,\u6211\u4eec\u4f1a\u5f97\u5230\u4e00\u4e9b\u5408\u7406\u7684\u7ed3\u679c. Note \u5982\u679c\u4f60\u8fd0\u884c\u8fd9\u4e2anotebook,\u4f60\u53ef\u4ee5\u8bad\u7ec3,\u6253\u65ad\u5185\u6838,\u8bc4\u4f30\u5e76\u5728\u4ee5\u540e\u7ee7\u7eed\u8bad\u7ec3. \u6ce8\u91ca\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u521d\u59cb\u5316\u7684\u884c\u5e76\u518d\u6b21\u8fd0\u884c trainIters . hidden_size = 256 encoder1 = EncoderRNN(input_lang.n_words, hidden_size) attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1) if use_cuda: encoder1 = encoder1.cuda() attn_decoder1 = attn_decoder1.cuda() trainIters(encoder1, attn_decoder1, 75000, print_every=5000) evaluateRandomly(encoder1, attn_decoder1) \u53ef\u89c6\u5316\u6ce8\u610f\u529b \u6ce8\u610f\u529b\u673a\u5236\u7684\u4e00\u4e2a\u6709\u7528\u7279\u6027\u662f\u5176\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u8f93\u51fa. \u7531\u4e8e\u5b83\u7528\u4e8e\u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u7279\u5b9a\u7f16\u7801\u5668\u8f93\u51fa\u8fdb\u884c\u52a0\u6743,\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u60f3\u8c61\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e2d\u67e5\u770b\u7f51\u7edc\u6700\u5173\u6ce8\u7684\u4f4d\u7f6e. \u60a8\u53ef\u4ee5\u7b80\u5355\u5730\u8fd0\u884c plt.matshow(attentions) ,\u5c06\u6ce8\u610f\u529b\u8f93\u51fa\u663e\u793a\u4e3a\u77e9\u9635, \u5176\u4e2d\u5217\u662f\u8f93\u5165\u6b65\u9aa4,\u884c\u662f\u8f93\u51fa\u6b65\u9aa4. output_words, attentions = evaluate( encoder1, attn_decoder1, \"je suis trop froid .\") plt.matshow(attentions.numpy()) \u4e3a\u4e86\u83b7\u5f97\u66f4\u597d\u7684\u89c2\u770b\u4f53\u9a8c,\u6211\u4eec\u5c06\u989d\u5916\u6dfb\u52a0\u8f74\u548c\u6807\u7b7e: def showAttention(input_sentence, output_words, attentions): # \u7528\u989c\u8272\u6761\u8bbe\u7f6e\u56fe\u5f62 fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(attentions.numpy(), cmap='bone') fig.colorbar(cax) # \u8bbe\u7f6e\u8f74 ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90) ax.set_yticklabels([''] + output_words) # \u5728\u6bcf\u4e2a\u6253\u52fe\u5904\u663e\u793a\u6807\u7b7e ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) plt.show() def evaluateAndShowAttention(input_sentence): output_words, attentions = evaluate( encoder1, attn_decoder1, input_sentence) print('input =', input_sentence) print('output =', ' '.join(output_words)) showAttention(input_sentence, output_words, attentions) evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\") evaluateAndShowAttention(\"elle est trop petit .\") evaluateAndShowAttention(\"je ne crains pas de mourir .\") evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\") \u7ec3\u4e60 \u5c1d\u8bd5\u4f7f\u7528\u4e0d\u540c\u7684\u6570\u636e\u96c6 \u53e6\u4e00\u79cd\u8bed\u8a00\u5bf9 \u4eba \u2192 \u673a\u5668 (\u4f8b\u5982. IOT \u547d\u4ee4) \u804a\u5929 \u2192 \u54cd\u5e94 \u95ee\u9898 \u2192 \u56de\u7b54 \u7528\u9884\u5148\u8bad\u7ec3\u7684\u8bcd\u5d4c\u5165\u66ff\u6362\u5d4c\u5165,\u4f8b\u5982word2vec\u6216GloVe \u5c1d\u8bd5\u66f4\u591a\u56fe\u5c42,\u66f4\u591a\u9690\u85cf\u5355\u4f4d\u548c\u66f4\u591a\u53e5\u5b50. \u6bd4\u8f83\u8bad\u7ec3\u65f6\u95f4\u548c\u7ed3\u679c. \u5982\u679c\u60a8\u4f7f\u7528\u7684\u7ffb\u8bd1\u6587\u4ef6\u5bf9\u4e2d\u6709\u4e24\u4e2a\u76f8\u540c\u7684\u77ed\u8bed( I am test \\t I am test ), \u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u4f5c\u4e3a\u81ea\u52a8\u7f16\u7801\u5668.\u5c1d\u8bd5\u8fd9\u4e2a: - \u8bad\u7ec3\u81ea\u7f16\u7801\u5668 - \u53ea\u4fdd\u5b58\u7f16\u7801\u5668\u7f51\u7edc - \u4ece\u90a3\u91cc\u8bad\u7ec3\u4e00\u4e2a\u65b0\u7684\u89e3\u7801\u5668\u8fdb\u884c\u7ffb\u8bd1 Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: seq2seq_translation_tutorial.py Download Jupyter notebook: seq2seq_translation_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684seq2seq\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u7ffb\u8bd1"},{"location":"33/#seq2seq","text":"\u4f5c\u8005 : Sean Robertson \u8fd9\u4e2a\u6559\u7a0b\u4e3b\u8981\u8bb2\u89e3\u7528\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u5c06\u6cd5\u8bed\u7ffb\u8bd1\u6210\u82f1\u8bed. [KEY: > input, = target, < output] > il est en train de peindre un tableau . = he is painting a picture . < he is painting a picture . > pourquoi ne pas essayer ce vin delicieux ? = why not try that delicious wine ? < why not try that delicious wine ? > elle n est pas poete mais romanciere . = she is not a poet but a novelist . < she not not a poet but a novelist . > vous etes trop maigre . = you re too skinny . < you re all alone . \u2026 \u53d6\u5f97\u4e0d\u540c\u9636\u6bb5\u7684\u6210\u529f. \u8fd9\u662f\u901a\u8fc7 seq2seq\u7f51\u7edc &lt;[http://arxiv.org/abs/1409.3215](http://arxiv.org/abs/1409.3215)&gt; __\u5b9e\u73b0\u7684\u7b80\u5355\u5374\u5f3a\u5927\u7684\u60f3\u6cd5, \u901a\u8fc7\u4e24\u4e2a\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u4e00\u8d77\u5de5\u4f5c\u5b9e\u73b0\u5c06\u4e00\u4e2a\u5e8f\u5217\u8f6c\u6362\u4e3a\u53e6\u4e00\u4e2a.\u4e00\u4e2a\u7f16\u7801\u5668\u7f51\u7edc\u5c06\u8f93\u5165\u5e8f\u5217\u538b \u7f29\u6210\u5411\u91cf,\u89e3\u7801\u5668\u7f51\u7edc\u5c06\u8be5\u77e2\u91cf\u5c55\u5f00\u4e3a\u65b0\u7684\u5e8f\u5217. \u4e3a\u4e86\u6539\u8fdb\u8fd9\u4e2a\u6a21\u578b,\u6211\u4eec\u5c06\u4f7f\u7528\u4e00\u79cd \u6ce8\u610f\u529b\u673a\u5236&lt;[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)&gt; __, \u5b83\u53ef\u4ee5\u8ba9\u89e3\u7801\u5668\u5b66\u4e60\u5c06\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u8f93\u5165\u5e8f\u5217\u7684\u7279\u5b9a\u8303\u56f4\u4e0a. \u63a8\u8350\u9605\u8bfb: \u6211\u4eec\u5047\u8bbe\u4f60\u81f3\u5c11\u5df2\u7ecf\u5b89\u88c5\u4e86PyTorch,\u4e86\u89e3Python,\u5e76\u4e14\u4e86\u89e3\u5f20\u91cf: http://pytorch.org/ PyTorch\u5b89\u88c5\u8bf4\u660e PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b \u5f00\u59cb\u4f7f\u7528PyTorch \u8ddf\u7740\u4f8b\u5b50\u5b66\u4e60 PyTorch \u8fdb\u884c\u5e7f\u6cdb\u800c\u6df1\u5165\u7684\u4e86\u89e3 PyTorch for former Torch users \u5982\u679c\u4f60\u662f\u524dLua Torch\u7528\u6237 \u8fd9\u4e9b\u5185\u5bb9\u4e5f\u6709\u5229\u4e8e\u4e86\u89e3seq2seq\u7f51\u7edc\u548c\u5176\u5de5\u4f5c\u673a\u5236: \u7528RNN\u7f16\u7801\u5668 - \u89e3\u7801\u5668\u6765\u5b66\u4e60\u7528\u4e8e\u7edf\u8ba1\u673a\u5668\u7ffb\u8bd1\u7684\u77ed\u8bed\u8868\u793a \u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884cseq2seq\u5b66\u4e60 \u795e\u7ecf\u7f51\u7edc\u673a\u5668\u7ffb\u8bd1\u8054\u5408\u5b66\u4e60\u5bf9\u9f50\u548c\u7ffb\u8bd1 \u795e\u7ecf\u4f1a\u8bdd\u6a21\u578b \u4f60\u8fd8\u53ef\u4ee5\u627e\u5230\u4ee5\u524d\u7684\u6559\u7a0b\u5173\u4e8eCharacter-Level RNN\u540d\u79f0\u5206\u7c7b \u7528\u5b57\u7b26\u7ea7RNN\u5206\u7c7b\u540d\u79f0 \u548c\u751f\u6210\u540d\u79f0 \u57fa\u4e0e\u5b57\u7b26\u7ea7RNN\uff08Char-RNN\uff09\u7684\u4eba\u540d\u751f\u6210 \u8fd9\u4e9b\u6982\u5ff5\u4e0e\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u975e\u5e38\u76f8\u4f3c. \u66f4\u591a\u5185\u5bb9\u8bf7\u9605\u8bfb\u4ecb\u7ecd\u8fd9\u4e9b\u4e3b\u9898\u7684\u8bba\u6587: \u7528RNN\u7f16\u7801\u5668 - \u89e3\u7801\u5668\u6765\u5b66\u4e60\u7528\u4e8e\u7edf\u8ba1\u673a\u5668\u7ffb\u8bd1\u7684\u77ed\u8bed\u8868\u793a \u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884cseq2seq\u5b66\u4e60 \u795e\u7ecf\u7f51\u7edc\u673a\u5668\u7ffb\u8bd1\u8054\u5408\u5b66\u4e60\u5bf9\u9f50\u548c\u7ffb\u8bd1 \u795e\u7ecf\u4f1a\u8bdd\u6a21\u578b \u8981\u6c42 from __future__ import unicode_literals, print_function, division from io import open import unicodedata import string import re import random import torch import torch.nn as nn from torch.autograd import Variable from torch import optim import torch.nn.functional as F use_cuda = torch.cuda.is_available()","title":"\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684seq2seq\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u7ffb\u8bd1"},{"location":"33/#_1","text":"\u8fd9\u4e2a\u9879\u76ee\u7684\u6570\u636e\u662f\u4e00\u7ec4\u6570\u4ee5\u5343\u8ba1\u7684\u82f1\u8bed\u5230\u6cd5\u8bed\u7684\u7ffb\u8bd1\u5bf9. [ ](#id2)\u8fd9\u4e2a\u95ee\u9898\u5728 Open Data Stack Exchange\u4e0a <[http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages](http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages)> __ \u6307\u5bfc\u6211\u4eec\u4f7f\u7528\u5f00\u653e\u7684\u7ffb\u8bd1\u7f51\u7ad9 http://tatoeba.org/ \u53ef\u4e0b\u8f7d\u5730\u5740\u4e3a http://tatoeba.org/eng/downloads - \u66f4\u597d\u7684\u662f, \u6709\u4eba\u505a\u4e86\u989d\u5916\u7684\u5de5\u4f5c,\u5207\u5206\u8bed\u8a00\u5bf9\u5230\u5355\u4e2a\u6587\u672c\u6587\u4ef6\u4e2d: http://www.manythings.org/anki/ \u82f1\u6587\u5230\u6cd5\u6587\u5bf9\u592a\u5927\u800c\u4e0d\u80fd\u5305\u542b\u5728repo\u4e2d,\u56e0\u6b64\u5f00\u59cb\u524d\u8bf7\u4e0b\u8f7d data/eng-fra.txt . \u8be5\u6587\u4ef6\u662f\u4e00\u4e2a\u5236\u8868\u7b26\u5206\u9694\u7684\u7ffb\u8bd1\u5bf9\u5217\u8868: : I am cold. Je suis froid. Note \u4e0b\u8f7d\u6570\u636e\u6587\u4ef6\u5728 \u8fd9\u91cc \u5e76\u89e3\u538b\u5230\u6b63\u786e\u7684\u8def\u5f84. \u4e0echaracter-level RNN\u6559\u7a0b\u4e2d\u4f7f\u7528\u7684\u5b57\u7b26\u7f16\u7801\u7c7b\u4f3c,\u6211\u4eec\u5c06\u7528\u8bed\u8a00\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd \u4f5c\u4e3a\u72ec\u70ed\u5411\u91cf,\u6216\u8005\u9664\u4e86\u5355\u4e2a\u5355\u8bcd\u4e4b\u5916(\u5728\u5355\u8bcd\u7684\u7d22\u5f15\u5904)\u7684\u5927\u7684\u96f6\u5411\u91cf. \u76f8\u8f83\u4e8e\u53ef\u80fd \u5b58\u5728\u4e8e\u4e00\u79cd\u8bed\u8a00\u4e2d\u4ec5\u6709\u5341\u4e2a\u5b57\u7b26\u76f8\u6bd4,\u591a\u6570\u90fd\u662f\u6709\u5927\u91cf\u7684\u5b57,\u56e0\u6b64\u7f16\u7801\u5411\u91cf\u5f88\u5927. \u7136\u800c,\u6211\u4eec\u4f1a\u6b3a\u9a97\u6027\u7684\u505a\u4e00\u4e9b\u6570\u636e\u4fee\u526a,\u4fdd\u8bc1\u6bcf\u79cd\u8bed\u8a00\u53ea\u4f7f\u7528\u51e0\u5343\u5b57. \u6211\u4eec\u9700\u8981\u6bcf\u4e2a\u5355\u8bcd\u5bf9\u5e94\u552f\u4e00\u7684\u7d22\u5f15\u4f5c\u4e3a\u7a0d\u540e\u7684\u7f51\u7edc\u8f93\u5165\u548c\u76ee\u6807.\u4e3a\u4e86\u8ffd\u8e2a\u8fd9\u4e9b\u7d22\u5f15\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u5e2e\u52a9\u7c7b Lang \u7c7b\u4e2d\u6709 \u8bcd \u2192 \u7d22\u5f15 ( word2index ) \u548c \u7d22\u5f15 \u2192 \u8bcd ( index2word ) \u7684\u5b57\u5178, \u4ee5\u53ca\u6bcf\u4e2a\u8bcd word2count \u7528\u6765\u66ff\u6362\u7a00\u758f\u8bcd\u6c47. SOS_token = 0 EOS_token = 1 class Lang: def __init__(self, name): self.name = name self.word2index = {} self.word2count = {} self.index2word = {0: \"SOS\", 1: \"EOS\"} self.n_words = 2 # Count SOS and EOS def addSentence(self, sentence): for word in sentence.split(' '): self.addWord(word) def addWord(self, word): if word not in self.word2index: self.word2index[word] = self.n_words self.word2count[word] = 1 self.index2word[self.n_words] = word self.n_words += 1 else: self.word2count[word] += 1 \u8fd9\u4e9b\u6587\u4ef6\u5168\u90e8\u91c7\u7528Unicode\u7f16\u7801,\u4e3a\u4e86\u7b80\u5316\u6211\u4eec\u5c06Unicode\u5b57\u7b26\u8f6c\u6362\u4e3aASCII, \u4f7f\u6240\u6709\u5185\u5bb9\u5c0f\u5199,\u5e76\u4fee\u526a\u5927\u90e8\u5206\u6807\u70b9\u7b26\u53f7. # \u611f\u8c22\u60a8\u5c06Unicode\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3a\u7eafASCII # http://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' ) # \u5c0f\u5199,\u4fee\u526a\u548c\u5220\u9664\u975e\u5b57\u6bcd\u5b57\u7b26 def normalizeString(s): s = unicodeToAscii(s.lower().strip()) s = re.sub(r\"([.!?])\", r\" \\1\", s) s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) return s \u8981\u8bfb\u53d6\u6570\u636e\u6587\u4ef6,\u6211\u4eec\u5c06\u628a\u6587\u4ef6\u5206\u6210\u884c,\u7136\u540e\u5c06\u884c\u6210\u5bf9\u5206\u5f00. \u8fd9\u4e9b\u6587\u4ef6\u90fd\u662f\u82f1\u6587\u2192\u5176\u4ed6\u8bed\u8a00,\u6240\u4ee5\u5982\u679c\u6211\u4eec\u60f3\u4ece\u5176\u4ed6\u8bed\u8a00\u7ffb\u8bd1\u2192\u82f1\u6587,\u6211\u4eec\u6dfb\u52a0\u4e86 \u7ffb\u8f6c\u6807\u5fd7 [ ](#id5)reverse \u6765\u7ffb\u8f6c\u8bcd\u8bed\u5bf9. def readLangs(lang1, lang2, reverse=False): print(\"Reading lines...\") # \u8bfb\u53d6\u6587\u4ef6\u5e76\u6309\u884c\u5206\u5f00 lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\ read().strip().split('\\n') # \u5c06\u6bcf\u4e00\u884c\u5206\u6210\u4e24\u5217\u5e76\u8fdb\u884c\u6807\u51c6\u5316 pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines] # \u7ffb\u8f6c\u5bf9,Lang\u5b9e\u4f8b\u5316 if reverse: pairs = [list(reversed(p)) for p in pairs] input_lang = Lang(lang2) output_lang = Lang(lang1) else: input_lang = Lang(lang1) output_lang = Lang(lang2) return input_lang, output_lang, pairs \u7531\u4e8e\u6709\u5f88\u591a\u4f8b\u53e5,\u6211\u4eec\u5e0c\u671b\u5feb\u901f\u8bad\u7ec3,\u6211\u4eec\u4f1a\u5c06\u6570\u636e\u96c6\u88c1\u526a\u4e3a\u76f8\u5bf9\u7b80\u77ed\u7684\u53e5\u5b50. \u8fd9\u91cc\u7684\u5355\u8bcd\u7684\u6700\u5927\u957f\u5ea6\u662f10\u8bcd(\u5305\u62ec\u7ed3\u675f\u6807\u70b9\u7b26\u53f7),\u6211\u4eec\u6b63\u5728\u8fc7\u6ee4\u5230\u7ffb\u8bd1 \u6210\u201dI am\u201d\u6216\u201dHe is\u201d\u7b49\u5f62\u5f0f\u7684\u53e5\u5b50.(\u8003\u8651\u5230\u5148\u524d\u66ff\u6362\u4e86\u6487\u53f7). MAX_LENGTH = 10 eng_prefixes = ( \"i am \", \"i m \", \"he is\", \"he s \", \"she is\", \"she s\", \"you are\", \"you re \", \"we are\", \"we re \", \"they are\", \"they re \" ) def filterPair(p): return len(p[0].split(' ')) < MAX_LENGTH and \\ len(p[1].split(' ')) < MAX_LENGTH and \\ p[1].startswith(eng_prefixes) def filterPairs(pairs): return [pair for pair in pairs if filterPair(pair)] \u5b8c\u6574\u7684\u51c6\u5907\u6570\u636e\u7684\u8fc7\u7a0b: \u52a0\u8f7d\u6587\u672c\u6587\u4ef6\u5207\u5206\u6210\u884c,\u5e76\u5207\u5206\u6210\u5355\u8bcd\u5bf9: \u6587\u672c\u5f52\u4e00\u5316, \u6309\u7167\u957f\u5ea6\u548c\u5185\u5bb9\u8fc7\u6ee4 \u4ece\u6210\u5bf9\u7684\u53e5\u5b50\u4e2d\u5236\u4f5c\u5355\u8bcd\u5217\u8868 def prepareData(lang1, lang2, reverse=False): input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse) print(\"Read %s sentence pairs\" % len(pairs)) pairs = filterPairs(pairs) print(\"Trimmed to %s sentence pairs\" % len(pairs)) print(\"Counting words...\") for pair in pairs: input_lang.addSentence(pair[0]) output_lang.addSentence(pair[1]) print(\"Counted words:\") print(input_lang.name, input_lang.n_words) print(output_lang.name, output_lang.n_words) return input_lang, output_lang, pairs input_lang, output_lang, pairs = prepareData('eng', 'fra', True) print(random.choice(pairs))","title":"\u52a0\u8f7d\u6570\u636e\u6587\u4ef6"},{"location":"33/#seq2seq_1","text":"\u9012\u5f52\u795e\u7ecf\u7f51\u7edc(RNN),\u662f\u4e00\u4e2a\u6309\u7167\u4e00\u4e2a\u5e8f\u5217\u8fdb\u884c\u64cd\u4f5c\u7684\u7f51\u8def,\u5e76 \u5c06\u5176\u81ea\u5df1\u7684\u8f93\u51fa\u7528\u4f5c\u540e\u7eed\u6b65\u9aa4\u7684\u8f93\u5165. \u4e00\u4e2a \u5e8f\u5217\u5230\u5e8f\u5217\u7f51\u7edc , \u6216 seq2seq \u7f51\u7edc, \u6216 \u7f16\u7801\u89e3\u7801\u5668\u7f51\u7edc , \u662f\u7531\u4e24\u4e2a\u79f0\u4e3a\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684RNN\u7ec4\u6210\u7684\u6a21\u578b. \u7f16\u7801\u5668\u8bfb\u53d6\u8f93\u5165\u5e8f\u5217\u5e76\u8f93\u51fa\u5355\u4e2a\u5411\u91cf, \u89e3\u7801\u5668\u8bfb\u53d6\u8be5\u5411\u91cf\u4ee5\u4ea7\u751f\u8f93\u51fa\u5e8f\u5217. \u4e0e\u5355\u4e2aRNN\u7684\u5e8f\u5217\u9884\u6d4b\u4e0d\u540c,\u6bcf\u4e2a\u8f93\u5165\u5bf9\u5e94\u4e00\u4e2a\u8f93\u51fa, seq2seq\u6a21\u578b\u5c06\u6211\u4eec\u4ece\u5e8f\u5217\u957f\u5ea6\u548c\u987a\u5e8f\u4e2d\u89e3\u653e\u51fa\u6765, \u8fd9\u4f7f\u5f97\u5b83\u6210\u4e3a\u4e24\u79cd\u8bed\u8a00\u4e4b\u95f4\u7ffb\u8bd1\u7684\u7406\u60f3\u9009\u62e9. \u8003\u8651\u8fd9\u53e5\u8bdd \u201cJe ne suis pas le chat noir\u201d \u2192 \u201cI am not the black cat\u201d. \u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u5927\u90e8\u5206\u5355\u8bcd\u5728\u8f93\u51fa\u53e5\u5b50\u4e2d\u6709\u76f4\u63a5\u7ffb\u8bd1, \u4f46\u987a\u5e8f\u7565\u6709\u4e0d\u540c,\u4f8b\u5982: \u201cchat noir\u201d \u548c \u201cblack cat\u201d. \u7531\u4e8e \u201cne/pas\u201d\u7ed3\u6784, \u5176\u4e2d\u53e6\u4e00\u4e2a\u5355\u8bcd\u5728\u8f93\u5165\u7684\u53e5\u5b50\u4e2d. \u76f4\u63a5\u4ece\u8f93\u5165\u8bcd\u7684\u5e8f\u5217\u4e2d\u76f4\u63a5\u751f\u6210\u6b63\u786e\u7684\u7ffb\u8bd1\u662f\u5f88\u56f0\u96be\u7684. \u4f7f\u7528seq2seq\u6a21\u578b,\u7f16\u7801\u5668\u4f1a\u521b\u5efa\u4e00\u4e2a\u5355\u72ec\u7684\u5411\u91cf, \u5728\u7406\u60f3\u60c5\u51b5\u4e0b,\u5b83\u5c06\u8f93\u5165\u5e8f\u5217\u7684\u201d\u542b\u4e49\u201d\u7f16\u7801\u4e3a\u5355\u4e2a\u5411\u91cf - \u53e5\u5b50\u7684N\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e00\u4e2a\u70b9.","title":"Seq2Seq\u6a21\u578b"},{"location":"33/#_2","text":"seq2seq\u7f51\u7edc\u7684\u7f16\u7801\u5668\u662f\u4e00\u4e2aRNN,\u5b83\u4e3a\u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd\u8f93\u51fa\u4e00\u4e9b\u503c. \u5bf9\u4e8e\u6bcf\u4e2a\u8f93\u5165\u5b57,\u7f16\u7801\u5668\u8f93\u51fa\u4e00\u4e2a\u5411\u91cf\u548c\u4e00\u4e2a\u9690\u85cf\u72b6\u6001,\u5e76\u5c06\u9690\u85cf\u72b6\u6001\u7528\u4e8e\u4e0b\u4e00\u4e2a\u8f93\u5165\u5b57. class EncoderRNN(nn.Module): def __init__(self, input_size, hidden_size): super(EncoderRNN, self).__init__() self.hidden_size = hidden_size self.embedding = nn.Embedding(input_size, hidden_size) self.gru = nn.GRU(hidden_size, hidden_size) def forward(self, input, hidden): embedded = self.embedding(input).view(1, 1, -1) output = embedded output, hidden = self.gru(output, hidden) return output, hidden def initHidden(self): result = Variable(torch.zeros(1, 1, self.hidden_size)) if use_cuda: return result.cuda() else: return result","title":"\u7f16\u7801\u5668"},{"location":"33/#_3","text":"\u89e3\u7801\u5668\u662f\u53e6\u4e00\u4e2aRNN,\u5b83\u63a5\u6536\u7f16\u7801\u5668\u8f93\u51fa\u5411\u91cf\u5e76\u8f93\u51fa\u4e00\u4e2a\u5355\u8bcd\u5e8f\u5217\u6765\u521b\u5efa\u7ffb\u8bd1.","title":"\u89e3\u7801\u5668"},{"location":"33/#_4","text":"\u5728\u6700\u7b80\u5355\u7684seq2seq\u89e3\u7801\u5668\u4e2d,\u6211\u4eec\u53ea\u4f7f\u7528\u7f16\u7801\u5668\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa. \u8fd9\u4e2a\u6700\u540e\u7684\u8f93\u51fa\u6709\u65f6\u79f0\u4e3a\u4e0a\u4e0b\u6587\u5411\u91cf,\u56e0\u4e3a\u5b83\u4ece\u6574\u4e2a\u5e8f\u5217\u7f16\u7801\u4e0a\u4e0b\u6587. \u8be5\u4e0a\u4e0b\u6587\u5411\u91cf\u88ab\u7528\u4f5c\u89e3\u7801\u5668\u7684\u521d\u59cb\u9690\u85cf\u72b6\u6001. \u5728\u89e3\u7801\u7684\u6bcf\u4e00\u6b65,\u89e3\u7801\u5668\u90fd\u88ab\u8d4b\u4e88\u4e00\u4e2a\u8f93\u5165\u6307\u4ee4\u548c\u9690\u85cf\u72b6\u6001. \u521d\u59cb\u8f93\u5165\u6307\u4ee4\u5b57\u7b26\u4e32\u5f00\u59cb\u7684 &lt;SOS&gt; \u6307\u4ee4,\u7b2c\u4e00\u4e2a\u9690\u85cf\u72b6\u6001\u662f\u4e0a\u4e0b\u6587\u5411\u91cf(\u7f16\u7801\u5668\u7684\u6700\u540e\u9690\u85cf\u72b6\u6001). class DecoderRNN(nn.Module): def __init__(self, hidden_size, output_size): super(DecoderRNN, self).__init__() self.hidden_size = hidden_size self.embedding = nn.Embedding(output_size, hidden_size) self.gru = nn.GRU(hidden_size, hidden_size) self.out = nn.Linear(hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): output = self.embedding(input).view(1, 1, -1) output = F.relu(output) output, hidden = self.gru(output, hidden) output = self.softmax(self.out(output[0])) return output, hidden def initHidden(self): result = Variable(torch.zeros(1, 1, self.hidden_size)) if use_cuda: return result.cuda() else: return result \u6211\u4eec\u9f13\u52b1\u4f60\u8bad\u7ec3\u548c\u89c2\u5bdf\u8fd9\u4e2a\u6a21\u578b\u7684\u7ed3\u679c,\u4f46\u4e3a\u4e86\u8282\u7701\u7a7a\u95f4,\u6211\u4eec\u5c06\u76f4\u63a5\u8fdb\u6b63\u9898\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236.","title":"\u7b80\u5355\u7684\u89e3\u7801\u5668"},{"location":"33/#_5","text":"\u5982\u679c\u4ec5\u5728\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e4b\u95f4\u4f20\u9012\u4e0a\u4e0b\u6587\u5411\u91cf,\u5219\u8be5\u5355\u4e2a\u5411\u91cf\u627f\u62c5\u7f16\u7801\u6574\u4e2a\u53e5\u5b50\u7684\u8d1f\u62c5. \u6ce8\u610f\u529b\u5141\u8bb8\u89e3\u7801\u5668\u7f51\u7edc\u9488\u5bf9\u89e3\u7801\u5668\u81ea\u8eab\u8f93\u51fa\u7684\u6bcf\u4e00\u6b65\u201d\u805a\u7126\u201d\u7f16\u7801\u5668\u8f93\u51fa\u7684\u4e0d\u540c\u90e8\u5206. \u9996\u5148\u6211\u4eec\u8ba1\u7b97\u4e00\u7ec4\u6ce8\u610f\u529b\u6743\u91cd. \u8fd9\u4e9b\u5c06\u88ab\u4e58\u4ee5\u7f16\u7801\u5668\u8f93\u51fa\u77e2\u91cf\u83b7\u5f97\u52a0\u6743\u7684\u7ec4\u5408. \u7ed3\u679c(\u5728\u4ee3\u7801\u4e2d\u4e3a attn_applied ) \u5e94\u8be5\u5305\u542b\u5173\u4e8e\u8f93\u5165\u5e8f\u5217\u7684\u7279\u5b9a\u90e8\u5206\u7684\u4fe1\u606f, \u4ece\u800c\u5e2e\u52a9\u89e3\u7801\u5668\u9009\u62e9\u6b63\u786e\u7684\u8f93\u51fa\u5355\u8bcd. \u4f7f\u7528\u89e3\u7801\u5668\u7684\u8f93\u5165\u548c\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u8f93\u5165,\u5229\u7528\u53e6\u4e00\u4e2a\u524d\u9988\u5c42 [ ](#id12)attn \u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd, \u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u6709\u5404\u79cd\u5927\u5c0f\u7684\u53e5\u5b50,\u4e3a\u4e86\u5b9e\u9645\u521b\u5efa\u548c\u8bad\u7ec3\u6b64\u5c42, \u6211\u4eec\u5fc5\u987b\u9009\u62e9\u6700\u5927\u957f\u5ea6\u7684\u53e5\u5b50(\u8f93\u5165\u957f\u5ea6,\u7528\u4e8e\u7f16\u7801\u5668\u8f93\u51fa),\u4ee5\u9002\u7528\u4e8e\u6b64\u5c42. \u6700\u5927\u957f\u5ea6\u7684\u53e5\u5b50\u5c06\u4f7f\u7528\u6240\u6709\u6ce8\u610f\u529b\u6743\u91cd,\u800c\u8f83\u77ed\u7684\u53e5\u5b50\u53ea\u4f7f\u7528\u524d\u51e0\u4e2a. class AttnDecoderRNN(nn.Module): def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH): super(AttnDecoderRNN, self).__init__() self.hidden_size = hidden_size self.output_size = output_size self.dropout_p = dropout_p self.max_length = max_length self.embedding = nn.Embedding(self.output_size, self.hidden_size) self.attn = nn.Linear(self.hidden_size * 2, self.max_length) self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size) self.dropout = nn.Dropout(self.dropout_p) self.gru = nn.GRU(self.hidden_size, self.hidden_size) self.out = nn.Linear(self.hidden_size, self.output_size) def forward(self, input, hidden, encoder_outputs): embedded = self.embedding(input).view(1, 1, -1) embedded = self.dropout(embedded) attn_weights = F.softmax( self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1) attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)) output = torch.cat((embedded[0], attn_applied[0]), 1) output = self.attn_combine(output).unsqueeze(0) output = F.relu(output) output, hidden = self.gru(output, hidden) output = F.log_softmax(self.out(output[0]), dim=1) return output, hidden, attn_weights def initHidden(self): result = Variable(torch.zeros(1, 1, self.hidden_size)) if use_cuda: return result.cuda() else: return result Note \u8fd8\u6709\u5176\u4ed6\u5f62\u5f0f\u7684\u6ce8\u610f\u529b\u901a\u8fc7\u4f7f\u7528\u76f8\u5bf9\u4f4d\u7f6e\u65b9\u6cd5\u6765\u89e3\u51b3\u957f\u5ea6\u9650\u5236. \u9605\u8bfb\u5173\u4e8e \u201clocal attention\u201d \u5728 \u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7684\u6709\u6548\u9014\u5f84 .\u4e3a\u4e86\u8bad\u7ec3,\u5bf9\u4e8e\u6bcf\u4e00\u5bf9\u6211\u4eec\u5c06\u9700\u8981\u8f93\u5165\u7684\u5f20\u91cf(\u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u8bcd\u7684\u7d22\u5f15)\u548c \u76ee\u6807\u5f20\u91cf(\u76ee\u6807\u8bed\u53e5\u4e2d\u7684\u8bcd\u7684\u7d22\u5f15). \u5728\u521b\u5efa\u8fd9\u4e9b\u5411\u91cf\u65f6,\u6211\u4eec\u4f1a\u5c06EOS\u6807\u8bb0\u6dfb\u52a0\u5230\u4e24\u4e2a\u5e8f\u5217\u4e2d. def indexesFromSentence(lang, sentence): return [lang.word2index[word] for word in sentence.split(' ')] def variableFromSentence(lang, sentence): indexes = indexesFromSentence(lang, sentence) indexes.append(EOS_token) result = Variable(torch.LongTensor(indexes).view(-1, 1)) if use_cuda: return result.cuda() else: return result def variablesFromPair(pair): input_variable = variableFromSentence(input_lang, pair[0]) target_variable = variableFromSentence(output_lang, pair[1]) return (input_variable, target_variable)","title":"\u6ce8\u610f\u529b\u89e3\u7801\u5668"},{"location":"33/#_6","text":"\u4e3a\u4e86\u8bad\u7ec3\u6211\u4eec\u901a\u8fc7\u7f16\u7801\u5668\u8fd0\u884c\u8f93\u5165\u53e5\u5b50,\u5e76\u8ddf\u8e2a\u6bcf\u4e2a\u8f93\u51fa\u548c\u6700\u65b0\u7684\u9690\u85cf\u72b6\u6001. \u7136\u540e\u89e3\u7801\u5668\u88ab\u8d4b\u4e88 &lt;SOS&gt; \u6307\u4ee4\u4f5c\u4e3a\u5176\u7b2c\u4e00\u4e2a\u8f93\u5165, \u5e76\u5c06\u7f16\u7801\u5668\u7684\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u5176\u7b2c\u4e00\u4e2a\u9690\u85cf\u72b6\u6001. \u201cTeacher forcing\u201d \u662f\u5c06\u5b9e\u9645\u76ee\u6807\u8f93\u51fa\u7528\u4f5c\u6bcf\u4e2a\u4e0b\u4e00\u4e2a\u8f93\u5165\u7684\u6982\u5ff5,\u800c\u4e0d\u662f\u5c06\u89e3\u7801\u5668\u7684 \u731c\u6d4b\u7528\u4f5c\u4e0b\u4e00\u4e2a\u8f93\u5165.\u4f7f\u7528\u6559\u5e08\u5f3a\u8feb\u4f1a\u4f7f\u5176\u66f4\u5feb\u5730\u6536\u655b,\u4f46\u662f \u5f53\u8bad\u7ec3\u597d\u7684\u7f51\u7edc\u88ab\u5229\u7528\u65f6,\u5b83\u53ef\u80fd\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u6027. . \u4f60\u53ef\u4ee5\u89c2\u5bdf\u6559\u5e08\u5f3a\u8feb\u7f51\u7edc\u7684\u8f93\u51fa,\u8fd9\u4e9b\u7f51\u7edc\u662f\u7528\u8fde\u8d2f\u7684\u8bed\u6cd5\u9605\u8bfb\u7684,\u4f46\u5374\u8fdc\u79bb\u4e86\u6b63\u786e\u7684\u7ffb\u8bd1 - \u76f4\u89c2\u5730\u6765\u770b\u5b83\u5df2\u7ecf\u5b66\u4f1a\u4e86\u4ee3\u8868\u8f93\u51fa\u8bed\u6cd5,\u5e76\u4e14\u4e00\u65e6\u8001\u5e08\u544a\u8bc9\u5b83\u524d\u51e0\u4e2a\u5355\u8bcd,\u5c31\u53ef\u4ee5\u201d\u62fe\u53d6\u201d\u5b83\u7684\u610f\u601d, \u4f46\u5b83\u6ca1\u6709\u9002\u5f53\u5730\u5b66\u4f1a\u5982\u4f55\u4ece\u7ffb\u8bd1\u4e2d\u521b\u5efa\u53e5\u5b50. \u7531\u4e8ePyTorch\u7684autograd\u7ed9\u6211\u4eec\u7684\u81ea\u7531,\u6211\u4eec\u53ef\u4ee5\u968f\u610f\u9009\u62e9\u4f7f\u7528\u8001\u5e08\u5f3a\u5236\u6216\u4e0d\u4f7f\u7528\u7b80\u5355\u7684if\u8bed\u53e5. \u6253\u5f00 teacher_forcing_ratio \u66f4\u591a\u7684\u4f7f\u7528\u5b83. teacher_forcing_ratio = 0.5 def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH): encoder_hidden = encoder.initHidden() encoder_optimizer.zero_grad() decoder_optimizer.zero_grad() input_length = input_variable.size()[0] target_length = target_variable.size()[0] encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size)) encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs loss = 0 for ei in range(input_length): encoder_output, encoder_hidden = encoder( input_variable[ei], encoder_hidden) encoder_outputs[ei] = encoder_output[0][0] decoder_input = Variable(torch.LongTensor([[SOS_token]])) decoder_input = decoder_input.cuda() if use_cuda else decoder_input decoder_hidden = encoder_hidden use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False if use_teacher_forcing: # \u6559\u5e08\u5f3a\u5236: \u5c06\u76ee\u6807\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u8f93\u5165 for di in range(target_length): decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs) loss += criterion(decoder_output, target_variable[di]) decoder_input = target_variable[di] # Teacher forcing else: # \u6ca1\u6709\u6559\u5e08\u5f3a\u8feb: \u4f7f\u7528\u81ea\u5df1\u7684\u9884\u6d4b\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u8f93\u5165 for di in range(target_length): decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs) topv, topi = decoder_output.data.topk(1) ni = topi[0][0] decoder_input = Variable(torch.LongTensor([[ni]])) decoder_input = decoder_input.cuda() if use_cuda else decoder_input loss += criterion(decoder_output, target_variable[di]) if ni == EOS_token: break loss.backward() encoder_optimizer.step() decoder_optimizer.step() return loss.data[0] / target_length \u6839\u636e\u5f53\u524d\u65f6\u95f4\u548c\u8fdb\u5ea6\u767e\u5206\u6bd4,\u8fd9\u662f\u4e00\u4e2a\u5e2e\u52a9\u529f\u80fd,\u7528\u4e8e\u6253\u5370\u7ecf\u8fc7\u7684\u65f6\u95f4\u548c\u4f30\u8ba1\u7684\u5269\u4f59\u65f6\u95f4. import time import math def asMinutes(s): m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) def timeSince(since, percent): now = time.time() s = now - since es = s / (percent) rs = es - s return '%s (- %s)' % (asMinutes(s), asMinutes(rs)) \u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u5982\u4e0b\u6240\u793a: \u542f\u52a8\u4e00\u4e2a\u8ba1\u65f6\u5668 \u521d\u59cb\u5316\u4f18\u5316\u5668\u548c\u6807\u51c6 \u521b\u5efa\u4e00\u7ec4\u8bad\u7ec3\u5bf9 \u4e3a\u7ed8\u56fe\u5efa\u7a7a\u635f\u5931\u6570\u7ec4 \u7136\u540e\u6211\u4eec\u591a\u6b21\u8c03\u7528 train ,\u5076\u5c14\u6253\u5370\u8fdb\u5ea6(\u6837\u672c\u7684\u767e\u5206\u6bd4,\u5230\u76ee\u524d\u4e3a\u6b62\u7684\u65f6\u95f4,\u4f30\u8ba1\u7684\u65f6\u95f4)\u548c\u5e73\u5747\u635f\u5931. def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01): start = time.time() plot_losses = [] print_loss_total = 0 # Reset every print_every plot_loss_total = 0 # Reset every plot_every encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate) training_pairs = [variablesFromPair(random.choice(pairs)) for i in range(n_iters)] criterion = nn.NLLLoss() for iter in range(1, n_iters + 1): training_pair = training_pairs[iter - 1] input_variable = training_pair[0] target_variable = training_pair[1] loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) print_loss_total += loss plot_loss_total += loss if iter % print_every == 0: print_loss_avg = print_loss_total / print_every print_loss_total = 0 print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg)) if iter % plot_every == 0: plot_loss_avg = plot_loss_total / plot_every plot_losses.append(plot_loss_avg) plot_loss_total = 0 showPlot(plot_losses)","title":"\u8bad\u7ec3\u6a21\u578b"},{"location":"33/#_7","text":"\u4f7f\u7528matplotlib\u5b8c\u6210\u7ed8\u56fe, \u4f7f\u7528\u8bad\u7ec3\u65f6\u4fdd\u5b58\u7684\u635f\u5931\u503c plot_losses \u6570\u7ec4. import matplotlib.pyplot as plt import matplotlib.ticker as ticker import numpy as np def showPlot(points): plt.figure() fig, ax = plt.subplots() # \u8fd9\u4e2a\u5b9a\u4f4d\u5668\u4f1a\u5b9a\u671f\u53d1\u51fa\u63d0\u793a\u4fe1\u606f loc = ticker.MultipleLocator(base=0.2) ax.yaxis.set_major_locator(loc) plt.plot(points)","title":"\u7ed8\u5236\u7ed3\u679c"},{"location":"33/#_8","text":"\u8bc4\u4f30\u4e0e\u8bad\u7ec3\u5927\u90e8\u5206\u76f8\u540c,\u4f46\u6ca1\u6709\u76ee\u6807,\u56e0\u6b64\u6211\u4eec\u53ea\u662f\u5c06\u89e3\u7801\u5668\u7684\u6bcf\u4e00\u6b65\u9884\u6d4b\u53cd\u9988\u7ed9\u5b83\u81ea\u8eab. \u6bcf\u5f53\u5b83\u9884\u6d4b\u5230\u4e00\u4e2a\u5355\u8bcd\u65f6,\u6211\u4eec\u5c31\u4f1a\u5c06\u5b83\u6dfb\u52a0\u5230\u8f93\u51fa\u5b57\u7b26\u4e32\u4e2d,\u5e76\u4e14\u5982\u679c\u5b83\u9884\u6d4b\u5230\u6211\u4eec\u5728\u90a3\u91cc\u505c\u6b62\u7684EOS\u6307\u4ee4. \u6211\u4eec\u8fd8\u5b58\u50a8\u89e3\u7801\u5668\u7684\u6ce8\u610f\u529b\u8f93\u51fa\u4ee5\u4f9b\u7a0d\u540e\u663e\u793a. def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH): input_variable = variableFromSentence(input_lang, sentence) input_length = input_variable.size()[0] encoder_hidden = encoder.initHidden() encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size)) encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs for ei in range(input_length): encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden) encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0] decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS decoder_input = decoder_input.cuda() if use_cuda else decoder_input decoder_hidden = encoder_hidden decoded_words = [] decoder_attentions = torch.zeros(max_length, max_length) for di in range(max_length): decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs) decoder_attentions[di] = decoder_attention.data topv, topi = decoder_output.data.topk(1) ni = topi[0][0] if ni == EOS_token: decoded_words.append('<EOS>') break else: decoded_words.append(output_lang.index2word[ni]) decoder_input = Variable(torch.LongTensor([[ni]])) decoder_input = decoder_input.cuda() if use_cuda else decoder_input return decoded_words, decoder_attentions[:di + 1] \u6211\u4eec\u53ef\u4ee5\u4ece\u8bad\u7ec3\u96c6\u4e2d\u8bc4\u4f30\u968f\u673a\u7684\u53e5\u5b50\u5e76\u6253\u5370\u51fa\u8f93\u5165,\u76ee\u6807\u548c\u8f93\u51fa\u4ee5\u4f5c\u51fa\u4e00\u4e9b\u4e3b\u89c2\u8d28\u91cf\u5224\u65ad: def evaluateRandomly(encoder, decoder, n=10): for i in range(n): pair = random.choice(pairs) print('>', pair[0]) print('=', pair[1]) output_words, attentions = evaluate(encoder, decoder, pair[0]) output_sentence = ' '.join(output_words) print('<', output_sentence) print('')","title":"\u8bc4\u4f30"},{"location":"33/#_9","text":"\u6709\u4e86\u6240\u6709\u8fd9\u4e9b\u8f85\u52a9\u529f\u80fd(\u5b83\u770b\u8d77\u6765\u50cf\u662f\u989d\u5916\u7684\u5de5\u4f5c,\u4f46\u5b83\u4f7f\u8fd0\u884c\u591a\u4e2a\u5b9e\u9a8c\u66f4\u5bb9\u6613), \u6211\u4eec\u5c31\u7acb\u9a6c\u53ef\u4ee5\u521d\u59cb\u5316\u7f51\u7edc\u5e76\u5f00\u59cb\u57f9\u8bad. \u8bf7\u8bb0\u4f4f\u8f93\u5165\u53e5\u5b50\u88ab\u4e25\u91cd\u8fc7\u6ee4, \u5bf9\u4e8e\u8fd9\u4e2a\u5c0f\u6570\u636e\u96c6,\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5305\u542b256\u4e2a\u9690\u85cf\u8282\u70b9 \u548c\u5355\u4e2aGRU\u5c42\u7684\u76f8\u5bf9\u8f83\u5c0f\u7684\u7f51\u7edc.\u5728MacBook CPU\u4e0a\u7ea640\u5206\u949f\u540e,\u6211\u4eec\u4f1a\u5f97\u5230\u4e00\u4e9b\u5408\u7406\u7684\u7ed3\u679c. Note \u5982\u679c\u4f60\u8fd0\u884c\u8fd9\u4e2anotebook,\u4f60\u53ef\u4ee5\u8bad\u7ec3,\u6253\u65ad\u5185\u6838,\u8bc4\u4f30\u5e76\u5728\u4ee5\u540e\u7ee7\u7eed\u8bad\u7ec3. \u6ce8\u91ca\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u521d\u59cb\u5316\u7684\u884c\u5e76\u518d\u6b21\u8fd0\u884c trainIters . hidden_size = 256 encoder1 = EncoderRNN(input_lang.n_words, hidden_size) attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1) if use_cuda: encoder1 = encoder1.cuda() attn_decoder1 = attn_decoder1.cuda() trainIters(encoder1, attn_decoder1, 75000, print_every=5000) evaluateRandomly(encoder1, attn_decoder1)","title":"\u8bad\u7ec3\u548c\u8bc4\u4f30"},{"location":"33/#_10","text":"\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e00\u4e2a\u6709\u7528\u7279\u6027\u662f\u5176\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u8f93\u51fa. \u7531\u4e8e\u5b83\u7528\u4e8e\u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u7279\u5b9a\u7f16\u7801\u5668\u8f93\u51fa\u8fdb\u884c\u52a0\u6743,\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u60f3\u8c61\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e2d\u67e5\u770b\u7f51\u7edc\u6700\u5173\u6ce8\u7684\u4f4d\u7f6e. \u60a8\u53ef\u4ee5\u7b80\u5355\u5730\u8fd0\u884c plt.matshow(attentions) ,\u5c06\u6ce8\u610f\u529b\u8f93\u51fa\u663e\u793a\u4e3a\u77e9\u9635, \u5176\u4e2d\u5217\u662f\u8f93\u5165\u6b65\u9aa4,\u884c\u662f\u8f93\u51fa\u6b65\u9aa4. output_words, attentions = evaluate( encoder1, attn_decoder1, \"je suis trop froid .\") plt.matshow(attentions.numpy()) \u4e3a\u4e86\u83b7\u5f97\u66f4\u597d\u7684\u89c2\u770b\u4f53\u9a8c,\u6211\u4eec\u5c06\u989d\u5916\u6dfb\u52a0\u8f74\u548c\u6807\u7b7e: def showAttention(input_sentence, output_words, attentions): # \u7528\u989c\u8272\u6761\u8bbe\u7f6e\u56fe\u5f62 fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(attentions.numpy(), cmap='bone') fig.colorbar(cax) # \u8bbe\u7f6e\u8f74 ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90) ax.set_yticklabels([''] + output_words) # \u5728\u6bcf\u4e2a\u6253\u52fe\u5904\u663e\u793a\u6807\u7b7e ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) plt.show() def evaluateAndShowAttention(input_sentence): output_words, attentions = evaluate( encoder1, attn_decoder1, input_sentence) print('input =', input_sentence) print('output =', ' '.join(output_words)) showAttention(input_sentence, output_words, attentions) evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\") evaluateAndShowAttention(\"elle est trop petit .\") evaluateAndShowAttention(\"je ne crains pas de mourir .\") evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")","title":"\u53ef\u89c6\u5316\u6ce8\u610f\u529b"},{"location":"33/#_11","text":"\u5c1d\u8bd5\u4f7f\u7528\u4e0d\u540c\u7684\u6570\u636e\u96c6 \u53e6\u4e00\u79cd\u8bed\u8a00\u5bf9 \u4eba \u2192 \u673a\u5668 (\u4f8b\u5982. IOT \u547d\u4ee4) \u804a\u5929 \u2192 \u54cd\u5e94 \u95ee\u9898 \u2192 \u56de\u7b54 \u7528\u9884\u5148\u8bad\u7ec3\u7684\u8bcd\u5d4c\u5165\u66ff\u6362\u5d4c\u5165,\u4f8b\u5982word2vec\u6216GloVe \u5c1d\u8bd5\u66f4\u591a\u56fe\u5c42,\u66f4\u591a\u9690\u85cf\u5355\u4f4d\u548c\u66f4\u591a\u53e5\u5b50. \u6bd4\u8f83\u8bad\u7ec3\u65f6\u95f4\u548c\u7ed3\u679c. \u5982\u679c\u60a8\u4f7f\u7528\u7684\u7ffb\u8bd1\u6587\u4ef6\u5bf9\u4e2d\u6709\u4e24\u4e2a\u76f8\u540c\u7684\u77ed\u8bed( I am test \\t I am test ), \u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u4f5c\u4e3a\u81ea\u52a8\u7f16\u7801\u5668.\u5c1d\u8bd5\u8fd9\u4e2a: - \u8bad\u7ec3\u81ea\u7f16\u7801\u5668 - \u53ea\u4fdd\u5b58\u7f16\u7801\u5668\u7f51\u7edc - \u4ece\u90a3\u91cc\u8bad\u7ec3\u4e00\u4e2a\u65b0\u7684\u89e3\u7801\u5668\u8fdb\u884c\u7ffb\u8bd1 Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: seq2seq_translation_tutorial.py Download Jupyter notebook: seq2seq_translation_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u7ec3\u4e60"},{"location":"34/","text":"\u5f3a\u5316\u5b66\u4e60\uff08DQN\uff09\u6559\u7a0b \u4f5c\u8005 : Adam Paszke \u672c\u6559\u7a0b\u6f14\u793a\u5982\u4f55\u4f7f\u7528 PyTorch \u5bf9\u4efb\u52a1 CartPole-v0 \u8bad\u7ec3 Deep Q Learning\uff08DQN\uff09\u4ee3\u7406\uff08\u5373\u4e00\u4e2a\u7b97\u6cd5\u9ed1\u7bb1\uff09, \u8be5\u4efb\u52a1\u6765\u81ea\u4e8e OpenAI Gym . \u4efb\u52a1 \u8be5\u4ee3\u7406\u9700\u8981\u51b3\u5b9a\u5c06\u5c0f\u8f66\u5f80\u5de6\u8fd8\u662f\u5f80\u53f3\u63a8, \u56e0\u6b64\u5c0f\u8f66\u4e0a\u7684\u6746\u5b50\u59cb\u7ec8\u4fdd\u6301\u7ad6\u76f4. \u4f60\u53ef\u4ee5\u5728 Gym website \u627e\u5230\u4e00\u4e2a\u5b98\u65b9\u7684\u516c\u793a\u699c\u5355, \u5176\u7f57\u5217\u4e86\u4e0d\u540c\u7684\u7b97\u6cd5\u548c\u53ef\u89c6\u5316. \u4ee3\u7406\u901a\u8fc7\u89c2\u5bdf\u5f53\u524d\u73af\u5883\uff08\u5c0f\u8f66\u548c\u6746\u5b50\u7684\u7ec4\u5408\u4f53\uff09\u4e0b\u7684\u72b6\u6001\u9009\u62e9\u4e00\u4e2a\u5408\u9002\u7684\u884c\u4e3a\uff08\u5f80\u5de6\u8fd8\u662f\u5f80\u53f3\u63a8\uff09, \u968f\u540e\u73af\u5883\u72b6\u6001\u5f97\u5230\u8f6c\u53d8, \u5e76\u8fd4\u56de\u4e00\u4e2a\u56de\u9988\u56e0\u5b50\u6765\u91cf\u5316\u8be5\u884c\u4e3a\u6240\u5e26\u6765\u7684\u540e\u679c\uff08\u597d\u5904\u6216\u662f\u574f\u5904\uff09. \u5728\u8fd9\u4e2a\u4efb\u52a1\u4e2d, \u5982\u679c\u6746\u5b50\u79fb\u52a8\u592a\u8fdc\u5219\u6574\u4e2a\u4ee3\u7406\u73af\u5883\u7ec8\u6b62. \u5c0f\u8f66\u63a8\u6746\u4efb\u52a1\u88ab\u8bbe\u8ba1\u4e3a\u67094\u4e2a\u8f93\u5165\u53c2\u6570\u4f20\u7ed9\u4ee3\u7406, \u5b83\u4eec\u662f\u73af\u5883\u72b6\u6001, \u73af\u5883\u4f4d\u7f6e, \u73af\u5883\u901f\u7387\u7b49. \u7136\u800c, \u795e\u7ecf\u7f51\u7edc\u5355\u9760\u89c2\u5bdf\u8fd9\u4e2a\u573a\u666f\u5c31\u53ef\u4ee5\u89e3\u51b3\u8be5\u4efb\u52a1, \u56e0\u6b64\u6211\u4eec\u7528\u4e00\u4e9b\u4ee5\u5c0f\u8f66\u4e3a\u4e2d\u5fc3\u7684\u5c4f\u5e55\u56fe\u4f5c\u4e3a\u8f93\u5165\u53c2\u6570\u5c31\u884c\u4e86. \u6b63\u56e0\u4e3a\u6b64, \u6211\u4eec\u5e76\u4e0d\u80fd\u4ec5\u4ec5\u51ed\u501f\u5c06\u6211\u4eec\u6240\u5f97\u7ed3\u679c\u4e0e\u516c\u793a\u699c\u5355\u4e0a\u7684\u7ed3\u679c\u5bf9\u6bd4\u6765\u5f97\u51fa\u7ed3\u8bba, \u6211\u4eec\u7684\u4efb\u52a1\u8fdc\u6bd4\u8fd9\u4e2a\u96be. \u4e0d\u5e78\u7684\u662f, \u8fd9\u5c06\u4f1a\u5bfc\u81f4\u6211\u4eec\u7684\u8bad\u7ec3\u901f\u5ea6\u53d8\u6162, \u56e0\u4e3a\u6211\u4eec\u5fc5\u987b\u5f97\u6e32\u67d3\u5c4f\u5e55\u56fe\u6240\u6709\u7684\u5e27\u6570. \u4e25\u683c\u6765\u8bf4, \u6211\u4eec\u5c06\u72b6\u6001\u5b9a\u4e49\u4e3a\u524d\u4e00\u4e2a\u5c4f\u5e55\u56fe\u4e0e\u5f53\u524d\u5c4f\u5e55\u56fe\u4e4b\u95f4\u7684\u5dee\u522b. \u8fd9\u4e5f\u4f1a\u4f7f\u5f97\u4ee3\u7406\u5c06\u56fe\u4e2d\u63a8\u6746\u7684\u901f\u7387\u4e5f\u8003\u8651\u8fdb\u53bb. \u5305 \u9996\u5148, \u6211\u4eec\u5bfc\u5165\u4e00\u4e9b\u9700\u8981\u7528\u5230\u7684\u5305. \u7b2c\u4e00, \u6211\u4eec\u9700\u8981 gym\u5305 \u73af\u5883\u9700\u8981\u8fd9\u4e2a\u5305\uff08\u4f7f\u7528 \u2018pip install gym\u2019 \u5b89\u88c5\u8be5\u5305). \u6211\u4eec\u4e5f\u4f1a\u4f7f\u7528\u6765\u81ea\u4e8e PyTorch \u7684\u4ee5\u4e0b\u5305: \u795e\u7ecf\u7f51\u7edc neural networks ( torch.nn ) \u4f18\u5316 optimization ( torch.optim ) \u81ea\u5fae\u5206 automatic differentiation ( torch.autograd ) \u89c6\u89c9\u4efb\u52a1\u5de5\u5177 ( torchvision - \u4e00\u4e2a\u72ec\u7acb\u7684\u5305 ). import gym import math import random import numpy as np import matplotlib import matplotlib.pyplot as plt from collections import namedtuple from itertools import count from copy import deepcopy from PIL import Image import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from torch.autograd import Variable import torchvision.transforms as T env = gym.make('CartPole-v0').unwrapped # \u8bbe\u7f6e matplotlib is_ipython = 'inline' in matplotlib.get_backend() if is_ipython: from IPython import display plt.ion() # \u5982\u679c\u8981\u4f7f\u7528 gpu \u7684\u8bdd use_cuda = torch.cuda.is_available() FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor Tensor = FloatTensor \u91cd\u64ad\u8bb0\u5fc6 (Replay Memory) \u6211\u4eec\u5c06\u4f7f\u7528\u4f53\u9a8c\u91cd\u64ad\u8bb0\u5fc6\u6765\u8bad\u7ec3\u6211\u4eec\u7684DQN. \u5b83\u5b58\u50a8\u4e86\u4ee3\u7406\u89c2\u5bdf\u5230\u7684\u53d8\u5316\u8fc7\u7a0b, \u5141\u8bb8\u6211\u4eec\u4e4b\u540e\u80fd\u591f \u91cd\u590d\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e. \u901a\u8fc7\u5bf9\u91cd\u64ad\u8bb0\u5fc6\u968f\u673a\u53d6\u6837, \u5efa\u7acb\u4e86\u6279\u5904\u7406\u7684\u53d8\u5316\u8fc7\u7a0b\u5c06\u4f1a\u88ab\u89e3\u8026\u5408. \u8fd9\u4e00\u673a\u5236 \u4e5f\u88ab\u8bc1\u660e\u80fd\u591f\u5927\u5e45\u5ea6\u5730\u63d0\u9ad8\u548c\u4f18\u5316 DNQ \u8bad\u7ec3\u6b65\u9aa4\u7684\u7a33\u5b9a\u6027. \u5bf9\u6b64, \u6211\u4eec\u5c06\u9700\u8981\u4e24\u4e2a\u7c7b: Transition - \u4e00\u4e2a\u547d\u540d\u5143\u7956\uff08tuple\uff09, \u4ee3\u8868\u4e86\u73af\u5883\u7684\u5355\u6b21\u53d8\u5316 ReplayMemory - \u4e00\u4e2a\u6709\u9650\u5927\u5c0f\u7684\u5faa\u73af\u7f13\u51b2\u533a, \u7528\u4e8e\u4fdd\u5b58\u6700\u8fd1\u89c2\u5bdf\u5230\u7684\u8f6c\u6362\u8fc7\u7a0b. \u5b83\u4e5f\u5b9e\u73b0\u4e86 .sample\uff08\uff09 \u65b9\u6cd5, \u7528\u4e8e\u9009\u62e9\u968f\u673a\u6279\u6b21\u7684\u8f6c\u6362\u8fdb\u884c\u8bad\u7ec3 Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward')) class ReplayMemory(object): def __init__(self, capacity): self.capacity = capacity self.memory = [] self.position = 0 def push(self, *args): \"\"\"Saves a transition.\"\"\" if len(self.memory) < self.capacity: self.memory.append(None) self.memory[self.position] = Transition(*args) self.position = (self.position + 1) % self.capacity def sample(self, batch_size): return random.sample(self.memory, batch_size) def __len__(self): return len(self.memory) \u73b0\u5728, \u8ba9\u6211\u4eec\u5f00\u59cb\u6784\u5efa\u6211\u4eec\u7684\u6a21\u578b. \u4f46\u662f\u5728\u6b64\u4e4b\u524d\u6211\u4eec\u9996\u5148\u5f97\u91cd\u65b0\u5b9a\u4e49\u4ec0\u4e48\u662f DNQ. DQN \u7b97\u6cd5 \u6211\u4eec\u7684\u73af\u5883\u662f\u786e\u5b9a\u6027\u7684, \u6240\u4ee5\u8fd9\u91cc\u63d0\u51fa\u7684\u6240\u6709\u65b9\u7a0b\u4e5f\u90fd\u662f\u4e3a\u7b80\u5355\u8d77\u89c1\u800c\u786e\u5b9a\u6027\u5730\u5236\u5b9a\u7684. \u5728\u5f3a\u5316\u5b66\u4e60\u6982\u5ff5\u4e2d, \u5176\u8fd8\u4f1a\u5305\u542b\u6709\u5bf9\u73af\u5883\u4e2d\u968f\u673a\u53d8\u5316\u8fc7\u7a0b\u7684\u671f\u671b\u503c. \u6211\u4eec\u7684\u76ee\u6807\u662f\u8bad\u7ec3\u51fa\u4e00\u4e2a\u673a\u5236, \u5c3d\u53ef\u80fd\u505a\u5230\u6700\u5927\u5316\u6298\u6263\u56e0\u5b50\u548c\u7d2f\u79ef\u56de\u9988\u56e0\u5b50. \\(R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t\\) , \\(R_{t_0}\\) \u4e5f\u88ab\u79f0\u4e3a \u56de\u9988\u56e0\u5b50 . \u6298\u6263\u56e0\u5b50 \\(\\gamma\\) \u5e94\u8be5\u662f\u4e00\u4e2a \u4f4d\u4e8e \\(0\\) \u548c \\(1\\) \u4e4b\u95f4\u7684\u5e38\u91cf, \u4e14\u786e\u4fdd\u4e86\u5176\u603b\u548c\u662f\u6536\u655b\u7684. \u5b83\u7684\u5b58\u5728\u610f\u4e49\u662f\u8ba9 \u56de\u9988\u7684\u91cd\u8981\u7a0b\u5ea6\u4e0e\u65f6\u95f4\u6210\u6b63\u6bd4, \u5373\u79bb\u73b0\u57281\u5206\u949f\u7684\u56de\u9988\u6bd4\u79bb\u73b0\u57281\u5c0f\u65f6\u7684\u56de\u9988\u8981\u66f4\u91cd\u8981, \u56e0\u4e3a\u79bb\u5f53\u524d\u65f6\u95f4\u8d8a\u8fd1, \u6211\u4eec\u7684\u9884\u6d4b\u503c\u53ef\u4ee5\u66f4\u51c6\u786e, \u66f4\u53ef\u4fe1. Q-learning \u7684\u4e3b\u8981\u539f\u7406\u662f, \u5047\u5982\u6211\u4eec\u6709\u4e00\u4e2a\u51fd\u6570, \\(Q^*: State \\times Action \\rightarrow \\mathbb{R}\\) , \u8fd9\u5c06\u4f1a\u5b9a\u4e49\u8fd4\u56de\u503c, \u5982\u679c\u6211\u4eec\u5728\u7ed9\u5b9a\u72b6\u6001\u4e0b\u505a\u51fa\u52a8\u4f5c, \u90a3\u4e48\u6211\u4eec\u5c06\u4f1a\u66f4\u5bb9\u6613\u636e\u6b64\u8bad\u7ec3\u51fa\u4e00\u4e2a\u673a\u5236, \u5e76\u505a\u5230\u6700\u5927\u5316\u56de\u9988\u56e0\u5b50. \\[\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\] \u7531\u4e8e\u6211\u4eec\u5bf9\u6574\u4e2a\u73af\u5883\u4e00\u65e0\u6240\u77e5, \u6211\u4eec\u4e0d\u9700\u8981\u77e5\u9053\u786e\u5b9a\u7684 \\(Q^*\\) . \u4f46\u662f, \u56e0\u4e3a\u795e\u7ecf\u7f51\u7edc \u662f\u4e00\u4e2a\u6cdb\u5316\u7684\u903c\u8fd1\u51fd\u6570, \u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u6784\u9020\u4e00\u4e2a\u7f51\u7edc\u5e76\u8bad\u7ec3\u5b83\u53bb\u6a21\u62df :math:[ ](#id2)Q^* \u5373\u53ef. \u5bf9\u4e8e\u6211\u4eec\u8bad\u7ec3\u7684\u66f4\u65b0\u89c4\u5219\u6765\u8bf4, \u6211\u4eec\u53ea\u9700\u8981\u8ba9\u6bcf\u4e00\u4e2a \\(Q\\) \u9075\u4ece\u8d1d\u5c14\u66fc\u65b9\u7a0b (Bellman equation) \u5c31\u53ef\u4ee5\u4e86. \\[Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\] \u65b9\u7a0b\u4e24\u8fb9\u7684\u5b9e\u9645\u5dee\u503c\u5373\u4e3a\u65f6\u95f4\u5dee\u5206\u8bef\u5dee (temporal difference error), \\(\\delta\\) : \\[\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\] \u4e3a\u4e86\u4f7f\u5f97\u8be5\u8bef\u5dee\u503c\u6700\u5c0f\u5316, \u6211\u4eec\u8981\u4f7f\u7528 Huber loss . \u5f53\u65f6\u95f4\u5dee\u5206\u8bef\u5dee\u8f83\u5c0f\u65f6, Huber loss \u8868\u73b0\u5730\u4e0e\u5747\u65b9\u8bef\u5dee (mean squared error) \u4e00\u6837, \u800c\u5f53\u65f6\u95f4\u5dee\u5206\u8bef\u5dee\u8f83\u5927\u65f6, Huber loss \u8868\u73b0\u5730\u4e0e\u7edd\u5bf9\u5747\u5dee (mean absolute error) \u4e00\u6837. \u8fd9\u4e00\u6027\u8d28\u4f7f\u5f97\u5b83\u5728\u9884\u6d4b\u5e26\u6709\u8f83\u591a\u566a\u97f3\u7684 \\(Q\\) \u503c\u4e0a\u66f4\u5177\u6709\u9c81\u68d2\u6027. \u6211\u4eec\u901a\u8fc7\u4ece\u91cd\u64ad\u8bb0\u5fc6\u4e2d\u53d6\u51fa\u4e00\u6279\u6837\u672c\u6765\u8ba1\u7b97 \\(B\\) . \\[\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\] \\[\\begin{split}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases} \\frac{1}{2}{\\delta^2} & \\text{for } |\\delta| \\le 1, \\\\ |\\delta| - \\frac{1}{2} & \\text{otherwise.} \\end{cases}\\end{split}\\] Q-network \u6211\u4eec\u7684\u6a21\u578b\u662f\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN), \u5c06\u5f53\u524d\u5c4f\u5e55\u56fe\u4e0e\u4e4b\u524d\u5c4f\u5e55\u56fe\u7684\u5dee\u503c\u4f5c\u4e3a\u552f\u4e00\u8f93\u5165, \u8f93\u51fa\u503c\u6709\u4e24\u4e2a, \u5206\u522b\u4ee3\u8868 \\(Q(s, \\mathrm{left})\\) \u548c \\(Q(s, \\mathrm{right})\\) (\u5176\u4e2d \\(s\\) \u662f\u7f51\u7edc\u7684\u8f93\u5165). \u4ece\u6548\u679c\u4e0a\u6765\u770b, \u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u53ef\u4ee5\u9884\u6d4b\u5728\u5f53\u524d\u8f93\u5165\u4e0b\u91c7\u53d6\u7279\u5b9a\u884c\u4e3a\u5e26\u6765\u7684 quality , \u5373\u5bf9\u6574\u4e2a\u73af\u5883\u7684\u5f71\u54cd. class DQN(nn.Module): def __init__(self): super(DQN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2) self.bn2 = nn.BatchNorm2d(32) self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2) self.bn3 = nn.BatchNorm2d(32) self.head = nn.Linear(448, 2) def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) return self.head(x.view(x.size(0), -1)) \u83b7\u53d6\u8f93\u5165 \u4ee5\u4e0b\u4ee3\u7801\u7528\u6765\u83b7\u53d6\u548c\u5904\u7406\u6765\u81ea\u73af\u5883\u4e2d\u7684\u88ab\u6e32\u67d3\u7684\u56fe\u50cf. \u5176\u4e2d\u4f7f\u7528\u4e86 torchvision \u5305, \u8fd9\u4f7f\u5f97\u56fe\u50cf\u53d8\u6362\u53d8\u5f97\u66f4\u52a0\u7b80\u5355. \u53ea\u8981\u8fd0\u884c\u8be5\u4ee3\u7801\u5757\u5c31\u4f1a\u663e\u793a\u5f53\u524d\u6240\u63d0\u53d6\u56fe\u50cf. resize = T.Compose([T.ToPILImage(), T.Scale(40, interpolation=Image.CUBIC), T.ToTensor()]) # This is based on the code from gym. screen_width = 600 def get_cart_location(): world_width = env.x_threshold * 2 scale = screen_width / world_width return int(env.state[0] * scale + screen_width / 2.0) # MIDDLE OF CART def get_screen(): screen = env.render(mode='rgb_array').transpose( (2, 0, 1)) # transpose into torch order (CHW) # Strip off the top and bottom of the screen screen = screen[:, 160:320] view_width = 320 cart_location = get_cart_location() if cart_location < view_width // 2: slice_range = slice(view_width) elif cart_location > (screen_width - view_width // 2): slice_range = slice(-view_width, None) else: slice_range = slice(cart_location - view_width // 2, cart_location + view_width // 2) # Strip off the edges, so that we have a square image centered on a cart screen = screen[:, :, slice_range] # Convert to float, rescare, convert to torch tensor # (this doesn't require a copy) screen = np.ascontiguousarray(screen, dtype=np.float32) / 255 screen = torch.from_numpy(screen) # \u8c03\u6574\u5927\u5c0f\u5e76\u6dfb\u52a0\u6279\u91cf\u7ef4\u5ea6 (BCHW) return resize(screen).unsqueeze(0).type(Tensor) env.reset() plt.figure() plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none') plt.title('Example extracted screen') plt.show() \u8bad\u7ec3 \u8d85\u53c2\u6570\u548c\u51fd\u6570 \u8fd9\u4e00\u4ee3\u7801\u5757\u5b9e\u4f8b\u5316\u6211\u4eec\u7684\u6a21\u578b\u548c\u4f18\u5316\u5668, \u5e76\u4e14\u5b9a\u4e49\u4e86\u4e00\u4e9b\u51fd\u6570. \u51fd\u6570: Variable - \u8fd9\u662f\u4e00\u4e2a\u5bf9 torch.autograd.Variable \u7684\u7b80\u5355\u5305\u88c5\u5668, \u5b83\u4f1a\u5728\u6211\u4eec\u6bcf\u6b21\u6784\u5efa\u53d8\u91cf\u65f6\u81ea\u52a8\u5c06\u6570\u636e\u53d1\u9001\u5230GPU. select_action - \u6839\u636e\u03b5\u8d2a\u5a6a\u6cd5\u5219\u9009\u62e9\u540e\u7eed\u884c\u52a8. \u7b80\u800c\u8a00\u4e4b, \u6211\u4eec\u6709\u65f6\u4f1a\u4f7f\u7528 \u6211\u4eec\u7684\u6a21\u578b\u6765\u9009\u62e9\u52a8\u4f5c, \u6709\u65f6\u6211\u4eec\u4ec5\u5747\u5300\u91c7\u6837. \u9009\u62e9\u968f\u673a\u52a8\u4f5c\u7684 \u6982\u7387\u5927\u5c0f\u5c06\u4ece \u201cEPS_START\u201d \u5f00\u59cb, \u5e76\u6cbf\u7740\u5230 \u201cEPS_END\u201d \u7684\u65b9\u5411 \u5448\u6307\u6570\u8870\u51cf. EPS_DECAY \u63a7\u5236\u8870\u51cf\u901f\u5ea6. plot_durations - \u4e00\u4e2a\u534f\u52a9\u7ed8\u5236\u52a8\u6001\u5e27\u6301\u7eed\u65f6\u95f4\u7684\u51fd\u6570, \u4ee5\u53ca\u8fc7\u53bb100\u52a8\u6001\u5e27\uff08\u5b98\u65b9 \u8bc4\u4f30\u4e2d\u4f7f\u7528\u7684\u6d4b\u91cf\u65b9\u6cd5\uff09\u7684\u5e73\u5747\u503c. \u7ed8\u5236\u7684\u56fe\u50cf\u5c06\u4f1a\u663e\u793a\u5728\u5305\u542b \u4e3b\u8981\u8bad\u7ec3\u5faa\u73af\u7684\u5355\u5143\u4ee3\u7801\u5757\u4e0b\u9762, \u5e76\u4e14\u5728\u6bcf\u8282\u52a8\u6001\u5e27\u4e4b\u540e\u66f4\u65b0. BATCH_SIZE = 128 GAMMA = 0.999 EPS_START = 0.9 EPS_END = 0.05 EPS_DECAY = 200 model = DQN() if use_cuda: model.cuda() optimizer = optim.RMSprop(model.parameters()) memory = ReplayMemory(10000) steps_done = 0 def select_action(state): global steps_done sample = random.random() eps_threshold = EPS_END + (EPS_START - EPS_END) * \\ math.exp(-1. * steps_done / EPS_DECAY) steps_done += 1 if sample > eps_threshold: return model( Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1) else: return LongTensor([[random.randrange(2)]]) episode_durations = [] def plot_durations(): plt.figure(2) plt.clf() durations_t = torch.FloatTensor(episode_durations) plt.title('Training...') plt.xlabel('Episode') plt.ylabel('Duration') plt.plot(durations_t.numpy()) # Take 100 episode averages and plot them too if len(durations_t) >= 100: means = durations_t.unfold(0, 100, 1).mean(1).view(-1) means = torch.cat((torch.zeros(99), means)) plt.plot(means.numpy()) plt.pause(0.001) # pause a bit so that plots are updated if is_ipython: display.clear_output(wait=True) display.display(plt.gcf()) \u8bad\u7ec3\u5faa\u73af \u6700\u7ec8\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u7684\u4ee3\u7801 \u5728\u4e0b\u9762\u4ee3\u7801\u4e2d\u6709\u4e00\u4e2a optimize_model \u51fd\u6570, \u5b83\u7528\u4e8e\u5b9e\u73b0\u4f18\u5316\u8fc7\u7a0b\u7684\u5176\u4e2d\u4e00\u6b65. \u5b83\u9996\u5148 \u53d6\u51fa\u4e00\u4e2a\u6279\u6b21\u7684\u6837\u672c, \u7136\u540e\u5c06\u6240\u6709\u7684\u5f20\u91cf\u5168\u90e8\u5408\u5e76\u5230\u4e00\u4e2a\u4e2d, \u5e76\u8ba1\u7b97 \\(Q(s_t, a_t)\\) \u548c \\(V(s_{t+1}) = \\max_a Q(s_{t+1}, a)\\) , \u6700\u7ec8\u5c06\u8fd9\u4e9b\u7ed3\u679c\u5168\u90fd\u878d\u5165\u5230loss\u4e2d\u53bb. \u5047\u5982 \\(s\\) \u662f\u4e00\u4e2a\u7ec8\u6b62\u72b6\u6001, \u5219 \\(V(s) = 0\\) . last_sync = 0 def optimize_model(): global last_sync if len(memory) < BATCH_SIZE: return transitions = memory.sample(BATCH_SIZE) # \u5c06betch\u8f6c\u7f6e (\u8be6\u89c1 http://stackoverflow.com/a/19343/3343043). batch = Transition(*zip(*transitions)) # \u8ba1\u7b97\u975e\u6700\u7ec8\u72b6\u6001\u7684\u63a9\u7801\u5e76\u8fde\u63a5\u6279\u5904\u7406\u5143\u7d20s non_final_mask = ByteTensor(tuple(map(lambda s: s is not None, batch.next_state))) # \u6211\u4eec\u4e0d\u60f3\u901a\u8fc7\u9884\u671f\u7684\u52a8\u4f5c\u503c\u53cd\u5411\u4f20\u64ad, volatile \u53d8\u91cf\u4f1a\u4e34\u65f6\u5c06\u6a21\u578b\u53c2\u6570 # 'requires_grad' \u66f4\u6539\u4e3aFalse\uff01 non_final_next_states = Variable(torch.cat([s for s in batch.next_state if s is not None]), volatile=True) state_batch = Variable(torch.cat(batch.state)) action_batch = Variable(torch.cat(batch.action)) reward_batch = Variable(torch.cat(batch.reward)) # \u8ba1\u7b97 Q(s_t, a) - \u6a21\u578b\u8ba1\u7b97\u51fa Q(s_t), \u7136\u540e\u6211\u4eec\u9009\u62e9\u67d0\u4e00\u680f\u52a8\u4f5c\u6267\u884c state_action_values = model(state_batch).gather(1, action_batch) # \u5bf9\u6240\u6709\u4e0b\u4e00\u72b6\u6001\u8ba1\u7b97\u51fa V(s_{t+1}) next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor)) next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0] # \u6b64\u65f6\u6211\u4eec\u4e0d\u60f3\u8ba9 volatile flag \u6df7\u4e71\u4e86\u6211\u4eec\u7684loss, \u56e0\u6b64\u6211\u4eec\u5c06\u5176\u7f6e\u4e3aFalse # \u5728\u6b64\u4e4b\u540e, \u6211\u4eec\u5c06\u4f1a\u76f4\u63a5\u4e22\u5f03\u6ee1\u8db3\u8be5\u53d8\u91cf, \u5e76\u8bbe requires_grad=False next_state_values.volatile = False # \u8ba1\u7b97 Q \u7684\u671f\u671b\u503c expected_state_action_values = (next_state_values * GAMMA) + reward_batch # \u8ba1\u7b97 Huber \u635f\u5931 loss = F.smooth_l1_loss(state_action_values, expected_state_action_values) # \u4f18\u5316\u6a21\u578b optimizer.zero_grad() loss.backward() for param in model.parameters(): param.grad.data.clamp_(-1, 1) optimizer.step() \u4e0b\u9762\u4ee3\u7801\u4e2d\u5305\u542b\u4e3b\u8981\u7684\u8bad\u7ec3\u5faa\u73af. \u9996\u5148, \u6211\u4eec\u91cd\u65b0\u8bbe\u7f6e\u73af\u5883, \u5e76\u5b9e\u4f8b\u5316 state \u53d8\u91cf. \u7136\u540e, \u6211\u4eec\u5bf9\u52a8\u4f5c\u53d6\u6837\u5e76\u6267\u884c, \u89c2\u5bdf\u4e0b\u4e00\u5c4f\u5e55\u56fe\u5e76\u5f97\u5230\u56de\u9988\u56e0\u5b50 (\u901a\u5e38\u4e3a1), \u540c\u65f6\u4f18\u5316\u4e00\u6b21\u6a21\u578b. \u5f53\u52a8\u6001\u5e27\u7ed3\u675f\u65f6 (\u5373\u6211\u4eec\u7684\u6a21\u578bfail\u4e86), \u5f00\u59cb\u65b0\u4e00\u8f6e\u7684\u5faa\u73af. \u4e0b\u9762\u7684 <cite>num_episodes</cite> \u53d8\u91cf\u8bbe\u7f6e\u7684\u5f88\u5c0f. \u4f60\u53ef\u4ee5\u628a\u8fd9\u4e2a notebook \u4e0b\u8f7d\u4e0b\u6765\u7136\u540e\u8fd0\u884c\u66f4\u591a\u5e27. num_episodes = 10 for i_episode in range(num_episodes): # Initialize the environment and state env.reset() last_screen = get_screen() current_screen = get_screen() state = current_screen - last_screen for t in count(): # Select and perform an action action = select_action(state) _, reward, done, _ = env.step(action[0, 0]) reward = Tensor([reward]) # \u89c2\u5bdf\u8bb0\u5f55\u65b0\u72b6\u6001 last_screen = current_screen current_screen = get_screen() if not done: next_state = current_screen - last_screen else: next_state = None # \u5c06\u53d8\u5316\u8fc7\u7a0b\u5b58\u5230\u5185\u5b58\u4e2d memory.push(state, action, next_state, reward) # \u8f6c\u79fb\u5230\u4e0b\u4e00\u72b6\u6001 state = next_state # \u5bf9\u76ee\u6807\u795e\u7ecf\u7f51\u7edc\u6267\u884c\u4e00\u6b65\u4f18\u5316 optimize_model() if done: episode_durations.append(t + 1) plot_durations() break print('Complete') env.render(close=True) env.close() plt.ioff() plt.show() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: reinforcement_q_learning.py Download Jupyter notebook: reinforcement_q_learning.ipynb Gallery generated by Sphinx-Gallery","title":"\u5f3a\u5316\u5b66\u4e60\uff08DQN\uff09\u6559\u7a0b"},{"location":"34/#dqn","text":"\u4f5c\u8005 : Adam Paszke \u672c\u6559\u7a0b\u6f14\u793a\u5982\u4f55\u4f7f\u7528 PyTorch \u5bf9\u4efb\u52a1 CartPole-v0 \u8bad\u7ec3 Deep Q Learning\uff08DQN\uff09\u4ee3\u7406\uff08\u5373\u4e00\u4e2a\u7b97\u6cd5\u9ed1\u7bb1\uff09, \u8be5\u4efb\u52a1\u6765\u81ea\u4e8e OpenAI Gym . \u4efb\u52a1 \u8be5\u4ee3\u7406\u9700\u8981\u51b3\u5b9a\u5c06\u5c0f\u8f66\u5f80\u5de6\u8fd8\u662f\u5f80\u53f3\u63a8, \u56e0\u6b64\u5c0f\u8f66\u4e0a\u7684\u6746\u5b50\u59cb\u7ec8\u4fdd\u6301\u7ad6\u76f4. \u4f60\u53ef\u4ee5\u5728 Gym website \u627e\u5230\u4e00\u4e2a\u5b98\u65b9\u7684\u516c\u793a\u699c\u5355, \u5176\u7f57\u5217\u4e86\u4e0d\u540c\u7684\u7b97\u6cd5\u548c\u53ef\u89c6\u5316. \u4ee3\u7406\u901a\u8fc7\u89c2\u5bdf\u5f53\u524d\u73af\u5883\uff08\u5c0f\u8f66\u548c\u6746\u5b50\u7684\u7ec4\u5408\u4f53\uff09\u4e0b\u7684\u72b6\u6001\u9009\u62e9\u4e00\u4e2a\u5408\u9002\u7684\u884c\u4e3a\uff08\u5f80\u5de6\u8fd8\u662f\u5f80\u53f3\u63a8\uff09, \u968f\u540e\u73af\u5883\u72b6\u6001\u5f97\u5230\u8f6c\u53d8, \u5e76\u8fd4\u56de\u4e00\u4e2a\u56de\u9988\u56e0\u5b50\u6765\u91cf\u5316\u8be5\u884c\u4e3a\u6240\u5e26\u6765\u7684\u540e\u679c\uff08\u597d\u5904\u6216\u662f\u574f\u5904\uff09. \u5728\u8fd9\u4e2a\u4efb\u52a1\u4e2d, \u5982\u679c\u6746\u5b50\u79fb\u52a8\u592a\u8fdc\u5219\u6574\u4e2a\u4ee3\u7406\u73af\u5883\u7ec8\u6b62. \u5c0f\u8f66\u63a8\u6746\u4efb\u52a1\u88ab\u8bbe\u8ba1\u4e3a\u67094\u4e2a\u8f93\u5165\u53c2\u6570\u4f20\u7ed9\u4ee3\u7406, \u5b83\u4eec\u662f\u73af\u5883\u72b6\u6001, \u73af\u5883\u4f4d\u7f6e, \u73af\u5883\u901f\u7387\u7b49. \u7136\u800c, \u795e\u7ecf\u7f51\u7edc\u5355\u9760\u89c2\u5bdf\u8fd9\u4e2a\u573a\u666f\u5c31\u53ef\u4ee5\u89e3\u51b3\u8be5\u4efb\u52a1, \u56e0\u6b64\u6211\u4eec\u7528\u4e00\u4e9b\u4ee5\u5c0f\u8f66\u4e3a\u4e2d\u5fc3\u7684\u5c4f\u5e55\u56fe\u4f5c\u4e3a\u8f93\u5165\u53c2\u6570\u5c31\u884c\u4e86. \u6b63\u56e0\u4e3a\u6b64, \u6211\u4eec\u5e76\u4e0d\u80fd\u4ec5\u4ec5\u51ed\u501f\u5c06\u6211\u4eec\u6240\u5f97\u7ed3\u679c\u4e0e\u516c\u793a\u699c\u5355\u4e0a\u7684\u7ed3\u679c\u5bf9\u6bd4\u6765\u5f97\u51fa\u7ed3\u8bba, \u6211\u4eec\u7684\u4efb\u52a1\u8fdc\u6bd4\u8fd9\u4e2a\u96be. \u4e0d\u5e78\u7684\u662f, \u8fd9\u5c06\u4f1a\u5bfc\u81f4\u6211\u4eec\u7684\u8bad\u7ec3\u901f\u5ea6\u53d8\u6162, \u56e0\u4e3a\u6211\u4eec\u5fc5\u987b\u5f97\u6e32\u67d3\u5c4f\u5e55\u56fe\u6240\u6709\u7684\u5e27\u6570. \u4e25\u683c\u6765\u8bf4, \u6211\u4eec\u5c06\u72b6\u6001\u5b9a\u4e49\u4e3a\u524d\u4e00\u4e2a\u5c4f\u5e55\u56fe\u4e0e\u5f53\u524d\u5c4f\u5e55\u56fe\u4e4b\u95f4\u7684\u5dee\u522b. \u8fd9\u4e5f\u4f1a\u4f7f\u5f97\u4ee3\u7406\u5c06\u56fe\u4e2d\u63a8\u6746\u7684\u901f\u7387\u4e5f\u8003\u8651\u8fdb\u53bb. \u5305 \u9996\u5148, \u6211\u4eec\u5bfc\u5165\u4e00\u4e9b\u9700\u8981\u7528\u5230\u7684\u5305. \u7b2c\u4e00, \u6211\u4eec\u9700\u8981 gym\u5305 \u73af\u5883\u9700\u8981\u8fd9\u4e2a\u5305\uff08\u4f7f\u7528 \u2018pip install gym\u2019 \u5b89\u88c5\u8be5\u5305). \u6211\u4eec\u4e5f\u4f1a\u4f7f\u7528\u6765\u81ea\u4e8e PyTorch \u7684\u4ee5\u4e0b\u5305: \u795e\u7ecf\u7f51\u7edc neural networks ( torch.nn ) \u4f18\u5316 optimization ( torch.optim ) \u81ea\u5fae\u5206 automatic differentiation ( torch.autograd ) \u89c6\u89c9\u4efb\u52a1\u5de5\u5177 ( torchvision - \u4e00\u4e2a\u72ec\u7acb\u7684\u5305 ). import gym import math import random import numpy as np import matplotlib import matplotlib.pyplot as plt from collections import namedtuple from itertools import count from copy import deepcopy from PIL import Image import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from torch.autograd import Variable import torchvision.transforms as T env = gym.make('CartPole-v0').unwrapped # \u8bbe\u7f6e matplotlib is_ipython = 'inline' in matplotlib.get_backend() if is_ipython: from IPython import display plt.ion() # \u5982\u679c\u8981\u4f7f\u7528 gpu \u7684\u8bdd use_cuda = torch.cuda.is_available() FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor Tensor = FloatTensor","title":"\u5f3a\u5316\u5b66\u4e60\uff08DQN\uff09\u6559\u7a0b"},{"location":"34/#replay-memory","text":"\u6211\u4eec\u5c06\u4f7f\u7528\u4f53\u9a8c\u91cd\u64ad\u8bb0\u5fc6\u6765\u8bad\u7ec3\u6211\u4eec\u7684DQN. \u5b83\u5b58\u50a8\u4e86\u4ee3\u7406\u89c2\u5bdf\u5230\u7684\u53d8\u5316\u8fc7\u7a0b, \u5141\u8bb8\u6211\u4eec\u4e4b\u540e\u80fd\u591f \u91cd\u590d\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e. \u901a\u8fc7\u5bf9\u91cd\u64ad\u8bb0\u5fc6\u968f\u673a\u53d6\u6837, \u5efa\u7acb\u4e86\u6279\u5904\u7406\u7684\u53d8\u5316\u8fc7\u7a0b\u5c06\u4f1a\u88ab\u89e3\u8026\u5408. \u8fd9\u4e00\u673a\u5236 \u4e5f\u88ab\u8bc1\u660e\u80fd\u591f\u5927\u5e45\u5ea6\u5730\u63d0\u9ad8\u548c\u4f18\u5316 DNQ \u8bad\u7ec3\u6b65\u9aa4\u7684\u7a33\u5b9a\u6027. \u5bf9\u6b64, \u6211\u4eec\u5c06\u9700\u8981\u4e24\u4e2a\u7c7b: Transition - \u4e00\u4e2a\u547d\u540d\u5143\u7956\uff08tuple\uff09, \u4ee3\u8868\u4e86\u73af\u5883\u7684\u5355\u6b21\u53d8\u5316 ReplayMemory - \u4e00\u4e2a\u6709\u9650\u5927\u5c0f\u7684\u5faa\u73af\u7f13\u51b2\u533a, \u7528\u4e8e\u4fdd\u5b58\u6700\u8fd1\u89c2\u5bdf\u5230\u7684\u8f6c\u6362\u8fc7\u7a0b. \u5b83\u4e5f\u5b9e\u73b0\u4e86 .sample\uff08\uff09 \u65b9\u6cd5, \u7528\u4e8e\u9009\u62e9\u968f\u673a\u6279\u6b21\u7684\u8f6c\u6362\u8fdb\u884c\u8bad\u7ec3 Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward')) class ReplayMemory(object): def __init__(self, capacity): self.capacity = capacity self.memory = [] self.position = 0 def push(self, *args): \"\"\"Saves a transition.\"\"\" if len(self.memory) < self.capacity: self.memory.append(None) self.memory[self.position] = Transition(*args) self.position = (self.position + 1) % self.capacity def sample(self, batch_size): return random.sample(self.memory, batch_size) def __len__(self): return len(self.memory) \u73b0\u5728, \u8ba9\u6211\u4eec\u5f00\u59cb\u6784\u5efa\u6211\u4eec\u7684\u6a21\u578b. \u4f46\u662f\u5728\u6b64\u4e4b\u524d\u6211\u4eec\u9996\u5148\u5f97\u91cd\u65b0\u5b9a\u4e49\u4ec0\u4e48\u662f DNQ.","title":"\u91cd\u64ad\u8bb0\u5fc6 (Replay Memory)"},{"location":"34/#dqn_1","text":"\u6211\u4eec\u7684\u73af\u5883\u662f\u786e\u5b9a\u6027\u7684, \u6240\u4ee5\u8fd9\u91cc\u63d0\u51fa\u7684\u6240\u6709\u65b9\u7a0b\u4e5f\u90fd\u662f\u4e3a\u7b80\u5355\u8d77\u89c1\u800c\u786e\u5b9a\u6027\u5730\u5236\u5b9a\u7684. \u5728\u5f3a\u5316\u5b66\u4e60\u6982\u5ff5\u4e2d, \u5176\u8fd8\u4f1a\u5305\u542b\u6709\u5bf9\u73af\u5883\u4e2d\u968f\u673a\u53d8\u5316\u8fc7\u7a0b\u7684\u671f\u671b\u503c. \u6211\u4eec\u7684\u76ee\u6807\u662f\u8bad\u7ec3\u51fa\u4e00\u4e2a\u673a\u5236, \u5c3d\u53ef\u80fd\u505a\u5230\u6700\u5927\u5316\u6298\u6263\u56e0\u5b50\u548c\u7d2f\u79ef\u56de\u9988\u56e0\u5b50. \\(R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t\\) , \\(R_{t_0}\\) \u4e5f\u88ab\u79f0\u4e3a \u56de\u9988\u56e0\u5b50 . \u6298\u6263\u56e0\u5b50 \\(\\gamma\\) \u5e94\u8be5\u662f\u4e00\u4e2a \u4f4d\u4e8e \\(0\\) \u548c \\(1\\) \u4e4b\u95f4\u7684\u5e38\u91cf, \u4e14\u786e\u4fdd\u4e86\u5176\u603b\u548c\u662f\u6536\u655b\u7684. \u5b83\u7684\u5b58\u5728\u610f\u4e49\u662f\u8ba9 \u56de\u9988\u7684\u91cd\u8981\u7a0b\u5ea6\u4e0e\u65f6\u95f4\u6210\u6b63\u6bd4, \u5373\u79bb\u73b0\u57281\u5206\u949f\u7684\u56de\u9988\u6bd4\u79bb\u73b0\u57281\u5c0f\u65f6\u7684\u56de\u9988\u8981\u66f4\u91cd\u8981, \u56e0\u4e3a\u79bb\u5f53\u524d\u65f6\u95f4\u8d8a\u8fd1, \u6211\u4eec\u7684\u9884\u6d4b\u503c\u53ef\u4ee5\u66f4\u51c6\u786e, \u66f4\u53ef\u4fe1. Q-learning \u7684\u4e3b\u8981\u539f\u7406\u662f, \u5047\u5982\u6211\u4eec\u6709\u4e00\u4e2a\u51fd\u6570, \\(Q^*: State \\times Action \\rightarrow \\mathbb{R}\\) , \u8fd9\u5c06\u4f1a\u5b9a\u4e49\u8fd4\u56de\u503c, \u5982\u679c\u6211\u4eec\u5728\u7ed9\u5b9a\u72b6\u6001\u4e0b\u505a\u51fa\u52a8\u4f5c, \u90a3\u4e48\u6211\u4eec\u5c06\u4f1a\u66f4\u5bb9\u6613\u636e\u6b64\u8bad\u7ec3\u51fa\u4e00\u4e2a\u673a\u5236, \u5e76\u505a\u5230\u6700\u5927\u5316\u56de\u9988\u56e0\u5b50. \\[\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\] \u7531\u4e8e\u6211\u4eec\u5bf9\u6574\u4e2a\u73af\u5883\u4e00\u65e0\u6240\u77e5, \u6211\u4eec\u4e0d\u9700\u8981\u77e5\u9053\u786e\u5b9a\u7684 \\(Q^*\\) . \u4f46\u662f, \u56e0\u4e3a\u795e\u7ecf\u7f51\u7edc \u662f\u4e00\u4e2a\u6cdb\u5316\u7684\u903c\u8fd1\u51fd\u6570, \u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u6784\u9020\u4e00\u4e2a\u7f51\u7edc\u5e76\u8bad\u7ec3\u5b83\u53bb\u6a21\u62df :math:[ ](#id2)Q^* \u5373\u53ef. \u5bf9\u4e8e\u6211\u4eec\u8bad\u7ec3\u7684\u66f4\u65b0\u89c4\u5219\u6765\u8bf4, \u6211\u4eec\u53ea\u9700\u8981\u8ba9\u6bcf\u4e00\u4e2a \\(Q\\) \u9075\u4ece\u8d1d\u5c14\u66fc\u65b9\u7a0b (Bellman equation) \u5c31\u53ef\u4ee5\u4e86. \\[Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\] \u65b9\u7a0b\u4e24\u8fb9\u7684\u5b9e\u9645\u5dee\u503c\u5373\u4e3a\u65f6\u95f4\u5dee\u5206\u8bef\u5dee (temporal difference error), \\(\\delta\\) : \\[\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\] \u4e3a\u4e86\u4f7f\u5f97\u8be5\u8bef\u5dee\u503c\u6700\u5c0f\u5316, \u6211\u4eec\u8981\u4f7f\u7528 Huber loss . \u5f53\u65f6\u95f4\u5dee\u5206\u8bef\u5dee\u8f83\u5c0f\u65f6, Huber loss \u8868\u73b0\u5730\u4e0e\u5747\u65b9\u8bef\u5dee (mean squared error) \u4e00\u6837, \u800c\u5f53\u65f6\u95f4\u5dee\u5206\u8bef\u5dee\u8f83\u5927\u65f6, Huber loss \u8868\u73b0\u5730\u4e0e\u7edd\u5bf9\u5747\u5dee (mean absolute error) \u4e00\u6837. \u8fd9\u4e00\u6027\u8d28\u4f7f\u5f97\u5b83\u5728\u9884\u6d4b\u5e26\u6709\u8f83\u591a\u566a\u97f3\u7684 \\(Q\\) \u503c\u4e0a\u66f4\u5177\u6709\u9c81\u68d2\u6027. \u6211\u4eec\u901a\u8fc7\u4ece\u91cd\u64ad\u8bb0\u5fc6\u4e2d\u53d6\u51fa\u4e00\u6279\u6837\u672c\u6765\u8ba1\u7b97 \\(B\\) . \\[\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\] \\[\\begin{split}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases} \\frac{1}{2}{\\delta^2} & \\text{for } |\\delta| \\le 1, \\\\ |\\delta| - \\frac{1}{2} & \\text{otherwise.} \\end{cases}\\end{split}\\]","title":"DQN \u7b97\u6cd5"},{"location":"34/#q-network","text":"\u6211\u4eec\u7684\u6a21\u578b\u662f\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN), \u5c06\u5f53\u524d\u5c4f\u5e55\u56fe\u4e0e\u4e4b\u524d\u5c4f\u5e55\u56fe\u7684\u5dee\u503c\u4f5c\u4e3a\u552f\u4e00\u8f93\u5165, \u8f93\u51fa\u503c\u6709\u4e24\u4e2a, \u5206\u522b\u4ee3\u8868 \\(Q(s, \\mathrm{left})\\) \u548c \\(Q(s, \\mathrm{right})\\) (\u5176\u4e2d \\(s\\) \u662f\u7f51\u7edc\u7684\u8f93\u5165). \u4ece\u6548\u679c\u4e0a\u6765\u770b, \u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u53ef\u4ee5\u9884\u6d4b\u5728\u5f53\u524d\u8f93\u5165\u4e0b\u91c7\u53d6\u7279\u5b9a\u884c\u4e3a\u5e26\u6765\u7684 quality , \u5373\u5bf9\u6574\u4e2a\u73af\u5883\u7684\u5f71\u54cd. class DQN(nn.Module): def __init__(self): super(DQN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2) self.bn2 = nn.BatchNorm2d(32) self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2) self.bn3 = nn.BatchNorm2d(32) self.head = nn.Linear(448, 2) def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) return self.head(x.view(x.size(0), -1))","title":"Q-network"},{"location":"34/#_1","text":"\u4ee5\u4e0b\u4ee3\u7801\u7528\u6765\u83b7\u53d6\u548c\u5904\u7406\u6765\u81ea\u73af\u5883\u4e2d\u7684\u88ab\u6e32\u67d3\u7684\u56fe\u50cf. \u5176\u4e2d\u4f7f\u7528\u4e86 torchvision \u5305, \u8fd9\u4f7f\u5f97\u56fe\u50cf\u53d8\u6362\u53d8\u5f97\u66f4\u52a0\u7b80\u5355. \u53ea\u8981\u8fd0\u884c\u8be5\u4ee3\u7801\u5757\u5c31\u4f1a\u663e\u793a\u5f53\u524d\u6240\u63d0\u53d6\u56fe\u50cf. resize = T.Compose([T.ToPILImage(), T.Scale(40, interpolation=Image.CUBIC), T.ToTensor()]) # This is based on the code from gym. screen_width = 600 def get_cart_location(): world_width = env.x_threshold * 2 scale = screen_width / world_width return int(env.state[0] * scale + screen_width / 2.0) # MIDDLE OF CART def get_screen(): screen = env.render(mode='rgb_array').transpose( (2, 0, 1)) # transpose into torch order (CHW) # Strip off the top and bottom of the screen screen = screen[:, 160:320] view_width = 320 cart_location = get_cart_location() if cart_location < view_width // 2: slice_range = slice(view_width) elif cart_location > (screen_width - view_width // 2): slice_range = slice(-view_width, None) else: slice_range = slice(cart_location - view_width // 2, cart_location + view_width // 2) # Strip off the edges, so that we have a square image centered on a cart screen = screen[:, :, slice_range] # Convert to float, rescare, convert to torch tensor # (this doesn't require a copy) screen = np.ascontiguousarray(screen, dtype=np.float32) / 255 screen = torch.from_numpy(screen) # \u8c03\u6574\u5927\u5c0f\u5e76\u6dfb\u52a0\u6279\u91cf\u7ef4\u5ea6 (BCHW) return resize(screen).unsqueeze(0).type(Tensor) env.reset() plt.figure() plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none') plt.title('Example extracted screen') plt.show()","title":"\u83b7\u53d6\u8f93\u5165"},{"location":"34/#_2","text":"","title":"\u8bad\u7ec3"},{"location":"34/#_3","text":"\u8fd9\u4e00\u4ee3\u7801\u5757\u5b9e\u4f8b\u5316\u6211\u4eec\u7684\u6a21\u578b\u548c\u4f18\u5316\u5668, \u5e76\u4e14\u5b9a\u4e49\u4e86\u4e00\u4e9b\u51fd\u6570. \u51fd\u6570: Variable - \u8fd9\u662f\u4e00\u4e2a\u5bf9 torch.autograd.Variable \u7684\u7b80\u5355\u5305\u88c5\u5668, \u5b83\u4f1a\u5728\u6211\u4eec\u6bcf\u6b21\u6784\u5efa\u53d8\u91cf\u65f6\u81ea\u52a8\u5c06\u6570\u636e\u53d1\u9001\u5230GPU. select_action - \u6839\u636e\u03b5\u8d2a\u5a6a\u6cd5\u5219\u9009\u62e9\u540e\u7eed\u884c\u52a8. \u7b80\u800c\u8a00\u4e4b, \u6211\u4eec\u6709\u65f6\u4f1a\u4f7f\u7528 \u6211\u4eec\u7684\u6a21\u578b\u6765\u9009\u62e9\u52a8\u4f5c, \u6709\u65f6\u6211\u4eec\u4ec5\u5747\u5300\u91c7\u6837. \u9009\u62e9\u968f\u673a\u52a8\u4f5c\u7684 \u6982\u7387\u5927\u5c0f\u5c06\u4ece \u201cEPS_START\u201d \u5f00\u59cb, \u5e76\u6cbf\u7740\u5230 \u201cEPS_END\u201d \u7684\u65b9\u5411 \u5448\u6307\u6570\u8870\u51cf. EPS_DECAY \u63a7\u5236\u8870\u51cf\u901f\u5ea6. plot_durations - \u4e00\u4e2a\u534f\u52a9\u7ed8\u5236\u52a8\u6001\u5e27\u6301\u7eed\u65f6\u95f4\u7684\u51fd\u6570, \u4ee5\u53ca\u8fc7\u53bb100\u52a8\u6001\u5e27\uff08\u5b98\u65b9 \u8bc4\u4f30\u4e2d\u4f7f\u7528\u7684\u6d4b\u91cf\u65b9\u6cd5\uff09\u7684\u5e73\u5747\u503c. \u7ed8\u5236\u7684\u56fe\u50cf\u5c06\u4f1a\u663e\u793a\u5728\u5305\u542b \u4e3b\u8981\u8bad\u7ec3\u5faa\u73af\u7684\u5355\u5143\u4ee3\u7801\u5757\u4e0b\u9762, \u5e76\u4e14\u5728\u6bcf\u8282\u52a8\u6001\u5e27\u4e4b\u540e\u66f4\u65b0. BATCH_SIZE = 128 GAMMA = 0.999 EPS_START = 0.9 EPS_END = 0.05 EPS_DECAY = 200 model = DQN() if use_cuda: model.cuda() optimizer = optim.RMSprop(model.parameters()) memory = ReplayMemory(10000) steps_done = 0 def select_action(state): global steps_done sample = random.random() eps_threshold = EPS_END + (EPS_START - EPS_END) * \\ math.exp(-1. * steps_done / EPS_DECAY) steps_done += 1 if sample > eps_threshold: return model( Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1) else: return LongTensor([[random.randrange(2)]]) episode_durations = [] def plot_durations(): plt.figure(2) plt.clf() durations_t = torch.FloatTensor(episode_durations) plt.title('Training...') plt.xlabel('Episode') plt.ylabel('Duration') plt.plot(durations_t.numpy()) # Take 100 episode averages and plot them too if len(durations_t) >= 100: means = durations_t.unfold(0, 100, 1).mean(1).view(-1) means = torch.cat((torch.zeros(99), means)) plt.plot(means.numpy()) plt.pause(0.001) # pause a bit so that plots are updated if is_ipython: display.clear_output(wait=True) display.display(plt.gcf())","title":"\u8d85\u53c2\u6570\u548c\u51fd\u6570"},{"location":"34/#_4","text":"\u6700\u7ec8\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u7684\u4ee3\u7801 \u5728\u4e0b\u9762\u4ee3\u7801\u4e2d\u6709\u4e00\u4e2a optimize_model \u51fd\u6570, \u5b83\u7528\u4e8e\u5b9e\u73b0\u4f18\u5316\u8fc7\u7a0b\u7684\u5176\u4e2d\u4e00\u6b65. \u5b83\u9996\u5148 \u53d6\u51fa\u4e00\u4e2a\u6279\u6b21\u7684\u6837\u672c, \u7136\u540e\u5c06\u6240\u6709\u7684\u5f20\u91cf\u5168\u90e8\u5408\u5e76\u5230\u4e00\u4e2a\u4e2d, \u5e76\u8ba1\u7b97 \\(Q(s_t, a_t)\\) \u548c \\(V(s_{t+1}) = \\max_a Q(s_{t+1}, a)\\) , \u6700\u7ec8\u5c06\u8fd9\u4e9b\u7ed3\u679c\u5168\u90fd\u878d\u5165\u5230loss\u4e2d\u53bb. \u5047\u5982 \\(s\\) \u662f\u4e00\u4e2a\u7ec8\u6b62\u72b6\u6001, \u5219 \\(V(s) = 0\\) . last_sync = 0 def optimize_model(): global last_sync if len(memory) < BATCH_SIZE: return transitions = memory.sample(BATCH_SIZE) # \u5c06betch\u8f6c\u7f6e (\u8be6\u89c1 http://stackoverflow.com/a/19343/3343043). batch = Transition(*zip(*transitions)) # \u8ba1\u7b97\u975e\u6700\u7ec8\u72b6\u6001\u7684\u63a9\u7801\u5e76\u8fde\u63a5\u6279\u5904\u7406\u5143\u7d20s non_final_mask = ByteTensor(tuple(map(lambda s: s is not None, batch.next_state))) # \u6211\u4eec\u4e0d\u60f3\u901a\u8fc7\u9884\u671f\u7684\u52a8\u4f5c\u503c\u53cd\u5411\u4f20\u64ad, volatile \u53d8\u91cf\u4f1a\u4e34\u65f6\u5c06\u6a21\u578b\u53c2\u6570 # 'requires_grad' \u66f4\u6539\u4e3aFalse\uff01 non_final_next_states = Variable(torch.cat([s for s in batch.next_state if s is not None]), volatile=True) state_batch = Variable(torch.cat(batch.state)) action_batch = Variable(torch.cat(batch.action)) reward_batch = Variable(torch.cat(batch.reward)) # \u8ba1\u7b97 Q(s_t, a) - \u6a21\u578b\u8ba1\u7b97\u51fa Q(s_t), \u7136\u540e\u6211\u4eec\u9009\u62e9\u67d0\u4e00\u680f\u52a8\u4f5c\u6267\u884c state_action_values = model(state_batch).gather(1, action_batch) # \u5bf9\u6240\u6709\u4e0b\u4e00\u72b6\u6001\u8ba1\u7b97\u51fa V(s_{t+1}) next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor)) next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0] # \u6b64\u65f6\u6211\u4eec\u4e0d\u60f3\u8ba9 volatile flag \u6df7\u4e71\u4e86\u6211\u4eec\u7684loss, \u56e0\u6b64\u6211\u4eec\u5c06\u5176\u7f6e\u4e3aFalse # \u5728\u6b64\u4e4b\u540e, \u6211\u4eec\u5c06\u4f1a\u76f4\u63a5\u4e22\u5f03\u6ee1\u8db3\u8be5\u53d8\u91cf, \u5e76\u8bbe requires_grad=False next_state_values.volatile = False # \u8ba1\u7b97 Q \u7684\u671f\u671b\u503c expected_state_action_values = (next_state_values * GAMMA) + reward_batch # \u8ba1\u7b97 Huber \u635f\u5931 loss = F.smooth_l1_loss(state_action_values, expected_state_action_values) # \u4f18\u5316\u6a21\u578b optimizer.zero_grad() loss.backward() for param in model.parameters(): param.grad.data.clamp_(-1, 1) optimizer.step() \u4e0b\u9762\u4ee3\u7801\u4e2d\u5305\u542b\u4e3b\u8981\u7684\u8bad\u7ec3\u5faa\u73af. \u9996\u5148, \u6211\u4eec\u91cd\u65b0\u8bbe\u7f6e\u73af\u5883, \u5e76\u5b9e\u4f8b\u5316 state \u53d8\u91cf. \u7136\u540e, \u6211\u4eec\u5bf9\u52a8\u4f5c\u53d6\u6837\u5e76\u6267\u884c, \u89c2\u5bdf\u4e0b\u4e00\u5c4f\u5e55\u56fe\u5e76\u5f97\u5230\u56de\u9988\u56e0\u5b50 (\u901a\u5e38\u4e3a1), \u540c\u65f6\u4f18\u5316\u4e00\u6b21\u6a21\u578b. \u5f53\u52a8\u6001\u5e27\u7ed3\u675f\u65f6 (\u5373\u6211\u4eec\u7684\u6a21\u578bfail\u4e86), \u5f00\u59cb\u65b0\u4e00\u8f6e\u7684\u5faa\u73af. \u4e0b\u9762\u7684 <cite>num_episodes</cite> \u53d8\u91cf\u8bbe\u7f6e\u7684\u5f88\u5c0f. \u4f60\u53ef\u4ee5\u628a\u8fd9\u4e2a notebook \u4e0b\u8f7d\u4e0b\u6765\u7136\u540e\u8fd0\u884c\u66f4\u591a\u5e27. num_episodes = 10 for i_episode in range(num_episodes): # Initialize the environment and state env.reset() last_screen = get_screen() current_screen = get_screen() state = current_screen - last_screen for t in count(): # Select and perform an action action = select_action(state) _, reward, done, _ = env.step(action[0, 0]) reward = Tensor([reward]) # \u89c2\u5bdf\u8bb0\u5f55\u65b0\u72b6\u6001 last_screen = current_screen current_screen = get_screen() if not done: next_state = current_screen - last_screen else: next_state = None # \u5c06\u53d8\u5316\u8fc7\u7a0b\u5b58\u5230\u5185\u5b58\u4e2d memory.push(state, action, next_state, reward) # \u8f6c\u79fb\u5230\u4e0b\u4e00\u72b6\u6001 state = next_state # \u5bf9\u76ee\u6807\u795e\u7ecf\u7f51\u7edc\u6267\u884c\u4e00\u6b65\u4f18\u5316 optimize_model() if done: episode_durations.append(t + 1) plot_durations() break print('Complete') env.render(close=True) env.close() plt.ioff() plt.show() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: reinforcement_q_learning.py Download Jupyter notebook: reinforcement_q_learning.ipynb Gallery generated by Sphinx-Gallery","title":"\u8bad\u7ec3\u5faa\u73af"},{"location":"35/","text":"Writing Distributed Applications with PyTorch Author : S\u00e9b Arnold In this short tutorial, we will be going over the distributed package of PyTorch. We\u2019ll see how to set up the distributed setting, use the different communication strategies, and go over some the internals of the package. Setup The distributed package included in PyTorch (i.e., torch.distributed ) enables researchers and practitioners to easily parallelize their computations across processes and clusters of machines. To do so, it leverages the messaging passing semantics allowing each process to communicate data to any of the other processes. As opposed to the multiprocessing ( torch.multiprocessing ) package, processes can use different communication backends and are not restricted to being executed on the same machine. In order to get started we need the ability to run multiple processes simultaneously. If you have access to compute cluster you should check with your local sysadmin or use your favorite coordination tool. (e.g., pdsh , clustershell , or others ) For the purpose of this tutorial, we will use a single machine and fork multiple processes using the following template. \"\"\"run.py:\"\"\" #!/usr/bin/env python import os import torch import torch.distributed as dist from torch.multiprocessing import Process def run(rank, size): \"\"\" Distributed function to be implemented later. \"\"\" pass def init_processes(rank, size, fn, backend='tcp'): \"\"\" Initialize the distributed environment. \"\"\" os.environ['MASTER_ADDR'] = '127.0.0.1' os.environ['MASTER_PORT'] = '29500' dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) if __name__ == \"__main__\": size = 2 processes = [] for rank in range(size): p = Process(target=init_processes, args=(rank, size, run)) p.start() processes.append(p) for p in processes: p.join() The above script spawns two processes who will each setup the distributed environment, initialize the process group ( dist.init_process_group ), and finally execute the given run function. Let\u2019s have a look at the init_processes function. It ensures that every process will be able to coordinate through a master, using the same ip address and port. Note that we used the TCP backend, but we could have used MPI or Gloo instead. (c.f. Section 5.1 ) We will go over the magic happening in dist.init_process_group at the end of this tutorial, but it essentially allows processes to communicate with each other by sharing their locations. Point-to-Point Communication Send and Recv A transfer of data from one process to another is called a point-to-point communication. These are achieved through the send and recv functions or their immediate counter-parts, isend and irecv . \"\"\"Blocking point-to-point communication.\"\"\" def run(rank, size): tensor = torch.zeros(1) if rank == 0: tensor += 1 # Send the tensor to process 1 dist.send(tensor=tensor, dst=1) else: # Receive tensor from process 0 dist.recv(tensor=tensor, src=0) print('Rank ', rank, ' has data ', tensor[0]) In the above example, both processes start with a zero tensor, then process 0 increments the tensor and sends it to process 1 so that they both end up with 1.0. Notice that process 1 needs to allocate memory in order to store the data it will receive. Also notice that send / recv are blocking : both processes stop until the communication is completed. On the other hand immediates are non-blocking ; the script continues its execution and the methods return a DistributedRequest object upon which we can choose to wait() . \"\"\"Non-blocking point-to-point communication.\"\"\" def run(rank, size): tensor = torch.zeros(1) req = None if rank == 0: tensor += 1 # Send the tensor to process 1 req = dist.isend(tensor=tensor, dst=1) print('Rank 0 started sending') else: # Receive tensor from process 0 req = dist.irecv(tensor=tensor, src=0) print('Rank 1 started receiving') req.wait() print('Rank ', rank, ' has data ', tensor[0]) When using immediates we have to be careful about with our usage of the sent and received tensors. Since we do not know when the data will be communicated to the other process, we should not modify the sent tensor nor access the received tensor before req.wait() has completed. In other words, writing to tensor after dist.isend() will result in undefined behaviour. reading from tensor after dist.irecv() will result in undefined behaviour. However, after req.wait() has been executed we are guaranteed that the communication took place, and that the value stored in tensor[0] is 1.0. Point-to-point communication is useful when we want a fine-grained control over the communication of our processes. They can be used to implement fancy algorithms, such as the one used in Baidu\u2019s DeepSpeech or Facebook\u2019s large-scale experiments .(c.f. Section 4.1 ) Collective Communication | Scatter | Gather | | Reduce | All-Reduce | | Broadcast | All-Gather | As opposed to point-to-point communcation, collectives allow for communication patterns across all processes in a group . A group is a subset of all our processes. To create a group, we can pass a list of ranks to dist.new_group(group) . By default, collectives are executed on the all processes, also known as the world . For example, in order to obtain the sum of all tensors at all processes, we can use the dist.all_reduce(tensor, op, group) collective. \"\"\" All-Reduce example.\"\"\" def run(rank, size): \"\"\" Simple point-to-point communication. \"\"\" group = dist.new_group([0, 1]) tensor = torch.ones(1) dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group) print('Rank ', rank, ' has data ', tensor[0]) Since we want the sum of all tensors in the group, we use dist.reduce_op.SUM as the reduce operator. Generally speaking, any commutative mathematical operation can be used as an operator. Out-of-the-box, PyTorch comes with 4 such operators, all working at the element-wise level: dist.reduce_op.SUM , dist.reduce_op.PRODUCT , dist.reduce_op.MAX , dist.reduce_op.MIN . In addition to dist.all_reduce(tensor, op, group) , there are a total of 6 collectives currently implemented in PyTorch. dist.broadcast(tensor, src, group) : Copies tensor from src to all other processes. dist.reduce(tensor, dst, op, group) : Applies op to all tensor and stores the result in dst . dist.all_reduce(tensor, op, group) : Same as reduce, but the result is stored in all processes. dist.scatter(tensor, src, scatter_list, group) : Copies the \\(i^{\\text{th}}\\) tensor scatter_list[i] to the \\(i^{\\text{th}}\\) process. dist.gather(tensor, dst, gather_list, group) : Copies tensor from all processes in dst . dist.all_gather(tensor_list, tensor, group) : Copies tensor from all processes to tensor_list , on all processes. Distributed Training Note: You can find the example script of this section in this GitHub repository . Now that we understand how the distributed module works, let us write something useful with it. Our goal will be to replicate the functionality of DistributedDataParallel . Of course, this will be a didactic example and in a real-world situtation you should use the official, well-tested and well-optimized version linked above. Quite simply we want to implement a distributed version of stochastic gradient descent. Our script will let all processes compute the gradients of their model on their batch of data and then average their gradients. In order to ensure similar convergence results when changing the number of processes, we will first have to partition our dataset. (You could also use tnt.dataset.SplitDataset , instead of the snippet below.) \"\"\" Dataset partitioning helper \"\"\" class Partition(object): def __init__(self, data, index): self.data = data self.index = index def __len__(self): return len(self.index) def __getitem__(self, index): data_idx = self.index[index] return self.data[data_idx] class DataPartitioner(object): def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234): self.data = data self.partitions = [] rng = Random() rng.seed(seed) data_len = len(data) indexes = [x for x in range(0, data_len)] rng.shuffle(indexes) for frac in sizes: part_len = int(frac * data_len) self.partitions.append(indexes[0:part_len]) indexes = indexes[part_len:] def use(self, partition): return Partition(self.data, self.partitions[partition]) With the above snippet, we can now simply partition any dataset using the following few lines: \"\"\" Partitioning MNIST \"\"\" def partition_dataset(): dataset = datasets.MNIST('./data', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])) size = dist.get_world_size() bsz = 128 / float(size) partition_sizes = [1.0 / size for _ in range(size)] partition = DataPartitioner(dataset, partition_sizes) partition = partition.use(dist.get_rank()) train_set = torch.utils.data.DataLoader(partition, batch_size=bsz, shuffle=True) return train_set, bsz Assuming we have 2 replicas, then each process will have a train_set of 60000 / 2 = 30000 samples. We also divide the batch size by the number of replicas in order to maintain the overall batch size of 128. We can now write our usual forward-backward-optimize training code, and add a function call to average the gradients of our models. (The following is largely inspired from the official PyTorch MNIST example .) \"\"\" Distributed Synchronous SGD Example \"\"\" def run(rank, size): torch.manual_seed(1234) train_set, bsz = partition_dataset() model = Net() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) num_batches = ceil(len(train_set.dataset) / float(bsz)) for epoch in range(10): epoch_loss = 0.0 for data, target in train_set: data, target = Variable(data), Variable(target) optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) epoch_loss += loss.data[0] loss.backward() average_gradients(model) optimizer.step() print('Rank ', dist.get_rank(), ', epoch ', epoch, ': ', epoch_loss / num_batches) It remains to implement the average_gradients(model) function, which simply takes in a model and averages its gradients across the whole world. \"\"\" Gradient averaging. \"\"\" def average_gradients(model): size = float(dist.get_world_size()) for param in model.parameters(): dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM) param.grad.data /= size Et voil\u00e0 ! We successfully implemented distributed synchronous SGD and could train any model on a large computer cluster. Note: While the last sentence is technically true, there are a lot more tricks required to implement a production-level implementation of synchronous SGD. Again, use what has been tested and optimized . Our Own Ring-Allreduce As an additional challenge, imagine that we wanted to implement DeepSpeech\u2019s efficient ring allreduce. This is fairly easily implemented using point-to-point collectives. \"\"\" Implementation of a ring-reduce with addition. \"\"\" def allreduce(send, recv): rank = dist.get_rank() size = dist.get_world_size() send_buff = th.zeros(send.size()) recv_buff = th.zeros(send.size()) accum = th.zeros(send.size()) accum[:] = send[:] left = ((rank - 1) + size) % size right = (rank + 1) % size for i in range(size - 1): if i % 2 == 0: # Send send_buff send_req = dist.isend(send_buff, right) dist.recv(recv_buff, left) accum[:] += recv[:] else: # Send recv_buff send_req = dist.isend(recv_buff, right) dist.recv(send_buff, left) accum[:] += send[:] send_req.wait() recv[:] = accum[:] In the above script, the allreduce(send, recv) function has a slightly different signature than the ones in PyTorch. It takes a recv tensor and will store the sum of all send tensors in it. As an exercise left to the reader, there is still one difference between our version and the one in DeepSpeech: their implementation divide the gradient tensor into chunks , so as to optimially utilize the communication bandwidth. (Hint: toch.chunk ) Advanced Topics We are now ready to discover some of the more advanced functionalities of torch.distributed . Since there is a lot to cover, this section is divided into two subsections: Communication Backends: where we learn how to use MPI and Gloo for GPU-GPU communication. Initialization Methods: where we understand how to best setup the initial coordination phase in dist.init_process_group() . Communication Backends One of the most elegant aspects of torch.distributed is its ability to abstract and build on top of different backends. As mentioned before, there are currently three backends implemented in PyTorch: TCP, MPI, and Gloo. They each have different specifications and tradeoffs, depending on the desired use-case. A comparative table of supported functions can be found here . TCP Backend So far we have made extensive usage of the TCP backend. It is quite handy as a development platform, as it is guaranteed to work on most machines and operating systems. It also supports all point-to-point and collective functions on CPU. However, there is no support for GPUs and its communication routines are not as optimized as the MPI one. Gloo Backend The Gloo backend provides an optimized implementation of collective communication procedures, both for CPUs and GPUs. It particularly shines on GPUs as it can perform communication without transferring data to the CPU\u2019s memory using GPUDirect . It is also capable of using NCCL to perform fast intra-node communication and implements its own algorithms for inter-node routines. Since version 0.2.0, the Gloo backend is automatically included with the pre-compiled binaries of PyTorch. As you have surely noticed, our distributed SGD example does not work if you put model on the GPU. Let\u2019s fix it by first replacing backend='gloo' in init_processes(rank, size, fn, backend='tcp') . At this point, the script will still run on CPU but uses the Gloo backend behind the scenes. In order to use multiple GPUs, let us also do the following modifications: init_processes(rank, size, fn, backend='tcp') \\(\\rightarrow\\) init_processes(rank, size, fn, backend='gloo') model = Net() \\(\\rightarrow\\) model = Net().cuda(rank) data, target = Variable(data), Variable(target) \\(\\rightarrow\\) data, target = Variable(data.cuda(rank)), Variable(target.cuda(rank)) With the above modifications, our model is now training on two GPUs and you can monitor their utilization with watch nvidia-smi . MPI Backend The Message Passing Interface (MPI) is a standardized tool from the field of high-performance computing. It allows to do point-to-point and collective communications and was the main inspiration for the API of torch.distributed . Several implementations of MPI exist (e.g. Open-MPI , MVAPICH2 , Intel MPI ) each optimized for different purposes. The advantage of using the MPI backend lies in MPI\u2019s wide availability - and high-level of optimization - on large computer clusters. Some recent implementations are also able to take advantage of CUDA IPC and GPU Direct technologies in order to avoid memory copies through the CPU. Unfortunately, PyTorch\u2019s binaries can not include an MPI implementation and we\u2019ll have to recompile it by hand. Fortunately, this process is fairly simple given that upon compilation, PyTorch will look by itself for an available MPI implementation. The following steps install the MPI backend, by installing PyTorch from sources . Create and activate your Anaconda environment, install all the pre-requisites following the guide , but do not run python setup.py install yet. Choose and install your favorite MPI implementation. Note that enabling CUDA-aware MPI might require some additional steps. In our case, we\u2019ll stick to Open-MPI without GPU support: conda install -c conda-forge openmpi Now, go to your cloned PyTorch repo and execute python setup.py install . In order to test our newly installed backend, a few modifications are required. Replace the content under if __name__ == '__main__': with init_processes(0, 0, run, backend='mpi') . Run mpirun -n 4 python myscript.py . The reason for these changes is that MPI needs to create its own environment before spawning the processes. MPI will also spawn its own processes and perform the handshake described in Initialization Methods , making the rank and size arguments of init_process_group superfluous. This is actually quite powerful as you can pass additional arguments to mpirun in order to tailor computational resources for each process. (Things like number of cores per process, hand-assigning machines to specific ranks, and some more ) Doing so, you should obtain the same familiar output as with the other communication backends. Initialization Methods To finish this tutorial, let\u2019s talk about the very first function we called: dist.init_process_group(backend, init_method) . In particular, we will go over the different initialization methods which are responsible for the initial coordination step between each process. Those methods allow you to define how this coordination is done. Depending on your hardware setup, one of these methods should be naturally more suitable than the others. In addition to the following sections, you should also have a look at the official documentation . Before diving into the initialization methods, let\u2019s have a quick look at what happens behind init_process_group from the C/C++ perspective. First, the arguments are parsed and validated. The backend is resolved via the name2channel.at() function. A Channel class is returned, and will be used to perform the data transmission. The GIL is dropped, and THDProcessGroupInit() is called. This instantiates the channel and adds the address of the master node. The process with rank 0 will execute the master procedure, while all other ranks will be workers . The master Creates sockets for all workers. Waits for all workers to connect. Sends them information about the location of the other processes. Each worker Creates a socket to the master. Sends their own location information. Receives information about the other workers. Opens a socket and handshakes with all other workers. The initialization is done, and everyone is connected to everyone. Environment Variable We have been using the environment variable initialization method throughout this tutorial. By setting the following four environment variables on all machines, all processes will be able to properly connect to the master, obtain information about the other processes, and finally handshake with them. MASTER_PORT : A free port on the machine that will host the process with rank 0. MASTER_ADDR : IP address of the machine that will host the process with rank 0. WORLD_SIZE : The total number of processes, so that the master knows how many workers to wait for. RANK : Rank of each process, so they will know whether it is the master of a worker. Shared File System The shared filesystem requires all processes to have access to a shared file system, and will coordinate them through a shared file. This means that each process will open the file, write its information, and wait until everybody did so. After what all required information will be readily available to all processes. In order to avoid race conditions, the file system must support locking through fcntl . Note that you can specify ranks manually or let the processes figure it out by themselves. Be defining a unique groupname per job you can use the same file path for multiple jobs and safely avoid collision. dist.init_process_group(init_method='file:///mnt/nfs/sharedfile', world_size=4, group_name='mygroup') TCP Init & Multicast Initializing via TCP can be achieved in two different ways: By providing the IP address of the process with rank 0 and the world size. By providing any valid IP multicast address and the world size. In the first case, all workers will be able to connect to the process with rank 0 and follow the procedure described above. dist.init_process_group(init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4) In the second case, the multicast address specifies the group of nodes who might potentially be active and the coordination can be handled by allowing each process to have an initial handshake before following the above procedure. In addition TCP multicast initialization also supports a group_name argument (as with the shared file method) allowing multiple jobs to be scheduled on the same cluster. dist.init_process_group(init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456', world_size=4) Acknowledgements I\u2019d like to thank the PyTorch developers for doing such a good job on their implementation, documentation, and tests. When the code was unclear, I could always count on the docs or the tests to find an answer. In particular, I\u2019d like to thank Soumith Chintala, Adam Paszke, and Natalia Gimelshein for providing insightful comments and answering questions on early drafts.","title":"Writing Distributed Applications with PyTorch"},{"location":"35/#writing-distributed-applications-with-pytorch","text":"Author : S\u00e9b Arnold In this short tutorial, we will be going over the distributed package of PyTorch. We\u2019ll see how to set up the distributed setting, use the different communication strategies, and go over some the internals of the package.","title":"Writing Distributed Applications with PyTorch"},{"location":"35/#setup","text":"The distributed package included in PyTorch (i.e., torch.distributed ) enables researchers and practitioners to easily parallelize their computations across processes and clusters of machines. To do so, it leverages the messaging passing semantics allowing each process to communicate data to any of the other processes. As opposed to the multiprocessing ( torch.multiprocessing ) package, processes can use different communication backends and are not restricted to being executed on the same machine. In order to get started we need the ability to run multiple processes simultaneously. If you have access to compute cluster you should check with your local sysadmin or use your favorite coordination tool. (e.g., pdsh , clustershell , or others ) For the purpose of this tutorial, we will use a single machine and fork multiple processes using the following template. \"\"\"run.py:\"\"\" #!/usr/bin/env python import os import torch import torch.distributed as dist from torch.multiprocessing import Process def run(rank, size): \"\"\" Distributed function to be implemented later. \"\"\" pass def init_processes(rank, size, fn, backend='tcp'): \"\"\" Initialize the distributed environment. \"\"\" os.environ['MASTER_ADDR'] = '127.0.0.1' os.environ['MASTER_PORT'] = '29500' dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) if __name__ == \"__main__\": size = 2 processes = [] for rank in range(size): p = Process(target=init_processes, args=(rank, size, run)) p.start() processes.append(p) for p in processes: p.join() The above script spawns two processes who will each setup the distributed environment, initialize the process group ( dist.init_process_group ), and finally execute the given run function. Let\u2019s have a look at the init_processes function. It ensures that every process will be able to coordinate through a master, using the same ip address and port. Note that we used the TCP backend, but we could have used MPI or Gloo instead. (c.f. Section 5.1 ) We will go over the magic happening in dist.init_process_group at the end of this tutorial, but it essentially allows processes to communicate with each other by sharing their locations.","title":"Setup"},{"location":"35/#point-to-point-communication","text":"Send and Recv A transfer of data from one process to another is called a point-to-point communication. These are achieved through the send and recv functions or their immediate counter-parts, isend and irecv . \"\"\"Blocking point-to-point communication.\"\"\" def run(rank, size): tensor = torch.zeros(1) if rank == 0: tensor += 1 # Send the tensor to process 1 dist.send(tensor=tensor, dst=1) else: # Receive tensor from process 0 dist.recv(tensor=tensor, src=0) print('Rank ', rank, ' has data ', tensor[0]) In the above example, both processes start with a zero tensor, then process 0 increments the tensor and sends it to process 1 so that they both end up with 1.0. Notice that process 1 needs to allocate memory in order to store the data it will receive. Also notice that send / recv are blocking : both processes stop until the communication is completed. On the other hand immediates are non-blocking ; the script continues its execution and the methods return a DistributedRequest object upon which we can choose to wait() . \"\"\"Non-blocking point-to-point communication.\"\"\" def run(rank, size): tensor = torch.zeros(1) req = None if rank == 0: tensor += 1 # Send the tensor to process 1 req = dist.isend(tensor=tensor, dst=1) print('Rank 0 started sending') else: # Receive tensor from process 0 req = dist.irecv(tensor=tensor, src=0) print('Rank 1 started receiving') req.wait() print('Rank ', rank, ' has data ', tensor[0]) When using immediates we have to be careful about with our usage of the sent and received tensors. Since we do not know when the data will be communicated to the other process, we should not modify the sent tensor nor access the received tensor before req.wait() has completed. In other words, writing to tensor after dist.isend() will result in undefined behaviour. reading from tensor after dist.irecv() will result in undefined behaviour. However, after req.wait() has been executed we are guaranteed that the communication took place, and that the value stored in tensor[0] is 1.0. Point-to-point communication is useful when we want a fine-grained control over the communication of our processes. They can be used to implement fancy algorithms, such as the one used in Baidu\u2019s DeepSpeech or Facebook\u2019s large-scale experiments .(c.f. Section 4.1 )","title":"Point-to-Point Communication"},{"location":"35/#collective-communication","text":"| Scatter | Gather | | Reduce | All-Reduce | | Broadcast | All-Gather | As opposed to point-to-point communcation, collectives allow for communication patterns across all processes in a group . A group is a subset of all our processes. To create a group, we can pass a list of ranks to dist.new_group(group) . By default, collectives are executed on the all processes, also known as the world . For example, in order to obtain the sum of all tensors at all processes, we can use the dist.all_reduce(tensor, op, group) collective. \"\"\" All-Reduce example.\"\"\" def run(rank, size): \"\"\" Simple point-to-point communication. \"\"\" group = dist.new_group([0, 1]) tensor = torch.ones(1) dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group) print('Rank ', rank, ' has data ', tensor[0]) Since we want the sum of all tensors in the group, we use dist.reduce_op.SUM as the reduce operator. Generally speaking, any commutative mathematical operation can be used as an operator. Out-of-the-box, PyTorch comes with 4 such operators, all working at the element-wise level: dist.reduce_op.SUM , dist.reduce_op.PRODUCT , dist.reduce_op.MAX , dist.reduce_op.MIN . In addition to dist.all_reduce(tensor, op, group) , there are a total of 6 collectives currently implemented in PyTorch. dist.broadcast(tensor, src, group) : Copies tensor from src to all other processes. dist.reduce(tensor, dst, op, group) : Applies op to all tensor and stores the result in dst . dist.all_reduce(tensor, op, group) : Same as reduce, but the result is stored in all processes. dist.scatter(tensor, src, scatter_list, group) : Copies the \\(i^{\\text{th}}\\) tensor scatter_list[i] to the \\(i^{\\text{th}}\\) process. dist.gather(tensor, dst, gather_list, group) : Copies tensor from all processes in dst . dist.all_gather(tensor_list, tensor, group) : Copies tensor from all processes to tensor_list , on all processes.","title":"Collective Communication"},{"location":"35/#distributed-training","text":"Note: You can find the example script of this section in this GitHub repository . Now that we understand how the distributed module works, let us write something useful with it. Our goal will be to replicate the functionality of DistributedDataParallel . Of course, this will be a didactic example and in a real-world situtation you should use the official, well-tested and well-optimized version linked above. Quite simply we want to implement a distributed version of stochastic gradient descent. Our script will let all processes compute the gradients of their model on their batch of data and then average their gradients. In order to ensure similar convergence results when changing the number of processes, we will first have to partition our dataset. (You could also use tnt.dataset.SplitDataset , instead of the snippet below.) \"\"\" Dataset partitioning helper \"\"\" class Partition(object): def __init__(self, data, index): self.data = data self.index = index def __len__(self): return len(self.index) def __getitem__(self, index): data_idx = self.index[index] return self.data[data_idx] class DataPartitioner(object): def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234): self.data = data self.partitions = [] rng = Random() rng.seed(seed) data_len = len(data) indexes = [x for x in range(0, data_len)] rng.shuffle(indexes) for frac in sizes: part_len = int(frac * data_len) self.partitions.append(indexes[0:part_len]) indexes = indexes[part_len:] def use(self, partition): return Partition(self.data, self.partitions[partition]) With the above snippet, we can now simply partition any dataset using the following few lines: \"\"\" Partitioning MNIST \"\"\" def partition_dataset(): dataset = datasets.MNIST('./data', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])) size = dist.get_world_size() bsz = 128 / float(size) partition_sizes = [1.0 / size for _ in range(size)] partition = DataPartitioner(dataset, partition_sizes) partition = partition.use(dist.get_rank()) train_set = torch.utils.data.DataLoader(partition, batch_size=bsz, shuffle=True) return train_set, bsz Assuming we have 2 replicas, then each process will have a train_set of 60000 / 2 = 30000 samples. We also divide the batch size by the number of replicas in order to maintain the overall batch size of 128. We can now write our usual forward-backward-optimize training code, and add a function call to average the gradients of our models. (The following is largely inspired from the official PyTorch MNIST example .) \"\"\" Distributed Synchronous SGD Example \"\"\" def run(rank, size): torch.manual_seed(1234) train_set, bsz = partition_dataset() model = Net() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) num_batches = ceil(len(train_set.dataset) / float(bsz)) for epoch in range(10): epoch_loss = 0.0 for data, target in train_set: data, target = Variable(data), Variable(target) optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) epoch_loss += loss.data[0] loss.backward() average_gradients(model) optimizer.step() print('Rank ', dist.get_rank(), ', epoch ', epoch, ': ', epoch_loss / num_batches) It remains to implement the average_gradients(model) function, which simply takes in a model and averages its gradients across the whole world. \"\"\" Gradient averaging. \"\"\" def average_gradients(model): size = float(dist.get_world_size()) for param in model.parameters(): dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM) param.grad.data /= size Et voil\u00e0 ! We successfully implemented distributed synchronous SGD and could train any model on a large computer cluster. Note: While the last sentence is technically true, there are a lot more tricks required to implement a production-level implementation of synchronous SGD. Again, use what has been tested and optimized .","title":"Distributed Training"},{"location":"35/#our-own-ring-allreduce","text":"As an additional challenge, imagine that we wanted to implement DeepSpeech\u2019s efficient ring allreduce. This is fairly easily implemented using point-to-point collectives. \"\"\" Implementation of a ring-reduce with addition. \"\"\" def allreduce(send, recv): rank = dist.get_rank() size = dist.get_world_size() send_buff = th.zeros(send.size()) recv_buff = th.zeros(send.size()) accum = th.zeros(send.size()) accum[:] = send[:] left = ((rank - 1) + size) % size right = (rank + 1) % size for i in range(size - 1): if i % 2 == 0: # Send send_buff send_req = dist.isend(send_buff, right) dist.recv(recv_buff, left) accum[:] += recv[:] else: # Send recv_buff send_req = dist.isend(recv_buff, right) dist.recv(send_buff, left) accum[:] += send[:] send_req.wait() recv[:] = accum[:] In the above script, the allreduce(send, recv) function has a slightly different signature than the ones in PyTorch. It takes a recv tensor and will store the sum of all send tensors in it. As an exercise left to the reader, there is still one difference between our version and the one in DeepSpeech: their implementation divide the gradient tensor into chunks , so as to optimially utilize the communication bandwidth. (Hint: toch.chunk )","title":"Our Own Ring-Allreduce"},{"location":"35/#advanced-topics","text":"We are now ready to discover some of the more advanced functionalities of torch.distributed . Since there is a lot to cover, this section is divided into two subsections: Communication Backends: where we learn how to use MPI and Gloo for GPU-GPU communication. Initialization Methods: where we understand how to best setup the initial coordination phase in dist.init_process_group() .","title":"Advanced Topics"},{"location":"35/#communication-backends","text":"One of the most elegant aspects of torch.distributed is its ability to abstract and build on top of different backends. As mentioned before, there are currently three backends implemented in PyTorch: TCP, MPI, and Gloo. They each have different specifications and tradeoffs, depending on the desired use-case. A comparative table of supported functions can be found here . TCP Backend So far we have made extensive usage of the TCP backend. It is quite handy as a development platform, as it is guaranteed to work on most machines and operating systems. It also supports all point-to-point and collective functions on CPU. However, there is no support for GPUs and its communication routines are not as optimized as the MPI one. Gloo Backend The Gloo backend provides an optimized implementation of collective communication procedures, both for CPUs and GPUs. It particularly shines on GPUs as it can perform communication without transferring data to the CPU\u2019s memory using GPUDirect . It is also capable of using NCCL to perform fast intra-node communication and implements its own algorithms for inter-node routines. Since version 0.2.0, the Gloo backend is automatically included with the pre-compiled binaries of PyTorch. As you have surely noticed, our distributed SGD example does not work if you put model on the GPU. Let\u2019s fix it by first replacing backend='gloo' in init_processes(rank, size, fn, backend='tcp') . At this point, the script will still run on CPU but uses the Gloo backend behind the scenes. In order to use multiple GPUs, let us also do the following modifications: init_processes(rank, size, fn, backend='tcp') \\(\\rightarrow\\) init_processes(rank, size, fn, backend='gloo') model = Net() \\(\\rightarrow\\) model = Net().cuda(rank) data, target = Variable(data), Variable(target) \\(\\rightarrow\\) data, target = Variable(data.cuda(rank)), Variable(target.cuda(rank)) With the above modifications, our model is now training on two GPUs and you can monitor their utilization with watch nvidia-smi . MPI Backend The Message Passing Interface (MPI) is a standardized tool from the field of high-performance computing. It allows to do point-to-point and collective communications and was the main inspiration for the API of torch.distributed . Several implementations of MPI exist (e.g. Open-MPI , MVAPICH2 , Intel MPI ) each optimized for different purposes. The advantage of using the MPI backend lies in MPI\u2019s wide availability - and high-level of optimization - on large computer clusters. Some recent implementations are also able to take advantage of CUDA IPC and GPU Direct technologies in order to avoid memory copies through the CPU. Unfortunately, PyTorch\u2019s binaries can not include an MPI implementation and we\u2019ll have to recompile it by hand. Fortunately, this process is fairly simple given that upon compilation, PyTorch will look by itself for an available MPI implementation. The following steps install the MPI backend, by installing PyTorch from sources . Create and activate your Anaconda environment, install all the pre-requisites following the guide , but do not run python setup.py install yet. Choose and install your favorite MPI implementation. Note that enabling CUDA-aware MPI might require some additional steps. In our case, we\u2019ll stick to Open-MPI without GPU support: conda install -c conda-forge openmpi Now, go to your cloned PyTorch repo and execute python setup.py install . In order to test our newly installed backend, a few modifications are required. Replace the content under if __name__ == '__main__': with init_processes(0, 0, run, backend='mpi') . Run mpirun -n 4 python myscript.py . The reason for these changes is that MPI needs to create its own environment before spawning the processes. MPI will also spawn its own processes and perform the handshake described in Initialization Methods , making the rank and size arguments of init_process_group superfluous. This is actually quite powerful as you can pass additional arguments to mpirun in order to tailor computational resources for each process. (Things like number of cores per process, hand-assigning machines to specific ranks, and some more ) Doing so, you should obtain the same familiar output as with the other communication backends.","title":"Communication Backends"},{"location":"35/#initialization-methods","text":"To finish this tutorial, let\u2019s talk about the very first function we called: dist.init_process_group(backend, init_method) . In particular, we will go over the different initialization methods which are responsible for the initial coordination step between each process. Those methods allow you to define how this coordination is done. Depending on your hardware setup, one of these methods should be naturally more suitable than the others. In addition to the following sections, you should also have a look at the official documentation . Before diving into the initialization methods, let\u2019s have a quick look at what happens behind init_process_group from the C/C++ perspective. First, the arguments are parsed and validated. The backend is resolved via the name2channel.at() function. A Channel class is returned, and will be used to perform the data transmission. The GIL is dropped, and THDProcessGroupInit() is called. This instantiates the channel and adds the address of the master node. The process with rank 0 will execute the master procedure, while all other ranks will be workers . The master Creates sockets for all workers. Waits for all workers to connect. Sends them information about the location of the other processes. Each worker Creates a socket to the master. Sends their own location information. Receives information about the other workers. Opens a socket and handshakes with all other workers. The initialization is done, and everyone is connected to everyone. Environment Variable We have been using the environment variable initialization method throughout this tutorial. By setting the following four environment variables on all machines, all processes will be able to properly connect to the master, obtain information about the other processes, and finally handshake with them. MASTER_PORT : A free port on the machine that will host the process with rank 0. MASTER_ADDR : IP address of the machine that will host the process with rank 0. WORLD_SIZE : The total number of processes, so that the master knows how many workers to wait for. RANK : Rank of each process, so they will know whether it is the master of a worker. Shared File System The shared filesystem requires all processes to have access to a shared file system, and will coordinate them through a shared file. This means that each process will open the file, write its information, and wait until everybody did so. After what all required information will be readily available to all processes. In order to avoid race conditions, the file system must support locking through fcntl . Note that you can specify ranks manually or let the processes figure it out by themselves. Be defining a unique groupname per job you can use the same file path for multiple jobs and safely avoid collision. dist.init_process_group(init_method='file:///mnt/nfs/sharedfile', world_size=4, group_name='mygroup') TCP Init & Multicast Initializing via TCP can be achieved in two different ways: By providing the IP address of the process with rank 0 and the world size. By providing any valid IP multicast address and the world size. In the first case, all workers will be able to connect to the process with rank 0 and follow the procedure described above. dist.init_process_group(init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4) In the second case, the multicast address specifies the group of nodes who might potentially be active and the coordination can be handled by allowing each process to have an initial handshake before following the above procedure. In addition TCP multicast initialization also supports a group_name argument (as with the shared file method) allowing multiple jobs to be scheduled on the same cluster. dist.init_process_group(init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456', world_size=4) Acknowledgements I\u2019d like to thank the PyTorch developers for doing such a good job on their implementation, documentation, and tests. When the code was unclear, I could always count on the docs or the tests to find an answer. In particular, I\u2019d like to thank Soumith Chintala, Adam Paszke, and Natalia Gimelshein for providing insightful comments and answering questions on early drafts.","title":"Initialization Methods"},{"location":"36/","text":"\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (Spatial Transformer Networks) \u6559\u7a0b \u539f\u4f5c\u8005 : Ghassen HAMROUNI \u5728\u8fd9\u7bc7\u6559\u7a0b\u4e2d, \u4f60\u4f1a\u5b66\u5230\u5982\u4f55\u7528\u540d\u4e3a\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (spatial transformer networks) \u7684\u89c6\u89c9\u6ce8\u610f\u529b\u7ed3\u6784\u6765\u52a0\u5f3a\u4f60\u7684\u7f51\u7edc. \u4f60\u53ef\u4ee5\u4ece\u8fd9\u7bc7\u8bba\u6587\u4e0a\u770b\u5230\u66f4\u591a\u5173\u4e8e\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (spatial transformer networks)\u7684\u77e5\u8bc6: DeepMind paper \u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (spatial transformer networks) \u662f\u5bf9\u5173\u6ce8\u7a7a\u95f4\u53d8\u6362\u53ef\u533a\u5206\u6027\u7684\u4e00\u79cd\u63a8\u5e7f \u5f62\u5f0f. \u77ed\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (STN for short) \u5141\u8bb8\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5982\u4f55\u5728\u8f93\u5165\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u7a7a \u95f4\u53d8\u6362, \u4ee5\u6b64\u6765\u589e\u5f3a\u6a21\u578b\u7684\u51e0\u4f55\u4e0d\u53d8\u6027. \u4f8b\u5982, \u5b83\u53ef\u4ee5\u88c1\u526a\u4e00\u4e2a\u611f\u5174\u8da3\u7684\u533a\u57df, \u7f29\u653e\u548c\u4fee\u6b63\u56fe\u50cf\u7684\u65b9\u5411. \u7531\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5bf9\u65cb\u8f6c\u3001\u7f29\u653e \u548c\u66f4\u666e\u904d\u4eff\u5c04\u53d8\u6362\u5e76\u4e0d\u5177\u6709\u4e0d\u53d8\u6027, \u56e0\u6b64\u5b83\u76f8\u5bf9\u6765\u8bf4\u662f\u4e00\u79cd\u6709\u7528\u7684\u7ed3\u6784. STN (\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc) \u6700\u597d\u7684\u4e00\u70b9\u662f\u5b83\u80fd\u5728\u975e\u5e38\u5c0f\u7684\u6539\u52a8\u4e4b\u540e, \u88ab\u7b80\u5355\u5730\u5d4c\u5165\u5230\u4efb\u4f55\u5df2\u5b58\u5728\u7684\u5377\u79ef\u795e \u7ecf\u7f51\u7edc\u4e2d. # \u8bb8\u53ef\u534f\u8bae: BSD # \u4f5c\u8005: Ghassen Hamrouni from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import torchvision from torchvision import datasets, transforms from torch.autograd import Variable import matplotlib.pyplot as plt import numpy as np plt.ion() # \u4ea4\u4e92\u6a21\u5f0f \u8bfb\u6570\u636e \u5728\u8fd9\u91cc\u6211\u4eec\u7528\u7ecf\u5178\u7684 MNIST \u6570\u636e\u96c6\u505a\u8bd5\u9a8c. \u4f7f\u7528\u4e00\u4e2a\u88ab\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc\u589e\u5f3a\u7684\u6807\u51c6\u5377\u79ef\u795e\u7ecf \u7f51\u7edc. use_cuda = torch.cuda.is_available() # \u8bad\u7ec3\u96c6 train_loader = torch.utils.data.DataLoader( datasets.MNIST(root='.', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=64, shuffle=True, num_workers=4) # \u6d4b\u8bd5\u96c6 test_loader = torch.utils.data.DataLoader( datasets.MNIST(root='.', train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=64, shuffle=True, num_workers=4) \u63cf\u8ff0\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (spatial transformer networks) \u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (spatial transformer networks) \u5f52\u7eb3\u4e3a\u4e09\u4e2a\u4e3b\u8981\u7684\u90e8\u4ef6 : \u672c\u5730\u7f51\u7edc (The localization network) \u662f\u4e00\u4e2a\u5e38\u89c4CNN, \u5b83\u53ef\u4ee5\u56de\u5f52\u8f6c\u6362\u53c2\u6570. \u8fd9\u79cd\u7a7a\u95f4\u8f6c\u6362\u4e0d\u662f\u7b80\u5355\u5730\u4ece\u6570\u636e\u96c6\u663e\u5f0f\u5b66\u4e60\u5230\u7684, \u800c\u662f\u81ea\u52a8\u5730\u5b66\u4e60\u4ee5\u589e\u5f3a\u5168\u5c40\u51c6\u786e\u7387. \u7f51\u683c\u751f\u6210\u5668 (The grid generator) \u5728\u8f93\u5165\u56fe\u50cf\u4e2d\u751f\u6210\u5bf9\u5e94\u4e8e\u6765\u81ea\u8f93\u51fa\u56fe\u50cf\u7684\u6bcf\u4e2a\u50cf \u7d20\u7684\u5750\u6807\u7f51\u683c. \u91c7\u6837\u5668 (The sampler) \u5c06\u8f6c\u6362\u7684\u53c2\u6570\u5e94\u7528\u4e8e\u8f93\u5165\u56fe\u50cf. Note \u6211\u4eec\u9700\u8981\u5305\u542b affine_grid \u548c grid_sample \u6a21\u5757\u7684 PyTorch \u6700\u65b0\u7248\u672c. class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) # \u7a7a\u95f4\u8f6c\u6362\u672c\u5730\u7f51\u7edc (Spatial transformer localization-network) self.localization = nn.Sequential( nn.Conv2d(1, 8, kernel_size=7), nn.MaxPool2d(2, stride=2), nn.ReLU(True), nn.Conv2d(8, 10, kernel_size=5), nn.MaxPool2d(2, stride=2), nn.ReLU(True) ) # 3 * 2 \u4eff\u5c04\u77e9\u9635 (affine matrix) \u7684\u56de\u5f52\u5668 self.fc_loc = nn.Sequential( nn.Linear(10 * 3 * 3, 32), nn.ReLU(True), nn.Linear(32, 3 * 2) ) # \u7528\u8eab\u4efd\u8f6c\u6362 (identity transformation) \u521d\u59cb\u5316\u6743\u91cd (weights) / \u504f\u7f6e (bias) self.fc_loc[2].weight.data.fill_(0) self.fc_loc[2].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0]) # \u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc\u7684\u524d\u5411\u51fd\u6570 (Spatial transformer network forward function) def stn(self, x): xs = self.localization(x) xs = xs.view(-1, 10 * 3 * 3) theta = self.fc_loc(xs) theta = theta.view(-1, 2, 3) grid = F.affine_grid(theta, x.size()) x = F.grid_sample(x, grid) return x def forward(self, x): # \u8f6c\u6362\u8f93\u5165 x = self.stn(x) # \u6267\u884c\u5e38\u89c4\u7684\u6b63\u5411\u4f20\u9012 x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=1) model = Net() if use_cuda: model.cuda() \u8bad\u7ec3\u6a21\u578b \u73b0\u5728, \u8ba9\u6211\u4eec\u7528 SGD \u7b97\u6cd5\u6765\u8bad\u7ec3\u6a21\u578b. \u8fd9\u4e2a\u7f51\u7edc\u7528\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\u5b66\u4e60\u5206\u7c7b\u4efb\u52a1. \u540c\u65f6, \u8fd9\u4e2a\u6a21\u578b\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u81ea\u52a8\u5730\u5b66\u4e60\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (STN) . optimizer = optim.SGD(model.parameters(), lr=0.01) def train(epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): if use_cuda: data, target = data.cuda(), target.cuda() data, target = Variable(data), Variable(target) optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if batch_idx % 500 == 0: print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.data[0])) # # \u4e00\u4e2a\u7b80\u5355\u7684\u6d4b\u8bd5\u7a0b\u5e8f\u6765\u6d4b\u91cf\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (STN) \u5728 MNIST \u4e0a\u7684\u8868\u73b0. # def test(): model.eval() test_loss = 0 correct = 0 for data, target in test_loader: if use_cuda: data, target = data.cuda(), target.cuda() data, target = Variable(data, volatile=True), Variable(target) output = model(data) # \u7d2f\u52a0\u6279loss test_loss += F.nll_loss(output, target, size_average=False).data[0] # \u5f97\u5230\u6700\u5927\u5bf9\u6570\u51e0\u7387 (log-probability) \u7684\u7d22\u5f15. pred = output.data.max(1, keepdim=True)[1] correct += pred.eq(target.data.view_as(pred)).cpu().sum() test_loss /= len(test_loader.dataset) print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n' .format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) \u53ef\u89c6\u5316\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (STN) \u7684\u7ed3\u679c \u73b0\u5728, \u6211\u4eec\u8981\u68c0\u67e5\u5b66\u5230\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u7ed3\u679c. \u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u5c0f\u7684\u8f85\u52a9\u51fd\u6570, \u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u89c6\u5316\u8f6c\u6362\u8fc7\u7a0b. def convert_image_np(inp): \"\"\"Convert a Tensor to numpy image.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) return inp # \u6211\u4eec\u60f3\u8981\u5728\u8bad\u7ec3\u4e4b\u540e\u53ef\u89c6\u5316\u7a7a\u95f4\u8f6c\u6362\u5c42 (spatial transformers layer) \u7684\u8f93\u51fa, \u6211\u4eec # \u7528 STN \u53ef\u89c6\u5316\u4e00\u6279\u8f93\u5165\u56fe\u50cf\u548c\u76f8\u5bf9\u4e8e\u7684\u8f6c\u6362\u540e\u7684\u6570\u636e. def visualize_stn(): # \u5f97\u5230\u4e00\u6279\u8f93\u5165\u6570\u636e data, _ = next(iter(test_loader)) data = Variable(data, volatile=True) if use_cuda: data = data.cuda() input_tensor = data.cpu().data transformed_input_tensor = model.stn(data).cpu().data in_grid = convert_image_np( torchvision.utils.make_grid(input_tensor)) out_grid = convert_image_np( torchvision.utils.make_grid(transformed_input_tensor)) # \u5e76\u884c\u5730 (side-by-side) \u753b\u51fa\u7ed3\u679c f, axarr = plt.subplots(1, 2) axarr[0].imshow(in_grid) axarr[0].set_title('Dataset Images') axarr[1].imshow(out_grid) axarr[1].set_title('Transformed Images') for epoch in range(1, 20 + 1): train(epoch) test() # \u5728\u4e00\u4e9b\u8f93\u5165\u6279\u6b21\u4e2d\u53ef\u89c6\u5316\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (STN) \u7684\u8f6c\u6362 visualize_stn() plt.ioff() plt.show() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: spatial_transformer_tutorial.py Download Jupyter notebook: spatial_transformer_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (Spatial Transformer Networks) \u6559\u7a0b"},{"location":"36/#spatial-transformer-networks","text":"\u539f\u4f5c\u8005 : Ghassen HAMROUNI \u5728\u8fd9\u7bc7\u6559\u7a0b\u4e2d, \u4f60\u4f1a\u5b66\u5230\u5982\u4f55\u7528\u540d\u4e3a\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (spatial transformer networks) \u7684\u89c6\u89c9\u6ce8\u610f\u529b\u7ed3\u6784\u6765\u52a0\u5f3a\u4f60\u7684\u7f51\u7edc. \u4f60\u53ef\u4ee5\u4ece\u8fd9\u7bc7\u8bba\u6587\u4e0a\u770b\u5230\u66f4\u591a\u5173\u4e8e\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (spatial transformer networks)\u7684\u77e5\u8bc6: DeepMind paper \u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (spatial transformer networks) \u662f\u5bf9\u5173\u6ce8\u7a7a\u95f4\u53d8\u6362\u53ef\u533a\u5206\u6027\u7684\u4e00\u79cd\u63a8\u5e7f \u5f62\u5f0f. \u77ed\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (STN for short) \u5141\u8bb8\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5982\u4f55\u5728\u8f93\u5165\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u7a7a \u95f4\u53d8\u6362, \u4ee5\u6b64\u6765\u589e\u5f3a\u6a21\u578b\u7684\u51e0\u4f55\u4e0d\u53d8\u6027. \u4f8b\u5982, \u5b83\u53ef\u4ee5\u88c1\u526a\u4e00\u4e2a\u611f\u5174\u8da3\u7684\u533a\u57df, \u7f29\u653e\u548c\u4fee\u6b63\u56fe\u50cf\u7684\u65b9\u5411. \u7531\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5bf9\u65cb\u8f6c\u3001\u7f29\u653e \u548c\u66f4\u666e\u904d\u4eff\u5c04\u53d8\u6362\u5e76\u4e0d\u5177\u6709\u4e0d\u53d8\u6027, \u56e0\u6b64\u5b83\u76f8\u5bf9\u6765\u8bf4\u662f\u4e00\u79cd\u6709\u7528\u7684\u7ed3\u6784. STN (\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc) \u6700\u597d\u7684\u4e00\u70b9\u662f\u5b83\u80fd\u5728\u975e\u5e38\u5c0f\u7684\u6539\u52a8\u4e4b\u540e, \u88ab\u7b80\u5355\u5730\u5d4c\u5165\u5230\u4efb\u4f55\u5df2\u5b58\u5728\u7684\u5377\u79ef\u795e \u7ecf\u7f51\u7edc\u4e2d. # \u8bb8\u53ef\u534f\u8bae: BSD # \u4f5c\u8005: Ghassen Hamrouni from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import torchvision from torchvision import datasets, transforms from torch.autograd import Variable import matplotlib.pyplot as plt import numpy as np plt.ion() # \u4ea4\u4e92\u6a21\u5f0f","title":"\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (Spatial Transformer Networks) \u6559\u7a0b"},{"location":"36/#_1","text":"\u5728\u8fd9\u91cc\u6211\u4eec\u7528\u7ecf\u5178\u7684 MNIST \u6570\u636e\u96c6\u505a\u8bd5\u9a8c. \u4f7f\u7528\u4e00\u4e2a\u88ab\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc\u589e\u5f3a\u7684\u6807\u51c6\u5377\u79ef\u795e\u7ecf \u7f51\u7edc. use_cuda = torch.cuda.is_available() # \u8bad\u7ec3\u96c6 train_loader = torch.utils.data.DataLoader( datasets.MNIST(root='.', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=64, shuffle=True, num_workers=4) # \u6d4b\u8bd5\u96c6 test_loader = torch.utils.data.DataLoader( datasets.MNIST(root='.', train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=64, shuffle=True, num_workers=4)","title":"\u8bfb\u6570\u636e"},{"location":"36/#spatial-transformer-networks_1","text":"\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (spatial transformer networks) \u5f52\u7eb3\u4e3a\u4e09\u4e2a\u4e3b\u8981\u7684\u90e8\u4ef6 : \u672c\u5730\u7f51\u7edc (The localization network) \u662f\u4e00\u4e2a\u5e38\u89c4CNN, \u5b83\u53ef\u4ee5\u56de\u5f52\u8f6c\u6362\u53c2\u6570. \u8fd9\u79cd\u7a7a\u95f4\u8f6c\u6362\u4e0d\u662f\u7b80\u5355\u5730\u4ece\u6570\u636e\u96c6\u663e\u5f0f\u5b66\u4e60\u5230\u7684, \u800c\u662f\u81ea\u52a8\u5730\u5b66\u4e60\u4ee5\u589e\u5f3a\u5168\u5c40\u51c6\u786e\u7387. \u7f51\u683c\u751f\u6210\u5668 (The grid generator) \u5728\u8f93\u5165\u56fe\u50cf\u4e2d\u751f\u6210\u5bf9\u5e94\u4e8e\u6765\u81ea\u8f93\u51fa\u56fe\u50cf\u7684\u6bcf\u4e2a\u50cf \u7d20\u7684\u5750\u6807\u7f51\u683c. \u91c7\u6837\u5668 (The sampler) \u5c06\u8f6c\u6362\u7684\u53c2\u6570\u5e94\u7528\u4e8e\u8f93\u5165\u56fe\u50cf. Note \u6211\u4eec\u9700\u8981\u5305\u542b affine_grid \u548c grid_sample \u6a21\u5757\u7684 PyTorch \u6700\u65b0\u7248\u672c. class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) # \u7a7a\u95f4\u8f6c\u6362\u672c\u5730\u7f51\u7edc (Spatial transformer localization-network) self.localization = nn.Sequential( nn.Conv2d(1, 8, kernel_size=7), nn.MaxPool2d(2, stride=2), nn.ReLU(True), nn.Conv2d(8, 10, kernel_size=5), nn.MaxPool2d(2, stride=2), nn.ReLU(True) ) # 3 * 2 \u4eff\u5c04\u77e9\u9635 (affine matrix) \u7684\u56de\u5f52\u5668 self.fc_loc = nn.Sequential( nn.Linear(10 * 3 * 3, 32), nn.ReLU(True), nn.Linear(32, 3 * 2) ) # \u7528\u8eab\u4efd\u8f6c\u6362 (identity transformation) \u521d\u59cb\u5316\u6743\u91cd (weights) / \u504f\u7f6e (bias) self.fc_loc[2].weight.data.fill_(0) self.fc_loc[2].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0]) # \u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc\u7684\u524d\u5411\u51fd\u6570 (Spatial transformer network forward function) def stn(self, x): xs = self.localization(x) xs = xs.view(-1, 10 * 3 * 3) theta = self.fc_loc(xs) theta = theta.view(-1, 2, 3) grid = F.affine_grid(theta, x.size()) x = F.grid_sample(x, grid) return x def forward(self, x): # \u8f6c\u6362\u8f93\u5165 x = self.stn(x) # \u6267\u884c\u5e38\u89c4\u7684\u6b63\u5411\u4f20\u9012 x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=1) model = Net() if use_cuda: model.cuda()","title":"\u63cf\u8ff0\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (spatial transformer networks)"},{"location":"36/#_2","text":"\u73b0\u5728, \u8ba9\u6211\u4eec\u7528 SGD \u7b97\u6cd5\u6765\u8bad\u7ec3\u6a21\u578b. \u8fd9\u4e2a\u7f51\u7edc\u7528\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\u5b66\u4e60\u5206\u7c7b\u4efb\u52a1. \u540c\u65f6, \u8fd9\u4e2a\u6a21\u578b\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u81ea\u52a8\u5730\u5b66\u4e60\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (STN) . optimizer = optim.SGD(model.parameters(), lr=0.01) def train(epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): if use_cuda: data, target = data.cuda(), target.cuda() data, target = Variable(data), Variable(target) optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if batch_idx % 500 == 0: print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.data[0])) # # \u4e00\u4e2a\u7b80\u5355\u7684\u6d4b\u8bd5\u7a0b\u5e8f\u6765\u6d4b\u91cf\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (STN) \u5728 MNIST \u4e0a\u7684\u8868\u73b0. # def test(): model.eval() test_loss = 0 correct = 0 for data, target in test_loader: if use_cuda: data, target = data.cuda(), target.cuda() data, target = Variable(data, volatile=True), Variable(target) output = model(data) # \u7d2f\u52a0\u6279loss test_loss += F.nll_loss(output, target, size_average=False).data[0] # \u5f97\u5230\u6700\u5927\u5bf9\u6570\u51e0\u7387 (log-probability) \u7684\u7d22\u5f15. pred = output.data.max(1, keepdim=True)[1] correct += pred.eq(target.data.view_as(pred)).cpu().sum() test_loss /= len(test_loader.dataset) print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n' .format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))","title":"\u8bad\u7ec3\u6a21\u578b"},{"location":"36/#stn","text":"\u73b0\u5728, \u6211\u4eec\u8981\u68c0\u67e5\u5b66\u5230\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u7ed3\u679c. \u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u5c0f\u7684\u8f85\u52a9\u51fd\u6570, \u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u89c6\u5316\u8f6c\u6362\u8fc7\u7a0b. def convert_image_np(inp): \"\"\"Convert a Tensor to numpy image.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) return inp # \u6211\u4eec\u60f3\u8981\u5728\u8bad\u7ec3\u4e4b\u540e\u53ef\u89c6\u5316\u7a7a\u95f4\u8f6c\u6362\u5c42 (spatial transformers layer) \u7684\u8f93\u51fa, \u6211\u4eec # \u7528 STN \u53ef\u89c6\u5316\u4e00\u6279\u8f93\u5165\u56fe\u50cf\u548c\u76f8\u5bf9\u4e8e\u7684\u8f6c\u6362\u540e\u7684\u6570\u636e. def visualize_stn(): # \u5f97\u5230\u4e00\u6279\u8f93\u5165\u6570\u636e data, _ = next(iter(test_loader)) data = Variable(data, volatile=True) if use_cuda: data = data.cuda() input_tensor = data.cpu().data transformed_input_tensor = model.stn(data).cpu().data in_grid = convert_image_np( torchvision.utils.make_grid(input_tensor)) out_grid = convert_image_np( torchvision.utils.make_grid(transformed_input_tensor)) # \u5e76\u884c\u5730 (side-by-side) \u753b\u51fa\u7ed3\u679c f, axarr = plt.subplots(1, 2) axarr[0].imshow(in_grid) axarr[0].set_title('Dataset Images') axarr[1].imshow(out_grid) axarr[1].set_title('Transformed Images') for epoch in range(1, 20 + 1): train(epoch) test() # \u5728\u4e00\u4e9b\u8f93\u5165\u6279\u6b21\u4e2d\u53ef\u89c6\u5316\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (STN) \u7684\u8f6c\u6362 visualize_stn() plt.ioff() plt.show() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: spatial_transformer_tutorial.py Download Jupyter notebook: spatial_transformer_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u53ef\u89c6\u5316\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edc (STN) \u7684\u7ed3\u679c"},{"location":"37/","text":"\u9ad8\u7ea7\u6559\u7a0b","title":"\u9ad8\u7ea7\u6559\u7a0b"},{"location":"37/#_1","text":"","title":"\u9ad8\u7ea7\u6559\u7a0b"},{"location":"38/","text":"\u7528 PyTorch \u505a \u795e\u7ecf\u8f6c\u6362 (Neural Transfer) \u539f\u4f5c\u8005 : Alexis Jacq \u4ecb\u7ecd \u6b22\u8fce\u89c2\u770b! \u8fd9\u7bc7\u6559\u7a0b\u89e3\u91ca\u4e86\u5982\u4f55\u5b9e\u73b0 Leon A. Gatys, Alexander S. Ecker \u548c Matthias Bethge \u51e0\u4f4d\u5b66\u8005\u53d1\u660e\u7684 Neural-Style \u7b97\u6cd5 . \u9898\u4e2d\u7684\u795e\u7ecf\u63cf\u8ff0\u7684\u662f\u4ec0\u4e48? \u795e\u7ecf\u98ce\u683c, \u6216\u8005\u8bf4\u795e\u7ecf\u8f6c\u6362\u662f\u4e00\u79cd\u7b97\u6cd5, \u5b83\u8f93\u5165\u4e00\u5f20\u5185\u5bb9\u56fe\u50cf (\u4f8b\u5982\u6d77\u9f9f), \u4e00\u5f20\u98ce\u683c\u56fe\u50cf (\u4f8b\u5982\u827a\u672f\u6ce2\u6d6a), \u7136\u540e\u8fd4\u56de\u5185\u5bb9\u56fe\u50cf\u7684\u5185\u5bb9, \u6b64\u65f6\u8fd4\u56de\u7684\u5185\u5bb9\u50cf\u662f\u88ab\u827a\u672f\u98ce\u683c\u56fe\u50cf\u7684\u98ce\u683c\u6e32\u67d3\u8fc7: \u5b83\u662f\u5982\u4f55\u5de5\u4f5c\u7684? \u539f\u7406\u5f88\u7b80\u5355: \u6211\u4eec\u5b9a\u4e49\u4e24\u4e2a\u8ddd\u79bb, \u4e00\u4e2a\u662f\u5173\u4e8e\u5185\u5bb9\u7684 ( \\(D_C\\) ) , \u53e6\u4e00\u4e2a\u662f\u5173\u4e8e\u98ce\u683c\u7684 ( \\(D_S\\) ) . \\(D_C\\) \u8861\u91cf\u4e24\u5f20\u56fe\u50cf\u7684\u5185\u5bb9\u6709\u591a\u4e48\u4e0d\u540c, \u800c \\(D_S\\) \u8861\u91cf\u4e24\u5f20\u56fe\u50cf\u7684\u98ce\u683c\u6709\u591a\u4e48\u4e0d\u540c. \u63a5\u7740\u6211\u4eec\u62ff\u51fa\u6211\u4eec\u7684\u8f93\u5165, \u4e5f\u5c31\u662f\u7b2c\u4e09\u5f20\u56fe\u50cf (\u4f8b\u5982\u5168\u566a\u58f0), \u7136\u540e\u6211\u4eec\u8f6c\u6362\u5b83, \u540c\u65f6\u6700\u5c0f\u5316\u5b83\u4e0e\u5185\u5bb9\u56fe\u50cf\u7684\u5185\u5bb9\u8ddd\u79bb\u548c\u5b83\u4e0e\u98ce\u683c\u56fe\u50cf\u7684\u98ce\u683c\u8ddd\u79bb. \u597d\u5427, \u5b83\u5177\u4f53\u662f\u600e\u4e48\u5de5\u4f5c\u7684? \u7ee7\u7eed\u6df1\u5165\u9700\u8981\u4e00\u4e9b\u6570\u5b66\u77e5\u8bc6. \u4ee4 \\(C_{nn}\\) \u4ee3\u8868\u4e00\u4e2a\u9884\u8bad\u7ec3\u597d\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc, \\(X\\) \u4ee3\u8868\u4efb\u4f55\u56fe\u50cf. \\(C_{nn}(X)\\) \u662f\u795e\u7ecf\u7f51\u7edc\u8f93\u5165 \\(X\\) \u540e\u7684\u7ed3\u679c (\u5305\u62ec\u5728\u6240\u6709\u5c42\u7684\u7279\u5f81\u6620\u5c04). \u4ee4 \\(F_{XL} \\in C_{nn}(X)\\) \u4ee3\u8868\u5728\u6df1\u5ea6\u4e3a \\(L\\) \u5c42\u5904\u7684\u7279\u5f81\u6620\u5c04, \u90fd\u77e2\u91cf\u5316\u548c\u7ea7\u8054\u4e3a\u4e00\u4e2a\u5355\u4e00\u77e2\u91cf. \u6211\u4eec\u7b80\u5355\u5730\u7528 \\(F_{XL}\\) \u5b9a\u4e49 \\(X\\) \u5728 \\(L\\) \u5c42\u7684\u5185\u5bb9. \u5982\u679c \\(Y\\) \u662f\u53e6\u4e00\u5f20\u548c \\(X\\) \u76f8\u540c\u5927\u5c0f\u7684\u56fe\u50cf, \u6211\u4eec\u5b9a\u4e49\u8fd9\u4e24\u5f20\u56fe\u50cf\u5728 \\(L\\) \u5c42\u7684\u5185\u5bb9\u8ddd\u79bb\u5982\u4e0b: \\[D_C^L(X,Y) = \\|F_{XL} - F_{YL}\\|^2 = \\sum_i (F_{XL}(i) - F_{YL}(i))^2\\] \u5f0f\u4e2d \\(F_{XL}(i)\\) \u662f \\(F_{XL}\\) \u7684\u7b2c \\(i^{th}\\) \u4e2a\u5143\u7d20. \u5b9a\u4e49\u98ce\u683c\u8981\u66f4\u7e41\u7410\u4e00\u4e9b. \u4ee4\u6ee1\u8db3 \\(k \\leq K\\) \u7684 \\(F_{XL}^k\\) \u4ee3\u8868 \\(L\\) \u5c42\u77e2\u91cf\u5316\u7684 \\(K\\) \u4e2a\u7279\u5f81\u6620\u5c04\u4e2d\u7684\u7b2c \\(k^{th}\\) \u4e2a. \u56fe\u50cf \\(X\\) \u5728 \\(L\\) \u5c42\u7684\u98ce\u683c \\(G_{XL}\\) \u5b9a\u4e49\u4e3a\u6ee1\u8db3 \\(k \\leq K\\) \u7684\u6240\u6709\u77e2\u91cf\u5316\u7279\u5f81\u6620\u5c04 \\(F_{XL}^k\\) \u7684\u514b\u4ea7\u7269 (Gram produce). \u6362\u53e5\u8bdd\u8bf4, \\(G_{XL}\\) \u662f\u4e00\u4e2a \\(K\\) x \\(K\\) \u7684\u77e9\u9635, \u5176\u5728 \\(k^{th}\\) \u884c\u548c \\(l^{th}\\) \u5217\u7684\u6bcf\u4e2a\u5143\u7d20 \\(G_{XL}(k,l)\\) \u662f \\(F_{XL}^k\\) \u548c \\(F_{XL}^l\\) \u4e4b\u95f4\u7684\u77e2\u91cf\u4ea7\u7269 : \\[G_{XL}(k,l) = \\langle F_{XL}^k, F_{XL}^l\\rangle = \\sum_i F_{XL}^k(i) . F_{XL}^l(i)\\] \u5f0f\u4e2d \\(F_{XL}^k(i)\\) \u662f \\(F_{XL}^k\\) \u7684\u7b2c \\(i^{th}\\) \u4e2a\u5143\u7d20. \u6211\u4eec\u53ef\u4ee5\u628a \\(G_{XL}(k,l)\\) \u5f53\u505a\u7279\u5f81\u6620\u5c04 \\(k\\) \u548c \\(l\\) \u76f8\u5173\u6027\u7684\u8861\u91cf. \u90a3\u6837\u7684\u8bdd, \\(G_{XL}\\) \u4ee3\u8868\u4e86 \\(X\\) \u5728 \\(L\\) \u5c42\u7279\u5f81\u5411\u91cf\u7684\u76f8\u5173\u6027\u77e9\u9635. \u6ce8\u610f \\(G_{XL}\\) \u7684\u5c3a\u5bf8\u53ea\u51b3\u5b9a\u4e8e\u7279\u5f81\u6620\u5c04\u7684\u6570\u91cf, \u4e0d\u88ab \\(X\\) \u7684\u5c3a\u5bf8\u6240\u5f71\u54cd. \u7136\u540e\u5982\u679c \\(Y\\) \u662f \u4efb\u610f\u5c3a\u5bf8 \u7684\u53e6\u4e00\u5f20\u56fe\u50cf, \u6211\u4eec\u5b9a\u4e49\u5728 \\(L\\) \u5c42\u7684\u98ce\u683c\u8ddd\u79bb\u5982\u4e0b: \\[D_S^L(X,Y) = \\|G_{XL} - G_{YL}\\|^2 = \\sum_{k,l} (G_{XL}(k,l) - G_{YL}(k,l))^2\\] \u8981\u60f3\u4e00\u6b21\u6027\u5730\u5728\u4e00\u4e9b\u5c42\u6700\u5c0f\u5316\u4e00\u4e2a\u53ef\u53d8\u56fe\u50cf \\(X\\) \u4e0e\u76ee\u6807\u5185\u5bb9\u56fe\u50cf \\(C\\) \u95f4\u7684 \\(D_C(X,C)\\) , \u548c \\(X\\) \u4e0e\u76ee\u6807\u98ce\u683c\u56fe\u50cf \\(S\\) \u95f4\u7684 \\(D_S(X,S)\\) , \u6211\u4eec\u8ba1\u7b97\u5e76\u52a0\u548c\u6bcf\u4e2a\u76ee\u6807\u5c42\u6bcf\u4e2a\u8ddd\u79bb\u7684\u68af\u5ea6 (\u5bf9 \\(X\\) \u6c42\u5bfc). \\[\\nabla_{ extit{total}}(X,S,C) = \\sum_{L_C} w_{CL_C}.\\nabla_{ extit{content}}^{L_C}(X,C) + \\sum_{L_S} w_{SL_S}.\\nabla_{ extit{style}}^{L_S}(X,S)\\] \u5f0f\u4e2d \\(L_C\\) \u548c \\(L_S\\) \u5206\u522b\u662f\u5185\u5bb9\u548c\u98ce\u683c\u7684\u76ee\u6807\u5c42(\u4efb\u610f\u9648\u8ff0), \\(w_{CL_C}\\) \u548c \\(w_{SL_S}\\) \u662f\u98ce\u683c\u548c\u5185\u5bb9\u5173\u4e8e\u6bcf\u4e2a\u76ee\u6807\u5c42\u7684\u6743\u91cd(\u4efb\u610f\u9648\u8ff0). \u7136\u540e\u6211\u4eec\u5bf9 \\(X\\) \u8fdb\u884c\u68af\u5ea6\u4e0b\u964d: \\[X \\leftarrow X - \\alpha \\nabla_{ extit{total}}(X,S,C)\\] \u597d\u5427, \u6570\u5b66\u7684\u90e8\u5206\u5c31\u5230\u6b64\u4e3a\u6b62. \u5982\u679c\u4f60\u60f3\u8981\u66f4\u52a0\u6df1\u5165(\u6bd4\u5982\u600e\u4e48\u8ba1\u7b97\u68af\u5ea6), \u6211\u4eec\u63a8\u8350\u4f60\u9605\u8bfb\u539f\u59cb\u8bba\u6587 (\u4f5c\u8005\u662f Leon A. Gatys \u548c AL), \u8bba\u6587\u4e2d\u8fd9\u90e8\u5206\u89e3\u91ca\u5730\u66f4\u597d\u66f4\u6e05\u6670. \u5bf9\u4e8e\u5728 PyTorch \u4e2d\u7684\u5b9e\u73b0, \u6211\u4eec\u5df2\u7ecf\u6709\u4e86\u6211\u4eec\u9700\u8981\u7684\u4e00\u5207: \u4e8b\u5b9e\u4e0a\u5c31\u662f PyTorch, \u6240\u6709\u7684\u68af\u5ea6\u90fd\u88ab\u4e3a\u4f60\u81ea\u52a8\u4e14\u52a8\u6001\u5730\u8ba1\u7b97(\u5f53\u4f60\u4ece\u5e93\u4e2d\u4f7f\u7528\u51fd\u6570\u65f6). \u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u7b97\u6cd5\u7684\u5b9e\u73b0\u5728 PyTorch \u4e2d\u53d8\u5f97\u975e\u5e38\u8f7b\u677e. PyTorch \u5b9e\u73b0 \u5982\u679c\u4f60\u4e0d\u786e\u5b9a\u662f\u5426\u7406\u89e3\u4e86\u4ee5\u4e0a\u6570\u5b66\u516c\u5f0f, \u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5b9e\u73b0\u5b83, \u5728\u8fc7\u7a0b\u4e2d\u6709\u6240\u9886\u609f. \u5982\u679c\u4f60\u6b63\u5728\u63a2\u7d22 PyTorch , \u6211\u4eec\u63a8\u8350\u4f60\u5148\u9605\u8bfb\u8fd9\u7bc7\u6559\u7a0b Introduction to PyTorch . \u5305 \u6211\u4eec\u5c06\u4f1a\u4f9d\u8d56\u4e0b\u5217\u8fd9\u4e9b\u5305: torch , torch.nn , numpy (indispensables packages for neural networks with PyTorch) torch.autograd.Variable (dynamic computation of the gradient wrt a variable) torch.optim (efficient gradient descents) PIL , PIL.Image , matplotlib.pyplot (load and display images) torchvision.transforms (treat PIL images and transform into torch tensors) torchvision.models (train or load pre-trained models) copy (to deep copy the models; system package) from __future__ import print_function import torch import torch.nn as nn from torch.autograd import Variable import torch.optim as optim from PIL import Image import matplotlib.pyplot as plt import torchvision.transforms as transforms import torchvision.models as models import copy Cuda \u5982\u679c\u4f60\u7684\u8ba1\u7b97\u673a\u91cc\u6709 GPU, \u63a8\u8350\u5728\u4e0a\u9762\u8fd0\u884c\u7b97\u6cd5, \u5c24\u5176\u662f\u5f53\u4f60\u8981\u5c1d\u8bd5 \u5927\u578b\u7f51\u7edc\u65f6 (\u5c31\u50cf VGG). \u6709\u9274\u4e8e\u6b64, \u6211\u4eec\u6709 torch.cuda.is_available() , \u5982\u679c\u4f60\u7684\u8ba1\u7b97\u673a\u6709\u53ef\u7528 GPU \u5219\u4f1a\u8fd4\u56de True. \u7136\u540e\u6211\u4eec\u7528 .cuda() \u65b9\u6cd5 \u5c06\u53ef\u5206\u914d\u7684\u8fdb\u7a0b\u548c\u6a21\u5757\u4ece CPU \u79fb\u52a8\u5230 GPU. \u5f53\u6211\u4eec\u60f3\u5c06\u8fd9\u4e9b\u6a21\u5757\u91cd\u65b0\u79fb\u56de CPU \u7684\u65f6\u5019(\u6bd4\u5982\u8981\u7528 numpy), \u6211\u4eec\u7528 .cpu() \u65b9\u6cd5. \u6700\u540e, .type(dtype) \u4f1a\u7528\u6765\u5c06\u4e00\u4e2a torch.FloatTensor \u8f6c\u5316\u4e3a \u7528\u4e8e GPU \u8fdb\u7a0b\u8f93\u5165\u7684 torch.cuda.FloatTensor . use_cuda = torch.cuda.is_available() dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor \u8bfb\u53d6\u56fe\u50cf \u4e3a\u4e86\u7b80\u5316\u5176\u5b9e\u73b0, \u8ba9\u6211\u4eec\u4ece\u5bfc\u5165\u4e00\u4e2a\u76f8\u540c\u7ef4\u5ea6\u7684\u98ce\u683c\u548c\u5185\u5bb9\u56fe\u50cf\u5f00\u59cb. \u7136\u540e\u6211\u4eec\u5c06\u5b83\u4eec\u7f29\u653e\u5230\u60f3\u8981\u7684\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8 (\u5728\u4f8b\u5b50\u4e2d\u662f 128 \u548c 512, \u53d6\u51b3\u4f60\u7684 GPU \u662f\u5426\u53ef\u7528) \u7136\u540e\u628a\u5b83\u4eec\u8f6c\u5316\u4e3a torch \u5f20\u91cf, \u4ee5\u5f85\u5582\u5165\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc. Note \u8fd9\u91cc\u662f\u6559\u7a0b\u9700\u8981\u7684\u56fe\u50cf\u7684\u4e0b\u8f7d\u94fe\u63a5: picasso.jpg \u548c dancing.jpg . \u4e0b\u8f7d\u8fd9\u4e24\u5f20\u56fe\u50cf\u7136\u540e\u628a\u5b83\u4eec\u52a0\u5165\u5230\u540d\u4e3a images \u7684\u76ee\u5f55\u4e2d. # \u60f3\u8981\u7684\u8f93\u51fa\u56fe\u50cf\u5c3a\u5bf8 imsize = 512 if use_cuda else 128 # \u5982\u679c\u6ca1\u6709 GPU \u5219\u4f7f\u7528\u5c0f\u5c3a\u5bf8 loader = transforms.Compose([ transforms.Scale(imsize), # \u7f29\u653e\u56fe\u50cf transforms.ToTensor()]) # \u5c06\u5176\u8f6c\u5316\u4e3a torch \u5f20\u91cf def image_loader(image_name): image = Image.open(image_name) image = Variable(loader(image)) # \u7531\u4e8e\u795e\u7ecf\u7f51\u7edc\u8f93\u5165\u7684\u9700\u8981, \u6dfb\u52a0 batch \u7684\u7ef4\u5ea6 image = image.unsqueeze(0) return image style_img = image_loader(\"images/picasso.jpg\").type(dtype) content_img = image_loader(\"images/dancing.jpg\").type(dtype) assert style_img.size() == content_img.size(), \\ \"we need to import style and content images of the same size\" \u5bfc\u5165\u7684 PIL \u56fe\u50cf\u50cf\u7d20\u503c\u7684\u8303\u56f4\u4e3a 0 \u5230 255. \u8f6c\u5316\u4e3a torch \u5f20\u91cf\u540e, \u5b83\u4eec\u7684\u503c\u8303\u56f4\u53d8\u4e3a\u4e86 0 \u5230and 1. \u8fd9\u662f\u4e2a\u91cd\u8981\u7684\u7ec6\u8282: torch \u5e93\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u88ab\u4f7f\u7528 0-1 \u7684\u5f20\u91cf\u56fe\u50cf\u8bad\u7ec3. \u5982\u679c\u4f60\u5c1d\u8bd5\u7528 0-255 \u7684\u5f20\u91cf\u56fe\u50cf\u5582\u5165\u795e\u7ecf\u7f51\u7edc, \u6fc0\u6d3b\u7684\u7279\u5f81\u6620\u5c04\u5c31\u6ca1\u7528\u4e86. \u8fd9\u4e0d\u662f \u4f7f\u7528 Caffe \u5e93\u4e2d\u9884\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc, Caffe \u4e2d\u662f\u7528 0-255 \u7684\u5f20\u91cf\u56fe\u50cf\u8bad\u7ec3\u7684. \u663e\u793a\u56fe\u50cf \u6211\u4eec\u5c06\u4f7f\u7528 plt.imshow \u6765\u663e\u793a\u56fe\u50cf. \u6240\u4ee5\u6211\u4eec\u9700\u8981\u5148\u628a\u5b83\u4eec\u8f6c\u56de PIL \u56fe\u50cf. unloader = transforms.ToPILImage() # \u8f6c\u56de PIL \u56fe\u50cf plt.ion() def imshow(tensor, title=None): image = tensor.clone().cpu() # \u514b\u9686\u662f\u4e3a\u4e86\u4e0d\u6539\u53d8\u5b83 image = image.view(3, imsize, imsize) # \u79fb\u9664 batch \u7ef4\u5ea6 image = unloader(image) plt.imshow(image) if title is not None: plt.title(title) plt.pause(0.001) # \u6682\u505c\u4e00\u4f1a, \u8ba9\u7ed8\u56fe\u66f4\u65b0 plt.figure() imshow(style_img.data, title='Style Image') plt.figure() imshow(content_img.data, title='Content Image') \u5185\u5bb9\u635f\u5931 \u5185\u5bb9\u635f\u5931\u662f\u4e00\u4e2a\u5728\u7f51\u7edc\u8f93\u5165\u4e3a \\(X\\) \u7684\u5c42 \\(L\\) \u8f93\u5165\u7279\u5f81\u6620\u5c04 \\(F_{XL}\\) \u7684\u51fd\u6570, \u8fd4\u56de\u6b64\u56fe\u50cf\u4e0e\u5185\u5bb9\u56fe\u50cf\u95f4\u7684\u52a0\u6743\u5185\u5bb9\u8ddd\u79bb \\(w_{CL}.D_C^L(X,C)\\) . \u4e4b\u540e, \u6743\u91cd \\(w_{CL}\\) \u548c\u76ee\u6807\u5185\u5bb9 \\(F_{CL}\\) \u5c31\u6210\u4e3a\u4e86\u51fd\u6570\u7684\u53c2\u6570. \u6211\u4eec\u628a\u8fd9\u4e2a\u51fd\u6570\u4f5c\u4e3a torch \u6a21\u5757\u6765\u5b9e\u73b0, \u628a\u8fd9\u4e9b\u53c2 \u6570\u4f5c\u4e3a\u6784\u9020\u5668\u7684\u8f93\u5165. \u8fd9\u4e2a\u8ddd\u79bb \\(\\|F_{XL} - F_{YL}\\|^2\\) \u662f\u4e24\u4e2a\u7279\u5f81\u6620\u5c04\u96c6\u7684 \u5747\u65b9\u8bef\u5dee, \u53ef\u4ee5\u7528\u4f5c\u4e3a\u7b2c\u4e09\u4e2a\u53c2\u6570\u7684\u6807\u51c6\u7684 nn.MSELoss \u6765\u8ba1\u7b97. \u6211\u4eec\u4f1a\u5728\u6bcf\u4e2a\u76ee\u6807\u5c42\u52a0\u5165\u6211\u4eec\u7684\u5185\u5bb9\u635f\u5931\u4f5c\u4e3a\u989d\u5916\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u5757. \u8fd9\u6837, \u6bcf\u6b21\u6211\u4eec\u90fd\u4f1a\u7ed9\u795e\u7ecf \u7f51\u7edc\u6295\u5582\u4e00\u5f20\u8f93\u5165\u56fe\u50cf \\(X\\) , \u6240\u6709\u7684\u635f\u5931\u90fd\u4f1a\u5728\u76ee\u6807\u5c42\u88ab\u8ba1\u7b97, \u591a\u4e8f\u4e86\u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97, \u6240\u6709\u68af\u5ea6\u90fd\u4f1a\u88ab\u641e\u5b9a. \u8981\u5b9e\u73b0, \u6211\u4eec\u53ea\u9700\u5199\u51fa\u8f6c\u6362\u6a21\u5757\u7684 forward \u65b9\u6cd5, \u8fd9\u4e2a\u6a21\u5757\u5c31\u53d8 \u6210\u4e86\u7f51\u7edc\u7684 \u2018\u2019transparent layer (\u900f\u660e\u5c42)\u2019\u2018, \u8ba1\u7b97\u597d\u7684\u635f\u5931\u88ab\u5b58\u4e3a\u6a21\u5757\u7684\u53c2\u6570. \u6700\u540e, \u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u5047\u7684 backward \u65b9\u6cd5, \u5b83\u4ec5\u4ec5\u53ea\u8c03\u7528\u540e\u5411\u65b9\u6cd5 nn.MSELoss \u6765\u91cd\u6784\u68af\u5ea6. \u8fd9\u4e2a\u65b9\u6cd5\u8fd4\u56de\u8ba1\u7b97\u597d\u7684\u635f\u5931: \u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\u65f6\u8981\u60f3\u663e\u793a\u98ce\u683c\u548c\u5185\u5bb9\u635f\u5931\u7684\u53d8\u5316, \u8fd9\u4f1a\u975e\u5e38\u6709\u7528. class ContentLoss(nn.Module): def __init__(self, target, weight): super(ContentLoss, self).__init__() # \u6211\u4eec\u4f1a\u4ece\u6240\u4f7f\u7528\u7684\u6811\u4e2d\u201c\u5206\u79bb\u201d\u76ee\u6807\u5185\u5bb9 self.target = target.detach() * weight # \u52a8\u6001\u5730\u8ba1\u7b97\u68af\u5ea6: \u5b83\u662f\u4e2a\u72b6\u6001\u503c, \u4e0d\u662f\u53d8\u91cf. # \u5426\u5219\u8bc4\u4ef7\u6307\u6807\u7684\u524d\u5411\u65b9\u6cd5\u4f1a\u629b\u51fa\u9519\u8bef. self.weight = weight self.criterion = nn.MSELoss() def forward(self, input): self.loss = self.criterion(input * self.weight, self.target) self.output = input return self.output def backward(self, retain_graph=True): self.loss.backward(retain_graph=retain_graph) return self.loss Note \u91cd\u8981\u7ec6\u8282 : \u8fd9\u4e2a\u6a21\u5757\u867d\u7136\u53eb\u505a ContentLoss , \u5374\u4e0d\u662f\u4e2a\u771f\u6b63\u7684 Pytorch \u635f\u5931\u51fd\u6570. \u5982\u679c\u4f60\u60f3\u50cf Pytorch \u635f\u5931\u4e00\u6837\u5b9a\u4e49\u4f60\u7684\u5185\u5bb9\u635f\u5931, \u4f60\u5f97\u65b0\u5efa\u4e00\u4e2a Pytorch \u81ea\u52a8\u6c42\u5bfc\u51fd\u6570\u5e76\u624b\u52a8\u5f97\u5728 backward \u65b9\u6cd5\u4e2d\u91cd\u7b97/\u5b9e\u73b0\u68af\u5ea6. \u98ce\u683c\u635f\u5931 \u5bf9\u4e8e\u98ce\u683c\u635f\u5931, \u6211\u4eec\u9996\u5148\u9700\u8981\u5b9a\u4e49\u4e00\u4e2a\u7ed9\u5b9a\u8f93\u5165 \\(X\\) \u5728 \\(L\\) \u5c42\u7684\u7279\u5f81\u6620\u5c04 \\(F_{XL}\\) \u65f6\u8ba1\u7b97\u514b\u4ea7\u7269 \\(G_{XL}\\) \u7684\u6a21\u5757. \u4ee4 \\(\\hat{F}_{XL}\\) \u8868\u793a \\(F_{XL}\\) \u91cd\u53d8\u5f62\u4e3a \\(K\\) x \\(N\\) \u7684\u7248\u672c, \u8fd9\u91cc \\(K\\) \u662f \\(L\\) \u5c42\u7279\u5f81 \u6620\u5c04\u7684\u6570\u91cf, \\(N\\) \u662f\u4efb\u610f\u77e2\u91cf\u5316\u7279\u5f81\u6620\u5c04 \\(F_{XL}^k\\) \u7684\u957f\u5ea6. \\(\\hat{F}_{XL}\\) \u7684\u7b2c \\(k^{th}\\) \u884c\u662f \\(F_{XL}^k\\) . \u53ef\u4ee5\u9a8c\u8bc1 \\(\\hat{F}_{XL} \\cdot \\hat{F}_{XL}^T = G_{XL}\\) . \u9274\u4e8e\u6b64, \u5b9e\u73b0\u6211\u4eec\u7684\u6a21\u5757\u5c31\u5f88\u5bb9\u6613\u4e86: class GramMatrix(nn.Module): def forward(self, input): a, b, c, d = input.size() # a=batch size(=1) # b= \u7279\u5f81\u6620\u5c04\u7684\u6570\u91cf # (c,d)= \u4e00\u4e2a\u7279\u5f81\u6620\u5c04\u7684\u7ef4\u5ea6 (N=c*d) features = input.view(a * b, c * d) # \u5c06 F_XL \u8f6c\u6362\u4e3a \\hat F_XL G = torch.mm(features, features.t()) # \u8ba1\u7b97\u514b\u4ea7\u7269 (gram product) # \u6211\u4eec\u7528\u9664\u4ee5\u6bcf\u4e2a\u7279\u5f81\u6620\u5c04\u5143\u7d20\u6570\u91cf\u7684\u65b9\u6cd5 # \u6807\u51c6\u5316\u514b\u77e9\u9635 (gram matrix) \u7684\u503c return G.div(a * b * c * d) \u7279\u5f81\u6620\u5c04\u7684\u7ef4\u5ea6 \\(N\\) \u8d8a\u957f, \u5219\u514b\u77e9\u9635 (gram matrix) \u7684\u503c\u8d8a\u5927. \u56e0\u6b64\u5982\u679c\u6211\u4eec\u4e0d\u7528 \\(N\\) \u6765\u6807\u51c6\u5316, \u5728\u68af\u5ea6\u4e0b\u964d\u8fc7\u7a0b\u4e2d\u7b2c\u4e00\u5c42 (\u5728\u6c60\u5316\u5c42\u4e4b\u524d) \u7684\u635f\u5931\u8ba1\u7b97\u5c31\u4f1a\u8fc7\u4e8e\u91cd\u8981. \u6211\u4eec\u5f53\u7136\u4e0d\u5e0c\u671b\u8fd9\u6837, \u56e0\u4e3a\u6211\u4eec\u611f\u5174\u8da3\u7684\u98ce\u683c\u7279\u5f81\u90fd\u5728\u6700\u6df1\u7684\u90a3\u4e9b\u5c42! \u63a5\u7740, \u98ce\u683c\u635f\u5931\u6a21\u5757\u88ab\u4ee5\u548c\u5185\u5bb9\u635f\u5931\u6a21\u5757\u76f8\u540c\u7684\u65b9\u5f0f\u5b9e\u73b0, \u4f46\u662f\u6211\u4eec\u8fd8\u5f97\u628a gramMatrix \u52a0\u5165\u4f5c\u4e3a\u53c2\u6570: class StyleLoss(nn.Module): def __init__(self, target, weight): super(StyleLoss, self).__init__() self.target = target.detach() * weight self.weight = weight self.gram = GramMatrix() self.criterion = nn.MSELoss() def forward(self, input): self.output = input.clone() self.G = self.gram(input) self.G.mul_(self.weight) self.loss = self.criterion(self.G, self.target) return self.output def backward(self, retain_graph=True): self.loss.backward(retain_graph=retain_graph) return self.loss \u8bfb\u53d6\u795e\u7ecf\u7f51\u7edc \u73b0\u5728, \u6211\u4eec\u8981\u5bfc\u5165\u4e00\u4e2a\u9884\u8bad\u7ec3\u597d\u7684\u795e\u7ecf\u7f51\u7edc. \u548c\u8bba\u6587\u4e00\u6837, \u6211\u4eec\u7528\u9884\u8bad\u7ec3 \u7684 19 \u5c42 VGG \u7f51\u7edc (VGG19). PyTorch\u5bf9 VGG \u7684\u5b9e\u73b0\u6a21\u5757\u5206\u4e3a\u4e24\u4e2a\u5b50 Sequential \u6a21\u5757: features (\u5305\u62ec\u5377\u79ef\u548c\u6c60\u5316\u5c42) \u548c classifier (\u5305\u62ec\u5168\u8fde\u63a5\u5c42). \u6211\u4eec\u53ea\u5bf9 features \u611f\u5174\u8da3: cnn = models.vgg19(pretrained=True).features # \u53ef\u80fd\u7684\u8bdd\u5c06\u5b83\u79fb\u5230 GPU \u4e0a: if use_cuda: cnn = cnn.cuda() Sequential (\u987a\u5e8f) \u6a21\u5757\u5305\u542b\u4e00\u4e2a\u5b50\u6a21\u5757\u7684\u5217\u8868. \u6bd4\u5982, vgg19.features \u5305\u542b\u4e00\u4e2a\u4ee5\u6b63\u786e\u6df1\u5ea6\u6392\u5217\u7684\u5e8f\u5217 (Conv2d, ReLU, Maxpool2d, Conv2d, ReLU\u2026), \u5c31\u5982\u6211\u4eec\u5728 Content loss \u90e8\u5206\u8bb2\u5230\u7684, \u6211\u4eec\u60f3\u8981\u628a\u6211\u4eec\u7684\u98ce\u683c\u548c\u5185\u5bb9\u635f\u5931\u6a21\u5757\u4ee5\u60f3\u8981\u7684\u6df1\u5ea6\u4f5c\u4e3a \u2018\u900f\u660e\u5c42\u2019 \u52a0\u5165\u5230 \u6211\u4eec\u7684\u7f51\u7edc\u4e2d. \u4e3a\u4e86\u8fd9\u6837, \u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684 Sequential (\u987a\u5e8f) \u6a21\u5757, \u5728\u5176\u4e2d\u6211\u4eec\u628a vgg19 \u548c\u6211\u4eec\u7684\u635f\u5931\u6a21\u5757\u4ee5\u6b63\u786e\u7684\u987a\u5e8f\u52a0\u5165: # \u5e0c\u671b\u8ba1\u7b97\u98ce\u683c/\u5185\u5bb9\u635f\u5931\u7684\u5c42 : content_layers_default = ['conv_4'] style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5'] def get_style_model_and_losses(cnn, style_img, content_img, style_weight=1000, content_weight=1, content_layers=content_layers_default, style_layers=style_layers_default): cnn = copy.deepcopy(cnn) # \u4ec5\u4e3a\u4e86\u6709\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u5217\u8868 \u5185\u5bb9/\u98ce\u683c \u635f\u5931 content_losses = [] style_losses = [] model = nn.Sequential() # \u65b0\u5efa\u7684 Sequential \u7f51\u7edc\u6a21\u5757 gram = GramMatrix() # \u6211\u4eec\u9700\u8981\u4e00\u4e2a\u514b\u6a21\u5757 (gram module) \u6765\u8ba1\u7b97\u98ce\u683c\u76ee\u6807 # \u53ef\u80fd\u7684\u8bdd\u5c06\u8fd9\u4e9b\u6a21\u5757\u79fb\u5230 GPU \u4e0a: if use_cuda: model = model.cuda() gram = gram.cuda() i = 1 for layer in list(cnn): if isinstance(layer, nn.Conv2d): name = \"conv_\" + str(i) model.add_module(name, layer) if name in content_layers: # \u52a0\u5185\u5bb9\u635f\u5931: target = model(content_img).clone() content_loss = ContentLoss(target, content_weight) model.add_module(\"content_loss_\" + str(i), content_loss) content_losses.append(content_loss) if name in style_layers: # \u52a0\u98ce\u683c\u635f\u5931: target_feature = model(style_img).clone() target_feature_gram = gram(target_feature) style_loss = StyleLoss(target_feature_gram, style_weight) model.add_module(\"style_loss_\" + str(i), style_loss) style_losses.append(style_loss) if isinstance(layer, nn.ReLU): name = \"relu_\" + str(i) model.add_module(name, layer) if name in content_layers: # \u52a0\u5185\u5bb9\u635f\u5931: target = model(content_img).clone() content_loss = ContentLoss(target, content_weight) model.add_module(\"content_loss_\" + str(i), content_loss) content_losses.append(content_loss) if name in style_layers: # \u52a0\u98ce\u683c\u635f\u5931: target_feature = model(style_img).clone() target_feature_gram = gram(target_feature) style_loss = StyleLoss(target_feature_gram, style_weight) model.add_module(\"style_loss_\" + str(i), style_loss) style_losses.append(style_loss) i += 1 if isinstance(layer, nn.MaxPool2d): name = \"pool_\" + str(i) model.add_module(name, layer) # *** return model, style_losses, content_losses Note \u5728\u8fd9\u7bc7\u8bba\u6587\u4e2d\u4ed6\u4eec\u63a8\u8350\u5c06\u6700\u5927\u6c60\u5316\u5c42\u66f4\u6539\u4e3a\u5e73\u5747\u6c60\u5316\u5c42. AlexNet\u662f\u4e00\u4e2a\u6bd4 VGG19 \u66f4\u5c0f\u7684\u7f51\u7edc, \u7528\u5b83\u5b9e\u73b0\u7684\u8bdd\u6211\u4eec\u4e5f\u4e0d\u4f1a\u770b\u5230 \u4efb\u4f55\u7ed3\u679c\u8d28\u91cf\u7684\u4e0d\u540c. \u800c\u5982\u679c\u4f60\u60f3\u505a\u8fd9\u4e2a\u66ff\u4ee3\u7684\u8bdd, \u53ef\u4ee5\u7528\u8fd9\u4e9b\u4ee3\u7801: # avgpool = nn.AvgPool2d(kernel_size=layer.kernel_size, # stride=layer.stride, padding = layer.padding) # model.add_module(name,avgpool) \u8f93\u5165\u56fe\u50cf \u4e3a\u4e86\u7b80\u5316\u4ee3\u7801, \u6211\u4eec\u7528\u4e0e\u5185\u5bb9\u548c\u98ce\u683c\u56fe\u50cf\u540c\u6837\u5c3a\u5bf8\u7684\u56fe\u50cf\u505a\u8f93\u5165. \u8fd9\u4e2a\u56fe\u50cf\u53ef\u4ee5\u662f\u767d\u566a\u58f0\u7684, \u4e5f\u53ef\u4ee5\u662f\u4e00\u4efd\u5185\u5bb9\u56fe\u50cf\u7684\u62f7\u8d1d. input_img = content_img.clone() # \u5982\u679c\u4f60\u60f3\u7528\u767d\u566a\u58f0\u505a\u8f93\u5165, \u8bf7\u53d6\u6d88\u4e0b\u9762\u7684\u6ce8\u91ca\u884c: # input_img = Variable(torch.randn(content_img.data.size())).type(dtype) # \u5728\u7ed8\u56fe\u4e2d\u52a0\u5165\u539f\u59cb\u7684\u8f93\u5165\u56fe\u50cf: plt.figure() imshow(input_img.data, title='Input Image') \u68af\u5ea6\u4e0b\u964d \u7531\u4e8e\u672c\u7b97\u6cd5\u7684\u4f5c\u8005 Leon Gatys \u7684\u5efa\u8bae here , \u6211\u4eec\u5c06\u4f7f\u7528 L-BFGS \u7b97\u6cd5\u6765\u8dd1\u6211\u4eec\u7684\u68af\u5ea6\u4e0b\u964d. \u548c\u8bad\u7ec3\u4e00\u4e2a\u7f51\u7edc\u4e0d\u540c\u7684\u662f, \u6211\u4eec\u5e0c\u671b\u8bad\u7ec3\u8f93\u5165\u56fe\u50cf\u6765\u6700\u5c0f\u5316 \u5185\u5bb9/\u98ce\u683c \u635f\u5931. \u6211\u4eec\u60f3\u7b80\u5355\u5730\u5efa\u4e00\u4e2a PyTorch L-BFGS \u4f18\u5316\u5668, \u4f20\u5165\u6211\u4eec\u7684\u56fe\u50cf\u4f5c\u4e3a\u53d8\u91cf\u8fdb\u884c\u4f18\u5316. \u4f46\u662f optim.LBFGS \u7684\u7b2c\u4e00\u4e2a\u5f62\u53c2\u662f\u4e00\u4e2a\u9700\u8981\u68af\u5ea6\u7684 PyTorch Variable . \u6211\u4eec\u7684\u8f93\u5165\u56fe\u50cf\u662f\u4e00\u4e2a Variable , \u4f46\u4e0d\u662f\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\u7684\u6811\u7684\u53f6\u8282\u70b9. \u4e3a\u4e86\u4f7f\u8fd9\u4e2a\u53d8\u91cf\u9700\u8981\u68af\u5ea6\u8fd0\u7b97, \u4e00\u4e2a\u53ef\u80fd\u7684\u65b9\u6cd5\u662f\u4ece\u8f93\u5165\u56fe\u50cf\u6784\u5efa\u4e00\u4e2a Parameter (\u53c2\u6570) \u5bf9\u8c61. \u7136\u540e\u6211\u4eec\u53ea\u9700\u7ed9\u4f18\u5316\u5668\u7684\u6784\u9020\u5668\u4f20\u9012\u4e00\u4e2a \u5305\u542b\u8fd9\u4e2a\u53c2\u6570\u7684\u5217\u8868: def get_input_param_optimizer(input_img): # \u8fd9\u884c\u663e\u793a\u4e86\u8f93\u5165\u662f\u4e00\u4e2a\u9700\u8981\u68af\u5ea6\u8ba1\u7b97\u7684\u53c2\u6570 input_param = nn.Parameter(input_img.data) optimizer = optim.LBFGS([input_param]) return input_param, optimizer \u6700\u540e\u4e00\u6b65 : \u5faa\u73af\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d. \u6bcf\u4e00\u6b65\u4e2d\u6211\u4eec\u5fc5\u987b\u5582\u7ed9\u795e\u7ecf\u7f51\u7edc\u66f4\u65b0\u540e \u7684\u8f93\u5165\u4ee5\u8ba1\u7b97\u65b0\u7684\u635f\u5931, \u6211\u4eec\u8981\u8fd0\u884c\u6bcf\u4e2a\u635f\u5931\u7684 backward \u65b9\u6cd5\u6765\u52a8\u6001 \u8ba1\u7b97\u4ed6\u4eec\u7684\u68af\u5ea6\u5e76\u5448\u73b0\u68af\u5ea6\u4e0b\u964d\u7684\u6bcf\u4e00\u6b65. \u8fd9\u4e2a\u4f18\u5316\u5668\u9700\u8981\u4e00\u4e2a \u201cclosure\u201d : \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u5e76\u8fd4\u56de\u635f\u5931\u7684\u51fd\u6570. \u7136\u800c, \u8fd9\u91cc\u6709\u4e00\u4e2a\u5c0f\u95ee\u9898. \u88ab\u4f18\u5316\u7684\u56fe\u50cf\u7684\u50cf\u7d20\u503c\u4f1a\u5728 \\(-\\infty\\) \u548c \\(+\\infty\\) \u4e4b\u95f4\u6ce2\u52a8, \u800c\u4e0d\u662f\u7ee7\u7eed\u4fdd\u6301\u5728 0 \u5230 1. \u6362\u53e5\u8bdd\u8bf4, \u56fe\u50cf\u53ef\u80fd\u4f1a\u88ab\u5b8c\u7f8e\u5730\u4f18\u5316\u6210\u8352\u8c2c\u7684\u503c. \u4e8b\u5b9e\u4e0a, \u6211\u4eec\u5fc5\u987b\u5728\u9650\u5236\u4e0b\u4f7f\u7528 \u4f18\u5316\u5668\u6765\u4f7f\u6211\u4eec\u7684\u8f93\u5165\u56fe\u50cf\u4e00\u76f4\u4fdd\u6301\u6b63\u786e\u7684\u503c. \u6709\u4e00\u4e2a\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6848: \u5728\u6bcf\u4e00\u6b65, \u90fd\u6821\u6b63\u56fe\u50cf\u4f7f\u5176\u4fdd\u6301 0-1 \u8303\u56f4\u7684\u503c. def run_style_transfer(cnn, content_img, style_img, input_img, num_steps=300, style_weight=1000, content_weight=1): \"\"\"Run the style transfer.\"\"\" print('Building the style transfer model..') model, style_losses, content_losses = get_style_model_and_losses(cnn, style_img, content_img, style_weight, content_weight) input_param, optimizer = get_input_param_optimizer(input_img) print('Optimizing..') run = [0] while run[0] <= num_steps: def closure(): # \u6821\u6b63\u66f4\u65b0\u540e\u7684\u8f93\u5165\u56fe\u50cf\u503c input_param.data.clamp_(0, 1) optimizer.zero_grad() model(input_param) style_score = 0 content_score = 0 for sl in style_losses: style_score += sl.backward() for cl in content_losses: content_score += cl.backward() run[0] += 1 if run[0] % 50 == 0: print(\"run {}:\".format(run)) print('Style Loss : {:4f} Content Loss: {:4f}'.format( style_score.data[0], content_score.data[0])) print() return style_score + content_score optimizer.step(closure) # \u6700\u540e\u4e00\u6b21\u7684\u6821\u6b63... input_param.data.clamp_(0, 1) return input_param.data \u6700\u540e, \u8fd0\u884c\u7b97\u6cd5 output = run_style_transfer(cnn, content_img, style_img, input_img) plt.figure() imshow(output, title='Output Image') # sphinx_gallery_thumbnail_number = 4 plt.ioff() plt.show() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: neural_style_tutorial.py Download Jupyter notebook: neural_style_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u7528 PyTorch \u505a \u795e\u7ecf\u8f6c\u6362 (Neural Transfer)"},{"location":"38/#pytorch-neural-transfer","text":"\u539f\u4f5c\u8005 : Alexis Jacq","title":"\u7528 PyTorch \u505a \u795e\u7ecf\u8f6c\u6362 (Neural Transfer)"},{"location":"38/#_1","text":"\u6b22\u8fce\u89c2\u770b! \u8fd9\u7bc7\u6559\u7a0b\u89e3\u91ca\u4e86\u5982\u4f55\u5b9e\u73b0 Leon A. Gatys, Alexander S. Ecker \u548c Matthias Bethge \u51e0\u4f4d\u5b66\u8005\u53d1\u660e\u7684 Neural-Style \u7b97\u6cd5 .","title":"\u4ecb\u7ecd"},{"location":"38/#_2","text":"\u795e\u7ecf\u98ce\u683c, \u6216\u8005\u8bf4\u795e\u7ecf\u8f6c\u6362\u662f\u4e00\u79cd\u7b97\u6cd5, \u5b83\u8f93\u5165\u4e00\u5f20\u5185\u5bb9\u56fe\u50cf (\u4f8b\u5982\u6d77\u9f9f), \u4e00\u5f20\u98ce\u683c\u56fe\u50cf (\u4f8b\u5982\u827a\u672f\u6ce2\u6d6a), \u7136\u540e\u8fd4\u56de\u5185\u5bb9\u56fe\u50cf\u7684\u5185\u5bb9, \u6b64\u65f6\u8fd4\u56de\u7684\u5185\u5bb9\u50cf\u662f\u88ab\u827a\u672f\u98ce\u683c\u56fe\u50cf\u7684\u98ce\u683c\u6e32\u67d3\u8fc7:","title":"\u9898\u4e2d\u7684\u795e\u7ecf\u63cf\u8ff0\u7684\u662f\u4ec0\u4e48?"},{"location":"38/#_3","text":"\u539f\u7406\u5f88\u7b80\u5355: \u6211\u4eec\u5b9a\u4e49\u4e24\u4e2a\u8ddd\u79bb, \u4e00\u4e2a\u662f\u5173\u4e8e\u5185\u5bb9\u7684 ( \\(D_C\\) ) , \u53e6\u4e00\u4e2a\u662f\u5173\u4e8e\u98ce\u683c\u7684 ( \\(D_S\\) ) . \\(D_C\\) \u8861\u91cf\u4e24\u5f20\u56fe\u50cf\u7684\u5185\u5bb9\u6709\u591a\u4e48\u4e0d\u540c, \u800c \\(D_S\\) \u8861\u91cf\u4e24\u5f20\u56fe\u50cf\u7684\u98ce\u683c\u6709\u591a\u4e48\u4e0d\u540c. \u63a5\u7740\u6211\u4eec\u62ff\u51fa\u6211\u4eec\u7684\u8f93\u5165, \u4e5f\u5c31\u662f\u7b2c\u4e09\u5f20\u56fe\u50cf (\u4f8b\u5982\u5168\u566a\u58f0), \u7136\u540e\u6211\u4eec\u8f6c\u6362\u5b83, \u540c\u65f6\u6700\u5c0f\u5316\u5b83\u4e0e\u5185\u5bb9\u56fe\u50cf\u7684\u5185\u5bb9\u8ddd\u79bb\u548c\u5b83\u4e0e\u98ce\u683c\u56fe\u50cf\u7684\u98ce\u683c\u8ddd\u79bb.","title":"\u5b83\u662f\u5982\u4f55\u5de5\u4f5c\u7684?"},{"location":"38/#_4","text":"\u7ee7\u7eed\u6df1\u5165\u9700\u8981\u4e00\u4e9b\u6570\u5b66\u77e5\u8bc6. \u4ee4 \\(C_{nn}\\) \u4ee3\u8868\u4e00\u4e2a\u9884\u8bad\u7ec3\u597d\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc, \\(X\\) \u4ee3\u8868\u4efb\u4f55\u56fe\u50cf. \\(C_{nn}(X)\\) \u662f\u795e\u7ecf\u7f51\u7edc\u8f93\u5165 \\(X\\) \u540e\u7684\u7ed3\u679c (\u5305\u62ec\u5728\u6240\u6709\u5c42\u7684\u7279\u5f81\u6620\u5c04). \u4ee4 \\(F_{XL} \\in C_{nn}(X)\\) \u4ee3\u8868\u5728\u6df1\u5ea6\u4e3a \\(L\\) \u5c42\u5904\u7684\u7279\u5f81\u6620\u5c04, \u90fd\u77e2\u91cf\u5316\u548c\u7ea7\u8054\u4e3a\u4e00\u4e2a\u5355\u4e00\u77e2\u91cf. \u6211\u4eec\u7b80\u5355\u5730\u7528 \\(F_{XL}\\) \u5b9a\u4e49 \\(X\\) \u5728 \\(L\\) \u5c42\u7684\u5185\u5bb9. \u5982\u679c \\(Y\\) \u662f\u53e6\u4e00\u5f20\u548c \\(X\\) \u76f8\u540c\u5927\u5c0f\u7684\u56fe\u50cf, \u6211\u4eec\u5b9a\u4e49\u8fd9\u4e24\u5f20\u56fe\u50cf\u5728 \\(L\\) \u5c42\u7684\u5185\u5bb9\u8ddd\u79bb\u5982\u4e0b: \\[D_C^L(X,Y) = \\|F_{XL} - F_{YL}\\|^2 = \\sum_i (F_{XL}(i) - F_{YL}(i))^2\\] \u5f0f\u4e2d \\(F_{XL}(i)\\) \u662f \\(F_{XL}\\) \u7684\u7b2c \\(i^{th}\\) \u4e2a\u5143\u7d20. \u5b9a\u4e49\u98ce\u683c\u8981\u66f4\u7e41\u7410\u4e00\u4e9b. \u4ee4\u6ee1\u8db3 \\(k \\leq K\\) \u7684 \\(F_{XL}^k\\) \u4ee3\u8868 \\(L\\) \u5c42\u77e2\u91cf\u5316\u7684 \\(K\\) \u4e2a\u7279\u5f81\u6620\u5c04\u4e2d\u7684\u7b2c \\(k^{th}\\) \u4e2a. \u56fe\u50cf \\(X\\) \u5728 \\(L\\) \u5c42\u7684\u98ce\u683c \\(G_{XL}\\) \u5b9a\u4e49\u4e3a\u6ee1\u8db3 \\(k \\leq K\\) \u7684\u6240\u6709\u77e2\u91cf\u5316\u7279\u5f81\u6620\u5c04 \\(F_{XL}^k\\) \u7684\u514b\u4ea7\u7269 (Gram produce). \u6362\u53e5\u8bdd\u8bf4, \\(G_{XL}\\) \u662f\u4e00\u4e2a \\(K\\) x \\(K\\) \u7684\u77e9\u9635, \u5176\u5728 \\(k^{th}\\) \u884c\u548c \\(l^{th}\\) \u5217\u7684\u6bcf\u4e2a\u5143\u7d20 \\(G_{XL}(k,l)\\) \u662f \\(F_{XL}^k\\) \u548c \\(F_{XL}^l\\) \u4e4b\u95f4\u7684\u77e2\u91cf\u4ea7\u7269 : \\[G_{XL}(k,l) = \\langle F_{XL}^k, F_{XL}^l\\rangle = \\sum_i F_{XL}^k(i) . F_{XL}^l(i)\\] \u5f0f\u4e2d \\(F_{XL}^k(i)\\) \u662f \\(F_{XL}^k\\) \u7684\u7b2c \\(i^{th}\\) \u4e2a\u5143\u7d20. \u6211\u4eec\u53ef\u4ee5\u628a \\(G_{XL}(k,l)\\) \u5f53\u505a\u7279\u5f81\u6620\u5c04 \\(k\\) \u548c \\(l\\) \u76f8\u5173\u6027\u7684\u8861\u91cf. \u90a3\u6837\u7684\u8bdd, \\(G_{XL}\\) \u4ee3\u8868\u4e86 \\(X\\) \u5728 \\(L\\) \u5c42\u7279\u5f81\u5411\u91cf\u7684\u76f8\u5173\u6027\u77e9\u9635. \u6ce8\u610f \\(G_{XL}\\) \u7684\u5c3a\u5bf8\u53ea\u51b3\u5b9a\u4e8e\u7279\u5f81\u6620\u5c04\u7684\u6570\u91cf, \u4e0d\u88ab \\(X\\) \u7684\u5c3a\u5bf8\u6240\u5f71\u54cd. \u7136\u540e\u5982\u679c \\(Y\\) \u662f \u4efb\u610f\u5c3a\u5bf8 \u7684\u53e6\u4e00\u5f20\u56fe\u50cf, \u6211\u4eec\u5b9a\u4e49\u5728 \\(L\\) \u5c42\u7684\u98ce\u683c\u8ddd\u79bb\u5982\u4e0b: \\[D_S^L(X,Y) = \\|G_{XL} - G_{YL}\\|^2 = \\sum_{k,l} (G_{XL}(k,l) - G_{YL}(k,l))^2\\] \u8981\u60f3\u4e00\u6b21\u6027\u5730\u5728\u4e00\u4e9b\u5c42\u6700\u5c0f\u5316\u4e00\u4e2a\u53ef\u53d8\u56fe\u50cf \\(X\\) \u4e0e\u76ee\u6807\u5185\u5bb9\u56fe\u50cf \\(C\\) \u95f4\u7684 \\(D_C(X,C)\\) , \u548c \\(X\\) \u4e0e\u76ee\u6807\u98ce\u683c\u56fe\u50cf \\(S\\) \u95f4\u7684 \\(D_S(X,S)\\) , \u6211\u4eec\u8ba1\u7b97\u5e76\u52a0\u548c\u6bcf\u4e2a\u76ee\u6807\u5c42\u6bcf\u4e2a\u8ddd\u79bb\u7684\u68af\u5ea6 (\u5bf9 \\(X\\) \u6c42\u5bfc). \\[\\nabla_{ extit{total}}(X,S,C) = \\sum_{L_C} w_{CL_C}.\\nabla_{ extit{content}}^{L_C}(X,C) + \\sum_{L_S} w_{SL_S}.\\nabla_{ extit{style}}^{L_S}(X,S)\\] \u5f0f\u4e2d \\(L_C\\) \u548c \\(L_S\\) \u5206\u522b\u662f\u5185\u5bb9\u548c\u98ce\u683c\u7684\u76ee\u6807\u5c42(\u4efb\u610f\u9648\u8ff0), \\(w_{CL_C}\\) \u548c \\(w_{SL_S}\\) \u662f\u98ce\u683c\u548c\u5185\u5bb9\u5173\u4e8e\u6bcf\u4e2a\u76ee\u6807\u5c42\u7684\u6743\u91cd(\u4efb\u610f\u9648\u8ff0). \u7136\u540e\u6211\u4eec\u5bf9 \\(X\\) \u8fdb\u884c\u68af\u5ea6\u4e0b\u964d: \\[X \\leftarrow X - \\alpha \\nabla_{ extit{total}}(X,S,C)\\] \u597d\u5427, \u6570\u5b66\u7684\u90e8\u5206\u5c31\u5230\u6b64\u4e3a\u6b62. \u5982\u679c\u4f60\u60f3\u8981\u66f4\u52a0\u6df1\u5165(\u6bd4\u5982\u600e\u4e48\u8ba1\u7b97\u68af\u5ea6), \u6211\u4eec\u63a8\u8350\u4f60\u9605\u8bfb\u539f\u59cb\u8bba\u6587 (\u4f5c\u8005\u662f Leon A. Gatys \u548c AL), \u8bba\u6587\u4e2d\u8fd9\u90e8\u5206\u89e3\u91ca\u5730\u66f4\u597d\u66f4\u6e05\u6670. \u5bf9\u4e8e\u5728 PyTorch \u4e2d\u7684\u5b9e\u73b0, \u6211\u4eec\u5df2\u7ecf\u6709\u4e86\u6211\u4eec\u9700\u8981\u7684\u4e00\u5207: \u4e8b\u5b9e\u4e0a\u5c31\u662f PyTorch, \u6240\u6709\u7684\u68af\u5ea6\u90fd\u88ab\u4e3a\u4f60\u81ea\u52a8\u4e14\u52a8\u6001\u5730\u8ba1\u7b97(\u5f53\u4f60\u4ece\u5e93\u4e2d\u4f7f\u7528\u51fd\u6570\u65f6). \u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u7b97\u6cd5\u7684\u5b9e\u73b0\u5728 PyTorch \u4e2d\u53d8\u5f97\u975e\u5e38\u8f7b\u677e.","title":"\u597d\u5427, \u5b83\u5177\u4f53\u662f\u600e\u4e48\u5de5\u4f5c\u7684?"},{"location":"38/#pytorch","text":"\u5982\u679c\u4f60\u4e0d\u786e\u5b9a\u662f\u5426\u7406\u89e3\u4e86\u4ee5\u4e0a\u6570\u5b66\u516c\u5f0f, \u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5b9e\u73b0\u5b83, \u5728\u8fc7\u7a0b\u4e2d\u6709\u6240\u9886\u609f. \u5982\u679c\u4f60\u6b63\u5728\u63a2\u7d22 PyTorch , \u6211\u4eec\u63a8\u8350\u4f60\u5148\u9605\u8bfb\u8fd9\u7bc7\u6559\u7a0b Introduction to PyTorch .","title":"PyTorch \u5b9e\u73b0"},{"location":"38/#_5","text":"\u6211\u4eec\u5c06\u4f1a\u4f9d\u8d56\u4e0b\u5217\u8fd9\u4e9b\u5305: torch , torch.nn , numpy (indispensables packages for neural networks with PyTorch) torch.autograd.Variable (dynamic computation of the gradient wrt a variable) torch.optim (efficient gradient descents) PIL , PIL.Image , matplotlib.pyplot (load and display images) torchvision.transforms (treat PIL images and transform into torch tensors) torchvision.models (train or load pre-trained models) copy (to deep copy the models; system package) from __future__ import print_function import torch import torch.nn as nn from torch.autograd import Variable import torch.optim as optim from PIL import Image import matplotlib.pyplot as plt import torchvision.transforms as transforms import torchvision.models as models import copy","title":"\u5305"},{"location":"38/#cuda","text":"\u5982\u679c\u4f60\u7684\u8ba1\u7b97\u673a\u91cc\u6709 GPU, \u63a8\u8350\u5728\u4e0a\u9762\u8fd0\u884c\u7b97\u6cd5, \u5c24\u5176\u662f\u5f53\u4f60\u8981\u5c1d\u8bd5 \u5927\u578b\u7f51\u7edc\u65f6 (\u5c31\u50cf VGG). \u6709\u9274\u4e8e\u6b64, \u6211\u4eec\u6709 torch.cuda.is_available() , \u5982\u679c\u4f60\u7684\u8ba1\u7b97\u673a\u6709\u53ef\u7528 GPU \u5219\u4f1a\u8fd4\u56de True. \u7136\u540e\u6211\u4eec\u7528 .cuda() \u65b9\u6cd5 \u5c06\u53ef\u5206\u914d\u7684\u8fdb\u7a0b\u548c\u6a21\u5757\u4ece CPU \u79fb\u52a8\u5230 GPU. \u5f53\u6211\u4eec\u60f3\u5c06\u8fd9\u4e9b\u6a21\u5757\u91cd\u65b0\u79fb\u56de CPU \u7684\u65f6\u5019(\u6bd4\u5982\u8981\u7528 numpy), \u6211\u4eec\u7528 .cpu() \u65b9\u6cd5. \u6700\u540e, .type(dtype) \u4f1a\u7528\u6765\u5c06\u4e00\u4e2a torch.FloatTensor \u8f6c\u5316\u4e3a \u7528\u4e8e GPU \u8fdb\u7a0b\u8f93\u5165\u7684 torch.cuda.FloatTensor . use_cuda = torch.cuda.is_available() dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor","title":"Cuda"},{"location":"38/#_6","text":"\u4e3a\u4e86\u7b80\u5316\u5176\u5b9e\u73b0, \u8ba9\u6211\u4eec\u4ece\u5bfc\u5165\u4e00\u4e2a\u76f8\u540c\u7ef4\u5ea6\u7684\u98ce\u683c\u548c\u5185\u5bb9\u56fe\u50cf\u5f00\u59cb. \u7136\u540e\u6211\u4eec\u5c06\u5b83\u4eec\u7f29\u653e\u5230\u60f3\u8981\u7684\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8 (\u5728\u4f8b\u5b50\u4e2d\u662f 128 \u548c 512, \u53d6\u51b3\u4f60\u7684 GPU \u662f\u5426\u53ef\u7528) \u7136\u540e\u628a\u5b83\u4eec\u8f6c\u5316\u4e3a torch \u5f20\u91cf, \u4ee5\u5f85\u5582\u5165\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc. Note \u8fd9\u91cc\u662f\u6559\u7a0b\u9700\u8981\u7684\u56fe\u50cf\u7684\u4e0b\u8f7d\u94fe\u63a5: picasso.jpg \u548c dancing.jpg . \u4e0b\u8f7d\u8fd9\u4e24\u5f20\u56fe\u50cf\u7136\u540e\u628a\u5b83\u4eec\u52a0\u5165\u5230\u540d\u4e3a images \u7684\u76ee\u5f55\u4e2d. # \u60f3\u8981\u7684\u8f93\u51fa\u56fe\u50cf\u5c3a\u5bf8 imsize = 512 if use_cuda else 128 # \u5982\u679c\u6ca1\u6709 GPU \u5219\u4f7f\u7528\u5c0f\u5c3a\u5bf8 loader = transforms.Compose([ transforms.Scale(imsize), # \u7f29\u653e\u56fe\u50cf transforms.ToTensor()]) # \u5c06\u5176\u8f6c\u5316\u4e3a torch \u5f20\u91cf def image_loader(image_name): image = Image.open(image_name) image = Variable(loader(image)) # \u7531\u4e8e\u795e\u7ecf\u7f51\u7edc\u8f93\u5165\u7684\u9700\u8981, \u6dfb\u52a0 batch \u7684\u7ef4\u5ea6 image = image.unsqueeze(0) return image style_img = image_loader(\"images/picasso.jpg\").type(dtype) content_img = image_loader(\"images/dancing.jpg\").type(dtype) assert style_img.size() == content_img.size(), \\ \"we need to import style and content images of the same size\" \u5bfc\u5165\u7684 PIL \u56fe\u50cf\u50cf\u7d20\u503c\u7684\u8303\u56f4\u4e3a 0 \u5230 255. \u8f6c\u5316\u4e3a torch \u5f20\u91cf\u540e, \u5b83\u4eec\u7684\u503c\u8303\u56f4\u53d8\u4e3a\u4e86 0 \u5230and 1. \u8fd9\u662f\u4e2a\u91cd\u8981\u7684\u7ec6\u8282: torch \u5e93\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u88ab\u4f7f\u7528 0-1 \u7684\u5f20\u91cf\u56fe\u50cf\u8bad\u7ec3. \u5982\u679c\u4f60\u5c1d\u8bd5\u7528 0-255 \u7684\u5f20\u91cf\u56fe\u50cf\u5582\u5165\u795e\u7ecf\u7f51\u7edc, \u6fc0\u6d3b\u7684\u7279\u5f81\u6620\u5c04\u5c31\u6ca1\u7528\u4e86. \u8fd9\u4e0d\u662f \u4f7f\u7528 Caffe \u5e93\u4e2d\u9884\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc, Caffe \u4e2d\u662f\u7528 0-255 \u7684\u5f20\u91cf\u56fe\u50cf\u8bad\u7ec3\u7684.","title":"\u8bfb\u53d6\u56fe\u50cf"},{"location":"38/#_7","text":"\u6211\u4eec\u5c06\u4f7f\u7528 plt.imshow \u6765\u663e\u793a\u56fe\u50cf. \u6240\u4ee5\u6211\u4eec\u9700\u8981\u5148\u628a\u5b83\u4eec\u8f6c\u56de PIL \u56fe\u50cf. unloader = transforms.ToPILImage() # \u8f6c\u56de PIL \u56fe\u50cf plt.ion() def imshow(tensor, title=None): image = tensor.clone().cpu() # \u514b\u9686\u662f\u4e3a\u4e86\u4e0d\u6539\u53d8\u5b83 image = image.view(3, imsize, imsize) # \u79fb\u9664 batch \u7ef4\u5ea6 image = unloader(image) plt.imshow(image) if title is not None: plt.title(title) plt.pause(0.001) # \u6682\u505c\u4e00\u4f1a, \u8ba9\u7ed8\u56fe\u66f4\u65b0 plt.figure() imshow(style_img.data, title='Style Image') plt.figure() imshow(content_img.data, title='Content Image')","title":"\u663e\u793a\u56fe\u50cf"},{"location":"38/#_8","text":"\u5185\u5bb9\u635f\u5931\u662f\u4e00\u4e2a\u5728\u7f51\u7edc\u8f93\u5165\u4e3a \\(X\\) \u7684\u5c42 \\(L\\) \u8f93\u5165\u7279\u5f81\u6620\u5c04 \\(F_{XL}\\) \u7684\u51fd\u6570, \u8fd4\u56de\u6b64\u56fe\u50cf\u4e0e\u5185\u5bb9\u56fe\u50cf\u95f4\u7684\u52a0\u6743\u5185\u5bb9\u8ddd\u79bb \\(w_{CL}.D_C^L(X,C)\\) . \u4e4b\u540e, \u6743\u91cd \\(w_{CL}\\) \u548c\u76ee\u6807\u5185\u5bb9 \\(F_{CL}\\) \u5c31\u6210\u4e3a\u4e86\u51fd\u6570\u7684\u53c2\u6570. \u6211\u4eec\u628a\u8fd9\u4e2a\u51fd\u6570\u4f5c\u4e3a torch \u6a21\u5757\u6765\u5b9e\u73b0, \u628a\u8fd9\u4e9b\u53c2 \u6570\u4f5c\u4e3a\u6784\u9020\u5668\u7684\u8f93\u5165. \u8fd9\u4e2a\u8ddd\u79bb \\(\\|F_{XL} - F_{YL}\\|^2\\) \u662f\u4e24\u4e2a\u7279\u5f81\u6620\u5c04\u96c6\u7684 \u5747\u65b9\u8bef\u5dee, \u53ef\u4ee5\u7528\u4f5c\u4e3a\u7b2c\u4e09\u4e2a\u53c2\u6570\u7684\u6807\u51c6\u7684 nn.MSELoss \u6765\u8ba1\u7b97. \u6211\u4eec\u4f1a\u5728\u6bcf\u4e2a\u76ee\u6807\u5c42\u52a0\u5165\u6211\u4eec\u7684\u5185\u5bb9\u635f\u5931\u4f5c\u4e3a\u989d\u5916\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u5757. \u8fd9\u6837, \u6bcf\u6b21\u6211\u4eec\u90fd\u4f1a\u7ed9\u795e\u7ecf \u7f51\u7edc\u6295\u5582\u4e00\u5f20\u8f93\u5165\u56fe\u50cf \\(X\\) , \u6240\u6709\u7684\u635f\u5931\u90fd\u4f1a\u5728\u76ee\u6807\u5c42\u88ab\u8ba1\u7b97, \u591a\u4e8f\u4e86\u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97, \u6240\u6709\u68af\u5ea6\u90fd\u4f1a\u88ab\u641e\u5b9a. \u8981\u5b9e\u73b0, \u6211\u4eec\u53ea\u9700\u5199\u51fa\u8f6c\u6362\u6a21\u5757\u7684 forward \u65b9\u6cd5, \u8fd9\u4e2a\u6a21\u5757\u5c31\u53d8 \u6210\u4e86\u7f51\u7edc\u7684 \u2018\u2019transparent layer (\u900f\u660e\u5c42)\u2019\u2018, \u8ba1\u7b97\u597d\u7684\u635f\u5931\u88ab\u5b58\u4e3a\u6a21\u5757\u7684\u53c2\u6570. \u6700\u540e, \u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u5047\u7684 backward \u65b9\u6cd5, \u5b83\u4ec5\u4ec5\u53ea\u8c03\u7528\u540e\u5411\u65b9\u6cd5 nn.MSELoss \u6765\u91cd\u6784\u68af\u5ea6. \u8fd9\u4e2a\u65b9\u6cd5\u8fd4\u56de\u8ba1\u7b97\u597d\u7684\u635f\u5931: \u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\u65f6\u8981\u60f3\u663e\u793a\u98ce\u683c\u548c\u5185\u5bb9\u635f\u5931\u7684\u53d8\u5316, \u8fd9\u4f1a\u975e\u5e38\u6709\u7528. class ContentLoss(nn.Module): def __init__(self, target, weight): super(ContentLoss, self).__init__() # \u6211\u4eec\u4f1a\u4ece\u6240\u4f7f\u7528\u7684\u6811\u4e2d\u201c\u5206\u79bb\u201d\u76ee\u6807\u5185\u5bb9 self.target = target.detach() * weight # \u52a8\u6001\u5730\u8ba1\u7b97\u68af\u5ea6: \u5b83\u662f\u4e2a\u72b6\u6001\u503c, \u4e0d\u662f\u53d8\u91cf. # \u5426\u5219\u8bc4\u4ef7\u6307\u6807\u7684\u524d\u5411\u65b9\u6cd5\u4f1a\u629b\u51fa\u9519\u8bef. self.weight = weight self.criterion = nn.MSELoss() def forward(self, input): self.loss = self.criterion(input * self.weight, self.target) self.output = input return self.output def backward(self, retain_graph=True): self.loss.backward(retain_graph=retain_graph) return self.loss Note \u91cd\u8981\u7ec6\u8282 : \u8fd9\u4e2a\u6a21\u5757\u867d\u7136\u53eb\u505a ContentLoss , \u5374\u4e0d\u662f\u4e2a\u771f\u6b63\u7684 Pytorch \u635f\u5931\u51fd\u6570. \u5982\u679c\u4f60\u60f3\u50cf Pytorch \u635f\u5931\u4e00\u6837\u5b9a\u4e49\u4f60\u7684\u5185\u5bb9\u635f\u5931, \u4f60\u5f97\u65b0\u5efa\u4e00\u4e2a Pytorch \u81ea\u52a8\u6c42\u5bfc\u51fd\u6570\u5e76\u624b\u52a8\u5f97\u5728 backward \u65b9\u6cd5\u4e2d\u91cd\u7b97/\u5b9e\u73b0\u68af\u5ea6.","title":"\u5185\u5bb9\u635f\u5931"},{"location":"38/#_9","text":"\u5bf9\u4e8e\u98ce\u683c\u635f\u5931, \u6211\u4eec\u9996\u5148\u9700\u8981\u5b9a\u4e49\u4e00\u4e2a\u7ed9\u5b9a\u8f93\u5165 \\(X\\) \u5728 \\(L\\) \u5c42\u7684\u7279\u5f81\u6620\u5c04 \\(F_{XL}\\) \u65f6\u8ba1\u7b97\u514b\u4ea7\u7269 \\(G_{XL}\\) \u7684\u6a21\u5757. \u4ee4 \\(\\hat{F}_{XL}\\) \u8868\u793a \\(F_{XL}\\) \u91cd\u53d8\u5f62\u4e3a \\(K\\) x \\(N\\) \u7684\u7248\u672c, \u8fd9\u91cc \\(K\\) \u662f \\(L\\) \u5c42\u7279\u5f81 \u6620\u5c04\u7684\u6570\u91cf, \\(N\\) \u662f\u4efb\u610f\u77e2\u91cf\u5316\u7279\u5f81\u6620\u5c04 \\(F_{XL}^k\\) \u7684\u957f\u5ea6. \\(\\hat{F}_{XL}\\) \u7684\u7b2c \\(k^{th}\\) \u884c\u662f \\(F_{XL}^k\\) . \u53ef\u4ee5\u9a8c\u8bc1 \\(\\hat{F}_{XL} \\cdot \\hat{F}_{XL}^T = G_{XL}\\) . \u9274\u4e8e\u6b64, \u5b9e\u73b0\u6211\u4eec\u7684\u6a21\u5757\u5c31\u5f88\u5bb9\u6613\u4e86: class GramMatrix(nn.Module): def forward(self, input): a, b, c, d = input.size() # a=batch size(=1) # b= \u7279\u5f81\u6620\u5c04\u7684\u6570\u91cf # (c,d)= \u4e00\u4e2a\u7279\u5f81\u6620\u5c04\u7684\u7ef4\u5ea6 (N=c*d) features = input.view(a * b, c * d) # \u5c06 F_XL \u8f6c\u6362\u4e3a \\hat F_XL G = torch.mm(features, features.t()) # \u8ba1\u7b97\u514b\u4ea7\u7269 (gram product) # \u6211\u4eec\u7528\u9664\u4ee5\u6bcf\u4e2a\u7279\u5f81\u6620\u5c04\u5143\u7d20\u6570\u91cf\u7684\u65b9\u6cd5 # \u6807\u51c6\u5316\u514b\u77e9\u9635 (gram matrix) \u7684\u503c return G.div(a * b * c * d) \u7279\u5f81\u6620\u5c04\u7684\u7ef4\u5ea6 \\(N\\) \u8d8a\u957f, \u5219\u514b\u77e9\u9635 (gram matrix) \u7684\u503c\u8d8a\u5927. \u56e0\u6b64\u5982\u679c\u6211\u4eec\u4e0d\u7528 \\(N\\) \u6765\u6807\u51c6\u5316, \u5728\u68af\u5ea6\u4e0b\u964d\u8fc7\u7a0b\u4e2d\u7b2c\u4e00\u5c42 (\u5728\u6c60\u5316\u5c42\u4e4b\u524d) \u7684\u635f\u5931\u8ba1\u7b97\u5c31\u4f1a\u8fc7\u4e8e\u91cd\u8981. \u6211\u4eec\u5f53\u7136\u4e0d\u5e0c\u671b\u8fd9\u6837, \u56e0\u4e3a\u6211\u4eec\u611f\u5174\u8da3\u7684\u98ce\u683c\u7279\u5f81\u90fd\u5728\u6700\u6df1\u7684\u90a3\u4e9b\u5c42! \u63a5\u7740, \u98ce\u683c\u635f\u5931\u6a21\u5757\u88ab\u4ee5\u548c\u5185\u5bb9\u635f\u5931\u6a21\u5757\u76f8\u540c\u7684\u65b9\u5f0f\u5b9e\u73b0, \u4f46\u662f\u6211\u4eec\u8fd8\u5f97\u628a gramMatrix \u52a0\u5165\u4f5c\u4e3a\u53c2\u6570: class StyleLoss(nn.Module): def __init__(self, target, weight): super(StyleLoss, self).__init__() self.target = target.detach() * weight self.weight = weight self.gram = GramMatrix() self.criterion = nn.MSELoss() def forward(self, input): self.output = input.clone() self.G = self.gram(input) self.G.mul_(self.weight) self.loss = self.criterion(self.G, self.target) return self.output def backward(self, retain_graph=True): self.loss.backward(retain_graph=retain_graph) return self.loss","title":"\u98ce\u683c\u635f\u5931"},{"location":"38/#_10","text":"\u73b0\u5728, \u6211\u4eec\u8981\u5bfc\u5165\u4e00\u4e2a\u9884\u8bad\u7ec3\u597d\u7684\u795e\u7ecf\u7f51\u7edc. \u548c\u8bba\u6587\u4e00\u6837, \u6211\u4eec\u7528\u9884\u8bad\u7ec3 \u7684 19 \u5c42 VGG \u7f51\u7edc (VGG19). PyTorch\u5bf9 VGG \u7684\u5b9e\u73b0\u6a21\u5757\u5206\u4e3a\u4e24\u4e2a\u5b50 Sequential \u6a21\u5757: features (\u5305\u62ec\u5377\u79ef\u548c\u6c60\u5316\u5c42) \u548c classifier (\u5305\u62ec\u5168\u8fde\u63a5\u5c42). \u6211\u4eec\u53ea\u5bf9 features \u611f\u5174\u8da3: cnn = models.vgg19(pretrained=True).features # \u53ef\u80fd\u7684\u8bdd\u5c06\u5b83\u79fb\u5230 GPU \u4e0a: if use_cuda: cnn = cnn.cuda() Sequential (\u987a\u5e8f) \u6a21\u5757\u5305\u542b\u4e00\u4e2a\u5b50\u6a21\u5757\u7684\u5217\u8868. \u6bd4\u5982, vgg19.features \u5305\u542b\u4e00\u4e2a\u4ee5\u6b63\u786e\u6df1\u5ea6\u6392\u5217\u7684\u5e8f\u5217 (Conv2d, ReLU, Maxpool2d, Conv2d, ReLU\u2026), \u5c31\u5982\u6211\u4eec\u5728 Content loss \u90e8\u5206\u8bb2\u5230\u7684, \u6211\u4eec\u60f3\u8981\u628a\u6211\u4eec\u7684\u98ce\u683c\u548c\u5185\u5bb9\u635f\u5931\u6a21\u5757\u4ee5\u60f3\u8981\u7684\u6df1\u5ea6\u4f5c\u4e3a \u2018\u900f\u660e\u5c42\u2019 \u52a0\u5165\u5230 \u6211\u4eec\u7684\u7f51\u7edc\u4e2d. \u4e3a\u4e86\u8fd9\u6837, \u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684 Sequential (\u987a\u5e8f) \u6a21\u5757, \u5728\u5176\u4e2d\u6211\u4eec\u628a vgg19 \u548c\u6211\u4eec\u7684\u635f\u5931\u6a21\u5757\u4ee5\u6b63\u786e\u7684\u987a\u5e8f\u52a0\u5165: # \u5e0c\u671b\u8ba1\u7b97\u98ce\u683c/\u5185\u5bb9\u635f\u5931\u7684\u5c42 : content_layers_default = ['conv_4'] style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5'] def get_style_model_and_losses(cnn, style_img, content_img, style_weight=1000, content_weight=1, content_layers=content_layers_default, style_layers=style_layers_default): cnn = copy.deepcopy(cnn) # \u4ec5\u4e3a\u4e86\u6709\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u5217\u8868 \u5185\u5bb9/\u98ce\u683c \u635f\u5931 content_losses = [] style_losses = [] model = nn.Sequential() # \u65b0\u5efa\u7684 Sequential \u7f51\u7edc\u6a21\u5757 gram = GramMatrix() # \u6211\u4eec\u9700\u8981\u4e00\u4e2a\u514b\u6a21\u5757 (gram module) \u6765\u8ba1\u7b97\u98ce\u683c\u76ee\u6807 # \u53ef\u80fd\u7684\u8bdd\u5c06\u8fd9\u4e9b\u6a21\u5757\u79fb\u5230 GPU \u4e0a: if use_cuda: model = model.cuda() gram = gram.cuda() i = 1 for layer in list(cnn): if isinstance(layer, nn.Conv2d): name = \"conv_\" + str(i) model.add_module(name, layer) if name in content_layers: # \u52a0\u5185\u5bb9\u635f\u5931: target = model(content_img).clone() content_loss = ContentLoss(target, content_weight) model.add_module(\"content_loss_\" + str(i), content_loss) content_losses.append(content_loss) if name in style_layers: # \u52a0\u98ce\u683c\u635f\u5931: target_feature = model(style_img).clone() target_feature_gram = gram(target_feature) style_loss = StyleLoss(target_feature_gram, style_weight) model.add_module(\"style_loss_\" + str(i), style_loss) style_losses.append(style_loss) if isinstance(layer, nn.ReLU): name = \"relu_\" + str(i) model.add_module(name, layer) if name in content_layers: # \u52a0\u5185\u5bb9\u635f\u5931: target = model(content_img).clone() content_loss = ContentLoss(target, content_weight) model.add_module(\"content_loss_\" + str(i), content_loss) content_losses.append(content_loss) if name in style_layers: # \u52a0\u98ce\u683c\u635f\u5931: target_feature = model(style_img).clone() target_feature_gram = gram(target_feature) style_loss = StyleLoss(target_feature_gram, style_weight) model.add_module(\"style_loss_\" + str(i), style_loss) style_losses.append(style_loss) i += 1 if isinstance(layer, nn.MaxPool2d): name = \"pool_\" + str(i) model.add_module(name, layer) # *** return model, style_losses, content_losses Note \u5728\u8fd9\u7bc7\u8bba\u6587\u4e2d\u4ed6\u4eec\u63a8\u8350\u5c06\u6700\u5927\u6c60\u5316\u5c42\u66f4\u6539\u4e3a\u5e73\u5747\u6c60\u5316\u5c42. AlexNet\u662f\u4e00\u4e2a\u6bd4 VGG19 \u66f4\u5c0f\u7684\u7f51\u7edc, \u7528\u5b83\u5b9e\u73b0\u7684\u8bdd\u6211\u4eec\u4e5f\u4e0d\u4f1a\u770b\u5230 \u4efb\u4f55\u7ed3\u679c\u8d28\u91cf\u7684\u4e0d\u540c. \u800c\u5982\u679c\u4f60\u60f3\u505a\u8fd9\u4e2a\u66ff\u4ee3\u7684\u8bdd, \u53ef\u4ee5\u7528\u8fd9\u4e9b\u4ee3\u7801: # avgpool = nn.AvgPool2d(kernel_size=layer.kernel_size, # stride=layer.stride, padding = layer.padding) # model.add_module(name,avgpool)","title":"\u8bfb\u53d6\u795e\u7ecf\u7f51\u7edc"},{"location":"38/#_11","text":"\u4e3a\u4e86\u7b80\u5316\u4ee3\u7801, \u6211\u4eec\u7528\u4e0e\u5185\u5bb9\u548c\u98ce\u683c\u56fe\u50cf\u540c\u6837\u5c3a\u5bf8\u7684\u56fe\u50cf\u505a\u8f93\u5165. \u8fd9\u4e2a\u56fe\u50cf\u53ef\u4ee5\u662f\u767d\u566a\u58f0\u7684, \u4e5f\u53ef\u4ee5\u662f\u4e00\u4efd\u5185\u5bb9\u56fe\u50cf\u7684\u62f7\u8d1d. input_img = content_img.clone() # \u5982\u679c\u4f60\u60f3\u7528\u767d\u566a\u58f0\u505a\u8f93\u5165, \u8bf7\u53d6\u6d88\u4e0b\u9762\u7684\u6ce8\u91ca\u884c: # input_img = Variable(torch.randn(content_img.data.size())).type(dtype) # \u5728\u7ed8\u56fe\u4e2d\u52a0\u5165\u539f\u59cb\u7684\u8f93\u5165\u56fe\u50cf: plt.figure() imshow(input_img.data, title='Input Image')","title":"\u8f93\u5165\u56fe\u50cf"},{"location":"38/#_12","text":"\u7531\u4e8e\u672c\u7b97\u6cd5\u7684\u4f5c\u8005 Leon Gatys \u7684\u5efa\u8bae here , \u6211\u4eec\u5c06\u4f7f\u7528 L-BFGS \u7b97\u6cd5\u6765\u8dd1\u6211\u4eec\u7684\u68af\u5ea6\u4e0b\u964d. \u548c\u8bad\u7ec3\u4e00\u4e2a\u7f51\u7edc\u4e0d\u540c\u7684\u662f, \u6211\u4eec\u5e0c\u671b\u8bad\u7ec3\u8f93\u5165\u56fe\u50cf\u6765\u6700\u5c0f\u5316 \u5185\u5bb9/\u98ce\u683c \u635f\u5931. \u6211\u4eec\u60f3\u7b80\u5355\u5730\u5efa\u4e00\u4e2a PyTorch L-BFGS \u4f18\u5316\u5668, \u4f20\u5165\u6211\u4eec\u7684\u56fe\u50cf\u4f5c\u4e3a\u53d8\u91cf\u8fdb\u884c\u4f18\u5316. \u4f46\u662f optim.LBFGS \u7684\u7b2c\u4e00\u4e2a\u5f62\u53c2\u662f\u4e00\u4e2a\u9700\u8981\u68af\u5ea6\u7684 PyTorch Variable . \u6211\u4eec\u7684\u8f93\u5165\u56fe\u50cf\u662f\u4e00\u4e2a Variable , \u4f46\u4e0d\u662f\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\u7684\u6811\u7684\u53f6\u8282\u70b9. \u4e3a\u4e86\u4f7f\u8fd9\u4e2a\u53d8\u91cf\u9700\u8981\u68af\u5ea6\u8fd0\u7b97, \u4e00\u4e2a\u53ef\u80fd\u7684\u65b9\u6cd5\u662f\u4ece\u8f93\u5165\u56fe\u50cf\u6784\u5efa\u4e00\u4e2a Parameter (\u53c2\u6570) \u5bf9\u8c61. \u7136\u540e\u6211\u4eec\u53ea\u9700\u7ed9\u4f18\u5316\u5668\u7684\u6784\u9020\u5668\u4f20\u9012\u4e00\u4e2a \u5305\u542b\u8fd9\u4e2a\u53c2\u6570\u7684\u5217\u8868: def get_input_param_optimizer(input_img): # \u8fd9\u884c\u663e\u793a\u4e86\u8f93\u5165\u662f\u4e00\u4e2a\u9700\u8981\u68af\u5ea6\u8ba1\u7b97\u7684\u53c2\u6570 input_param = nn.Parameter(input_img.data) optimizer = optim.LBFGS([input_param]) return input_param, optimizer \u6700\u540e\u4e00\u6b65 : \u5faa\u73af\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d. \u6bcf\u4e00\u6b65\u4e2d\u6211\u4eec\u5fc5\u987b\u5582\u7ed9\u795e\u7ecf\u7f51\u7edc\u66f4\u65b0\u540e \u7684\u8f93\u5165\u4ee5\u8ba1\u7b97\u65b0\u7684\u635f\u5931, \u6211\u4eec\u8981\u8fd0\u884c\u6bcf\u4e2a\u635f\u5931\u7684 backward \u65b9\u6cd5\u6765\u52a8\u6001 \u8ba1\u7b97\u4ed6\u4eec\u7684\u68af\u5ea6\u5e76\u5448\u73b0\u68af\u5ea6\u4e0b\u964d\u7684\u6bcf\u4e00\u6b65. \u8fd9\u4e2a\u4f18\u5316\u5668\u9700\u8981\u4e00\u4e2a \u201cclosure\u201d : \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u5e76\u8fd4\u56de\u635f\u5931\u7684\u51fd\u6570. \u7136\u800c, \u8fd9\u91cc\u6709\u4e00\u4e2a\u5c0f\u95ee\u9898. \u88ab\u4f18\u5316\u7684\u56fe\u50cf\u7684\u50cf\u7d20\u503c\u4f1a\u5728 \\(-\\infty\\) \u548c \\(+\\infty\\) \u4e4b\u95f4\u6ce2\u52a8, \u800c\u4e0d\u662f\u7ee7\u7eed\u4fdd\u6301\u5728 0 \u5230 1. \u6362\u53e5\u8bdd\u8bf4, \u56fe\u50cf\u53ef\u80fd\u4f1a\u88ab\u5b8c\u7f8e\u5730\u4f18\u5316\u6210\u8352\u8c2c\u7684\u503c. \u4e8b\u5b9e\u4e0a, \u6211\u4eec\u5fc5\u987b\u5728\u9650\u5236\u4e0b\u4f7f\u7528 \u4f18\u5316\u5668\u6765\u4f7f\u6211\u4eec\u7684\u8f93\u5165\u56fe\u50cf\u4e00\u76f4\u4fdd\u6301\u6b63\u786e\u7684\u503c. \u6709\u4e00\u4e2a\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6848: \u5728\u6bcf\u4e00\u6b65, \u90fd\u6821\u6b63\u56fe\u50cf\u4f7f\u5176\u4fdd\u6301 0-1 \u8303\u56f4\u7684\u503c. def run_style_transfer(cnn, content_img, style_img, input_img, num_steps=300, style_weight=1000, content_weight=1): \"\"\"Run the style transfer.\"\"\" print('Building the style transfer model..') model, style_losses, content_losses = get_style_model_and_losses(cnn, style_img, content_img, style_weight, content_weight) input_param, optimizer = get_input_param_optimizer(input_img) print('Optimizing..') run = [0] while run[0] <= num_steps: def closure(): # \u6821\u6b63\u66f4\u65b0\u540e\u7684\u8f93\u5165\u56fe\u50cf\u503c input_param.data.clamp_(0, 1) optimizer.zero_grad() model(input_param) style_score = 0 content_score = 0 for sl in style_losses: style_score += sl.backward() for cl in content_losses: content_score += cl.backward() run[0] += 1 if run[0] % 50 == 0: print(\"run {}:\".format(run)) print('Style Loss : {:4f} Content Loss: {:4f}'.format( style_score.data[0], content_score.data[0])) print() return style_score + content_score optimizer.step(closure) # \u6700\u540e\u4e00\u6b21\u7684\u6821\u6b63... input_param.data.clamp_(0, 1) return input_param.data \u6700\u540e, \u8fd0\u884c\u7b97\u6cd5 output = run_style_transfer(cnn, content_img, style_img, input_img) plt.figure() imshow(output, title='Output Image') # sphinx_gallery_thumbnail_number = 4 plt.ioff() plt.show() Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: neural_style_tutorial.py Download Jupyter notebook: neural_style_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u68af\u5ea6\u4e0b\u964d"},{"location":"39/","text":"\u4f7f\u7528 numpy \u548c scipy \u521b\u5efa\u6269\u5c55 \u4f5c\u8005 : Adam Paszke \u8fd9\u4e2a\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u5b8c\u6210\u4ee5\u4e0b\u4e24\u4e2a\u4efb\u52a1: \u521b\u5efa\u4e0d\u5e26\u53c2\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u5c42 > * \u8fd9\u4f1a\u8c03\u7528 numpy , \u4f5c\u4e3a\u5176\u5b9e\u73b0\u7684\u4e00\u90e8\u5206 \u521b\u5efa\u5e26\u6709\u53ef\u5b66\u4e60\u7684\u6743\u91cd\u7684\u795e\u7ecf\u7f51\u7edc\u5c42 > * \u8fd9\u4f1a\u8c03\u7528 SciPy , \u4f5c\u4e3a\u5176\u5b9e\u73b0\u7684\u4e00\u90e8\u5206 import torch from torch.autograd import Function from torch.autograd import Variable \u65e0\u53c2\u793a\u4f8b \u8fd9\u4e00\u5c42\u5e76\u4e0d\u505a\u4efb\u4f55\u6709\u7528\u7684, \u6216\u8005\u6570\u5b66\u4e0a\u6b63\u786e\u7684\u4e8b\u60c5. \u5b83\u88ab\u6070\u5f53\u5730\u547d\u540d\u4e3a BadFFTFunction \u5c42\u7684\u5b9e\u73b0 from numpy.fft import rfft2, irfft2 class BadFFTFunction(Function): def forward(self, input): numpy_input = input.numpy() result = abs(rfft2(numpy_input)) return torch.FloatTensor(result) def backward(self, grad_output): numpy_go = grad_output.numpy() result = irfft2(numpy_go) return torch.FloatTensor(result) # \u7531\u4e8e\u8fd9\u4e00\u5c42\u6ca1\u6709\u4efb\u4f55\u53c2\u6570, \u6211\u4eec\u53ef\u4ee5 # \u4ec5\u4ec5\u5c06\u5176\u58f0\u660e\u4e3a\u4e00\u4e2a\u51fd\u6570, \u800c\u4e0d\u662f nn.Module \u7c7b def incorrect_fft(input): return BadFFTFunction()(input) \u6240\u521b\u5efa\u7684\u5c42\u7684\u4f7f\u7528\u793a\u4f8b: input = Variable(torch.randn(8, 8), requires_grad=True) result = incorrect_fft(input) print(result.data) result.backward(torch.randn(result.size())) print(input.grad) \u53c2\u6570\u5316\u793a\u4f8b \u5b83\u5b9e\u73b0\u4e86\u5e26\u6709\u53ef\u5b66\u4e60\u7684\u6743\u91cd\u7684\u5c42. \u5b83\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u6838, \u5b9e\u73b0\u4e86\u4e92\u76f8\u5173. \u5728\u6df1\u5ea6\u5b66\u4e60\u6587\u732e\u4e2d, \u5b83\u5bb9\u6613\u548c\u5377\u79ef\u6df7\u6dc6. \u53cd\u5411\u8fc7\u7a0b\u8ba1\u7b97\u4e86\u8f93\u5165\u548c\u6ee4\u6ce2\u7684\u68af\u5ea6. \u5b9e\u73b0: \u8981\u6ce8\u610f, \u5b9e\u73b0\u4f5c\u4e3a\u4e00\u4e2a\u6f14\u793a, \u6211\u4eec\u5e76\u4e0d\u9a8c\u8bc1\u5b83\u7684\u6b63\u786e\u6027 from scipy.signal import convolve2d, correlate2d from torch.nn.modules.module import Module from torch.nn.parameter import Parameter class ScipyConv2dFunction(Function): @staticmethod def forward(ctx, input, filter): result = correlate2d(input.numpy(), filter.numpy(), mode='valid') ctx.save_for_backward(input, filter) return torch.FloatTensor(result) @staticmethod def backward(ctx, grad_output): input, filter = ctx.saved_tensors grad_output = grad_output.data grad_input = convolve2d(grad_output.numpy(), filter.t().numpy(), mode='full') grad_filter = convolve2d(input.numpy(), grad_output.numpy(), mode='valid') return Variable(torch.FloatTensor(grad_input)), \\ Variable(torch.FloatTensor(grad_filter)) class ScipyConv2d(Module): def __init__(self, kh, kw): super(ScipyConv2d, self).__init__() self.filter = Parameter(torch.randn(kh, kw)) def forward(self, input): return ScipyConv2dFunction.apply(input, self.filter) \u793a\u4f8b\u7528\u6cd5: module = ScipyConv2d(3, 3) print(list(module.parameters())) input = Variable(torch.randn(10, 10), requires_grad=True) output = module(input) print(output) output.backward(torch.randn(8, 8)) print(input.grad) Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: numpy_extensions_tutorial.py Download Jupyter notebook: numpy_extensions_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u4f7f\u7528 numpy \u548c scipy \u521b\u5efa\u6269\u5c55"},{"location":"39/#numpy-scipy","text":"\u4f5c\u8005 : Adam Paszke \u8fd9\u4e2a\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u5b8c\u6210\u4ee5\u4e0b\u4e24\u4e2a\u4efb\u52a1: \u521b\u5efa\u4e0d\u5e26\u53c2\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u5c42 > * \u8fd9\u4f1a\u8c03\u7528 numpy , \u4f5c\u4e3a\u5176\u5b9e\u73b0\u7684\u4e00\u90e8\u5206 \u521b\u5efa\u5e26\u6709\u53ef\u5b66\u4e60\u7684\u6743\u91cd\u7684\u795e\u7ecf\u7f51\u7edc\u5c42 > * \u8fd9\u4f1a\u8c03\u7528 SciPy , \u4f5c\u4e3a\u5176\u5b9e\u73b0\u7684\u4e00\u90e8\u5206 import torch from torch.autograd import Function from torch.autograd import Variable","title":"\u4f7f\u7528 numpy \u548c scipy \u521b\u5efa\u6269\u5c55"},{"location":"39/#_1","text":"\u8fd9\u4e00\u5c42\u5e76\u4e0d\u505a\u4efb\u4f55\u6709\u7528\u7684, \u6216\u8005\u6570\u5b66\u4e0a\u6b63\u786e\u7684\u4e8b\u60c5. \u5b83\u88ab\u6070\u5f53\u5730\u547d\u540d\u4e3a BadFFTFunction \u5c42\u7684\u5b9e\u73b0 from numpy.fft import rfft2, irfft2 class BadFFTFunction(Function): def forward(self, input): numpy_input = input.numpy() result = abs(rfft2(numpy_input)) return torch.FloatTensor(result) def backward(self, grad_output): numpy_go = grad_output.numpy() result = irfft2(numpy_go) return torch.FloatTensor(result) # \u7531\u4e8e\u8fd9\u4e00\u5c42\u6ca1\u6709\u4efb\u4f55\u53c2\u6570, \u6211\u4eec\u53ef\u4ee5 # \u4ec5\u4ec5\u5c06\u5176\u58f0\u660e\u4e3a\u4e00\u4e2a\u51fd\u6570, \u800c\u4e0d\u662f nn.Module \u7c7b def incorrect_fft(input): return BadFFTFunction()(input) \u6240\u521b\u5efa\u7684\u5c42\u7684\u4f7f\u7528\u793a\u4f8b: input = Variable(torch.randn(8, 8), requires_grad=True) result = incorrect_fft(input) print(result.data) result.backward(torch.randn(result.size())) print(input.grad)","title":"\u65e0\u53c2\u793a\u4f8b"},{"location":"39/#_2","text":"\u5b83\u5b9e\u73b0\u4e86\u5e26\u6709\u53ef\u5b66\u4e60\u7684\u6743\u91cd\u7684\u5c42. \u5b83\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u6838, \u5b9e\u73b0\u4e86\u4e92\u76f8\u5173. \u5728\u6df1\u5ea6\u5b66\u4e60\u6587\u732e\u4e2d, \u5b83\u5bb9\u6613\u548c\u5377\u79ef\u6df7\u6dc6. \u53cd\u5411\u8fc7\u7a0b\u8ba1\u7b97\u4e86\u8f93\u5165\u548c\u6ee4\u6ce2\u7684\u68af\u5ea6. \u5b9e\u73b0: \u8981\u6ce8\u610f, \u5b9e\u73b0\u4f5c\u4e3a\u4e00\u4e2a\u6f14\u793a, \u6211\u4eec\u5e76\u4e0d\u9a8c\u8bc1\u5b83\u7684\u6b63\u786e\u6027 from scipy.signal import convolve2d, correlate2d from torch.nn.modules.module import Module from torch.nn.parameter import Parameter class ScipyConv2dFunction(Function): @staticmethod def forward(ctx, input, filter): result = correlate2d(input.numpy(), filter.numpy(), mode='valid') ctx.save_for_backward(input, filter) return torch.FloatTensor(result) @staticmethod def backward(ctx, grad_output): input, filter = ctx.saved_tensors grad_output = grad_output.data grad_input = convolve2d(grad_output.numpy(), filter.t().numpy(), mode='full') grad_filter = convolve2d(input.numpy(), grad_output.numpy(), mode='valid') return Variable(torch.FloatTensor(grad_input)), \\ Variable(torch.FloatTensor(grad_filter)) class ScipyConv2d(Module): def __init__(self, kh, kw): super(ScipyConv2d, self).__init__() self.filter = Parameter(torch.randn(kh, kw)) def forward(self, input): return ScipyConv2dFunction.apply(input, self.filter) \u793a\u4f8b\u7528\u6cd5: module = ScipyConv2d(3, 3) print(list(module.parameters())) input = Variable(torch.randn(10, 10), requires_grad=True) output = module(input) print(output) output.backward(torch.randn(8, 8)) print(input.grad) Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: numpy_extensions_tutorial.py Download Jupyter notebook: numpy_extensions_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u53c2\u6570\u5316\u793a\u4f8b"},{"location":"4/","text":"\u795e\u7ecf\u7f51\u7edc \u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u4f7f\u7528 torch.nn \u5305\u6784\u5efa. autograd \u5b9e\u73b0\u4e86\u53cd\u5411\u4f20\u64ad\u529f\u80fd, \u4f46\u662f\u76f4\u63a5\u7528\u6765\u5199\u6df1\u5ea6\u5b66\u4e60\u7684\u4ee3\u7801\u5728\u5f88\u591a\u60c5\u51b5\u4e0b\u8fd8\u662f\u7a0d\u663e\u590d\u6742, torch.nn \u662f\u4e13\u95e8\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\u63a5\u53e3. nn \u6784\u5efa\u4e8e Autograd \u4e4b\u4e0a, \u53ef\u7528\u6765\u5b9a\u4e49\u548c\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc. nn.Module \u662f nn \u4e2d\u6700\u91cd\u8981\u7684\u7c7b, \u53ef\u628a\u5b83\u770b\u6210\u662f\u4e00\u4e2a\u7f51\u7edc\u7684\u5c01\u88c5, \u5305\u542b\u7f51\u7edc\u5404\u5c42\u5b9a\u4e49\u4ee5\u53ca forward \u65b9\u6cd5, \u8c03\u7528 forward(input) \u65b9\u6cd5, \u53ef\u8fd4\u56de\u524d\u5411\u4f20\u64ad\u7684\u7ed3\u679c. \u4f8b\u5982, \u770b\u770b\u8fd9\u4e2a\u5206\u7c7b\u6570\u5b57\u56fe\u50cf\u7684\u7f51\u7edc: convnet \u8fd9\u662f\u4e00\u4e2a\u57fa\u7840\u7684\u524d\u5411\u4f20\u64ad(feed-forward)\u7f51\u7edc: \u63a5\u6536\u8f93\u5165, \u7ecf\u8fc7\u5c42\u5c42\u4f20\u9012\u8fd0\u7b97, \u5f97\u5230\u8f93\u51fa. \u4e00\u4e2a\u5178\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\u5982\u4e0b: \u5b9a\u4e49\u5177\u6709\u4e00\u4e9b\u53ef\u5b66\u4e60\u53c2\u6570(\u6216\u6743\u91cd)\u7684\u795e\u7ecf\u7f51\u7edc \u8fed\u4ee3\u8f93\u5165\u6570\u636e\u96c6 \u901a\u8fc7\u7f51\u7edc\u5904\u7406\u8f93\u5165 \u8ba1\u7b97\u635f\u5931(\u8f93\u51fa\u7684\u9884\u6d4b\u503c\u4e0e\u5b9e\u9645\u503c\u4e4b\u95f4\u7684\u8ddd\u79bb) \u5c06\u68af\u5ea6\u4f20\u64ad\u56de\u7f51\u7edc \u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd, \u901a\u5e38\u4f7f\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219: weight = weight - learning_rate * gradient \u5b9a\u4e49\u7f51\u7edc \u8ba9\u6211\u4eec\u6765\u5b9a\u4e49\u4e00\u4e2a\u7f51\u7edc: import torch from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() # \u5377\u79ef\u5c42 '1'\u8868\u793a\u8f93\u5165\u56fe\u7247\u4e3a\u5355\u901a\u9053, '6'\u8868\u793a\u8f93\u51fa\u901a\u9053\u6570, '5'\u8868\u793a\u5377\u79ef\u6838\u4e3a5*5 # \u6838\u5fc3 self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # \u4eff\u5c04\u5c42/\u5168\u8fde\u63a5\u5c42: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): #\u5728\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u4e0a\u5e94\u75282D\u6700\u5927\u6c60\u5316. # (2, 2) \u4ee3\u8868\u7684\u662f\u6c60\u5316\u64cd\u4f5c\u7684\u6b65\u5e45 x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # \u5982\u679c\u5927\u5c0f\u662f\u6b63\u65b9\u5f62, \u5219\u53ea\u80fd\u6307\u5b9a\u4e00\u4e2a\u6570\u5b57 x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # \u9664\u6279\u91cf\u7ef4\u5ea6\u5916\u7684\u6240\u6709\u7ef4\u5ea6 num_features = 1 for s in size: num_features *= s return num_features net = Net() print(net) \u4f60\u53ea\u8981\u5728 nn.Module \u7684\u5b50\u7c7b\u4e2d\u5b9a\u4e49\u4e86 forward \u51fd\u6570, backward \u51fd\u6570\u5c31\u4f1a\u81ea\u52a8\u88ab\u5b9e\u73b0(\u5229\u7528 autograd ). \u5728 forward \u51fd\u6570\u4e2d\u53ef\u4f7f\u7528\u4efb\u4f55 Tensor \u652f\u6301\u7684\u64cd\u4f5c. \u7f51\u7edc\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u901a\u8fc7 net.parameters() \u8fd4\u56de, net.named_parameters \u53ef\u540c\u65f6\u8fd4\u56de\u5b66\u4e60\u7684\u53c2\u6570\u4ee5\u53ca\u540d\u79f0. params = list(net.parameters()) print(len(params)) print(params[0].size()) # conv1\u7684weight \u5411\u524d\u7684\u8f93\u5165\u662f\u4e00\u4e2a autograd.Variable , \u8f93\u51fa\u4e5f\u662f\u5982\u6b64. \u6ce8\u610f: \u8fd9\u4e2a\u7f51\u7edc(LeNet)\u7684\u9884\u671f\u8f93\u5165\u5927\u5c0f\u662f 32x32, \u4f7f\u7528\u8fd9\u4e2a\u7f51\u4e0a MNIST \u6570\u636e\u96c6, \u8bf7\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\u8c03\u6574\u4e3a 32x32. input = Variable(torch.randn(1, 1, 32, 32)) out = net(input) print(out) \u5c06\u7f51\u7edc\u4e2d\u6240\u6709\u53c2\u6570\u7684\u68af\u5ea6\u6e05\u96f6. net.zero_grad() out.backward(torch.randn(1, 10)) Note torch.nn \u53ea\u652f\u6301\u5c0f\u6279\u91cf(mini-batches), \u4e0d\u652f\u6301\u4e00\u6b21\u8f93\u5165\u4e00\u4e2a\u6837\u672c, \u5373\u4e00\u6b21\u5fc5\u987b\u662f\u4e00\u4e2a batch. \u4f8b\u5982, nn.Conv2d \u7684\u8f93\u5165\u5fc5\u987b\u662f 4 \u7ef4\u7684, \u5f62\u5982 nSamples x nChannels x Height x Width . \u5982\u679c\u4f60\u53ea\u60f3\u8f93\u5165\u4e00\u4e2a\u6837\u672c, \u9700\u8981\u4f7f\u7528 input.unsqueeze(0) \u5c06 batch_size \u8bbe\u7f6e\u4e3a 1. \u5728\u7ee7\u7eed\u4e4b\u524d, \u8ba9\u6211\u4eec\u56de\u987e\u4e00\u4e0b\u8fc4\u4eca\u4e3a\u6b62\u6240\u6709\u89c1\u8fc7\u7684\u7c7b. \u6982\u62ec: torch.Tensor - \u4e00\u4e2a \u591a\u7ef4\u6570\u7ec4 . autograd.Variable - \u5305\u88c5\u5f20\u91cf\u5e76\u8bb0\u5f55\u5e94\u7528\u4e8e\u5176\u4e0a\u7684\u5386\u53f2\u64cd\u4f5c . \u5177\u6709\u548c Tensor \u76f8\u540c\u7684 API ,\u8fd8\u6709\u4e00\u4e9b\u8865\u5145, \u5982 backward() . \u53e6\u5916 \u62e5\u6709\u5f20\u91cf\u7684\u68af\u5ea6 . nn.Module - \u795e\u7ecf\u7f51\u7edc\u6a21\u5757. \u65b9\u4fbf\u7684\u65b9\u5f0f\u5c01\u88c5\u53c2\u6570 , \u5e2e\u52a9\u5c06\u5176\u79fb\u52a8\u5230GPU, \u5bfc\u51fa, \u52a0\u8f7d\u7b49. nn.Parameter - \u4e00\u79cd\u53d8\u91cf, \u5f53\u88ab\u6307\u5b9a\u4e3a Model \u7684\u5c5e\u6027\u65f6, \u5b83\u4f1a\u81ea\u52a8\u6ce8\u518c\u4e3a\u4e00\u4e2a\u53c2\u6570. autograd.Function - \u5b9e\u73b0 autograd \u64cd\u4f5c\u7684\u5411\u524d\u548c\u5411\u540e\u5b9a\u4e49 . \u6bcf\u4e2a Variable \u64cd\u4f5c, \u81f3\u5c11\u521b\u5efa\u4e00\u4e2a Function \u8282\u70b9, \u8fde\u63a5\u5230\u521b\u5efa Variable \u7684\u51fd\u6570, \u5e76 \u7f16\u7801\u5b83\u7684\u5386\u53f2 . \u5728\u8fd9\u4e00\u70b9\u4e0a, \u6211\u4eec\u6db5\u76d6: \u5b9a\u4e49\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc \u5904\u7406\u8f93\u5165\u5e76\u53cd\u5411\u4f20\u64ad \u8fd8\u5269\u4e0b: \u8ba1\u7b97\u635f\u5931\u51fd\u6570 \u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd \u635f\u5931\u51fd\u6570 \u635f\u5931\u51fd\u6570\u91c7\u7528 (output,target) \u8f93\u5165\u5bf9, \u5e76\u8ba1\u7b97\u9884\u6d4b\u8f93\u51fa\u7ed3\u679c\u4e0e\u5b9e\u9645\u76ee\u6807\u7684\u8ddd\u79bb. \u5728 nn \u5305\u4e0b\u6709\u51e0\u79cd\u4e0d\u540c\u7684 \u635f\u5931\u51fd\u6570 . \u4e00\u4e2a\u7b80\u5355\u7684\u635f\u5931\u51fd\u6570\u662f: nn.MSELoss \u8ba1\u7b97\u8f93\u51fa\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee \u4f8b\u5982: output = net(input) target = Variable(torch.arange(1, 11)) # \u4e00\u4e2a\u865a\u62df\u7684\u76ee\u6807 criterion = nn.MSELoss() loss = criterion(output, target) print(loss) \u73b0\u5728, \u5982\u679c\u4f60\u6cbf\u7740 loss \u53cd\u5411\u4f20\u64ad\u7684\u65b9\u5411\u4f7f\u7528 .grad_fn \u5c5e\u6027, \u4f60\u5c06\u4f1a\u770b\u5230\u4e00\u4e2a\u5982\u4e0b\u6240\u793a\u7684\u8ba1\u7b97\u56fe: input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss \u6240\u4ee5, \u5f53\u6211\u4eec\u8c03\u7528 loss.backward() , \u6574\u4e2a\u56fe\u4e0e\u635f\u5931\u662f\u6709\u533a\u522b\u7684, \u56fe\u4e2d\u7684\u6240\u6709\u53d8\u91cf\u90fd\u5c06\u7528 .grad \u68af\u5ea6\u7d2f\u52a0\u5b83\u4eec\u7684\u53d8\u91cf. \u4e3a\u4e86\u8bf4\u660e, \u8ba9\u6211\u4eec\u5411\u540e\u8d70\u51e0\u6b65: print(loss.grad_fn) # MSELoss print(loss.grad_fn.next_functions[0][0]) # Linear print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU \u53cd\u5411\u4f20\u64ad \u4e3a\u4e86\u53cd\u5411\u4f20\u64ad\u8bef\u5dee, \u6211\u4eec\u6240\u8981\u505a\u7684\u5c31\u662f loss.backward() . \u4f60\u9700\u8981\u6e05\u9664\u73b0\u6709\u7684\u68af\u5ea6, \u5426\u5219\u68af\u5ea6\u4f1a\u7d2f\u52a0\u4e4b\u524d\u7684\u68af\u5ea6. \u73b0\u5728\u6211\u4eec\u4f7f\u7528 loss.backward() , \u770b\u770b\u53cd\u5411\u4f20\u64ad\u4e4b\u524d\u548c\u4e4b\u540e conv1 \u7684\u68af\u5ea6. net.zero_grad() # \u628a\u4e4b\u524d\u7684\u68af\u5ea6\u6e05\u96f6 print('conv1.bias.grad before backward') print(net.conv1.bias.grad) loss.backward() print('conv1.bias.grad after backward') print(net.conv1.bias.grad) \u73b0\u5728, \u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4e86\u5982\u4f55\u4f7f\u7528\u635f\u5931\u51fd\u6570. \u7a0d\u540e\u9605\u8bfb: \u795e\u7ecf\u7f51\u7edc\u5305\u5305\u542b\u5404\u79cd\u6a21\u5757\u548c\u635f\u5931\u51fd\u6570, \u5f62\u6210\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6784\u5efa\u6a21\u5757. \u5b8c\u6574\u7684\u6587\u4ef6\u5217\u8868 \u5728\u8fd9\u91cc \u63a5\u4e0b\u6765\u5b66\u4e60\u7684\u552f\u4e00\u4e1c\u897f\u662f: \u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd \u66f4\u65b0\u6743\u91cd \u5b9e\u8df5\u4e2d\u4f7f\u7528\u7684\u6700\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219\u662f\u968f\u673a\u68af\u5ea6\u4e0b\u964d( SGD ): weight = weight - learning_rate * gradient \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u7b80\u5355\u7684 python \u4ee3\u7801\u6765\u5b9e\u73b0\u8fd9\u4e2a: learning_rate = 0.01 for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) \u7136\u800c, \u5f53\u4f60\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u65f6, \u4f60\u9700\u8981\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u7684\u66f4\u65b0\u89c4\u5219, \u6bd4\u5982 SGD, Nesterov-SGD, Adam, RMSProp\u7b49. \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e2a\u529f\u80fd, \u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305: torch.optim \u5b9e\u73b0\u6240\u6709\u8fd9\u4e9b\u65b9\u6cd5. \u4f7f\u7528\u5b83\u975e\u5e38\u7684\u7b80\u5355: import torch.optim as optim # \u65b0\u5efa\u4e00\u4e2a\u4f18\u5316\u5668, \u6307\u5b9a\u8981\u8c03\u6574\u7684\u53c2\u6570\u548c\u5b66\u4e60\u7387 optimizer = optim.SGD(net.parameters(), lr = 0.01) # \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d: optimizer.zero_grad() # \u9996\u5148\u68af\u5ea6\u6e05\u96f6(\u4e0e net.zero_grad() \u6548\u679c\u4e00\u6837) output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() # \u66f4\u65b0\u53c2\u6570 Note \u89c2\u5bdf\u5982\u4f55\u4f7f\u7528\u624b\u52a8\u8bbe\u7f6e\u68af\u5ea6\u6e05\u96f6 optimizer.zero_grad() . \u9700\u8981\u624b\u52a8\u6e05\u96f6\u7684\u539f\u56e0\u5728 Backprop _ \u4e2d\u5df2\u7ecf\u8bf4\u660e\u4e86(\u68af\u5ea6\u4f1a\u7d2f\u52a0\u4e4b\u524d\u7684\u68af\u5ea6). Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: neural_networks_tutorial.py Download Jupyter notebook: neural_networks_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u795e\u7ecf\u7f51\u7edc"},{"location":"4/#_1","text":"\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u4f7f\u7528 torch.nn \u5305\u6784\u5efa. autograd \u5b9e\u73b0\u4e86\u53cd\u5411\u4f20\u64ad\u529f\u80fd, \u4f46\u662f\u76f4\u63a5\u7528\u6765\u5199\u6df1\u5ea6\u5b66\u4e60\u7684\u4ee3\u7801\u5728\u5f88\u591a\u60c5\u51b5\u4e0b\u8fd8\u662f\u7a0d\u663e\u590d\u6742, torch.nn \u662f\u4e13\u95e8\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\u63a5\u53e3. nn \u6784\u5efa\u4e8e Autograd \u4e4b\u4e0a, \u53ef\u7528\u6765\u5b9a\u4e49\u548c\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc. nn.Module \u662f nn \u4e2d\u6700\u91cd\u8981\u7684\u7c7b, \u53ef\u628a\u5b83\u770b\u6210\u662f\u4e00\u4e2a\u7f51\u7edc\u7684\u5c01\u88c5, \u5305\u542b\u7f51\u7edc\u5404\u5c42\u5b9a\u4e49\u4ee5\u53ca forward \u65b9\u6cd5, \u8c03\u7528 forward(input) \u65b9\u6cd5, \u53ef\u8fd4\u56de\u524d\u5411\u4f20\u64ad\u7684\u7ed3\u679c. \u4f8b\u5982, \u770b\u770b\u8fd9\u4e2a\u5206\u7c7b\u6570\u5b57\u56fe\u50cf\u7684\u7f51\u7edc: convnet \u8fd9\u662f\u4e00\u4e2a\u57fa\u7840\u7684\u524d\u5411\u4f20\u64ad(feed-forward)\u7f51\u7edc: \u63a5\u6536\u8f93\u5165, \u7ecf\u8fc7\u5c42\u5c42\u4f20\u9012\u8fd0\u7b97, \u5f97\u5230\u8f93\u51fa. \u4e00\u4e2a\u5178\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\u5982\u4e0b: \u5b9a\u4e49\u5177\u6709\u4e00\u4e9b\u53ef\u5b66\u4e60\u53c2\u6570(\u6216\u6743\u91cd)\u7684\u795e\u7ecf\u7f51\u7edc \u8fed\u4ee3\u8f93\u5165\u6570\u636e\u96c6 \u901a\u8fc7\u7f51\u7edc\u5904\u7406\u8f93\u5165 \u8ba1\u7b97\u635f\u5931(\u8f93\u51fa\u7684\u9884\u6d4b\u503c\u4e0e\u5b9e\u9645\u503c\u4e4b\u95f4\u7684\u8ddd\u79bb) \u5c06\u68af\u5ea6\u4f20\u64ad\u56de\u7f51\u7edc \u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd, \u901a\u5e38\u4f7f\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219: weight = weight - learning_rate * gradient","title":"\u795e\u7ecf\u7f51\u7edc"},{"location":"4/#_2","text":"\u8ba9\u6211\u4eec\u6765\u5b9a\u4e49\u4e00\u4e2a\u7f51\u7edc: import torch from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() # \u5377\u79ef\u5c42 '1'\u8868\u793a\u8f93\u5165\u56fe\u7247\u4e3a\u5355\u901a\u9053, '6'\u8868\u793a\u8f93\u51fa\u901a\u9053\u6570, '5'\u8868\u793a\u5377\u79ef\u6838\u4e3a5*5 # \u6838\u5fc3 self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # \u4eff\u5c04\u5c42/\u5168\u8fde\u63a5\u5c42: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): #\u5728\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u4e0a\u5e94\u75282D\u6700\u5927\u6c60\u5316. # (2, 2) \u4ee3\u8868\u7684\u662f\u6c60\u5316\u64cd\u4f5c\u7684\u6b65\u5e45 x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # \u5982\u679c\u5927\u5c0f\u662f\u6b63\u65b9\u5f62, \u5219\u53ea\u80fd\u6307\u5b9a\u4e00\u4e2a\u6570\u5b57 x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # \u9664\u6279\u91cf\u7ef4\u5ea6\u5916\u7684\u6240\u6709\u7ef4\u5ea6 num_features = 1 for s in size: num_features *= s return num_features net = Net() print(net) \u4f60\u53ea\u8981\u5728 nn.Module \u7684\u5b50\u7c7b\u4e2d\u5b9a\u4e49\u4e86 forward \u51fd\u6570, backward \u51fd\u6570\u5c31\u4f1a\u81ea\u52a8\u88ab\u5b9e\u73b0(\u5229\u7528 autograd ). \u5728 forward \u51fd\u6570\u4e2d\u53ef\u4f7f\u7528\u4efb\u4f55 Tensor \u652f\u6301\u7684\u64cd\u4f5c. \u7f51\u7edc\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u901a\u8fc7 net.parameters() \u8fd4\u56de, net.named_parameters \u53ef\u540c\u65f6\u8fd4\u56de\u5b66\u4e60\u7684\u53c2\u6570\u4ee5\u53ca\u540d\u79f0. params = list(net.parameters()) print(len(params)) print(params[0].size()) # conv1\u7684weight \u5411\u524d\u7684\u8f93\u5165\u662f\u4e00\u4e2a autograd.Variable , \u8f93\u51fa\u4e5f\u662f\u5982\u6b64. \u6ce8\u610f: \u8fd9\u4e2a\u7f51\u7edc(LeNet)\u7684\u9884\u671f\u8f93\u5165\u5927\u5c0f\u662f 32x32, \u4f7f\u7528\u8fd9\u4e2a\u7f51\u4e0a MNIST \u6570\u636e\u96c6, \u8bf7\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\u8c03\u6574\u4e3a 32x32. input = Variable(torch.randn(1, 1, 32, 32)) out = net(input) print(out) \u5c06\u7f51\u7edc\u4e2d\u6240\u6709\u53c2\u6570\u7684\u68af\u5ea6\u6e05\u96f6. net.zero_grad() out.backward(torch.randn(1, 10)) Note torch.nn \u53ea\u652f\u6301\u5c0f\u6279\u91cf(mini-batches), \u4e0d\u652f\u6301\u4e00\u6b21\u8f93\u5165\u4e00\u4e2a\u6837\u672c, \u5373\u4e00\u6b21\u5fc5\u987b\u662f\u4e00\u4e2a batch. \u4f8b\u5982, nn.Conv2d \u7684\u8f93\u5165\u5fc5\u987b\u662f 4 \u7ef4\u7684, \u5f62\u5982 nSamples x nChannels x Height x Width . \u5982\u679c\u4f60\u53ea\u60f3\u8f93\u5165\u4e00\u4e2a\u6837\u672c, \u9700\u8981\u4f7f\u7528 input.unsqueeze(0) \u5c06 batch_size \u8bbe\u7f6e\u4e3a 1. \u5728\u7ee7\u7eed\u4e4b\u524d, \u8ba9\u6211\u4eec\u56de\u987e\u4e00\u4e0b\u8fc4\u4eca\u4e3a\u6b62\u6240\u6709\u89c1\u8fc7\u7684\u7c7b. \u6982\u62ec: torch.Tensor - \u4e00\u4e2a \u591a\u7ef4\u6570\u7ec4 . autograd.Variable - \u5305\u88c5\u5f20\u91cf\u5e76\u8bb0\u5f55\u5e94\u7528\u4e8e\u5176\u4e0a\u7684\u5386\u53f2\u64cd\u4f5c . \u5177\u6709\u548c Tensor \u76f8\u540c\u7684 API ,\u8fd8\u6709\u4e00\u4e9b\u8865\u5145, \u5982 backward() . \u53e6\u5916 \u62e5\u6709\u5f20\u91cf\u7684\u68af\u5ea6 . nn.Module - \u795e\u7ecf\u7f51\u7edc\u6a21\u5757. \u65b9\u4fbf\u7684\u65b9\u5f0f\u5c01\u88c5\u53c2\u6570 , \u5e2e\u52a9\u5c06\u5176\u79fb\u52a8\u5230GPU, \u5bfc\u51fa, \u52a0\u8f7d\u7b49. nn.Parameter - \u4e00\u79cd\u53d8\u91cf, \u5f53\u88ab\u6307\u5b9a\u4e3a Model \u7684\u5c5e\u6027\u65f6, \u5b83\u4f1a\u81ea\u52a8\u6ce8\u518c\u4e3a\u4e00\u4e2a\u53c2\u6570. autograd.Function - \u5b9e\u73b0 autograd \u64cd\u4f5c\u7684\u5411\u524d\u548c\u5411\u540e\u5b9a\u4e49 . \u6bcf\u4e2a Variable \u64cd\u4f5c, \u81f3\u5c11\u521b\u5efa\u4e00\u4e2a Function \u8282\u70b9, \u8fde\u63a5\u5230\u521b\u5efa Variable \u7684\u51fd\u6570, \u5e76 \u7f16\u7801\u5b83\u7684\u5386\u53f2 . \u5728\u8fd9\u4e00\u70b9\u4e0a, \u6211\u4eec\u6db5\u76d6: \u5b9a\u4e49\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc \u5904\u7406\u8f93\u5165\u5e76\u53cd\u5411\u4f20\u64ad \u8fd8\u5269\u4e0b: \u8ba1\u7b97\u635f\u5931\u51fd\u6570 \u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd","title":"\u5b9a\u4e49\u7f51\u7edc"},{"location":"4/#_3","text":"\u635f\u5931\u51fd\u6570\u91c7\u7528 (output,target) \u8f93\u5165\u5bf9, \u5e76\u8ba1\u7b97\u9884\u6d4b\u8f93\u51fa\u7ed3\u679c\u4e0e\u5b9e\u9645\u76ee\u6807\u7684\u8ddd\u79bb. \u5728 nn \u5305\u4e0b\u6709\u51e0\u79cd\u4e0d\u540c\u7684 \u635f\u5931\u51fd\u6570 . \u4e00\u4e2a\u7b80\u5355\u7684\u635f\u5931\u51fd\u6570\u662f: nn.MSELoss \u8ba1\u7b97\u8f93\u51fa\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee \u4f8b\u5982: output = net(input) target = Variable(torch.arange(1, 11)) # \u4e00\u4e2a\u865a\u62df\u7684\u76ee\u6807 criterion = nn.MSELoss() loss = criterion(output, target) print(loss) \u73b0\u5728, \u5982\u679c\u4f60\u6cbf\u7740 loss \u53cd\u5411\u4f20\u64ad\u7684\u65b9\u5411\u4f7f\u7528 .grad_fn \u5c5e\u6027, \u4f60\u5c06\u4f1a\u770b\u5230\u4e00\u4e2a\u5982\u4e0b\u6240\u793a\u7684\u8ba1\u7b97\u56fe: input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss \u6240\u4ee5, \u5f53\u6211\u4eec\u8c03\u7528 loss.backward() , \u6574\u4e2a\u56fe\u4e0e\u635f\u5931\u662f\u6709\u533a\u522b\u7684, \u56fe\u4e2d\u7684\u6240\u6709\u53d8\u91cf\u90fd\u5c06\u7528 .grad \u68af\u5ea6\u7d2f\u52a0\u5b83\u4eec\u7684\u53d8\u91cf. \u4e3a\u4e86\u8bf4\u660e, \u8ba9\u6211\u4eec\u5411\u540e\u8d70\u51e0\u6b65: print(loss.grad_fn) # MSELoss print(loss.grad_fn.next_functions[0][0]) # Linear print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU","title":"\u635f\u5931\u51fd\u6570"},{"location":"4/#_4","text":"\u4e3a\u4e86\u53cd\u5411\u4f20\u64ad\u8bef\u5dee, \u6211\u4eec\u6240\u8981\u505a\u7684\u5c31\u662f loss.backward() . \u4f60\u9700\u8981\u6e05\u9664\u73b0\u6709\u7684\u68af\u5ea6, \u5426\u5219\u68af\u5ea6\u4f1a\u7d2f\u52a0\u4e4b\u524d\u7684\u68af\u5ea6. \u73b0\u5728\u6211\u4eec\u4f7f\u7528 loss.backward() , \u770b\u770b\u53cd\u5411\u4f20\u64ad\u4e4b\u524d\u548c\u4e4b\u540e conv1 \u7684\u68af\u5ea6. net.zero_grad() # \u628a\u4e4b\u524d\u7684\u68af\u5ea6\u6e05\u96f6 print('conv1.bias.grad before backward') print(net.conv1.bias.grad) loss.backward() print('conv1.bias.grad after backward') print(net.conv1.bias.grad) \u73b0\u5728, \u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4e86\u5982\u4f55\u4f7f\u7528\u635f\u5931\u51fd\u6570. \u7a0d\u540e\u9605\u8bfb: \u795e\u7ecf\u7f51\u7edc\u5305\u5305\u542b\u5404\u79cd\u6a21\u5757\u548c\u635f\u5931\u51fd\u6570, \u5f62\u6210\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6784\u5efa\u6a21\u5757. \u5b8c\u6574\u7684\u6587\u4ef6\u5217\u8868 \u5728\u8fd9\u91cc \u63a5\u4e0b\u6765\u5b66\u4e60\u7684\u552f\u4e00\u4e1c\u897f\u662f: \u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd","title":"\u53cd\u5411\u4f20\u64ad"},{"location":"4/#_5","text":"\u5b9e\u8df5\u4e2d\u4f7f\u7528\u7684\u6700\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219\u662f\u968f\u673a\u68af\u5ea6\u4e0b\u964d( SGD ): weight = weight - learning_rate * gradient \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u7b80\u5355\u7684 python \u4ee3\u7801\u6765\u5b9e\u73b0\u8fd9\u4e2a: learning_rate = 0.01 for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) \u7136\u800c, \u5f53\u4f60\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u65f6, \u4f60\u9700\u8981\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u7684\u66f4\u65b0\u89c4\u5219, \u6bd4\u5982 SGD, Nesterov-SGD, Adam, RMSProp\u7b49. \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e2a\u529f\u80fd, \u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305: torch.optim \u5b9e\u73b0\u6240\u6709\u8fd9\u4e9b\u65b9\u6cd5. \u4f7f\u7528\u5b83\u975e\u5e38\u7684\u7b80\u5355: import torch.optim as optim # \u65b0\u5efa\u4e00\u4e2a\u4f18\u5316\u5668, \u6307\u5b9a\u8981\u8c03\u6574\u7684\u53c2\u6570\u548c\u5b66\u4e60\u7387 optimizer = optim.SGD(net.parameters(), lr = 0.01) # \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d: optimizer.zero_grad() # \u9996\u5148\u68af\u5ea6\u6e05\u96f6(\u4e0e net.zero_grad() \u6548\u679c\u4e00\u6837) output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() # \u66f4\u65b0\u53c2\u6570 Note \u89c2\u5bdf\u5982\u4f55\u4f7f\u7528\u624b\u52a8\u8bbe\u7f6e\u68af\u5ea6\u6e05\u96f6 optimizer.zero_grad() . \u9700\u8981\u624b\u52a8\u6e05\u96f6\u7684\u539f\u56e0\u5728 Backprop _ \u4e2d\u5df2\u7ecf\u8bf4\u660e\u4e86(\u68af\u5ea6\u4f1a\u7d2f\u52a0\u4e4b\u524d\u7684\u68af\u5ea6). Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: neural_networks_tutorial.py Download Jupyter notebook: neural_networks_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u66f4\u65b0\u6743\u91cd"},{"location":"40/","text":"\u4f7f\u7528 ONNX \u5c06\u6a21\u578b\u4ece PyTorch \u8fc1\u79fb\u5230 Caffe2 \u548c Mobile \u5728\u672c\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 ONNX \u5c06 PyTorch \u4e2d\u5b9a\u4e49\u7684\u6a21\u578b\u8f6c\u6362\u4e3a ONNX \u683c\u5f0f, \u7136\u540e\u5c06\u5176\u52a0\u8f7d\u5230 Caffe2 \u4e2d. \u4e00\u65e6\u8fdb\u5165 Caffe2 , \u6211\u4eec\u53ef\u4ee5\u8fd0\u884c\u8be5\u6a21\u578b\u4ee5\u4ed4\u7ec6\u68c0\u67e5\u5b83\u662f\u5426\u6b63\u786e\u5bfc\u51fa, \u7136\u540e\u6f14\u793a\u5982\u4f55\u4f7f\u7528 Caffe2 \u529f\u80fd\uff08\u4f8b\u5982\u79fb\u52a8\u5bfc\u51fa\u5668\uff09\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u6a21\u578b. \u5728\u672c\u6559\u7a0b\u4e2d, \u60a8\u9700\u8981\u5b89\u88c5 onnx , onnx-caffe2 \u548c Caffe2 . \u4f60\u53ef\u4ee5\u901a\u8fc7 conda install -c ezyang onnx onnx-caffe2 \u7528 onnx \u548c onnx-caffe2 \u83b7\u5f97\u4e8c\u8fdb\u5236\u7248\u672c. NOTE : \u672c\u6559\u7a0b\u9700\u8981 PyTorch \u4e3b\u5206\u652f, \u53ef\u4ee5\u6309\u7167 here \u7684\u8bf4\u660e\u8fdb\u884c\u5b89\u88c5 # Some standard imports import io import numpy as np from torch import nn from torch.autograd import Variable import torch.utils.model_zoo as model_zoo import torch.onnx Super-resolution \u662f\u63d0\u9ad8\u56fe\u50cf, \u89c6\u9891\u5206\u8fa8\u7387\u7684\u4e00\u79cd\u65b9\u5f0f, \u5e7f\u6cdb\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6216\u89c6\u9891\u7f16\u8f91. \u5bf9\u4e8e\u672c\u6559\u7a0b, \u6211\u4eec\u5c06\u9996\u5148\u4f7f\u7528\u5e26\u6709\u865a\u62df\u8f93\u5165\u7684\u5c0f\u578b super-resolution \u6a21\u578b. \u9996\u5148, \u8ba9\u6211\u4eec\u5728 PyTorch \u4e2d\u521b\u5efa\u4e00\u4e2a SuperResolution \u6a21\u578b. \u8fd9\u4e2a\u6a21\u578b \u76f4\u63a5\u6765\u81ea PyTorch \u7684\u4f8b\u5b50\u800c\u6ca1\u6709\u4fee\u6539: # Super Resolution model definition in PyTorch import torch.nn as nn import torch.nn.init as init class SuperResolutionNet(nn.Module): def __init__(self, upscale_factor, inplace=False): super(SuperResolutionNet, self).__init__() self.relu = nn.ReLU(inplace=inplace) self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) self._initialize_weights() def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x def _initialize_weights(self): init.orthogonal(self.conv1.weight, init.calculate_gain('relu')) init.orthogonal(self.conv2.weight, init.calculate_gain('relu')) init.orthogonal(self.conv3.weight, init.calculate_gain('relu')) init.orthogonal(self.conv4.weight) # Create the super-resolution model by using the above model definition. torch_model = SuperResolutionNet(upscale_factor=3) \u901a\u5e38, \u4f60\u73b0\u5728\u8981\u8bad\u7ec3\u8fd9\u4e2a\u6a21\u578b; \u4f46\u662f, \u5bf9\u4e8e\u672c\u6559\u7a0b, \u6211\u4eec\u5c06\u4e0b\u8f7d\u4e00\u4e9b\u9884\u5148\u8bad\u7ec3\u7684\u6743\u91cd. \u8bf7\u6ce8\u610f, \u8be5\u6a21\u578b\u6ca1\u6709\u5f97\u5230\u5145\u5206\u8bad\u7ec3\u4ee5\u83b7\u5f97\u826f\u597d\u7684\u51c6\u786e\u6027, \u56e0\u6b64\u4ec5\u7528\u4e8e\u6f14\u793a\u76ee\u7684. # Load pretrained model weights model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth' batch_size = 1 # just a random number # Initialize model with the pretrained weights map_location = lambda storage, loc: storage if torch.cuda.is_available(): map_location = None torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location)) # set the train mode to false since we will only run the forward pass. torch_model.train(False) \u5728PyTorch\u4e2d\u5bfc\u51fa\u6a21\u578b\u901a\u8fc7\u8ddf\u8e2a\u5de5\u4f5c.\u8981\u5bfc\u51fa\u6a21\u578b, \u8bf7\u8c03\u7528\u8be5 torch.onnx._export() \u51fd\u6570. \u8fd9\u5c06\u6267\u884c\u6a21\u578b, \u8bb0\u5f55\u8fd0\u7b97\u7b26\u7528\u4e8e\u8ba1\u7b97\u8f93\u51fa\u7684\u8f68\u8ff9.\u7531\u4e8e _export \u8fd0\u884c\u6a21\u578b, \u6211\u4eec\u9700\u8981\u63d0\u4f9b\u4e00\u4e2a\u8f93\u5165\u5f20\u91cf x . \u8fd9\u4e2a\u5f20\u91cf\u4e2d\u7684\u503c\u5e76\u4e0d\u91cd\u8981; \u53ea\u8981\u5c3a\u5bf8\u5408\u9002, \u5b83\u53ef\u4ee5\u662f\u56fe\u50cf\u6216\u968f\u673a\u5f20\u91cf. \u8981\u4e86\u89e3\u66f4\u591a\u5173\u4e8e PyTorch \u5bfc\u51fa\u754c\u9762\u7684\u7ec6\u8282, \u8bf7\u67e5\u770b torch.onnx\u6587\u6863 . # Input to the model x = Variable(torch.randn(batch_size, 1, 224, 224), requires_grad=True) # Export the model torch_out = torch.onnx._export(torch_model, # model being run x, # model input (or a tuple for multiple inputs) \"super_resolution.onnx\", # where to save the model (can be a file or file-like object) export_params=True) # store the trained parameter weights inside the model file torch_out \u662f\u6267\u884c\u6a21\u578b\u540e\u7684\u8f93\u51fa.\u901a\u5e38\u60c5\u51b5\u4e0b, \u60a8\u53ef\u4ee5\u5ffd\u7565\u6b64\u8f93\u51fa, \u4f46\u5728\u6b64\u6211\u4eec\u5c06\u4f7f\u7528\u5b83\u6765\u9a8c\u8bc1\u6211\u4eec\u5bfc\u51fa\u7684\u6a21\u578b\u5728 Caffe2 \u4e2d\u8fd0\u884c\u65f6\u8ba1\u7b97\u76f8\u540c\u7684\u503c. \u73b0\u5728\u6211\u4eec\u6765\u770b\u770b ONNX \u8868\u793a\u6cd5, \u5e76\u5728 Caffe2 \u4e2d\u4f7f\u7528\u5b83. \u8fd9\u90e8\u5206\u901a\u5e38\u53ef\u4ee5\u5728\u5355\u72ec\u7684\u8fdb\u7a0b\u6216\u53e6\u4e00\u53f0\u673a\u5668\u4e0a\u5b8c\u6210, \u4f46\u6211\u4eec\u5c06\u7ee7\u7eed\u4f7f\u7528\u76f8\u540c\u7684\u8fc7\u7a0b, \u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u9a8c\u8bc1 Caffe2 \u548c PyTorch \u662f\u5426\u4e3a\u7f51\u7edc\u8ba1\u7b97\u76f8\u540c\u7684\u503c: import onnx import onnx_caffe2.backend # Load the ONNX ModelProto object. model is a standard Python protobuf object model = onnx.load(\"super_resolution.onnx\") # prepare the caffe2 backend for executing the model this converts the ONNX model into a # Caffe2 NetDef that can execute it. Other ONNX backends, like one for CNTK will be # availiable soon. prepared_backend = onnx_caffe2.backend.prepare(model) # run the model in Caffe2 # Construct a map from input names to Tensor data. # The graph of the model itself contains inputs for all weight parameters, after the input image. # Since the weights are already embedded, we just need to pass the input image. # Set the first input. W = {model.graph.input[0].name: x.data.numpy()} # Run the Caffe2 net: c2_out = prepared_backend.run(W)[0] # Verify the numerical correctness upto 3 decimal places np.testing.assert_almost_equal(torch_out.data.cpu().numpy(), c2_out, decimal=3) print(\"Exported model has been executed on Caffe2 backend, and the result looks good!\") \u6211\u4eec\u5e94\u8be5\u770b\u5230 PyTorch \u548c Caffe2 \u7684\u8f93\u51fa\u5728\u6570\u5b57\u4e0a\u5339\u914d\u8fbe\u52303\u4f4d\u5c0f\u6570. \u4f5c\u4e3a\u65c1\u6ce8, \u5982\u679c\u5b83\u4eec\u4e0d\u5339\u914d, \u90a3\u4e48 Caffe2 \u548c PyTorch \u4e2d\u7684\u64cd\u4f5c\u7b26\u7684\u5b9e\u73b0\u65b9\u5f0f\u4f1a\u6709\u6240\u4e0d\u540c, \u8bf7\u5728\u6b64\u60c5\u51b5\u4e0b\u4e0e\u6211\u4eec\u8054\u7cfb. \u4f7f\u7528 ONNX \u8fc1\u79fb\u5230 SRResNet \u4f7f\u7528\u4e0e\u4e0a\u8ff0\u76f8\u540c\u7684\u8fc7\u7a0b, \u6211\u4eec\u8fd8\u4e3a \u672c\u6587 \u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u65b0\u7684 super-resolution \u6a21\u5f0f \u201cSRResNet\u201d (\u611f\u8c22 Twitter \u4e0a\u7684\u4f5c\u8005\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u53c2\u6570, \u4ee5\u7528\u4e8e\u672c\u6559\u7a0b). \u6a21\u578b\u5b9a\u4e49\u548c\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\u53ef\u4ee5\u5728 \u8fd9\u91cc \u627e\u5230. \u4ee5\u4e0b\u662f SRResNet \u6a21\u578b\u8f93\u5165, \u8f93\u51fa\u7684\u6837\u5b50. \u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u6a21\u578b \u5230\u76ee\u524d\u4e3a\u6b62, \u6211\u4eec\u5df2\u7ecf\u4ece PyTorch \u4e2d\u5bfc\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b, \u5e76\u5c55\u793a\u4e86\u5982\u4f55\u52a0\u8f7d\u5b83\u5e76\u5728 Caffe2 \u4e2d\u8fd0\u884c\u5b83. \u73b0\u5728\u8be5\u6a21\u578b\u5df2\u7ecf\u52a0\u8f7d\u5230 Caffe2 \u4e2d, \u6211\u4eec\u53ef\u4ee5\u5c06\u5176\u8f6c\u6362\u4e3a\u9002\u5408 \u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c \u7684\u683c\u5f0f. \u6211\u4eec\u5c06\u4f7f\u7528 Caffe2 \u7684 mobile_exporter \u6765\u751f\u6210\u53ef\u4ee5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u4e24\u4e2a\u6a21\u578b protobufs. \u7b2c\u4e00\u4e2a\u7528\u4e8e\u4f7f\u7528\u6b63\u786e\u7684\u6743\u91cd\u521d\u59cb\u5316\u7f51\u7edc, \u7b2c\u4e8c\u4e2a\u5b9e\u9645\u8fd0\u884c\u7528\u4e8e\u6267\u884c\u6a21\u578b. \u6211\u4eec\u5c06\u7ee7\u7eed\u5728\u672c\u6559\u7a0b\u7684\u5176\u4f59\u90e8\u5206\u4f7f\u7528\u5c0f\u578b super-resolution \u6a21\u578b. # extract the workspace and the model proto from the internal representation c2_workspace = prepared_backend.workspace c2_model = prepared_backend.predict_net # Now import the caffe2 mobile exporter from caffe2.python.predictor import mobile_exporter # call the Export to get the predict_net, init_net. These nets are needed for running things on mobile init_net, predict_net = mobile_exporter.Export(c2_workspace, c2_model, c2_model.external_input) # Let's also save the init_net and predict_net to a file that we will later use for running them on mobile with open('init_net.pb', \"wb\") as fopen: fopen.write(init_net.SerializeToString()) with open('predict_net.pb', \"wb\") as fopen: fopen.write(predict_net.SerializeToString()) init_net \u5c06\u6a21\u578b\u53c2\u6570\u548c\u6a21\u578b\u8f93\u5165\u5d4c\u5165\u5176\u4e2d, predict_net \u5e76\u5c06\u7528\u4e8e init_net \u5728\u8fd0\u884c\u65f6\u6307\u5bfc\u6267\u884c. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u5c06\u4f7f\u7528 init_net \u4e0e predict_net \u4e0a\u9762\u751f\u6210\u548c\u5728\u6b63\u5e38 Caffe2 \u540e\u7aef\u548c\u79fb\u52a8\u8fd0\u884c\u5b83\u4eec, \u5e76\u9a8c\u8bc1\u5728\u4e24\u4e2a\u8bd5\u9a8c\u4e2d\u4ea7\u751f\u7684\u8f93\u51fa\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u732b\u662f\u76f8\u540c\u7684. \u5728\u672c\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u4f7f\u7528\u4e00\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u8457\u540d\u7684\u732b\u54aa\u56fe\u50cf, \u5982\u4e0b\u6240\u793a: # Some standard imports from caffe2.proto import caffe2_pb2 from caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils import numpy as np import os import subprocess from PIL import Image from matplotlib import pyplot from skimage import io, transform \u9996\u5148, \u6211\u4eec\u52a0\u8f7d\u56fe\u50cf, \u4f7f\u7528\u6807\u51c6\u7684 skimage python \u5e93\u5bf9\u5176\u8fdb\u884c\u9884\u5904\u7406. \u8bf7\u6ce8\u610f, \u8fd9\u79cd\u9884\u5904\u7406\u662f training/testing \u795e\u7ecf\u7f51\u7edc\u5904\u7406\u6570\u636e\u7684\u6807\u51c6\u5b9e\u8df5. # load the image img_in = io.imread(\"./_static/img/cat.jpg\") # resize the image to dimensions 224x224 img = transform.resize(img_in, [224, 224]) # save this resized image to be used as input to the model io.imsave(\"./_static/img/cat_224x224.jpg\", img) \u73b0\u5728, \u4f5c\u4e3a\u4e0b\u4e00\u6b65, \u6211\u4eec\u6765\u8c03\u6574\u5927\u5c0f\u7684\u732b\u56fe\u50cf, \u5e76\u5728 Caffe2 \u540e\u7aef\u8fd0\u884c super-resolution \u6a21\u578b\u5e76\u4fdd\u5b58\u8f93\u51fa\u56fe\u50cf. \u56fe\u50cf\u5904\u7406\u6b65\u9aa4\u5982\u4e0b\u5df2\u4ece PyTorch \u5b9e\u73b0 super-resolution \u6a21\u578b\u91c7\u7528 \u8fd9\u91cc # load the resized image and convert it to Ybr format img = Image.open(\"./_static/img/cat_224x224.jpg\") img_ycbcr = img.convert('YCbCr') img_y, img_cb, img_cr = img_ycbcr.split() # Let's run the mobile nets that we generated above so that caffe2 workspace is properly initialized workspace.RunNetOnce(init_net) workspace.RunNetOnce(predict_net) # Caffe2 has a nice net_printer to be able to inspect what the net looks like and identify # what our input and output blob names are. print(net_printer.to_string(predict_net)) \u4ece\u4e0a\u9762\u7684\u8f93\u51fa\u4e2d, \u6211\u4eec\u53ef\u4ee5\u770b\u5230\u8f93\u5165\u540d\u4e3a \u201c9\u201d, \u8f93\u51fa\u540d\u4e3a \u201c27\u201d(\u6709\u70b9\u5947\u602a, \u6211\u4eec\u5c06\u6570\u5b57\u4f5c\u4e3a blob \u540d\u79f0, \u4f46\u8fd9\u662f\u56e0\u4e3a\u8ddf\u8e2a JIT \u4f1a\u4e3a\u6a21\u578b\u751f\u6210\u7f16\u53f7\u6761\u76ee) # Now, let's also pass in the resized cat image for processing by the model. workspace.FeedBlob(\"9\", np.array(img_y)[np.newaxis, np.newaxis, :, :].astype(np.float32)) # run the predict_net to get the model output workspace.RunNetOnce(predict_net) # Now let's get the model output blob img_out = workspace.FetchBlob(\"27\") \u73b0\u5728, \u6211\u4eec\u5c06\u8fd4\u56de\u53c2\u8003 PyTorch \u6267\u884c super-resolution \u6a21\u578b\u7684\u540e\u5904\u7406\u6b65\u9aa4, \u5728\u8fd9\u91cc \u6784\u5efa\u56de\u6700\u7ec8\u8f93\u51fa\u7684\u56fe\u50cf\u5e76\u4fdd\u5b58\u56fe\u50cf. img_out_y = Image.fromarray(np.uint8((img_out[0, 0]).clip(0, 255)), mode='L') # get the output image follow post-processing step from PyTorch implementation final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") # Save the image, we will compare this with the output image from mobile device final_img.save(\"./_static/img/cat_superres.jpg\") \u6211\u4eec\u5df2\u7ecf\u5b8c\u6210\u4e86\u5728\u7eaf Caffe2 \u540e\u7aef\u8fd0\u884c\u6211\u4eec\u7684\u79fb\u52a8\u7f51\u7edc, \u73b0\u5728, \u8ba9\u6211\u4eec\u5728 Android \u8bbe\u5907\u4e0a\u6267\u884c\u6a21\u578b\u5e76\u83b7\u53d6\u6a21\u578b\u8f93\u51fa. NOTE : \u5bf9\u4e8e Android \u5f00\u53d1, adb \u9700\u8981\u4f7f\u7528 shell, \u5426\u5219\u4ee5\u4e0b\u90e8\u5206\u6559\u7a0b\u5c06\u65e0\u6cd5\u8fd0\u884c. \u5728\u6211\u4eec\u7684\u79fb\u52a8\u8bbe\u5907 runnig \u6a21\u578b\u7684\u7b2c\u4e00\u6b65\u4e2d, \u6211\u4eec\u5c06\u628a\u79fb\u52a8\u8bbe\u5907\u7684\u672c\u5730\u901f\u5ea6\u57fa\u51c6\u4e8c\u8fdb\u5236\u6587\u4ef6\u63a8\u9001\u5230 adb. \u8fd9\u4e2a\u4e8c\u8fdb\u5236\u6587\u4ef6\u53ef\u4ee5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u6a21\u578b, \u4e5f\u53ef\u4ee5\u5bfc\u51fa\u7a0d\u540e\u53ef\u4ee5\u68c0\u7d22\u7684\u6a21\u578b\u8f93\u51fa. \u4e8c\u8fdb\u5236\u6587\u4ef6 \u5728\u8fd9\u91cc \u53ef\u7528. \u4e3a\u4e86\u6784\u5efa\u4e8c\u8fdb\u5236\u6587\u4ef6, \u8bf7 build_android.sh \u6309\u7167 \u6b64\u5904 \u7684\u8bf4\u660e\u6267\u884c\u811a\u672c. NOTE : \u60a8\u9700\u8981 ANDROID_NDK \u5b89\u88c5\u5e76\u8bbe\u7f6e\u60a8\u7684 env \u53d8\u91cf ANDROID_NDK=path to ndk root # let's first push a bunch of stuff to adb, specify the path for the binary CAFFE2_MOBILE_BINARY = ('caffe2/binaries/speed_benchmark') # we had saved our init_net and proto_net in steps above, we use them now. # Push the binary and the model protos os.system('adb push ' + CAFFE2_MOBILE_BINARY + ' /data/local/tmp/') os.system('adb push init_net.pb /data/local/tmp') os.system('adb push predict_net.pb /data/local/tmp') # Let's serialize the input image blob to a blob proto and then send it to mobile for execution. with open(\"input.blobproto\", \"wb\") as fid: fid.write(workspace.SerializeBlob(\"9\")) # push the input image blob to adb os.system('adb push input.blobproto /data/local/tmp/') # Now we run the net on mobile, look at the speed_benchmark --help for what various options mean os.system( 'adb shell /data/local/tmp/speed_benchmark ' # binary to execute '--init_net=/data/local/tmp/super_resolution_mobile_init.pb ' # mobile init_net '--net=/data/local/tmp/super_resolution_mobile_predict.pb ' # mobile predict_net '--input=9 ' # name of our input image blob '--input_file=/data/local/tmp/input.blobproto ' # serialized input image '--output_folder=/data/local/tmp ' # destination folder for saving mobile output '--output=27,9 ' # output blobs we are interested in '--iter=1 ' # number of net iterations to execute '--caffe2_log_level=0 ' ) # get the model output from adb and save to a file os.system('adb pull /data/local/tmp/27 ./output.blobproto') # We can recover the output content and post-process the model using same steps as we followed earlier blob_proto = caffe2_pb2.BlobProto() blob_proto.ParseFromString(open('./output.blobproto').read()) img_out = utils.Caffe2TensorToNumpyArray(blob_proto.tensor) img_out_y = Image.fromarray(np.uint8((img_out[0,0]).clip(0, 255)), mode='L') final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") final_img.save(\"./_static/img/cat_superres_mobile.jpg\") \u73b0\u5728, \u60a8\u53ef\u4ee5\u6bd4\u8f83\u56fe\u50cf cat_superres.jpg (\u6765\u81ea\u7eaf caffe2 \u540e\u7aef\u6267\u884c\u7684 cat_superres_mobile.jpg \u6a21\u578b\u8f93\u51fa) \u548c (\u6765\u81ea\u79fb\u52a8\u6267\u884c\u7684\u6a21\u578b\u8f93\u51fa) \u5e76\u67e5\u770b\u8fd9\u4e24\u4e2a\u56fe\u50cf\u770b\u8d77\u6765\u76f8\u540c. \u5982\u679c\u5b83\u4eec\u770b\u8d77\u6765\u4e0d\u4e00\u6837, \u90a3\u4e48\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u5c31\u4f1a\u51fa\u73b0\u95ee\u9898, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u8bf7\u8054\u7cfb Caffe2 \u793e\u533a. \u60a8\u5e94\u8be5\u671f\u671b\u770b\u5230\u8f93\u51fa\u56fe\u50cf\u5982\u4e0b\u6240\u793a: \u4f7f\u7528\u4e0a\u8ff0\u6b65\u9aa4, \u60a8\u53ef\u4ee5\u8f7b\u677e\u5730\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u6a21\u578b. \u53e6\u5916, \u6709\u5173 caffe2 \u79fb\u52a8\u540e\u7aef\u7684\u66f4\u591a\u4fe1\u606f, \u8bf7\u67e5\u770b caffe2-android-demo . Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: super_resolution_with_caffe2.py Download Jupyter notebook: super_resolution_with_caffe2.ipynb Gallery generated by Sphinx-Gallery","title":"\u4f7f\u7528 ONNX \u5c06\u6a21\u578b\u4ece PyTorch \u8fc1\u79fb\u5230 Caffe2 \u548c Mobile"},{"location":"40/#onnx-pytorch-caffe2-mobile","text":"\u5728\u672c\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 ONNX \u5c06 PyTorch \u4e2d\u5b9a\u4e49\u7684\u6a21\u578b\u8f6c\u6362\u4e3a ONNX \u683c\u5f0f, \u7136\u540e\u5c06\u5176\u52a0\u8f7d\u5230 Caffe2 \u4e2d. \u4e00\u65e6\u8fdb\u5165 Caffe2 , \u6211\u4eec\u53ef\u4ee5\u8fd0\u884c\u8be5\u6a21\u578b\u4ee5\u4ed4\u7ec6\u68c0\u67e5\u5b83\u662f\u5426\u6b63\u786e\u5bfc\u51fa, \u7136\u540e\u6f14\u793a\u5982\u4f55\u4f7f\u7528 Caffe2 \u529f\u80fd\uff08\u4f8b\u5982\u79fb\u52a8\u5bfc\u51fa\u5668\uff09\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u6a21\u578b. \u5728\u672c\u6559\u7a0b\u4e2d, \u60a8\u9700\u8981\u5b89\u88c5 onnx , onnx-caffe2 \u548c Caffe2 . \u4f60\u53ef\u4ee5\u901a\u8fc7 conda install -c ezyang onnx onnx-caffe2 \u7528 onnx \u548c onnx-caffe2 \u83b7\u5f97\u4e8c\u8fdb\u5236\u7248\u672c. NOTE : \u672c\u6559\u7a0b\u9700\u8981 PyTorch \u4e3b\u5206\u652f, \u53ef\u4ee5\u6309\u7167 here \u7684\u8bf4\u660e\u8fdb\u884c\u5b89\u88c5 # Some standard imports import io import numpy as np from torch import nn from torch.autograd import Variable import torch.utils.model_zoo as model_zoo import torch.onnx Super-resolution \u662f\u63d0\u9ad8\u56fe\u50cf, \u89c6\u9891\u5206\u8fa8\u7387\u7684\u4e00\u79cd\u65b9\u5f0f, \u5e7f\u6cdb\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6216\u89c6\u9891\u7f16\u8f91. \u5bf9\u4e8e\u672c\u6559\u7a0b, \u6211\u4eec\u5c06\u9996\u5148\u4f7f\u7528\u5e26\u6709\u865a\u62df\u8f93\u5165\u7684\u5c0f\u578b super-resolution \u6a21\u578b. \u9996\u5148, \u8ba9\u6211\u4eec\u5728 PyTorch \u4e2d\u521b\u5efa\u4e00\u4e2a SuperResolution \u6a21\u578b. \u8fd9\u4e2a\u6a21\u578b \u76f4\u63a5\u6765\u81ea PyTorch \u7684\u4f8b\u5b50\u800c\u6ca1\u6709\u4fee\u6539: # Super Resolution model definition in PyTorch import torch.nn as nn import torch.nn.init as init class SuperResolutionNet(nn.Module): def __init__(self, upscale_factor, inplace=False): super(SuperResolutionNet, self).__init__() self.relu = nn.ReLU(inplace=inplace) self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) self._initialize_weights() def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x def _initialize_weights(self): init.orthogonal(self.conv1.weight, init.calculate_gain('relu')) init.orthogonal(self.conv2.weight, init.calculate_gain('relu')) init.orthogonal(self.conv3.weight, init.calculate_gain('relu')) init.orthogonal(self.conv4.weight) # Create the super-resolution model by using the above model definition. torch_model = SuperResolutionNet(upscale_factor=3) \u901a\u5e38, \u4f60\u73b0\u5728\u8981\u8bad\u7ec3\u8fd9\u4e2a\u6a21\u578b; \u4f46\u662f, \u5bf9\u4e8e\u672c\u6559\u7a0b, \u6211\u4eec\u5c06\u4e0b\u8f7d\u4e00\u4e9b\u9884\u5148\u8bad\u7ec3\u7684\u6743\u91cd. \u8bf7\u6ce8\u610f, \u8be5\u6a21\u578b\u6ca1\u6709\u5f97\u5230\u5145\u5206\u8bad\u7ec3\u4ee5\u83b7\u5f97\u826f\u597d\u7684\u51c6\u786e\u6027, \u56e0\u6b64\u4ec5\u7528\u4e8e\u6f14\u793a\u76ee\u7684. # Load pretrained model weights model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth' batch_size = 1 # just a random number # Initialize model with the pretrained weights map_location = lambda storage, loc: storage if torch.cuda.is_available(): map_location = None torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location)) # set the train mode to false since we will only run the forward pass. torch_model.train(False) \u5728PyTorch\u4e2d\u5bfc\u51fa\u6a21\u578b\u901a\u8fc7\u8ddf\u8e2a\u5de5\u4f5c.\u8981\u5bfc\u51fa\u6a21\u578b, \u8bf7\u8c03\u7528\u8be5 torch.onnx._export() \u51fd\u6570. \u8fd9\u5c06\u6267\u884c\u6a21\u578b, \u8bb0\u5f55\u8fd0\u7b97\u7b26\u7528\u4e8e\u8ba1\u7b97\u8f93\u51fa\u7684\u8f68\u8ff9.\u7531\u4e8e _export \u8fd0\u884c\u6a21\u578b, \u6211\u4eec\u9700\u8981\u63d0\u4f9b\u4e00\u4e2a\u8f93\u5165\u5f20\u91cf x . \u8fd9\u4e2a\u5f20\u91cf\u4e2d\u7684\u503c\u5e76\u4e0d\u91cd\u8981; \u53ea\u8981\u5c3a\u5bf8\u5408\u9002, \u5b83\u53ef\u4ee5\u662f\u56fe\u50cf\u6216\u968f\u673a\u5f20\u91cf. \u8981\u4e86\u89e3\u66f4\u591a\u5173\u4e8e PyTorch \u5bfc\u51fa\u754c\u9762\u7684\u7ec6\u8282, \u8bf7\u67e5\u770b torch.onnx\u6587\u6863 . # Input to the model x = Variable(torch.randn(batch_size, 1, 224, 224), requires_grad=True) # Export the model torch_out = torch.onnx._export(torch_model, # model being run x, # model input (or a tuple for multiple inputs) \"super_resolution.onnx\", # where to save the model (can be a file or file-like object) export_params=True) # store the trained parameter weights inside the model file torch_out \u662f\u6267\u884c\u6a21\u578b\u540e\u7684\u8f93\u51fa.\u901a\u5e38\u60c5\u51b5\u4e0b, \u60a8\u53ef\u4ee5\u5ffd\u7565\u6b64\u8f93\u51fa, \u4f46\u5728\u6b64\u6211\u4eec\u5c06\u4f7f\u7528\u5b83\u6765\u9a8c\u8bc1\u6211\u4eec\u5bfc\u51fa\u7684\u6a21\u578b\u5728 Caffe2 \u4e2d\u8fd0\u884c\u65f6\u8ba1\u7b97\u76f8\u540c\u7684\u503c. \u73b0\u5728\u6211\u4eec\u6765\u770b\u770b ONNX \u8868\u793a\u6cd5, \u5e76\u5728 Caffe2 \u4e2d\u4f7f\u7528\u5b83. \u8fd9\u90e8\u5206\u901a\u5e38\u53ef\u4ee5\u5728\u5355\u72ec\u7684\u8fdb\u7a0b\u6216\u53e6\u4e00\u53f0\u673a\u5668\u4e0a\u5b8c\u6210, \u4f46\u6211\u4eec\u5c06\u7ee7\u7eed\u4f7f\u7528\u76f8\u540c\u7684\u8fc7\u7a0b, \u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u9a8c\u8bc1 Caffe2 \u548c PyTorch \u662f\u5426\u4e3a\u7f51\u7edc\u8ba1\u7b97\u76f8\u540c\u7684\u503c: import onnx import onnx_caffe2.backend # Load the ONNX ModelProto object. model is a standard Python protobuf object model = onnx.load(\"super_resolution.onnx\") # prepare the caffe2 backend for executing the model this converts the ONNX model into a # Caffe2 NetDef that can execute it. Other ONNX backends, like one for CNTK will be # availiable soon. prepared_backend = onnx_caffe2.backend.prepare(model) # run the model in Caffe2 # Construct a map from input names to Tensor data. # The graph of the model itself contains inputs for all weight parameters, after the input image. # Since the weights are already embedded, we just need to pass the input image. # Set the first input. W = {model.graph.input[0].name: x.data.numpy()} # Run the Caffe2 net: c2_out = prepared_backend.run(W)[0] # Verify the numerical correctness upto 3 decimal places np.testing.assert_almost_equal(torch_out.data.cpu().numpy(), c2_out, decimal=3) print(\"Exported model has been executed on Caffe2 backend, and the result looks good!\") \u6211\u4eec\u5e94\u8be5\u770b\u5230 PyTorch \u548c Caffe2 \u7684\u8f93\u51fa\u5728\u6570\u5b57\u4e0a\u5339\u914d\u8fbe\u52303\u4f4d\u5c0f\u6570. \u4f5c\u4e3a\u65c1\u6ce8, \u5982\u679c\u5b83\u4eec\u4e0d\u5339\u914d, \u90a3\u4e48 Caffe2 \u548c PyTorch \u4e2d\u7684\u64cd\u4f5c\u7b26\u7684\u5b9e\u73b0\u65b9\u5f0f\u4f1a\u6709\u6240\u4e0d\u540c, \u8bf7\u5728\u6b64\u60c5\u51b5\u4e0b\u4e0e\u6211\u4eec\u8054\u7cfb.","title":"\u4f7f\u7528 ONNX \u5c06\u6a21\u578b\u4ece PyTorch \u8fc1\u79fb\u5230 Caffe2 \u548c Mobile"},{"location":"40/#onnx-srresnet","text":"\u4f7f\u7528\u4e0e\u4e0a\u8ff0\u76f8\u540c\u7684\u8fc7\u7a0b, \u6211\u4eec\u8fd8\u4e3a \u672c\u6587 \u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u65b0\u7684 super-resolution \u6a21\u5f0f \u201cSRResNet\u201d (\u611f\u8c22 Twitter \u4e0a\u7684\u4f5c\u8005\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u53c2\u6570, \u4ee5\u7528\u4e8e\u672c\u6559\u7a0b). \u6a21\u578b\u5b9a\u4e49\u548c\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\u53ef\u4ee5\u5728 \u8fd9\u91cc \u627e\u5230. \u4ee5\u4e0b\u662f SRResNet \u6a21\u578b\u8f93\u5165, \u8f93\u51fa\u7684\u6837\u5b50.","title":"\u4f7f\u7528 ONNX \u8fc1\u79fb\u5230 SRResNet"},{"location":"40/#_1","text":"\u5230\u76ee\u524d\u4e3a\u6b62, \u6211\u4eec\u5df2\u7ecf\u4ece PyTorch \u4e2d\u5bfc\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b, \u5e76\u5c55\u793a\u4e86\u5982\u4f55\u52a0\u8f7d\u5b83\u5e76\u5728 Caffe2 \u4e2d\u8fd0\u884c\u5b83. \u73b0\u5728\u8be5\u6a21\u578b\u5df2\u7ecf\u52a0\u8f7d\u5230 Caffe2 \u4e2d, \u6211\u4eec\u53ef\u4ee5\u5c06\u5176\u8f6c\u6362\u4e3a\u9002\u5408 \u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c \u7684\u683c\u5f0f. \u6211\u4eec\u5c06\u4f7f\u7528 Caffe2 \u7684 mobile_exporter \u6765\u751f\u6210\u53ef\u4ee5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u4e24\u4e2a\u6a21\u578b protobufs. \u7b2c\u4e00\u4e2a\u7528\u4e8e\u4f7f\u7528\u6b63\u786e\u7684\u6743\u91cd\u521d\u59cb\u5316\u7f51\u7edc, \u7b2c\u4e8c\u4e2a\u5b9e\u9645\u8fd0\u884c\u7528\u4e8e\u6267\u884c\u6a21\u578b. \u6211\u4eec\u5c06\u7ee7\u7eed\u5728\u672c\u6559\u7a0b\u7684\u5176\u4f59\u90e8\u5206\u4f7f\u7528\u5c0f\u578b super-resolution \u6a21\u578b. # extract the workspace and the model proto from the internal representation c2_workspace = prepared_backend.workspace c2_model = prepared_backend.predict_net # Now import the caffe2 mobile exporter from caffe2.python.predictor import mobile_exporter # call the Export to get the predict_net, init_net. These nets are needed for running things on mobile init_net, predict_net = mobile_exporter.Export(c2_workspace, c2_model, c2_model.external_input) # Let's also save the init_net and predict_net to a file that we will later use for running them on mobile with open('init_net.pb', \"wb\") as fopen: fopen.write(init_net.SerializeToString()) with open('predict_net.pb', \"wb\") as fopen: fopen.write(predict_net.SerializeToString()) init_net \u5c06\u6a21\u578b\u53c2\u6570\u548c\u6a21\u578b\u8f93\u5165\u5d4c\u5165\u5176\u4e2d, predict_net \u5e76\u5c06\u7528\u4e8e init_net \u5728\u8fd0\u884c\u65f6\u6307\u5bfc\u6267\u884c. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u5c06\u4f7f\u7528 init_net \u4e0e predict_net \u4e0a\u9762\u751f\u6210\u548c\u5728\u6b63\u5e38 Caffe2 \u540e\u7aef\u548c\u79fb\u52a8\u8fd0\u884c\u5b83\u4eec, \u5e76\u9a8c\u8bc1\u5728\u4e24\u4e2a\u8bd5\u9a8c\u4e2d\u4ea7\u751f\u7684\u8f93\u51fa\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u732b\u662f\u76f8\u540c\u7684. \u5728\u672c\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u4f7f\u7528\u4e00\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u8457\u540d\u7684\u732b\u54aa\u56fe\u50cf, \u5982\u4e0b\u6240\u793a: # Some standard imports from caffe2.proto import caffe2_pb2 from caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils import numpy as np import os import subprocess from PIL import Image from matplotlib import pyplot from skimage import io, transform \u9996\u5148, \u6211\u4eec\u52a0\u8f7d\u56fe\u50cf, \u4f7f\u7528\u6807\u51c6\u7684 skimage python \u5e93\u5bf9\u5176\u8fdb\u884c\u9884\u5904\u7406. \u8bf7\u6ce8\u610f, \u8fd9\u79cd\u9884\u5904\u7406\u662f training/testing \u795e\u7ecf\u7f51\u7edc\u5904\u7406\u6570\u636e\u7684\u6807\u51c6\u5b9e\u8df5. # load the image img_in = io.imread(\"./_static/img/cat.jpg\") # resize the image to dimensions 224x224 img = transform.resize(img_in, [224, 224]) # save this resized image to be used as input to the model io.imsave(\"./_static/img/cat_224x224.jpg\", img) \u73b0\u5728, \u4f5c\u4e3a\u4e0b\u4e00\u6b65, \u6211\u4eec\u6765\u8c03\u6574\u5927\u5c0f\u7684\u732b\u56fe\u50cf, \u5e76\u5728 Caffe2 \u540e\u7aef\u8fd0\u884c super-resolution \u6a21\u578b\u5e76\u4fdd\u5b58\u8f93\u51fa\u56fe\u50cf. \u56fe\u50cf\u5904\u7406\u6b65\u9aa4\u5982\u4e0b\u5df2\u4ece PyTorch \u5b9e\u73b0 super-resolution \u6a21\u578b\u91c7\u7528 \u8fd9\u91cc # load the resized image and convert it to Ybr format img = Image.open(\"./_static/img/cat_224x224.jpg\") img_ycbcr = img.convert('YCbCr') img_y, img_cb, img_cr = img_ycbcr.split() # Let's run the mobile nets that we generated above so that caffe2 workspace is properly initialized workspace.RunNetOnce(init_net) workspace.RunNetOnce(predict_net) # Caffe2 has a nice net_printer to be able to inspect what the net looks like and identify # what our input and output blob names are. print(net_printer.to_string(predict_net)) \u4ece\u4e0a\u9762\u7684\u8f93\u51fa\u4e2d, \u6211\u4eec\u53ef\u4ee5\u770b\u5230\u8f93\u5165\u540d\u4e3a \u201c9\u201d, \u8f93\u51fa\u540d\u4e3a \u201c27\u201d(\u6709\u70b9\u5947\u602a, \u6211\u4eec\u5c06\u6570\u5b57\u4f5c\u4e3a blob \u540d\u79f0, \u4f46\u8fd9\u662f\u56e0\u4e3a\u8ddf\u8e2a JIT \u4f1a\u4e3a\u6a21\u578b\u751f\u6210\u7f16\u53f7\u6761\u76ee) # Now, let's also pass in the resized cat image for processing by the model. workspace.FeedBlob(\"9\", np.array(img_y)[np.newaxis, np.newaxis, :, :].astype(np.float32)) # run the predict_net to get the model output workspace.RunNetOnce(predict_net) # Now let's get the model output blob img_out = workspace.FetchBlob(\"27\") \u73b0\u5728, \u6211\u4eec\u5c06\u8fd4\u56de\u53c2\u8003 PyTorch \u6267\u884c super-resolution \u6a21\u578b\u7684\u540e\u5904\u7406\u6b65\u9aa4, \u5728\u8fd9\u91cc \u6784\u5efa\u56de\u6700\u7ec8\u8f93\u51fa\u7684\u56fe\u50cf\u5e76\u4fdd\u5b58\u56fe\u50cf. img_out_y = Image.fromarray(np.uint8((img_out[0, 0]).clip(0, 255)), mode='L') # get the output image follow post-processing step from PyTorch implementation final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") # Save the image, we will compare this with the output image from mobile device final_img.save(\"./_static/img/cat_superres.jpg\") \u6211\u4eec\u5df2\u7ecf\u5b8c\u6210\u4e86\u5728\u7eaf Caffe2 \u540e\u7aef\u8fd0\u884c\u6211\u4eec\u7684\u79fb\u52a8\u7f51\u7edc, \u73b0\u5728, \u8ba9\u6211\u4eec\u5728 Android \u8bbe\u5907\u4e0a\u6267\u884c\u6a21\u578b\u5e76\u83b7\u53d6\u6a21\u578b\u8f93\u51fa. NOTE : \u5bf9\u4e8e Android \u5f00\u53d1, adb \u9700\u8981\u4f7f\u7528 shell, \u5426\u5219\u4ee5\u4e0b\u90e8\u5206\u6559\u7a0b\u5c06\u65e0\u6cd5\u8fd0\u884c. \u5728\u6211\u4eec\u7684\u79fb\u52a8\u8bbe\u5907 runnig \u6a21\u578b\u7684\u7b2c\u4e00\u6b65\u4e2d, \u6211\u4eec\u5c06\u628a\u79fb\u52a8\u8bbe\u5907\u7684\u672c\u5730\u901f\u5ea6\u57fa\u51c6\u4e8c\u8fdb\u5236\u6587\u4ef6\u63a8\u9001\u5230 adb. \u8fd9\u4e2a\u4e8c\u8fdb\u5236\u6587\u4ef6\u53ef\u4ee5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u6a21\u578b, \u4e5f\u53ef\u4ee5\u5bfc\u51fa\u7a0d\u540e\u53ef\u4ee5\u68c0\u7d22\u7684\u6a21\u578b\u8f93\u51fa. \u4e8c\u8fdb\u5236\u6587\u4ef6 \u5728\u8fd9\u91cc \u53ef\u7528. \u4e3a\u4e86\u6784\u5efa\u4e8c\u8fdb\u5236\u6587\u4ef6, \u8bf7 build_android.sh \u6309\u7167 \u6b64\u5904 \u7684\u8bf4\u660e\u6267\u884c\u811a\u672c. NOTE : \u60a8\u9700\u8981 ANDROID_NDK \u5b89\u88c5\u5e76\u8bbe\u7f6e\u60a8\u7684 env \u53d8\u91cf ANDROID_NDK=path to ndk root # let's first push a bunch of stuff to adb, specify the path for the binary CAFFE2_MOBILE_BINARY = ('caffe2/binaries/speed_benchmark') # we had saved our init_net and proto_net in steps above, we use them now. # Push the binary and the model protos os.system('adb push ' + CAFFE2_MOBILE_BINARY + ' /data/local/tmp/') os.system('adb push init_net.pb /data/local/tmp') os.system('adb push predict_net.pb /data/local/tmp') # Let's serialize the input image blob to a blob proto and then send it to mobile for execution. with open(\"input.blobproto\", \"wb\") as fid: fid.write(workspace.SerializeBlob(\"9\")) # push the input image blob to adb os.system('adb push input.blobproto /data/local/tmp/') # Now we run the net on mobile, look at the speed_benchmark --help for what various options mean os.system( 'adb shell /data/local/tmp/speed_benchmark ' # binary to execute '--init_net=/data/local/tmp/super_resolution_mobile_init.pb ' # mobile init_net '--net=/data/local/tmp/super_resolution_mobile_predict.pb ' # mobile predict_net '--input=9 ' # name of our input image blob '--input_file=/data/local/tmp/input.blobproto ' # serialized input image '--output_folder=/data/local/tmp ' # destination folder for saving mobile output '--output=27,9 ' # output blobs we are interested in '--iter=1 ' # number of net iterations to execute '--caffe2_log_level=0 ' ) # get the model output from adb and save to a file os.system('adb pull /data/local/tmp/27 ./output.blobproto') # We can recover the output content and post-process the model using same steps as we followed earlier blob_proto = caffe2_pb2.BlobProto() blob_proto.ParseFromString(open('./output.blobproto').read()) img_out = utils.Caffe2TensorToNumpyArray(blob_proto.tensor) img_out_y = Image.fromarray(np.uint8((img_out[0,0]).clip(0, 255)), mode='L') final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") final_img.save(\"./_static/img/cat_superres_mobile.jpg\") \u73b0\u5728, \u60a8\u53ef\u4ee5\u6bd4\u8f83\u56fe\u50cf cat_superres.jpg (\u6765\u81ea\u7eaf caffe2 \u540e\u7aef\u6267\u884c\u7684 cat_superres_mobile.jpg \u6a21\u578b\u8f93\u51fa) \u548c (\u6765\u81ea\u79fb\u52a8\u6267\u884c\u7684\u6a21\u578b\u8f93\u51fa) \u5e76\u67e5\u770b\u8fd9\u4e24\u4e2a\u56fe\u50cf\u770b\u8d77\u6765\u76f8\u540c. \u5982\u679c\u5b83\u4eec\u770b\u8d77\u6765\u4e0d\u4e00\u6837, \u90a3\u4e48\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u5c31\u4f1a\u51fa\u73b0\u95ee\u9898, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u8bf7\u8054\u7cfb Caffe2 \u793e\u533a. \u60a8\u5e94\u8be5\u671f\u671b\u770b\u5230\u8f93\u51fa\u56fe\u50cf\u5982\u4e0b\u6240\u793a: \u4f7f\u7528\u4e0a\u8ff0\u6b65\u9aa4, \u60a8\u53ef\u4ee5\u8f7b\u677e\u5730\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u6a21\u578b. \u53e6\u5916, \u6709\u5173 caffe2 \u79fb\u52a8\u540e\u7aef\u7684\u66f4\u591a\u4fe1\u606f, \u8bf7\u67e5\u770b caffe2-android-demo . Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: super_resolution_with_caffe2.py Download Jupyter notebook: super_resolution_with_caffe2.ipynb Gallery generated by Sphinx-Gallery","title":"\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u6a21\u578b"},{"location":"41/","text":"\u4e3a pytorch \u81ea\u5b9a\u4e49 C \u6269\u5c55 \u4f5c\u8005 : Soumith Chintala \u7b2c\u4e00\u6b65. \u51c6\u5907\u4f60\u7684 C \u4ee3\u7801 \u9996\u5148, \u4f60\u9700\u8981\u7f16\u5199\u4f60\u7684 C \u51fd\u6570. \u4e0b\u9762\u4f60\u53ef\u4ee5\u627e\u5230\u6a21\u5757\u7684\u6b63\u5411\u548c\u53cd\u5411\u51fd\u6570\u7684\u793a\u4f8b\u5b9e\u73b0, \u5b83\u5c06\u4e24\u4e2a\u8f93\u5165\u76f8\u52a0. \u5728\u4f60\u7684 .c \u6587\u4ef6\u4e2d, \u4f60\u53ef\u4ee5\u4f7f\u7528 #include &lt;TH/TH.h&gt; \u76f4\u63a5\u5305\u542b TH, \u4ee5\u53ca\u4f7f\u7528 #include &lt;THC/THC.h&gt; \u5305\u542b THC. ffi (\u5916\u6765\u51fd\u6570\u63a5\u53e3) \u5de5\u5177\u4f1a\u786e\u4fdd\u7f16\u8bd1\u5668\u53ef\u4ee5\u5728\u6784\u5efa\u8fc7\u7a0b\u4e2d\u627e\u5230\u5b83\u4eec. /* src/my_lib.c */ #include <TH/TH.h> int my_lib_add_forward(THFloatTensor *input1, THFloatTensor *input2, THFloatTensor *output) { if (!THFloatTensor_isSameSizeAs(input1, input2)) return 0; THFloatTensor_resizeAs(output, input1); THFloatTensor_cadd(output, input1, 1.0, input2); return 1; } int my_lib_add_backward(THFloatTensor *grad_output, THFloatTensor *grad_input) { THFloatTensor_resizeAs(grad_input, grad_output); THFloatTensor_fill(grad_input, 1); return 1; } \u4ee3\u7801\u6ca1\u6709\u4efb\u4f55\u9650\u5236, \u9664\u4e86\u4f60\u5fc5\u987b\u51c6\u5907\u5355\u4e2a\u5934\u6587\u4ef6, \u5b83\u4f1a\u5217\u51fa\u6240\u6709\u4f60\u60f3\u8981\u4ece Python \u8c03\u7528\u7684\u51fd\u6570. \u5b83\u4f1a\u7531 ffi \u7528\u4e8e\u751f\u6210\u5408\u9002\u7684\u5305\u88c5. /* src/my_lib.h */ int my_lib_add_forward(THFloatTensor *input1, THFloatTensor *input2, THFloatTensor *output); int my_lib_add_backward(THFloatTensor *grad_output, THFloatTensor *grad_input); \u73b0\u5728, \u4f60\u9700\u8981\u4e00\u4e2a\u8d85\u77ed\u7684\u6587\u4ef6, \u5b83\u4f1a\u6784\u5efa\u4f60\u7684\u81ea\u5b9a\u4e49\u6269\u5c55: # build.py from torch.utils.ffi import create_extension ffi = create_extension( name='_ext.my_lib', headers='src/my_lib.h', sources=['src/my_lib.c'], with_cuda=False ) ffi.build() \u7b2c\u4e8c\u6b65: \u5728\u4f60\u7684 Python \u4ee3\u7801\u4e2d\u5305\u542b\u5b83 \u4f60\u8fd0\u884c\u5b83\u4e4b\u540e, pytorch \u4f1a\u521b\u5efa\u4e00\u4e2a _ext \u76ee\u5f55, \u5e76\u628a my_lib \u653e\u5230\u91cc\u9762. \u5305\u540d\u79f0\u53ef\u4ee5\u5728\u6700\u7ec8\u6a21\u5757\u540d\u79f0\u4e4b\u524d, \u5305\u542b\u4efb\u610f\u6570\u91cf\u7684\u5305 (\u5305\u62ec\u6ca1\u6709). \u5982\u679c\u6784\u5efa\u6210\u529f, \u4f60\u53ef\u4ee5\u5bfc\u5165\u4f60\u7684\u6269\u5c55, \u5c31\u50cf\u666e\u901a\u7684 Python \u6587\u4ef6. # functions/add.py import torch from torch.autograd import Function from _ext import my_lib class MyAddFunction(Function): def forward(self, input1, input2): output = torch.FloatTensor() my_lib.my_lib_add_forward(input1, input2, output) return output def backward(self, grad_output): grad_input = torch.FloatTensor() my_lib.my_lib_add_backward(grad_output, grad_input) return grad_input # modules/add.py from torch.nn import Module from functions.add import MyAddFunction class MyAddModule(Module): def forward(self, input1, input2): return MyAddFunction()(input1, input2) # main.py import torch import torch.nn as nn from torch.autograd import Variable from modules.add import MyAddModule class MyNetwork(nn.Module): def __init__(self): super(MyNetwork, self).__init__() self.add = MyAddModule() def forward(self, input1, input2): return self.add(input1, input2) model = MyNetwork() input1, input2 = Variable(torch.randn(5, 5)), Variable(torch.randn(5, 5)) print(model(input1, input2)) print(input1 + input2)","title":"\u4e3a pytorch \u81ea\u5b9a\u4e49 C \u6269\u5c55"},{"location":"41/#pytorch-c","text":"\u4f5c\u8005 : Soumith Chintala","title":"\u4e3a pytorch \u81ea\u5b9a\u4e49 C \u6269\u5c55"},{"location":"41/#c","text":"\u9996\u5148, \u4f60\u9700\u8981\u7f16\u5199\u4f60\u7684 C \u51fd\u6570. \u4e0b\u9762\u4f60\u53ef\u4ee5\u627e\u5230\u6a21\u5757\u7684\u6b63\u5411\u548c\u53cd\u5411\u51fd\u6570\u7684\u793a\u4f8b\u5b9e\u73b0, \u5b83\u5c06\u4e24\u4e2a\u8f93\u5165\u76f8\u52a0. \u5728\u4f60\u7684 .c \u6587\u4ef6\u4e2d, \u4f60\u53ef\u4ee5\u4f7f\u7528 #include &lt;TH/TH.h&gt; \u76f4\u63a5\u5305\u542b TH, \u4ee5\u53ca\u4f7f\u7528 #include &lt;THC/THC.h&gt; \u5305\u542b THC. ffi (\u5916\u6765\u51fd\u6570\u63a5\u53e3) \u5de5\u5177\u4f1a\u786e\u4fdd\u7f16\u8bd1\u5668\u53ef\u4ee5\u5728\u6784\u5efa\u8fc7\u7a0b\u4e2d\u627e\u5230\u5b83\u4eec. /* src/my_lib.c */ #include <TH/TH.h> int my_lib_add_forward(THFloatTensor *input1, THFloatTensor *input2, THFloatTensor *output) { if (!THFloatTensor_isSameSizeAs(input1, input2)) return 0; THFloatTensor_resizeAs(output, input1); THFloatTensor_cadd(output, input1, 1.0, input2); return 1; } int my_lib_add_backward(THFloatTensor *grad_output, THFloatTensor *grad_input) { THFloatTensor_resizeAs(grad_input, grad_output); THFloatTensor_fill(grad_input, 1); return 1; } \u4ee3\u7801\u6ca1\u6709\u4efb\u4f55\u9650\u5236, \u9664\u4e86\u4f60\u5fc5\u987b\u51c6\u5907\u5355\u4e2a\u5934\u6587\u4ef6, \u5b83\u4f1a\u5217\u51fa\u6240\u6709\u4f60\u60f3\u8981\u4ece Python \u8c03\u7528\u7684\u51fd\u6570. \u5b83\u4f1a\u7531 ffi \u7528\u4e8e\u751f\u6210\u5408\u9002\u7684\u5305\u88c5. /* src/my_lib.h */ int my_lib_add_forward(THFloatTensor *input1, THFloatTensor *input2, THFloatTensor *output); int my_lib_add_backward(THFloatTensor *grad_output, THFloatTensor *grad_input); \u73b0\u5728, \u4f60\u9700\u8981\u4e00\u4e2a\u8d85\u77ed\u7684\u6587\u4ef6, \u5b83\u4f1a\u6784\u5efa\u4f60\u7684\u81ea\u5b9a\u4e49\u6269\u5c55: # build.py from torch.utils.ffi import create_extension ffi = create_extension( name='_ext.my_lib', headers='src/my_lib.h', sources=['src/my_lib.c'], with_cuda=False ) ffi.build()","title":"\u7b2c\u4e00\u6b65. \u51c6\u5907\u4f60\u7684 C \u4ee3\u7801"},{"location":"41/#python","text":"\u4f60\u8fd0\u884c\u5b83\u4e4b\u540e, pytorch \u4f1a\u521b\u5efa\u4e00\u4e2a _ext \u76ee\u5f55, \u5e76\u628a my_lib \u653e\u5230\u91cc\u9762. \u5305\u540d\u79f0\u53ef\u4ee5\u5728\u6700\u7ec8\u6a21\u5757\u540d\u79f0\u4e4b\u524d, \u5305\u542b\u4efb\u610f\u6570\u91cf\u7684\u5305 (\u5305\u62ec\u6ca1\u6709). \u5982\u679c\u6784\u5efa\u6210\u529f, \u4f60\u53ef\u4ee5\u5bfc\u5165\u4f60\u7684\u6269\u5c55, \u5c31\u50cf\u666e\u901a\u7684 Python \u6587\u4ef6. # functions/add.py import torch from torch.autograd import Function from _ext import my_lib class MyAddFunction(Function): def forward(self, input1, input2): output = torch.FloatTensor() my_lib.my_lib_add_forward(input1, input2, output) return output def backward(self, grad_output): grad_input = torch.FloatTensor() my_lib.my_lib_add_backward(grad_output, grad_input) return grad_input # modules/add.py from torch.nn import Module from functions.add import MyAddFunction class MyAddModule(Module): def forward(self, input1, input2): return MyAddFunction()(input1, input2) # main.py import torch import torch.nn as nn from torch.autograd import Variable from modules.add import MyAddModule class MyNetwork(nn.Module): def __init__(self): super(MyNetwork, self).__init__() self.add = MyAddModule() def forward(self, input1, input2): return self.add(input1, input2) model = MyNetwork() input1, input2 = Variable(torch.randn(5, 5)), Variable(torch.randn(5, 5)) print(model(input1, input2)) print(input1 + input2)","title":"\u7b2c\u4e8c\u6b65: \u5728\u4f60\u7684 Python \u4ee3\u7801\u4e2d\u5305\u542b\u5b83"},{"location":"42/","text":"\u9879\u76ee\u76f8\u5173","title":"\u9879\u76ee\u76f8\u5173"},{"location":"42/#_1","text":"","title":"\u9879\u76ee\u76f8\u5173"},{"location":"43/","text":"\u9879\u76ee\u8d21\u732e\u8005 \u8be5\u9875\u9762\u662f apachecn/pytorch-doc-zh \u9879\u76ee\u76f8\u5173\u7684\u8d21\u732e\u8005\u4fe1\u606f. \u9879\u76ee\u8d1f\u8d23\u4eba @\u90a3\u4f0a\u62b9\u5fae\u7b11 \u8d21\u732e\u8005 PyTorch \u4e2d\u6587\u6559\u7a0b\u8d21\u732e\u8005 @\u90a3\u4f0a\u62b9\u5fae\u7b11 \u5efa\u8bae\u53cd\u9988 \u8054\u7cfb\u9879\u76ee\u8d1f\u8d23\u4eba @\u90a3\u4f0a\u62b9\u5fae\u7b11 . \u5728\u6211\u4eec\u7684 apachecn/pytorch-doc-zh github \u4e0a\u63d0 issue. \u53d1\u9001\u90ae\u4ef6\u5230 Email: pytorch#apachecn.org (#\u66ff\u6362\u6210@) . \u5728\u6211\u4eec\u7684 \u7ec4\u7ec7\u5b66\u4e60\u4ea4\u6d41\u7fa4 \u4e2d\u8054\u7cfb\u7fa4\u4e3b/\u7ba1\u7406\u5458\u5373\u53ef.","title":"\u9879\u76ee\u8d21\u732e\u8005"},{"location":"43/#_1","text":"\u8be5\u9875\u9762\u662f apachecn/pytorch-doc-zh \u9879\u76ee\u76f8\u5173\u7684\u8d21\u732e\u8005\u4fe1\u606f.","title":"\u9879\u76ee\u8d21\u732e\u8005"},{"location":"43/#_2","text":"@\u90a3\u4f0a\u62b9\u5fae\u7b11","title":"\u9879\u76ee\u8d1f\u8d23\u4eba"},{"location":"43/#_3","text":"","title":"\u8d21\u732e\u8005"},{"location":"43/#pytorch","text":"@\u90a3\u4f0a\u62b9\u5fae\u7b11","title":"PyTorch \u4e2d\u6587\u6559\u7a0b\u8d21\u732e\u8005"},{"location":"43/#_4","text":"\u8054\u7cfb\u9879\u76ee\u8d1f\u8d23\u4eba @\u90a3\u4f0a\u62b9\u5fae\u7b11 . \u5728\u6211\u4eec\u7684 apachecn/pytorch-doc-zh github \u4e0a\u63d0 issue. \u53d1\u9001\u90ae\u4ef6\u5230 Email: pytorch#apachecn.org (#\u66ff\u6362\u6210@) . \u5728\u6211\u4eec\u7684 \u7ec4\u7ec7\u5b66\u4e60\u4ea4\u6d41\u7fa4 \u4e2d\u8054\u7cfb\u7fa4\u4e3b/\u7ba1\u7406\u5458\u5373\u53ef.","title":"\u5efa\u8bae\u53cd\u9988"},{"location":"44/","text":"\u7ec4\u7ec7\u5b66\u4e60\u4ea4\u6d41\u7fa4 \u673a\u5668\u5b66\u4e60\u4ea4\u6d41\u7fa4: 629470233 (2000\u4eba) \u5927\u6570\u636e\u4ea4\u6d41\u7fa4: 214293307 (2000\u4eba) Kaggle \u7ade\u8d5b\u4ea4\u6d41\u7fa4: 686932392 (2000\u4eba) \u4e86\u89e3\u6211\u4eec: http://www.apachecn.org/organization/209.html \u52a0\u5165\u7ec4\u7ec7: http://www.apachecn.org/organization/209.html \u66f4\u591a\u5b66 (zhuang) \u4e60 (bi) \u4ea4\u6d41\u7fa4\u8bf7\u53c2\u9605: http://www.apachecn.org/organization/348.html","title":"\u7ec4\u7ec7\u5b66\u4e60\u4ea4\u6d41\u7fa4"},{"location":"44/#_1","text":"\u673a\u5668\u5b66\u4e60\u4ea4\u6d41\u7fa4: 629470233 (2000\u4eba) \u5927\u6570\u636e\u4ea4\u6d41\u7fa4: 214293307 (2000\u4eba) Kaggle \u7ade\u8d5b\u4ea4\u6d41\u7fa4: 686932392 (2000\u4eba) \u4e86\u89e3\u6211\u4eec: http://www.apachecn.org/organization/209.html \u52a0\u5165\u7ec4\u7ec7: http://www.apachecn.org/organization/209.html \u66f4\u591a\u5b66 (zhuang) \u4e60 (bi) \u4ea4\u6d41\u7fa4\u8bf7\u53c2\u9605: http://www.apachecn.org/organization/348.html","title":"\u7ec4\u7ec7\u5b66\u4e60\u4ea4\u6d41\u7fa4"},{"location":"45/","text":"\u4ecb\u7ecd","title":"\u4ecb\u7ecd"},{"location":"45/#_1","text":"","title":"\u4ecb\u7ecd"},{"location":"46/","text":"\u81ea\u52a8\u6c42\u5bfc\u673a\u5236 \u672c\u6587\u5c06\u4ecb\u7ecd autograd (\u81ea\u52a8\u6c42\u5bfc) \u5982\u4f55\u5de5\u4f5c\u5e76\u8bb0\u5f55\u64cd\u4f5c. \u7406\u89e3\u8fd9\u4e00\u5207\u5e76\u4e0d\u662f\u5fc5\u987b\u7684, \u4f46\u6211\u4eec\u5efa\u8bae\u60a8\u719f\u6089\u5b83, \u56e0\u4e3a\u5b83\u4f1a\u5e2e\u52a9\u60a8\u7f16\u5199\u51fa\u66f4\u9ad8\u6548, \u66f4\u7b80\u6d01\u7684\u7a0b\u5e8f, \u5e76\u4e14\u53ef\u4ee5\u5e2e\u52a9\u60a8\u8fdb\u884c\u8c03\u8bd5. \u53cd\u5411\u6392\u9664 subgraphs (\u5b50\u56fe) \u6bcf\u4e00\u4e2a\u53d8\u91cf\u90fd\u6709\u4e24\u4e2a\u6807\u8bb0: requires_grad \u548c volatile . \u5b83\u4eec\u90fd\u5141\u8bb8\u4ece\u68af\u5ea6\u8ba1\u7b97\u4e2d\u7cbe\u7ec6\u5730\u6392\u9664 subgraphs (\u5b50\u56fe) , \u5e76\u4e14\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387. requires_grad \u5982\u679c\u6709\u4e00\u4e2a\u5355\u4e00\u7684\u8f93\u5165\u64cd\u4f5c\u9700\u8981\u68af\u5ea6, \u5219\u5176\u8f93\u51fa\u4e5f\u9700\u8981\u68af\u5ea6. \u76f8\u53cd, \u53ea\u6709\u5f53\u6240\u6709\u8f93\u5165\u90fd\u4e0d\u9700\u8981\u68af\u5ea6\u65f6, \u8f93\u51fa\u4e5f\u624d\u4e0d\u9700\u8981\u5b83. \u5f53\u6240\u6709\u7684\u53d8\u91cf\u90fd\u4e0d\u9700\u8981\u68af\u5ea6\u65f6, \u5219\u53cd\u5411\u8ba1\u7b97\u4e0d\u4f1a\u5728 subgraphs (\u5b50\u56fe) \u4e2d\u6267\u884c. >>> x = Variable(torch.randn(5, 5)) >>> y = Variable(torch.randn(5, 5)) >>> z = Variable(torch.randn(5, 5), requires_grad=True) >>> a = x + y >>> a.requires_grad False >>> b = a + z >>> b.requires_grad True \u5f53\u60a8\u60f3\u8981\u51bb\u7ed3\u6a21\u578b\u7684\u4e00\u90e8\u5206, \u6216\u8005\u60a8\u4e8b\u5148\u77e5\u9053\u4e0d\u4f1a\u4f7f\u7528\u67d0\u4e9b\u53c2\u6570\u7684\u68af\u5ea6\u65f6, \u8fd9\u4e2a\u6807\u8bb0\u662f\u7279\u522b\u6709\u7528\u7684. \u4f8b\u5982, \u5982\u679c\u8981\u5bf9\u9884\u5148\u8bad\u7ec3\u7684 CNN \u8fdb\u884c\u5fae\u4f18\u5316, \u53ea\u9700\u5728\u51bb\u7ed3\u6a21\u578b\u7684\u57fa\u7840\u4e0a\u5207\u6362 requires_grad \u6807\u8bb0\u5c31\u53ef\u4ee5\u4e86, \u76f4\u5230\u8ba1\u7b97\u5230\u6700\u540e\u4e00\u5c42\u65f6, \u624d\u4f1a\u4fdd\u5b58\u4e2d\u95f4\u7f13\u51b2\u533a, \u5176\u4e2d\u7684 affine transform (\u4eff\u5c04\u53d8\u6362) \u5c06\u4f7f\u7528\u9700\u8981\u68af\u5ea6\u7684\u6743\u91cd, \u5e76\u4e14\u7f51\u7edc\u7684\u8f93\u51fa\u4e5f\u5c06\u9700\u8981\u5b83\u4eec. model = torchvision.models.resnet18(pretrained=True) for param in model.parameters(): param.requires_grad = False # \u66ff\u6362\u6700\u540e\u4e00\u4e2a fully-connected layer (\u5168\u8fde\u63a5\u5c42) # \u65b0\u6784\u9020\u7684\u6a21\u5757\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u53c2\u6570\u9ed8\u8ba4 requires_grad=True model.fc = nn.Linear(512, 100) # \u4ec5\u7528\u4e8e\u5206\u7c7b\u5668\u7684\u4f18\u5316\u5668 optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9) volatile \u5982\u679c\u60a8\u786e\u5b9a\u4e0d\u4f1a\u8c03\u7528 <cite>.backward()</cite>, \u5219\u63a8\u8350\u5728\u7eaf\u7cb9\u7684 inference mode (\u63a8\u65ad\u6a21\u5f0f) \u4e2d\u4f7f\u7528 Volatile. \u5b83\u6bd4\u4efb\u4f55\u5176\u5b83\u7684 autograd (\u81ea\u52a8\u6c42\u5bfc) \u8bbe\u7f6e\u66f4\u9ad8\u6548 - \u5b83\u5c06\u4f7f\u7528\u7edd\u5bf9\u6700\u5c0f\u91cf\u7684\u5185\u5b58\u6765\u8bc4\u4f30\u6a21\u578b. volatile \u4e5f\u4f1a\u786e\u5b9a require_grad \u4e3a False . Volatile \u4e0d\u540c\u4e8e requires_grad \u7684\u6807\u8bb0\u4f20\u64ad\u65b9\u5f0f. \u5373\u4f7f\u4e00\u4e2a\u64cd\u4f5c\u53ea\u6709\u4e00\u4e2a\u5355\u4e00\u7684 volatile \u8f93\u5165, \u5b83\u7684\u8f93\u51fa\u4e5f\u5c06\u4f1a\u662f volatile \u8fd9\u6837\u7684. Volatility \u5728\u6574\u4e2a\u56fe\u4e2d\u6bd4 non-requiring gradient (\u4e0d\u9700\u8981\u68af\u5ea6) `` \u66f4\u5bb9\u6613\u4f20\u64ad - \u60a8\u53ea\u9700\u8981\u4e00\u4e2a **\u5355\u4e2a\u7684** ``volatile \u53f6\u5b50\u5373\u53ef\u5f97\u5230\u4e00\u4e2a volatile \u8f93\u51fa, \u76f8\u5bf9\u7684, \u60a8\u9700\u8981 \u6240\u6709\u7684 \u53f6\u5b50\u4ee5 \u4e0d\u9700\u8981\u68af\u5ea6 \u7684\u65b9\u5f0f, \u6765\u4ea7\u751f\u4e00\u4e2a \u4e0d\u9700\u8981\u68af\u5ea6 \u7684\u8f93\u51fa. \u4f7f\u7528 volatile \u6807\u8bb0, \u60a8\u4e0d\u9700\u8981\u66f4\u6539\u6a21\u578b\u53c2\u6570\u7684\u4efb\u4f55\u53c2\u6570, \u4ee5\u4fbf\u5c06\u5176\u7528\u4e8e inference (\u63a8\u65ad) . \u521b\u5efa\u4e00\u4e2a volatile \u8f93\u5165\u5df2\u7ecf\u8db3\u591f\u4e86, \u8fd9\u79cd\u65b9\u5f0f\u4e5f\u5c06\u786e\u4fdd\u6ca1\u6709\u4e2d\u95f4\u72b6\u6001\u88ab\u4fdd\u5b58. >>> regular_input = Variable(torch.randn(1, 3, 227, 227)) >>> volatile_input = Variable(torch.randn(1, 3, 227, 227), volatile=True) >>> model = torchvision.models.resnet18(pretrained=True) >>> model(regular_input).requires_grad True >>> model(volatile_input).requires_grad False >>> model(volatile_input).volatile True >>> model(volatile_input).grad_fn is None True autograd (\u81ea\u52a8\u6c42\u5bfc) \u5982\u4f55\u7f16\u7801 history (\u5386\u53f2\u4fe1\u606f) Autograd (\u81ea\u52a8\u6c42\u5bfc) \u662f\u4e00\u4e2a\u53cd\u5411\u81ea\u52a8\u5fae\u5206\u7684\u7cfb\u7edf. \u4ece\u6982\u5ff5\u4e0a\u6765\u8bf4, autograd (\u81ea\u52a8\u6c42\u5bfc) \u8bb0\u5f55\u4e00\u4e2a graph (\u56fe) , \u5b83\u8bb0\u5f55\u4e86\u5728\u6267\u884c\u64cd\u4f5c\u65f6\u521b\u5efa\u6570\u636e\u7684\u6240\u6709\u64cd\u4f5c, \u7136\u540e\u7ed9\u51fa\u4e00\u4e2a DAG (\u6709\u5411\u65e0\u73af\u56fe) , \u5176\u4e2d leaves (\u53f6\u5b50) \u662f\u8f93\u5165\u53d8\u91cf, roots (\u6839) \u662f\u8f93\u51fa\u53d8\u91cf. \u901a\u8fc7\u8ffd\u8e2a\u8fd9\u4e2a\u4ece roots (\u6839) \u5230 leaves (\u53f6\u5b50) \u7684 graph (\u56fe) , \u60a8\u53ef\u4ee5\u4f7f\u7528 chain rule (\u94fe\u5f0f\u89c4\u5219) \u6765\u81ea\u52a8\u8ba1\u7b97\u68af\u5ea6. \u5728\u5176\u5185\u90e8, autograd (\u81ea\u52a8\u6c42\u5bfc) \u5c06\u8fd9\u4e2a graph (\u56fe) \u5f62\u8c61\u7684\u8868\u793a\u4e3a Function \u5bf9\u8c61 (\u771f\u6b63\u7684\u8868\u8fbe\u5f0f) , \u53ef\u4ee5\u901a\u8fc7 apply() \u65b9\u6cd5\u6765\u8ba1\u7b97\u8bc4\u4f30 graph (\u56fe) \u7684\u7ed3\u679c. \u5f53\u8ba1\u7b97 forwards pass (\u524d\u5411\u4f20\u9012) \u65f6, autograd (\u81ea\u52a8\u6c42\u5bfc) \u540c\u65f6\u6267\u884c\u6240\u9700\u8981\u7684\u8ba1\u7b97, \u5e76\u4e14\u6784\u5efa\u4e00\u4e2a\u56fe\u4ee5\u8868\u793a\u8ba1\u7b97\u68af\u5ea6\u7684\u51fd\u6570 ( \u6bcf\u4e2a Variable \u7c7b\u7684 .grad_fn \u5c5e\u6027\u662f\u8be5 graph \u7684\u5165\u53e3\u70b9) . \u5f53 forwards pass (\u524d\u5411\u4f20\u9012) \u8ba1\u7b97\u5b8c\u6210\u65f6, \u6211\u4eec\u901a\u8fc7 backwards pass (\u65b9\u5411\u4f20\u9012) \u8bc4\u4f30\u8be5 graph (\u56fe) \u6765\u8ba1\u7b97\u68af\u5ea6. \u5f88\u91cd\u8981\u7684\u4e00\u70b9\u9700\u8981\u6ce8\u610f, \u5c31\u662f\u6bcf\u6b21\u8fed\u4ee3\u90fd\u4f1a\u91cd\u65b0\u521b\u5efa\u4e00\u4e2a graph (\u56fe) , \u8fd9\u6b63\u662f\u5141\u8bb8\u4f7f\u7528\u4efb\u610f Python \u63a7\u5236\u6d41\u8bed\u53e5 \u7684\u539f\u56e0, \u8fd9\u6837\u53ef\u4ee5\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u6539\u53d8 graph (\u56fe) \u7684\u6574\u4f53\u5f62\u72b6\u548c\u5927\u5c0f. \u5728\u5f00\u59cb\u8bad\u7ec3\u4e4b\u524d, \u60a8\u4e0d\u5fc5\u7f16\u7801\u6240\u6709\u53ef\u80fd\u7684\u8def\u5f84 - \u60a8\u8fd0\u884c\u7684\u5373\u662f\u60a8\u6240\u5fae\u5206\u7684. \u53d8\u91cf\u4e0a\u7684 In-place Operations (\u5c31\u5730\u64cd\u4f5c) \u5728 autograd (\u81ea\u52a8\u6c42\u5bfc) \u4e2d\u652f\u6301 In-place Operations (\u5c31\u5730\u64cd\u4f5c) \u662f\u4e00\u4ef6\u5f88\u96be\u7684\u4e8b\u60c5, \u6211\u4eec\u4e0d\u9f13\u52b1\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f7f\u7528\u5b83\u4eec. Autograd (\u81ea\u52a8\u6c42\u5bfc) \u4e3b\u52a8\u7684 \u7f13\u5b58\u533a\u91ca\u653e \u548c \u91cd\u7528 \u4f7f\u5176\u975e\u5e38\u9ad8\u6548, \u800c\u4e14 In-place Operations (\u5c31\u5730\u64cd\u4f5c) \u5b9e\u9645\u4e0a\u5f88\u5c11\u80fd\u964d\u4f4e\u5927\u91cf\u7684\u5185\u5b58\u4f7f\u7528. \u9664\u975e\u60a8\u5728\u5185\u5b58\u538b\u529b\u5f88\u5927\u7684\u60c5\u51b5\u4e0b\u64cd\u4f5c, \u5426\u5219\u60a8\u53ef\u80fd\u6c38\u8fdc\u4e0d\u9700\u8981\u4f7f\u7528\u5b83\u4eec. \u9650\u5236 In-place Operations (\u5c31\u5730\u64cd\u4f5c) \u9002\u7528\u6027\u7684\u4e3b\u8981\u539f\u56e0\u6709\u4e24\u4e2a: \u8986\u76d6\u68af\u5ea6\u8ba1\u7b97\u6240\u9700\u7684\u503c. \u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u53d8\u91cf\u4e0d\u652f\u6301 log_ \u7684\u539f\u56e0. \u5b83\u7684\u68af\u5ea6\u516c\u5f0f\u9700\u8981\u539f\u59cb\u8f93\u5165, \u867d\u7136\u53ef\u4ee5\u901a\u8fc7\u8ba1\u7b97\u53cd\u5411\u64cd\u4f5c\u53ef\u4ee5\u91cd\u65b0\u521b\u5efa\u5b83, \u4f46\u5b83\u5728\u6570\u503c\u4e0a\u662f\u4e0d\u7a33\u5b9a\u7684, \u5e76\u4e14\u9700\u8981\u989d\u5916\u7684\u5de5\u4f5c, \u8fd9\u5f80\u5f80\u4f1a\u4f7f\u8fd9\u4e9b\u529f\u80fd\u7684\u4f7f\u7528\u5f97\u4e0d\u507f\u5931. \u6bcf\u4e00\u4e2a in-place Operations (\u5c31\u5730\u64cd\u4f5c) \u5b9e\u9645\u4e0a\u90fd\u9700\u8981\u5b9e\u73b0\u91cd\u5199\u8ba1\u7b97\u56fe. Out-of-place (\u5f53\u524d\u901a\u7528\u7684) \u7684\u7248\u672c\u53ea\u662f\u7b80\u5355\u7684\u5206\u914d\u65b0\u7684\u5bf9\u8c61, \u5e76\u4fdd\u6301\u65e7\u56fe\u7684\u5f15\u7528, \u800c in-place Operations (\u5c31\u5730\u64cd\u4f5c) \u9700\u8981\u5c06\u6240\u6709\u8f93\u5165\u7684 creator \u66f4\u6539\u4e3a\u8868\u793a\u6b64\u64cd\u4f5c\u7684 Function . \u8fd9\u53ef\u80fd\u4f1a\u5f88\u68d8\u624b, \u7279\u522b\u662f\u5982\u679c\u6709\u8bb8\u591a\u53d8\u91cf\u5f15\u7528\u76f8\u540c\u7684\u5b58\u50a8 (\u4f8b\u5982\u901a\u8fc7\u7d22\u5f15\u6216\u8f6c\u7f6e\u521b\u5efa\u7684) , \u5e76\u4e14\u5982\u679c\u4fee\u6539\u4e86\u8f93\u5165\u7684\u5b58\u50a8\u88ab\u4efb\u4f55\u5176\u5b83\u7684 Variable (\u53d8\u91cf) \u6240\u5f15\u7528, \u5219 in-place Functions (\u5c31\u5730\u51fd\u6570) \u5b9e\u9645\u4e0a\u4f1a\u629b\u51fa\u9519\u8bef. In-place Operations (\u5c31\u5730\u64cd\u4f5c) \u7684\u6b63\u786e\u6027\u68c0\u67e5 \u6bcf\u4e00\u4e2a\u53d8\u91cf\u90fd\u4fdd\u7559\u6709\u4e00\u4e2a version counter (\u7248\u672c\u8ba1\u6570\u5668) , \u6bcf\u4e00\u6b21\u7684\u4efb\u4f55\u64cd\u4f5c\u88ab\u6807\u8bb0\u4e3a dirty \u65f6\u5019\u90fd\u4f1a\u8fdb\u884c\u9012\u589e. \u5f53\u4e00\u4e2a Function \u4fdd\u5b58\u4e86\u4efb\u4f55\u7528\u4e8e backward (\u65b9\u5411\u7684) tensor \u65f6, \u8fd8\u4f1a\u4fdd\u5b58\u5176\u5305\u542b\u53d8\u91cf\u7684 version counter (\u7248\u672c\u8ba1\u6570\u5668) . \u4e00\u65e6\u60a8\u8bbf\u95ee self.saved_tensors \u65f6\u5b83\u5c06\u88ab\u68c0\u67e5, \u5982\u679c\u5b83\u5927\u4e8e\u5df2\u4fdd\u5b58\u7684\u503c, \u5219\u4f1a\u5f15\u8d77\u9519\u8bef.","title":"\u81ea\u52a8\u6c42\u5bfc\u673a\u5236"},{"location":"46/#_1","text":"\u672c\u6587\u5c06\u4ecb\u7ecd autograd (\u81ea\u52a8\u6c42\u5bfc) \u5982\u4f55\u5de5\u4f5c\u5e76\u8bb0\u5f55\u64cd\u4f5c. \u7406\u89e3\u8fd9\u4e00\u5207\u5e76\u4e0d\u662f\u5fc5\u987b\u7684, \u4f46\u6211\u4eec\u5efa\u8bae\u60a8\u719f\u6089\u5b83, \u56e0\u4e3a\u5b83\u4f1a\u5e2e\u52a9\u60a8\u7f16\u5199\u51fa\u66f4\u9ad8\u6548, \u66f4\u7b80\u6d01\u7684\u7a0b\u5e8f, \u5e76\u4e14\u53ef\u4ee5\u5e2e\u52a9\u60a8\u8fdb\u884c\u8c03\u8bd5.","title":"\u81ea\u52a8\u6c42\u5bfc\u673a\u5236"},{"location":"46/#subgraphs","text":"\u6bcf\u4e00\u4e2a\u53d8\u91cf\u90fd\u6709\u4e24\u4e2a\u6807\u8bb0: requires_grad \u548c volatile . \u5b83\u4eec\u90fd\u5141\u8bb8\u4ece\u68af\u5ea6\u8ba1\u7b97\u4e2d\u7cbe\u7ec6\u5730\u6392\u9664 subgraphs (\u5b50\u56fe) , \u5e76\u4e14\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387.","title":"\u53cd\u5411\u6392\u9664 subgraphs (\u5b50\u56fe)"},{"location":"46/#requires_grad","text":"\u5982\u679c\u6709\u4e00\u4e2a\u5355\u4e00\u7684\u8f93\u5165\u64cd\u4f5c\u9700\u8981\u68af\u5ea6, \u5219\u5176\u8f93\u51fa\u4e5f\u9700\u8981\u68af\u5ea6. \u76f8\u53cd, \u53ea\u6709\u5f53\u6240\u6709\u8f93\u5165\u90fd\u4e0d\u9700\u8981\u68af\u5ea6\u65f6, \u8f93\u51fa\u4e5f\u624d\u4e0d\u9700\u8981\u5b83. \u5f53\u6240\u6709\u7684\u53d8\u91cf\u90fd\u4e0d\u9700\u8981\u68af\u5ea6\u65f6, \u5219\u53cd\u5411\u8ba1\u7b97\u4e0d\u4f1a\u5728 subgraphs (\u5b50\u56fe) \u4e2d\u6267\u884c. >>> x = Variable(torch.randn(5, 5)) >>> y = Variable(torch.randn(5, 5)) >>> z = Variable(torch.randn(5, 5), requires_grad=True) >>> a = x + y >>> a.requires_grad False >>> b = a + z >>> b.requires_grad True \u5f53\u60a8\u60f3\u8981\u51bb\u7ed3\u6a21\u578b\u7684\u4e00\u90e8\u5206, \u6216\u8005\u60a8\u4e8b\u5148\u77e5\u9053\u4e0d\u4f1a\u4f7f\u7528\u67d0\u4e9b\u53c2\u6570\u7684\u68af\u5ea6\u65f6, \u8fd9\u4e2a\u6807\u8bb0\u662f\u7279\u522b\u6709\u7528\u7684. \u4f8b\u5982, \u5982\u679c\u8981\u5bf9\u9884\u5148\u8bad\u7ec3\u7684 CNN \u8fdb\u884c\u5fae\u4f18\u5316, \u53ea\u9700\u5728\u51bb\u7ed3\u6a21\u578b\u7684\u57fa\u7840\u4e0a\u5207\u6362 requires_grad \u6807\u8bb0\u5c31\u53ef\u4ee5\u4e86, \u76f4\u5230\u8ba1\u7b97\u5230\u6700\u540e\u4e00\u5c42\u65f6, \u624d\u4f1a\u4fdd\u5b58\u4e2d\u95f4\u7f13\u51b2\u533a, \u5176\u4e2d\u7684 affine transform (\u4eff\u5c04\u53d8\u6362) \u5c06\u4f7f\u7528\u9700\u8981\u68af\u5ea6\u7684\u6743\u91cd, \u5e76\u4e14\u7f51\u7edc\u7684\u8f93\u51fa\u4e5f\u5c06\u9700\u8981\u5b83\u4eec. model = torchvision.models.resnet18(pretrained=True) for param in model.parameters(): param.requires_grad = False # \u66ff\u6362\u6700\u540e\u4e00\u4e2a fully-connected layer (\u5168\u8fde\u63a5\u5c42) # \u65b0\u6784\u9020\u7684\u6a21\u5757\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u53c2\u6570\u9ed8\u8ba4 requires_grad=True model.fc = nn.Linear(512, 100) # \u4ec5\u7528\u4e8e\u5206\u7c7b\u5668\u7684\u4f18\u5316\u5668 optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)","title":"requires_grad"},{"location":"46/#volatile","text":"\u5982\u679c\u60a8\u786e\u5b9a\u4e0d\u4f1a\u8c03\u7528 <cite>.backward()</cite>, \u5219\u63a8\u8350\u5728\u7eaf\u7cb9\u7684 inference mode (\u63a8\u65ad\u6a21\u5f0f) \u4e2d\u4f7f\u7528 Volatile. \u5b83\u6bd4\u4efb\u4f55\u5176\u5b83\u7684 autograd (\u81ea\u52a8\u6c42\u5bfc) \u8bbe\u7f6e\u66f4\u9ad8\u6548 - \u5b83\u5c06\u4f7f\u7528\u7edd\u5bf9\u6700\u5c0f\u91cf\u7684\u5185\u5b58\u6765\u8bc4\u4f30\u6a21\u578b. volatile \u4e5f\u4f1a\u786e\u5b9a require_grad \u4e3a False . Volatile \u4e0d\u540c\u4e8e requires_grad \u7684\u6807\u8bb0\u4f20\u64ad\u65b9\u5f0f. \u5373\u4f7f\u4e00\u4e2a\u64cd\u4f5c\u53ea\u6709\u4e00\u4e2a\u5355\u4e00\u7684 volatile \u8f93\u5165, \u5b83\u7684\u8f93\u51fa\u4e5f\u5c06\u4f1a\u662f volatile \u8fd9\u6837\u7684. Volatility \u5728\u6574\u4e2a\u56fe\u4e2d\u6bd4 non-requiring gradient (\u4e0d\u9700\u8981\u68af\u5ea6) `` \u66f4\u5bb9\u6613\u4f20\u64ad - \u60a8\u53ea\u9700\u8981\u4e00\u4e2a **\u5355\u4e2a\u7684** ``volatile \u53f6\u5b50\u5373\u53ef\u5f97\u5230\u4e00\u4e2a volatile \u8f93\u51fa, \u76f8\u5bf9\u7684, \u60a8\u9700\u8981 \u6240\u6709\u7684 \u53f6\u5b50\u4ee5 \u4e0d\u9700\u8981\u68af\u5ea6 \u7684\u65b9\u5f0f, \u6765\u4ea7\u751f\u4e00\u4e2a \u4e0d\u9700\u8981\u68af\u5ea6 \u7684\u8f93\u51fa. \u4f7f\u7528 volatile \u6807\u8bb0, \u60a8\u4e0d\u9700\u8981\u66f4\u6539\u6a21\u578b\u53c2\u6570\u7684\u4efb\u4f55\u53c2\u6570, \u4ee5\u4fbf\u5c06\u5176\u7528\u4e8e inference (\u63a8\u65ad) . \u521b\u5efa\u4e00\u4e2a volatile \u8f93\u5165\u5df2\u7ecf\u8db3\u591f\u4e86, \u8fd9\u79cd\u65b9\u5f0f\u4e5f\u5c06\u786e\u4fdd\u6ca1\u6709\u4e2d\u95f4\u72b6\u6001\u88ab\u4fdd\u5b58. >>> regular_input = Variable(torch.randn(1, 3, 227, 227)) >>> volatile_input = Variable(torch.randn(1, 3, 227, 227), volatile=True) >>> model = torchvision.models.resnet18(pretrained=True) >>> model(regular_input).requires_grad True >>> model(volatile_input).requires_grad False >>> model(volatile_input).volatile True >>> model(volatile_input).grad_fn is None True","title":"volatile"},{"location":"46/#autograd-history","text":"Autograd (\u81ea\u52a8\u6c42\u5bfc) \u662f\u4e00\u4e2a\u53cd\u5411\u81ea\u52a8\u5fae\u5206\u7684\u7cfb\u7edf. \u4ece\u6982\u5ff5\u4e0a\u6765\u8bf4, autograd (\u81ea\u52a8\u6c42\u5bfc) \u8bb0\u5f55\u4e00\u4e2a graph (\u56fe) , \u5b83\u8bb0\u5f55\u4e86\u5728\u6267\u884c\u64cd\u4f5c\u65f6\u521b\u5efa\u6570\u636e\u7684\u6240\u6709\u64cd\u4f5c, \u7136\u540e\u7ed9\u51fa\u4e00\u4e2a DAG (\u6709\u5411\u65e0\u73af\u56fe) , \u5176\u4e2d leaves (\u53f6\u5b50) \u662f\u8f93\u5165\u53d8\u91cf, roots (\u6839) \u662f\u8f93\u51fa\u53d8\u91cf. \u901a\u8fc7\u8ffd\u8e2a\u8fd9\u4e2a\u4ece roots (\u6839) \u5230 leaves (\u53f6\u5b50) \u7684 graph (\u56fe) , \u60a8\u53ef\u4ee5\u4f7f\u7528 chain rule (\u94fe\u5f0f\u89c4\u5219) \u6765\u81ea\u52a8\u8ba1\u7b97\u68af\u5ea6. \u5728\u5176\u5185\u90e8, autograd (\u81ea\u52a8\u6c42\u5bfc) \u5c06\u8fd9\u4e2a graph (\u56fe) \u5f62\u8c61\u7684\u8868\u793a\u4e3a Function \u5bf9\u8c61 (\u771f\u6b63\u7684\u8868\u8fbe\u5f0f) , \u53ef\u4ee5\u901a\u8fc7 apply() \u65b9\u6cd5\u6765\u8ba1\u7b97\u8bc4\u4f30 graph (\u56fe) \u7684\u7ed3\u679c. \u5f53\u8ba1\u7b97 forwards pass (\u524d\u5411\u4f20\u9012) \u65f6, autograd (\u81ea\u52a8\u6c42\u5bfc) \u540c\u65f6\u6267\u884c\u6240\u9700\u8981\u7684\u8ba1\u7b97, \u5e76\u4e14\u6784\u5efa\u4e00\u4e2a\u56fe\u4ee5\u8868\u793a\u8ba1\u7b97\u68af\u5ea6\u7684\u51fd\u6570 ( \u6bcf\u4e2a Variable \u7c7b\u7684 .grad_fn \u5c5e\u6027\u662f\u8be5 graph \u7684\u5165\u53e3\u70b9) . \u5f53 forwards pass (\u524d\u5411\u4f20\u9012) \u8ba1\u7b97\u5b8c\u6210\u65f6, \u6211\u4eec\u901a\u8fc7 backwards pass (\u65b9\u5411\u4f20\u9012) \u8bc4\u4f30\u8be5 graph (\u56fe) \u6765\u8ba1\u7b97\u68af\u5ea6. \u5f88\u91cd\u8981\u7684\u4e00\u70b9\u9700\u8981\u6ce8\u610f, \u5c31\u662f\u6bcf\u6b21\u8fed\u4ee3\u90fd\u4f1a\u91cd\u65b0\u521b\u5efa\u4e00\u4e2a graph (\u56fe) , \u8fd9\u6b63\u662f\u5141\u8bb8\u4f7f\u7528\u4efb\u610f Python \u63a7\u5236\u6d41\u8bed\u53e5 \u7684\u539f\u56e0, \u8fd9\u6837\u53ef\u4ee5\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u6539\u53d8 graph (\u56fe) \u7684\u6574\u4f53\u5f62\u72b6\u548c\u5927\u5c0f. \u5728\u5f00\u59cb\u8bad\u7ec3\u4e4b\u524d, \u60a8\u4e0d\u5fc5\u7f16\u7801\u6240\u6709\u53ef\u80fd\u7684\u8def\u5f84 - \u60a8\u8fd0\u884c\u7684\u5373\u662f\u60a8\u6240\u5fae\u5206\u7684.","title":"autograd (\u81ea\u52a8\u6c42\u5bfc) \u5982\u4f55\u7f16\u7801 history (\u5386\u53f2\u4fe1\u606f)"},{"location":"46/#in-place-operations","text":"\u5728 autograd (\u81ea\u52a8\u6c42\u5bfc) \u4e2d\u652f\u6301 In-place Operations (\u5c31\u5730\u64cd\u4f5c) \u662f\u4e00\u4ef6\u5f88\u96be\u7684\u4e8b\u60c5, \u6211\u4eec\u4e0d\u9f13\u52b1\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f7f\u7528\u5b83\u4eec. Autograd (\u81ea\u52a8\u6c42\u5bfc) \u4e3b\u52a8\u7684 \u7f13\u5b58\u533a\u91ca\u653e \u548c \u91cd\u7528 \u4f7f\u5176\u975e\u5e38\u9ad8\u6548, \u800c\u4e14 In-place Operations (\u5c31\u5730\u64cd\u4f5c) \u5b9e\u9645\u4e0a\u5f88\u5c11\u80fd\u964d\u4f4e\u5927\u91cf\u7684\u5185\u5b58\u4f7f\u7528. \u9664\u975e\u60a8\u5728\u5185\u5b58\u538b\u529b\u5f88\u5927\u7684\u60c5\u51b5\u4e0b\u64cd\u4f5c, \u5426\u5219\u60a8\u53ef\u80fd\u6c38\u8fdc\u4e0d\u9700\u8981\u4f7f\u7528\u5b83\u4eec. \u9650\u5236 In-place Operations (\u5c31\u5730\u64cd\u4f5c) \u9002\u7528\u6027\u7684\u4e3b\u8981\u539f\u56e0\u6709\u4e24\u4e2a: \u8986\u76d6\u68af\u5ea6\u8ba1\u7b97\u6240\u9700\u7684\u503c. \u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u53d8\u91cf\u4e0d\u652f\u6301 log_ \u7684\u539f\u56e0. \u5b83\u7684\u68af\u5ea6\u516c\u5f0f\u9700\u8981\u539f\u59cb\u8f93\u5165, \u867d\u7136\u53ef\u4ee5\u901a\u8fc7\u8ba1\u7b97\u53cd\u5411\u64cd\u4f5c\u53ef\u4ee5\u91cd\u65b0\u521b\u5efa\u5b83, \u4f46\u5b83\u5728\u6570\u503c\u4e0a\u662f\u4e0d\u7a33\u5b9a\u7684, \u5e76\u4e14\u9700\u8981\u989d\u5916\u7684\u5de5\u4f5c, \u8fd9\u5f80\u5f80\u4f1a\u4f7f\u8fd9\u4e9b\u529f\u80fd\u7684\u4f7f\u7528\u5f97\u4e0d\u507f\u5931. \u6bcf\u4e00\u4e2a in-place Operations (\u5c31\u5730\u64cd\u4f5c) \u5b9e\u9645\u4e0a\u90fd\u9700\u8981\u5b9e\u73b0\u91cd\u5199\u8ba1\u7b97\u56fe. Out-of-place (\u5f53\u524d\u901a\u7528\u7684) \u7684\u7248\u672c\u53ea\u662f\u7b80\u5355\u7684\u5206\u914d\u65b0\u7684\u5bf9\u8c61, \u5e76\u4fdd\u6301\u65e7\u56fe\u7684\u5f15\u7528, \u800c in-place Operations (\u5c31\u5730\u64cd\u4f5c) \u9700\u8981\u5c06\u6240\u6709\u8f93\u5165\u7684 creator \u66f4\u6539\u4e3a\u8868\u793a\u6b64\u64cd\u4f5c\u7684 Function . \u8fd9\u53ef\u80fd\u4f1a\u5f88\u68d8\u624b, \u7279\u522b\u662f\u5982\u679c\u6709\u8bb8\u591a\u53d8\u91cf\u5f15\u7528\u76f8\u540c\u7684\u5b58\u50a8 (\u4f8b\u5982\u901a\u8fc7\u7d22\u5f15\u6216\u8f6c\u7f6e\u521b\u5efa\u7684) , \u5e76\u4e14\u5982\u679c\u4fee\u6539\u4e86\u8f93\u5165\u7684\u5b58\u50a8\u88ab\u4efb\u4f55\u5176\u5b83\u7684 Variable (\u53d8\u91cf) \u6240\u5f15\u7528, \u5219 in-place Functions (\u5c31\u5730\u51fd\u6570) \u5b9e\u9645\u4e0a\u4f1a\u629b\u51fa\u9519\u8bef.","title":"\u53d8\u91cf\u4e0a\u7684 In-place Operations (\u5c31\u5730\u64cd\u4f5c)"},{"location":"46/#in-place-operations_1","text":"\u6bcf\u4e00\u4e2a\u53d8\u91cf\u90fd\u4fdd\u7559\u6709\u4e00\u4e2a version counter (\u7248\u672c\u8ba1\u6570\u5668) , \u6bcf\u4e00\u6b21\u7684\u4efb\u4f55\u64cd\u4f5c\u88ab\u6807\u8bb0\u4e3a dirty \u65f6\u5019\u90fd\u4f1a\u8fdb\u884c\u9012\u589e. \u5f53\u4e00\u4e2a Function \u4fdd\u5b58\u4e86\u4efb\u4f55\u7528\u4e8e backward (\u65b9\u5411\u7684) tensor \u65f6, \u8fd8\u4f1a\u4fdd\u5b58\u5176\u5305\u542b\u53d8\u91cf\u7684 version counter (\u7248\u672c\u8ba1\u6570\u5668) . \u4e00\u65e6\u60a8\u8bbf\u95ee self.saved_tensors \u65f6\u5b83\u5c06\u88ab\u68c0\u67e5, \u5982\u679c\u5b83\u5927\u4e8e\u5df2\u4fdd\u5b58\u7684\u503c, \u5219\u4f1a\u5f15\u8d77\u9519\u8bef.","title":"In-place Operations (\u5c31\u5730\u64cd\u4f5c) \u7684\u6b63\u786e\u6027\u68c0\u67e5"},{"location":"47/","text":"\u5e7f\u64ad\u8bed\u4e49 \u4e00\u4e9b PyTorch \u7684\u64cd\u4f5c\u652f\u6301\u57fa\u4e8e NumPy Broadcasting Semantics . \u7b80\u800c\u8a00\u4e4b, \u5982\u679c\u4e00\u4e2a PyTorch \u64cd\u4f5c\u652f\u6301\u5e7f\u64ad\u8bed\u4e49, \u90a3\u4e48\u5b83\u7684\u5f20\u91cf\u53c2\u6570\u53ef\u4ee5\u81ea\u52a8\u6269\u5c55\u4e3a\u76f8\u540c\u7684\u5927\u5c0f (\u4e0d\u9700\u8981\u590d\u5236\u6570\u636e) \u4e00\u822c\u8bed\u4e49 \u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u6ee1\u8db3\u5982\u4e0b\u89c4\u5219, \u90a3\u4e48\u5c31\u8ba4\u4e3a\u5176\u662f <cite>broadcastable</cite> : \u6bcf\u4e2a\u5f20\u91cf\u81f3\u5c11\u5b58\u5728\u7ef4\u5ea6. \u5728\u904d\u5386\u7ef4\u5ea6\u5927\u5c0f\u65f6, \u4ece\u5c3e\u90e8\u7ef4\u5ea6\u5f00\u59cb\u904d\u5386, \u5e76\u4e14\u4e8c\u8005\u7ef4\u5ea6\u5fc5\u987b\u76f8\u7b49, \u5b83\u4eec\u5176\u4e2d\u4e00\u4e2a\u8981\u4e48\u662f1\u8981\u4e48\u4e0d\u5b58\u5728. \u793a\u4f8b: >>> x=torch.FloatTensor(5,7,3) >>> y=torch.FloatTensor(5,7,3) # \u76f8\u540c\u7684\u5f62\u72b6\u603b\u662f\u6ee1\u8db3\u7684(\u4e0a\u8ff0\u89c4\u5219\u603b\u662f\u6210\u7acb\u7684) >>> x=torch.FloatTensor() >>> y=torch.FloatTensor(2,2) # x\u548cy\u4e0d\u662f\u6ee1\u8db3\u5e7f\u64ad\u8bed\u4e49\u7684,\u56e0\u4e3ax\u8981\u6c42\u81f3\u5c11\u4e3a1\u7ef4. # \u53ef\u4ee5\u6392\u5217\u5c3e\u90e8\u7ef4\u5ea6 >>> x=torch.FloatTensor(5,3,4,1) >>> y=torch.FloatTensor( 3,1,1) # x\u548cy\u662f\u6ee1\u8db3\u5e7f\u64ad\u8bed\u4e49\u7684. # \u5c3e\u5217\u7b2c\u4e00\u7ef4 : \u90fd\u5305\u542b1. # \u5c3e\u5217\u7b2c\u4e8c\u7ef4 : y\u7684\u7ef4\u5ea6\u503c\u4e3a1. # \u5c3e\u5217\u7b2c\u4e09\u7ef4 : x size == y size. # \u5c3e\u5217\u7b2c\u56db\u7ef4 : y\u7ef4\u5ea6\u4e0d\u5b58\u5728\u5c3e\u5217\u7b2c\u56db\u7ef4. # \u4f46\u662f: >>> x=torch.FloatTensor(5,2,4,1) >>> y=torch.FloatTensor( 3,1,1) # x \u548c y \u662f\u4e0d\u6ee1\u8db3\u5e7f\u64ad\u8bed\u4e49\u7684, \u56e0\u4e3a\u5c3e\u5217\u7b2c\u4e09\u7ef4\u4e2d 2 != 3 . \u5982\u679c\u4e24\u4e2a\u5f20\u91cf x , y \u662f <cite>broadcastable</cite>, \u5219\u7ed3\u679c\u5f20\u91cf\u7684\u5927\u5c0f\u7531\u5982\u4e0b\u65b9\u5f0f\u8ba1\u7b97: - \u5982\u679c\u7ef4\u5ea6\u7684\u6570\u91cf x \u548c y \u4e0d\u76f8\u7b49, \u5728\u7ef4\u5ea6\u8f83\u5c11\u7684\u5f20\u91cf\u7684\u7ef4\u5ea6\u524d\u7f6e 1 - \u7136\u540e, \u5bf9\u4e8e\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f, \u751f\u6210\u7ef4\u5ea6\u7684\u5927\u5c0f\u662f attr:<cite>x</cite> \u548c y \u7684\u6700\u5927\u503c \u793a\u4f8b # \u53ef\u4ee5\u6392\u5217\u5c3e\u90e8\u7ef4\u5ea6, \u4f7f\u9605\u8bfb\u66f4\u5bb9\u6613 >>> x=torch.FloatTensor(5,1,4,1) >>> y=torch.FloatTensor( 3,1,1) >>> (x+y).size() torch.Size([5, 3, 4, 1]) # \u4f46\u662f\u4e5f\u53ef\u4e0d\u5fc5\u6392\u5217 >>> x=torch.FloatTensor(1) >>> y=torch.FloatTensor(3,1,7) >>> (x+y).size() torch.Size([3, 1, 7]) >>> x=torch.FloatTensor(5,2,4,1) >>> y=torch.FloatTensor(3,1,1) >>> (x+y).size() RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1 \u76f4\u63a5\u8bed\u4e49 (In-place semantics) \u76f4\u63a5 (\u5c31\u5730) \u64cd\u4f5c (in-place operations) \u7684\u4e00\u4e2a\u590d\u6742\u95ee\u9898\u5c31\u662f\u4e0d\u80fd\u50cf\u5e7f\u64ad\u90a3\u6837\u76f4\u63a5\u64cd\u4f5c\u4e24\u4e2a\u5f20\u91cf\u4f7f\u5176\u6539\u53d8\u7ef4\u5ea6\u6ee1\u8db3\u6761\u4ef6 \u793a\u4f8b >>> x=torch.FloatTensor(5,3,4,1) >>> y=torch.FloatTensor(3,1,1) >>> (x.add_(y)).size() torch.Size([5, 3, 4, 1]) # but: >>> x=torch.FloatTensor(1,3,1) >>> y=torch.FloatTensor(3,1,7) >>> (x.add_(y)).size() RuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2. \u5411\u540e\u517c\u5bb9 \u4ee5\u524d\u7248\u672c\u7684 PyTorch \u53ea\u8981\u5f20\u91cf\u4e2d\u7684\u5143\u7d20\u6570\u76ee\u662f\u76f8\u7b49\u7684, \u4fbf\u5141\u8bb8\u67d0\u4e9b\u70b9\u72b6\u51fd\u6570\u5728\u4e0d\u540c\u7684\u5f62\u72b6\u7684\u5f20\u91cf\u4e0a\u6267\u884c, \u5176\u4e2d\u70b9\u72b6\u64cd\u4f5c\u662f\u901a\u8fc7\u5c06\u6bcf\u4e2a\u5f20\u91cf\u89c6\u4e3a 1 \u7ef4\u6267\u884c \u73b0\u4eca PyTorch \u652f\u6301\u5e7f\u64ad\u8bed\u4e49\u548c\u4e0d\u63a8\u8350\u4f7f\u7528\u70b9\u72b6\u51fd\u6570\u64cd\u4f5c\u5411\u91cf, \u5e76\u4e14\u5c06\u5728\u5177\u6709\u76f8\u540c\u6570\u91cf\u7684\u5143\u7d20\u4f46\u4e0d\u652f\u6301\u5e7f\u64ad\u8bed\u4e49\u7684\u5f20\u91cf\u64cd\u4f5c\u751f\u6210\u4e00\u4e2a Python \u8b66\u544a \u6ce8\u610f, \u5e7f\u64ad\u8bed\u4e49\u7684\u5f15\u5165\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5411\u540e\u4e0d\u517c\u5bb9\u7684\u60c5\u51b5, \u5373\u4e24\u4e2a\u5f20\u91cf\u5f62\u72b6\u4e0d\u540c, \u4f46\u662f\u6570\u91cf\u76f8\u540c\u4e14\u652f\u6301\u5e7f\u64ad\u8bed\u4e49. \u793a\u4f8b >>> torch.add(torch.ones(4,1), torch.randn(4)) \u672c\u9884\u751f\u6210\u4e00\u4e2a: torch.Size([4,1]) \u7684\u5f20\u91cf,\u4f46\u662f\u73b0\u5728\u4f1a\u751f\u6210\u4e00\u4e2a: torch.Size([4,4]) \u7684\u5f20\u91cf. \u4e3a\u4e86\u5e2e\u52a9\u4f7f\u7528\u8005\u8bc6\u522b\u4ee3\u7801\u4e2d\u53ef\u80fd\u5b58\u5728\u7531\u5f15\u5165\u5e7f\u64ad\u8bed\u4e49\u7684\u5411\u540e\u4e0d\u517c\u5bb9\u60c5\u51b5, \u4f60\u53ef\u4ee5\u5c06 <cite>torch.utils.backcompat.broadcast_warning.enabled</cite> \u8bbe\u7f6e\u4e3a <cite>True</cite>, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u4f1a\u751f\u6210\u4e00\u4e2a Python \u8b66\u544a \u793a\u4f8b >>> torch.utils.backcompat.broadcast_warning.enabled=True >>> torch.add(torch.ones(4,1), torch.ones(4)) __main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements. Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.","title":"\u5e7f\u64ad\u8bed\u4e49"},{"location":"47/#_1","text":"\u4e00\u4e9b PyTorch \u7684\u64cd\u4f5c\u652f\u6301\u57fa\u4e8e NumPy Broadcasting Semantics . \u7b80\u800c\u8a00\u4e4b, \u5982\u679c\u4e00\u4e2a PyTorch \u64cd\u4f5c\u652f\u6301\u5e7f\u64ad\u8bed\u4e49, \u90a3\u4e48\u5b83\u7684\u5f20\u91cf\u53c2\u6570\u53ef\u4ee5\u81ea\u52a8\u6269\u5c55\u4e3a\u76f8\u540c\u7684\u5927\u5c0f (\u4e0d\u9700\u8981\u590d\u5236\u6570\u636e)","title":"\u5e7f\u64ad\u8bed\u4e49"},{"location":"47/#_2","text":"\u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u6ee1\u8db3\u5982\u4e0b\u89c4\u5219, \u90a3\u4e48\u5c31\u8ba4\u4e3a\u5176\u662f <cite>broadcastable</cite> : \u6bcf\u4e2a\u5f20\u91cf\u81f3\u5c11\u5b58\u5728\u7ef4\u5ea6. \u5728\u904d\u5386\u7ef4\u5ea6\u5927\u5c0f\u65f6, \u4ece\u5c3e\u90e8\u7ef4\u5ea6\u5f00\u59cb\u904d\u5386, \u5e76\u4e14\u4e8c\u8005\u7ef4\u5ea6\u5fc5\u987b\u76f8\u7b49, \u5b83\u4eec\u5176\u4e2d\u4e00\u4e2a\u8981\u4e48\u662f1\u8981\u4e48\u4e0d\u5b58\u5728. \u793a\u4f8b: >>> x=torch.FloatTensor(5,7,3) >>> y=torch.FloatTensor(5,7,3) # \u76f8\u540c\u7684\u5f62\u72b6\u603b\u662f\u6ee1\u8db3\u7684(\u4e0a\u8ff0\u89c4\u5219\u603b\u662f\u6210\u7acb\u7684) >>> x=torch.FloatTensor() >>> y=torch.FloatTensor(2,2) # x\u548cy\u4e0d\u662f\u6ee1\u8db3\u5e7f\u64ad\u8bed\u4e49\u7684,\u56e0\u4e3ax\u8981\u6c42\u81f3\u5c11\u4e3a1\u7ef4. # \u53ef\u4ee5\u6392\u5217\u5c3e\u90e8\u7ef4\u5ea6 >>> x=torch.FloatTensor(5,3,4,1) >>> y=torch.FloatTensor( 3,1,1) # x\u548cy\u662f\u6ee1\u8db3\u5e7f\u64ad\u8bed\u4e49\u7684. # \u5c3e\u5217\u7b2c\u4e00\u7ef4 : \u90fd\u5305\u542b1. # \u5c3e\u5217\u7b2c\u4e8c\u7ef4 : y\u7684\u7ef4\u5ea6\u503c\u4e3a1. # \u5c3e\u5217\u7b2c\u4e09\u7ef4 : x size == y size. # \u5c3e\u5217\u7b2c\u56db\u7ef4 : y\u7ef4\u5ea6\u4e0d\u5b58\u5728\u5c3e\u5217\u7b2c\u56db\u7ef4. # \u4f46\u662f: >>> x=torch.FloatTensor(5,2,4,1) >>> y=torch.FloatTensor( 3,1,1) # x \u548c y \u662f\u4e0d\u6ee1\u8db3\u5e7f\u64ad\u8bed\u4e49\u7684, \u56e0\u4e3a\u5c3e\u5217\u7b2c\u4e09\u7ef4\u4e2d 2 != 3 . \u5982\u679c\u4e24\u4e2a\u5f20\u91cf x , y \u662f <cite>broadcastable</cite>, \u5219\u7ed3\u679c\u5f20\u91cf\u7684\u5927\u5c0f\u7531\u5982\u4e0b\u65b9\u5f0f\u8ba1\u7b97: - \u5982\u679c\u7ef4\u5ea6\u7684\u6570\u91cf x \u548c y \u4e0d\u76f8\u7b49, \u5728\u7ef4\u5ea6\u8f83\u5c11\u7684\u5f20\u91cf\u7684\u7ef4\u5ea6\u524d\u7f6e 1 - \u7136\u540e, \u5bf9\u4e8e\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f, \u751f\u6210\u7ef4\u5ea6\u7684\u5927\u5c0f\u662f attr:<cite>x</cite> \u548c y \u7684\u6700\u5927\u503c \u793a\u4f8b # \u53ef\u4ee5\u6392\u5217\u5c3e\u90e8\u7ef4\u5ea6, \u4f7f\u9605\u8bfb\u66f4\u5bb9\u6613 >>> x=torch.FloatTensor(5,1,4,1) >>> y=torch.FloatTensor( 3,1,1) >>> (x+y).size() torch.Size([5, 3, 4, 1]) # \u4f46\u662f\u4e5f\u53ef\u4e0d\u5fc5\u6392\u5217 >>> x=torch.FloatTensor(1) >>> y=torch.FloatTensor(3,1,7) >>> (x+y).size() torch.Size([3, 1, 7]) >>> x=torch.FloatTensor(5,2,4,1) >>> y=torch.FloatTensor(3,1,1) >>> (x+y).size() RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1","title":"\u4e00\u822c\u8bed\u4e49"},{"location":"47/#in-place-semantics","text":"\u76f4\u63a5 (\u5c31\u5730) \u64cd\u4f5c (in-place operations) \u7684\u4e00\u4e2a\u590d\u6742\u95ee\u9898\u5c31\u662f\u4e0d\u80fd\u50cf\u5e7f\u64ad\u90a3\u6837\u76f4\u63a5\u64cd\u4f5c\u4e24\u4e2a\u5f20\u91cf\u4f7f\u5176\u6539\u53d8\u7ef4\u5ea6\u6ee1\u8db3\u6761\u4ef6 \u793a\u4f8b >>> x=torch.FloatTensor(5,3,4,1) >>> y=torch.FloatTensor(3,1,1) >>> (x.add_(y)).size() torch.Size([5, 3, 4, 1]) # but: >>> x=torch.FloatTensor(1,3,1) >>> y=torch.FloatTensor(3,1,7) >>> (x.add_(y)).size() RuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.","title":"\u76f4\u63a5\u8bed\u4e49 (In-place semantics)"},{"location":"47/#_3","text":"\u4ee5\u524d\u7248\u672c\u7684 PyTorch \u53ea\u8981\u5f20\u91cf\u4e2d\u7684\u5143\u7d20\u6570\u76ee\u662f\u76f8\u7b49\u7684, \u4fbf\u5141\u8bb8\u67d0\u4e9b\u70b9\u72b6\u51fd\u6570\u5728\u4e0d\u540c\u7684\u5f62\u72b6\u7684\u5f20\u91cf\u4e0a\u6267\u884c, \u5176\u4e2d\u70b9\u72b6\u64cd\u4f5c\u662f\u901a\u8fc7\u5c06\u6bcf\u4e2a\u5f20\u91cf\u89c6\u4e3a 1 \u7ef4\u6267\u884c \u73b0\u4eca PyTorch \u652f\u6301\u5e7f\u64ad\u8bed\u4e49\u548c\u4e0d\u63a8\u8350\u4f7f\u7528\u70b9\u72b6\u51fd\u6570\u64cd\u4f5c\u5411\u91cf, \u5e76\u4e14\u5c06\u5728\u5177\u6709\u76f8\u540c\u6570\u91cf\u7684\u5143\u7d20\u4f46\u4e0d\u652f\u6301\u5e7f\u64ad\u8bed\u4e49\u7684\u5f20\u91cf\u64cd\u4f5c\u751f\u6210\u4e00\u4e2a Python \u8b66\u544a \u6ce8\u610f, \u5e7f\u64ad\u8bed\u4e49\u7684\u5f15\u5165\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5411\u540e\u4e0d\u517c\u5bb9\u7684\u60c5\u51b5, \u5373\u4e24\u4e2a\u5f20\u91cf\u5f62\u72b6\u4e0d\u540c, \u4f46\u662f\u6570\u91cf\u76f8\u540c\u4e14\u652f\u6301\u5e7f\u64ad\u8bed\u4e49. \u793a\u4f8b >>> torch.add(torch.ones(4,1), torch.randn(4)) \u672c\u9884\u751f\u6210\u4e00\u4e2a: torch.Size([4,1]) \u7684\u5f20\u91cf,\u4f46\u662f\u73b0\u5728\u4f1a\u751f\u6210\u4e00\u4e2a: torch.Size([4,4]) \u7684\u5f20\u91cf. \u4e3a\u4e86\u5e2e\u52a9\u4f7f\u7528\u8005\u8bc6\u522b\u4ee3\u7801\u4e2d\u53ef\u80fd\u5b58\u5728\u7531\u5f15\u5165\u5e7f\u64ad\u8bed\u4e49\u7684\u5411\u540e\u4e0d\u517c\u5bb9\u60c5\u51b5, \u4f60\u53ef\u4ee5\u5c06 <cite>torch.utils.backcompat.broadcast_warning.enabled</cite> \u8bbe\u7f6e\u4e3a <cite>True</cite>, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u4f1a\u751f\u6210\u4e00\u4e2a Python \u8b66\u544a \u793a\u4f8b >>> torch.utils.backcompat.broadcast_warning.enabled=True >>> torch.add(torch.ones(4,1), torch.ones(4)) __main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements. Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.","title":"\u5411\u540e\u517c\u5bb9"},{"location":"48/","text":"CUDA \u8bed\u4e49 torch.cuda \u88ab\u7528\u4e8e\u8bbe\u7f6e\u548c\u8fd0\u884c CUDA \u64cd\u4f5c. \u5b83\u4f1a\u8bb0\u5f55\u5f53\u524d\u9009\u62e9\u7684 GPU, \u5e76\u4e14\u5206\u914d\u7684\u6240\u6709 CUDA \u5f20\u91cf\u5c06\u9ed8\u8ba4\u5728\u4e0a\u9762\u521b\u5efa. \u53ef\u4ee5\u4f7f\u7528 torch.cuda.device \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u66f4\u6539\u6240\u9009\u8bbe\u5907. \u4f46\u662f, \u4e00\u65e6\u5f20\u91cf\u88ab\u5206\u914d, \u60a8\u53ef\u4ee5\u76f4\u63a5\u5bf9\u5176\u8fdb\u884c\u64cd\u4f5c, \u800c\u4e0d\u9700\u8981\u8003\u8651\u5df2\u9009\u62e9\u7684\u8bbe\u5907, \u7ed3\u679c\u5c06\u59cb\u7ec8\u653e\u5728\u4e0e\u5f20\u91cf\u76f8\u5173\u7684\u8bbe\u5907\u4e0a. \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u4e0d\u652f\u6301\u8de8 GPU \u64cd\u4f5c, \u552f\u4e00\u7684\u4f8b\u5916\u662f copy_() . \u9664\u975e\u542f\u7528\u5bf9\u7b49\u5b58\u50a8\u5668\u8bbf\u95ee, \u5426\u5219\u5bf9\u5206\u5e03\u5728\u4e0d\u540c\u8bbe\u5907\u4e0a\u7684\u5f20\u91cf\u5c1d\u8bd5\u8fdb\u884c\u4efb\u4f55\u542f\u52a8\u64cd\u4f5c\u90fd\u5c06\u5f15\u53d1\u9519\u8bef. \u4e0b\u9762\u6211\u4eec\u7528\u4e00\u4e2a\u5c0f\u4f8b\u5b50\u6765\u5c55\u793a: x = torch.cuda.FloatTensor(1) # x.get_device() == 0 y = torch.FloatTensor(1).cuda() # y.get_device() == 0 with torch.cuda.device(1): # allocates a tensor on GPU 1 a = torch.cuda.FloatTensor(1) # transfers a tensor from CPU to GPU 1 b = torch.FloatTensor(1).cuda() # a.get_device() == b.get_device() == 1 c = a + b # c.get_device() == 1 z = x + y # z.get_device() == 0 # \u5373\u4f7f\u5728\u4e0a\u4e0b\u6587\u91cc\u9762, \u4f60\u4e5f\u53ef\u4ee5\u5728 .cuda \u7684\u53c2\u6570\u4e2d\u4f20\u5165\u8bbe\u5907id d = torch.randn(2).cuda(2) # d.get_device() == 2 \u5185\u5b58\u7ba1\u7406 PyTorch \u4f7f\u7528\u7f13\u5b58\u5185\u5b58\u5206\u914d\u5668\u6765\u52a0\u901f\u5185\u5b58\u5206\u914d. \u8fd9\u5141\u8bb8\u5728\u6ca1\u6709\u8bbe\u5907\u540c\u6b65\u7684\u60c5\u51b5\u4e0b\u5feb\u901f\u91ca\u653e\u5185\u5b58. \u4f46\u662f, \u7531\u5206\u914d\u5668\u7ba1\u7406\u7684\u672a\u4f7f\u7528\u7684\u5185\u5b58\u4ecd\u5c06\u663e\u793a\u4e3a\u5728 <cite>nvidia-smi</cite> \u4e2d\u4f7f\u7528. \u8c03\u7528 empty_cache() \u53ef\u4ee5\u4ece PyTorch \u4e2d\u91ca\u653e\u6240\u6709\u672a\u4f7f\u7528\u7684\u7f13\u5b58\u5185\u5b58, \u4ee5\u4fbf\u5176\u4ed6 GPU \u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u8fd9\u4e9b\u5185\u5b58. \u6700\u4f73\u5b9e\u8df5 \u8bbe\u5907\u65e0\u5173\u4ee3\u7801 \u7531\u4e8e PyTorch \u7684\u67b6\u6784, \u4f60\u53ef\u80fd\u9700\u8981\u660e\u786e\u5199\u5165\u8bbe\u5907\u65e0\u5173 (CPU \u6216 GPU) \u4ee3\u7801; \u4e3e\u4e2a\u4f8b\u5b50, \u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\u4f5c\u4e3a\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u521d\u59cb\u9690\u85cf\u72b6\u6001. \u7b2c\u4e00\u6b65\u5148\u786e\u5b9a\u662f\u5426\u4f7f\u7528 GPU. \u4e00\u4e2a\u5e38\u89c1\u7684\u65b9\u5f0f\u662f\u4f7f\u7528 Python \u7684 argparse \u6a21\u5757\u6765\u8bfb\u5165\u7528\u6237\u53c2\u6570, \u5e76\u4e14\u6709\u4e00\u4e2a\u53ef\u4ee5\u7528\u6765\u7981\u7528 CUDA\u3001\u80fd\u4e0e is_available() \u7ed3\u5408\u4f7f\u7528\u7684\u6807\u5fd7. \u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d, args.cuda \u4f1a\u4ea7\u751f\u4e00\u4e2a\u5f53\u9700\u8981\u65f6\u80fd\u5c06\u5f20\u91cf\u548c\u6a21\u5757\u8f6c\u6362\u4e3a CUDA \u7684\u6807\u5fd7: import argparse import torch parser = argparse.ArgumentParser(description='PyTorch Example') parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA') args = parser.parse_args() args.cuda = not args.disable_cuda and torch.cuda.is_available() \u5982\u679c\u9700\u8981\u5c06\u6a21\u5757\u548c\u5f20\u91cf\u53d1\u9001\u5230 GPU, args.cuda \u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b: x = torch.Tensor(8, 42) net = Network() if args.cuda: x = x.cuda() net.cuda() \u521b\u5efa\u5f20\u91cf\u65f6, \u53ef\u4ee5\u5b9a\u4e49\u4e00\u4e2a\u9ed8\u8ba4\u7684\u6570\u636e\u7c7b\u578b\u6765\u66ff\u4ee3 if \u8bed\u53e5, \u5e76\u4f7f\u7528\u5b83\u6765\u8f6c\u6362\u6240\u6709\u7684\u5f20\u91cf. \u4f7f\u7528 dataLoader \u7684\u4f8b\u5b50\u5982\u4e0b: dtype = torch.cuda.FloatTensor for i, x in enumerate(train_loader): x = Variable(x.type(dtype)) \u5728\u7cfb\u7edf\u4e0a\u4f7f\u7528\u591a\u4e2a GPU \u65f6, \u60a8\u53ef\u4ee5\u4f7f\u7528 CUDA_VISIBLE_DEVICES \u73af\u5883\u6807\u5fd7\u6765\u7ba1\u7406\u54ea\u4e9b GPU \u53ef\u7528\u4e8e PyTorch. \u5982\u4e0a\u6240\u8ff0, \u8981\u624b\u52a8\u63a7\u5236\u5728\u54ea\u4e2a GPU \u4e0a\u521b\u5efa\u5f20\u91cf, \u6700\u597d\u7684\u65b9\u6cd5\u662f\u4f7f\u7528 torch.cuda.device \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668: print(\"Outside device is 0\") # On device 0 (default in most scenarios) with torch.cuda.device(1): print(\"Inside device is 1\") # On device 1 print(\"Outside device is still 0\") # On device 0 \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u5f20\u91cf, \u5e76\u4e14\u60f3\u5728\u540c\u4e00\u4e2a\u8bbe\u5907\u4e0a\u521b\u5efa\u4e00\u4e2a\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf, \u90a3\u4e48\u60a8\u53ef\u4ee5\u4f7f\u7528 new() \u65b9\u6cd5, \u5b83\u7684\u4f7f\u7528\u548c\u666e\u901a\u7684\u5f20\u91cf\u6784\u9020\u51fd\u6570\u4e00\u6837. \u867d\u7136\u524d\u9762\u63d0\u5230\u7684\u65b9\u6cd5\u53d6\u51b3\u4e8e\u5f53\u524d\u7684 GPU \u73af\u5883, \u4f46\u662f new() \u4fdd\u7559\u4e86\u539f\u59cb\u5f20\u91cf\u7684\u8bbe\u5907\u4fe1\u606f. \u5f53\u521b\u5efa\u5728\u5411\u524d\u4f20\u9012\u671f\u95f4\u9700\u8981\u5728\u5185\u90e8\u521b\u5efa\u65b0\u7684\u5f20\u91cf/\u53d8\u91cf\u7684\u6a21\u5757\u65f6, \u5efa\u8bae\u4f7f\u7528\u8fd9\u79cd\u505a\u6cd5: x_cpu = torch.FloatTensor(1) x_gpu = torch.cuda.FloatTensor(1) x_cpu_long = torch.LongTensor(1) y_cpu = x_cpu.new(8, 10, 10).fill_(0.3) y_gpu = x_gpu.new(x_gpu.size()).fill_(-5) y_cpu_long = x_cpu_long.new([[1, 2, 3]]) \u5982\u679c\u4f60\u60f3\u521b\u5efa\u4e00\u4e2a\u4e0e\u53e6\u4e00\u4e2a\u5f20\u91cf\u6709\u7740\u76f8\u540c\u7c7b\u578b\u548c\u5927\u5c0f\u3001\u5e76\u7528 1 \u6216 0 \u586b\u5145\u7684\u5f20\u91cf, ones_like() \u6216 zeros_like() \u53ef\u63d0\u4f9b\u65b9\u4fbf\u7684\u8f85\u52a9\u529f\u80fd (\u540c\u65f6\u4fdd\u7559\u8bbe\u5907\u4fe1\u606f) x_cpu = torch.FloatTensor(1) x_gpu = torch.cuda.FloatTensor(1) y_cpu = torch.ones_like(x_cpu) y_gpu = torch.zeros_like(x_gpu) \u4f7f\u7528\u56fa\u5b9a\u7684\u5185\u5b58\u7f13\u51b2\u533a \u5f53\u526f\u672c\u6765\u81ea\u56fa\u5b9a (\u9875\u9501) \u5185\u5b58\u65f6, \u4e3b\u673a\u5230 GPU \u7684\u590d\u5236\u901f\u5ea6\u8981\u5feb\u5f88\u591a. CPU \u5f20\u91cf\u548c\u5b58\u50a8\u5f00\u653e\u4e86\u4e00\u4e2a pin_memory() \u65b9\u6cd5, \u5b83\u8fd4\u56de\u8be5\u5bf9\u8c61\u7684\u526f\u672c, \u800c\u5b83\u7684\u6570\u636e\u653e\u5728\u56fa\u5b9a\u533a\u57df\u4e2d. \u53e6\u5916, \u4e00\u65e6\u56fa\u5b9a\u4e86\u5f20\u91cf\u6216\u5b58\u50a8, \u5c31\u53ef\u4ee5\u4f7f\u7528\u5f02\u6b65\u7684 GPU \u526f\u672c. \u53ea\u9700\u4f20\u9012\u4e00\u4e2a\u989d\u5916\u7684 async=True \u53c2\u6570\u7ed9 cuda() \u8c03\u7528. \u8fd9\u53ef\u4ee5\u7528\u4e8e\u91cd\u53e0\u6570\u636e\u4f20\u8f93\u4e0e\u8ba1\u7b97. \u901a\u8fc7\u5c06 pin_memory=True \u4f20\u9012\u7ed9\u5176\u6784\u9020\u51fd\u6570, \u53ef\u4ee5\u4f7f DataLoader \u5c06 batch \u8fd4\u56de\u5230\u56fa\u5b9a\u5185\u5b58\u4e2d. \u4f7f\u7528 nn.DataParallel \u66ff\u4ee3 multiprocessing \u5927\u591a\u6570\u6d89\u53ca\u6279\u91cf\u8f93\u5165\u548c\u591a\u4e2a GPU \u7684\u60c5\u51b5\u5e94\u9ed8\u8ba4\u4f7f\u7528 DataParallel \u6765\u4f7f\u7528\u591a\u4e2a GPU. \u5c3d\u7ba1\u6709 GIL \u7684\u5b58\u5728, \u5355\u4e2a Python \u8fdb\u7a0b\u4e5f\u53ef\u80fd\u4f7f\u591a\u4e2a GPU \u9971\u548c. \u4ece 0.1.9 \u7248\u672c\u5f00\u59cb, \u5927\u91cf\u7684 GPU (8+) \u53ef\u80fd\u672a\u88ab\u5145\u5206\u5229\u7528. \u7136\u800c, \u8fd9\u662f\u4e00\u4e2a\u5df2\u77e5\u7684\u95ee\u9898, \u4e5f\u6b63\u5728\u79ef\u6781\u5f00\u53d1\u4e2d. \u548c\u5f80\u5e38\u4e00\u6837, \u6d4b\u8bd5\u60a8\u7684\u7528\u4f8b\u5427. \u8c03\u7528 multiprocessing \u4f7f\u7528 CUDA \u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u6ce8\u610f\u4e8b\u9879; \u9664\u975e\u60a8\u8db3\u591f\u8c28\u614e\u4ee5\u6ee1\u8db3\u6570\u636e\u5904\u7406\u9700\u6c42, \u5426\u5219\u60a8\u7684\u7a0b\u5e8f\u5f88\u53ef\u80fd\u4f1a\u51fa\u73b0\u9519\u8bef\u6216\u672a\u5b9a\u4e49\u7684\u884c\u4e3a.","title":"CUDA \u8bed\u4e49"},{"location":"48/#cuda","text":"torch.cuda \u88ab\u7528\u4e8e\u8bbe\u7f6e\u548c\u8fd0\u884c CUDA \u64cd\u4f5c. \u5b83\u4f1a\u8bb0\u5f55\u5f53\u524d\u9009\u62e9\u7684 GPU, \u5e76\u4e14\u5206\u914d\u7684\u6240\u6709 CUDA \u5f20\u91cf\u5c06\u9ed8\u8ba4\u5728\u4e0a\u9762\u521b\u5efa. \u53ef\u4ee5\u4f7f\u7528 torch.cuda.device \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u66f4\u6539\u6240\u9009\u8bbe\u5907. \u4f46\u662f, \u4e00\u65e6\u5f20\u91cf\u88ab\u5206\u914d, \u60a8\u53ef\u4ee5\u76f4\u63a5\u5bf9\u5176\u8fdb\u884c\u64cd\u4f5c, \u800c\u4e0d\u9700\u8981\u8003\u8651\u5df2\u9009\u62e9\u7684\u8bbe\u5907, \u7ed3\u679c\u5c06\u59cb\u7ec8\u653e\u5728\u4e0e\u5f20\u91cf\u76f8\u5173\u7684\u8bbe\u5907\u4e0a. \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u4e0d\u652f\u6301\u8de8 GPU \u64cd\u4f5c, \u552f\u4e00\u7684\u4f8b\u5916\u662f copy_() . \u9664\u975e\u542f\u7528\u5bf9\u7b49\u5b58\u50a8\u5668\u8bbf\u95ee, \u5426\u5219\u5bf9\u5206\u5e03\u5728\u4e0d\u540c\u8bbe\u5907\u4e0a\u7684\u5f20\u91cf\u5c1d\u8bd5\u8fdb\u884c\u4efb\u4f55\u542f\u52a8\u64cd\u4f5c\u90fd\u5c06\u5f15\u53d1\u9519\u8bef. \u4e0b\u9762\u6211\u4eec\u7528\u4e00\u4e2a\u5c0f\u4f8b\u5b50\u6765\u5c55\u793a: x = torch.cuda.FloatTensor(1) # x.get_device() == 0 y = torch.FloatTensor(1).cuda() # y.get_device() == 0 with torch.cuda.device(1): # allocates a tensor on GPU 1 a = torch.cuda.FloatTensor(1) # transfers a tensor from CPU to GPU 1 b = torch.FloatTensor(1).cuda() # a.get_device() == b.get_device() == 1 c = a + b # c.get_device() == 1 z = x + y # z.get_device() == 0 # \u5373\u4f7f\u5728\u4e0a\u4e0b\u6587\u91cc\u9762, \u4f60\u4e5f\u53ef\u4ee5\u5728 .cuda \u7684\u53c2\u6570\u4e2d\u4f20\u5165\u8bbe\u5907id d = torch.randn(2).cuda(2) # d.get_device() == 2","title":"CUDA \u8bed\u4e49"},{"location":"48/#_1","text":"PyTorch \u4f7f\u7528\u7f13\u5b58\u5185\u5b58\u5206\u914d\u5668\u6765\u52a0\u901f\u5185\u5b58\u5206\u914d. \u8fd9\u5141\u8bb8\u5728\u6ca1\u6709\u8bbe\u5907\u540c\u6b65\u7684\u60c5\u51b5\u4e0b\u5feb\u901f\u91ca\u653e\u5185\u5b58. \u4f46\u662f, \u7531\u5206\u914d\u5668\u7ba1\u7406\u7684\u672a\u4f7f\u7528\u7684\u5185\u5b58\u4ecd\u5c06\u663e\u793a\u4e3a\u5728 <cite>nvidia-smi</cite> \u4e2d\u4f7f\u7528. \u8c03\u7528 empty_cache() \u53ef\u4ee5\u4ece PyTorch \u4e2d\u91ca\u653e\u6240\u6709\u672a\u4f7f\u7528\u7684\u7f13\u5b58\u5185\u5b58, \u4ee5\u4fbf\u5176\u4ed6 GPU \u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u8fd9\u4e9b\u5185\u5b58.","title":"\u5185\u5b58\u7ba1\u7406"},{"location":"48/#_2","text":"","title":"\u6700\u4f73\u5b9e\u8df5"},{"location":"48/#_3","text":"\u7531\u4e8e PyTorch \u7684\u67b6\u6784, \u4f60\u53ef\u80fd\u9700\u8981\u660e\u786e\u5199\u5165\u8bbe\u5907\u65e0\u5173 (CPU \u6216 GPU) \u4ee3\u7801; \u4e3e\u4e2a\u4f8b\u5b50, \u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\u4f5c\u4e3a\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u521d\u59cb\u9690\u85cf\u72b6\u6001. \u7b2c\u4e00\u6b65\u5148\u786e\u5b9a\u662f\u5426\u4f7f\u7528 GPU. \u4e00\u4e2a\u5e38\u89c1\u7684\u65b9\u5f0f\u662f\u4f7f\u7528 Python \u7684 argparse \u6a21\u5757\u6765\u8bfb\u5165\u7528\u6237\u53c2\u6570, \u5e76\u4e14\u6709\u4e00\u4e2a\u53ef\u4ee5\u7528\u6765\u7981\u7528 CUDA\u3001\u80fd\u4e0e is_available() \u7ed3\u5408\u4f7f\u7528\u7684\u6807\u5fd7. \u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d, args.cuda \u4f1a\u4ea7\u751f\u4e00\u4e2a\u5f53\u9700\u8981\u65f6\u80fd\u5c06\u5f20\u91cf\u548c\u6a21\u5757\u8f6c\u6362\u4e3a CUDA \u7684\u6807\u5fd7: import argparse import torch parser = argparse.ArgumentParser(description='PyTorch Example') parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA') args = parser.parse_args() args.cuda = not args.disable_cuda and torch.cuda.is_available() \u5982\u679c\u9700\u8981\u5c06\u6a21\u5757\u548c\u5f20\u91cf\u53d1\u9001\u5230 GPU, args.cuda \u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b: x = torch.Tensor(8, 42) net = Network() if args.cuda: x = x.cuda() net.cuda() \u521b\u5efa\u5f20\u91cf\u65f6, \u53ef\u4ee5\u5b9a\u4e49\u4e00\u4e2a\u9ed8\u8ba4\u7684\u6570\u636e\u7c7b\u578b\u6765\u66ff\u4ee3 if \u8bed\u53e5, \u5e76\u4f7f\u7528\u5b83\u6765\u8f6c\u6362\u6240\u6709\u7684\u5f20\u91cf. \u4f7f\u7528 dataLoader \u7684\u4f8b\u5b50\u5982\u4e0b: dtype = torch.cuda.FloatTensor for i, x in enumerate(train_loader): x = Variable(x.type(dtype)) \u5728\u7cfb\u7edf\u4e0a\u4f7f\u7528\u591a\u4e2a GPU \u65f6, \u60a8\u53ef\u4ee5\u4f7f\u7528 CUDA_VISIBLE_DEVICES \u73af\u5883\u6807\u5fd7\u6765\u7ba1\u7406\u54ea\u4e9b GPU \u53ef\u7528\u4e8e PyTorch. \u5982\u4e0a\u6240\u8ff0, \u8981\u624b\u52a8\u63a7\u5236\u5728\u54ea\u4e2a GPU \u4e0a\u521b\u5efa\u5f20\u91cf, \u6700\u597d\u7684\u65b9\u6cd5\u662f\u4f7f\u7528 torch.cuda.device \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668: print(\"Outside device is 0\") # On device 0 (default in most scenarios) with torch.cuda.device(1): print(\"Inside device is 1\") # On device 1 print(\"Outside device is still 0\") # On device 0 \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u5f20\u91cf, \u5e76\u4e14\u60f3\u5728\u540c\u4e00\u4e2a\u8bbe\u5907\u4e0a\u521b\u5efa\u4e00\u4e2a\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf, \u90a3\u4e48\u60a8\u53ef\u4ee5\u4f7f\u7528 new() \u65b9\u6cd5, \u5b83\u7684\u4f7f\u7528\u548c\u666e\u901a\u7684\u5f20\u91cf\u6784\u9020\u51fd\u6570\u4e00\u6837. \u867d\u7136\u524d\u9762\u63d0\u5230\u7684\u65b9\u6cd5\u53d6\u51b3\u4e8e\u5f53\u524d\u7684 GPU \u73af\u5883, \u4f46\u662f new() \u4fdd\u7559\u4e86\u539f\u59cb\u5f20\u91cf\u7684\u8bbe\u5907\u4fe1\u606f. \u5f53\u521b\u5efa\u5728\u5411\u524d\u4f20\u9012\u671f\u95f4\u9700\u8981\u5728\u5185\u90e8\u521b\u5efa\u65b0\u7684\u5f20\u91cf/\u53d8\u91cf\u7684\u6a21\u5757\u65f6, \u5efa\u8bae\u4f7f\u7528\u8fd9\u79cd\u505a\u6cd5: x_cpu = torch.FloatTensor(1) x_gpu = torch.cuda.FloatTensor(1) x_cpu_long = torch.LongTensor(1) y_cpu = x_cpu.new(8, 10, 10).fill_(0.3) y_gpu = x_gpu.new(x_gpu.size()).fill_(-5) y_cpu_long = x_cpu_long.new([[1, 2, 3]]) \u5982\u679c\u4f60\u60f3\u521b\u5efa\u4e00\u4e2a\u4e0e\u53e6\u4e00\u4e2a\u5f20\u91cf\u6709\u7740\u76f8\u540c\u7c7b\u578b\u548c\u5927\u5c0f\u3001\u5e76\u7528 1 \u6216 0 \u586b\u5145\u7684\u5f20\u91cf, ones_like() \u6216 zeros_like() \u53ef\u63d0\u4f9b\u65b9\u4fbf\u7684\u8f85\u52a9\u529f\u80fd (\u540c\u65f6\u4fdd\u7559\u8bbe\u5907\u4fe1\u606f) x_cpu = torch.FloatTensor(1) x_gpu = torch.cuda.FloatTensor(1) y_cpu = torch.ones_like(x_cpu) y_gpu = torch.zeros_like(x_gpu)","title":"\u8bbe\u5907\u65e0\u5173\u4ee3\u7801"},{"location":"48/#_4","text":"\u5f53\u526f\u672c\u6765\u81ea\u56fa\u5b9a (\u9875\u9501) \u5185\u5b58\u65f6, \u4e3b\u673a\u5230 GPU \u7684\u590d\u5236\u901f\u5ea6\u8981\u5feb\u5f88\u591a. CPU \u5f20\u91cf\u548c\u5b58\u50a8\u5f00\u653e\u4e86\u4e00\u4e2a pin_memory() \u65b9\u6cd5, \u5b83\u8fd4\u56de\u8be5\u5bf9\u8c61\u7684\u526f\u672c, \u800c\u5b83\u7684\u6570\u636e\u653e\u5728\u56fa\u5b9a\u533a\u57df\u4e2d. \u53e6\u5916, \u4e00\u65e6\u56fa\u5b9a\u4e86\u5f20\u91cf\u6216\u5b58\u50a8, \u5c31\u53ef\u4ee5\u4f7f\u7528\u5f02\u6b65\u7684 GPU \u526f\u672c. \u53ea\u9700\u4f20\u9012\u4e00\u4e2a\u989d\u5916\u7684 async=True \u53c2\u6570\u7ed9 cuda() \u8c03\u7528. \u8fd9\u53ef\u4ee5\u7528\u4e8e\u91cd\u53e0\u6570\u636e\u4f20\u8f93\u4e0e\u8ba1\u7b97. \u901a\u8fc7\u5c06 pin_memory=True \u4f20\u9012\u7ed9\u5176\u6784\u9020\u51fd\u6570, \u53ef\u4ee5\u4f7f DataLoader \u5c06 batch \u8fd4\u56de\u5230\u56fa\u5b9a\u5185\u5b58\u4e2d.","title":"\u4f7f\u7528\u56fa\u5b9a\u7684\u5185\u5b58\u7f13\u51b2\u533a"},{"location":"48/#nndataparallel-multiprocessing","text":"\u5927\u591a\u6570\u6d89\u53ca\u6279\u91cf\u8f93\u5165\u548c\u591a\u4e2a GPU \u7684\u60c5\u51b5\u5e94\u9ed8\u8ba4\u4f7f\u7528 DataParallel \u6765\u4f7f\u7528\u591a\u4e2a GPU. \u5c3d\u7ba1\u6709 GIL \u7684\u5b58\u5728, \u5355\u4e2a Python \u8fdb\u7a0b\u4e5f\u53ef\u80fd\u4f7f\u591a\u4e2a GPU \u9971\u548c. \u4ece 0.1.9 \u7248\u672c\u5f00\u59cb, \u5927\u91cf\u7684 GPU (8+) \u53ef\u80fd\u672a\u88ab\u5145\u5206\u5229\u7528. \u7136\u800c, \u8fd9\u662f\u4e00\u4e2a\u5df2\u77e5\u7684\u95ee\u9898, \u4e5f\u6b63\u5728\u79ef\u6781\u5f00\u53d1\u4e2d. \u548c\u5f80\u5e38\u4e00\u6837, \u6d4b\u8bd5\u60a8\u7684\u7528\u4f8b\u5427. \u8c03\u7528 multiprocessing \u4f7f\u7528 CUDA \u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u6ce8\u610f\u4e8b\u9879; \u9664\u975e\u60a8\u8db3\u591f\u8c28\u614e\u4ee5\u6ee1\u8db3\u6570\u636e\u5904\u7406\u9700\u6c42, \u5426\u5219\u60a8\u7684\u7a0b\u5e8f\u5f88\u53ef\u80fd\u4f1a\u51fa\u73b0\u9519\u8bef\u6216\u672a\u5b9a\u4e49\u7684\u884c\u4e3a.","title":"\u4f7f\u7528 nn.DataParallel \u66ff\u4ee3 multiprocessing"},{"location":"49/","text":"\u6269\u5c55 PyTorch \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u6269\u5c55 torch.nn , torch.autograd \u6a21\u5757, \u5e76\u4e14\u4f7f\u7528\u6211\u4eec\u7684 C \u5e93\u6765\u7f16\u5199\u81ea\u5b9a\u4e49\u7684 C \u6269\u5c55\u5de5\u5177. \u6269\u5c55 torch.autograd \u6a21\u5757 \u5c06\u64cd\u4f5c\u6dfb\u52a0\u5230 autograd \u6a21\u5757\u9700\u8981\u4e3a\u6bcf\u4e00\u4e2a\u64cd\u4f5c\u5b9e\u73b0\u4e00\u4e2a\u65b0\u7684 Function \u7c7b\u7684\u5b50\u7c7b. \u56de\u60f3\u4e00\u4e0b, Function \u51fd\u6570\u662f autograd \u6a21\u5757\u7528\u6765\u8ba1\u7b97\u7ed3\u679c\u548c\u68af\u5ea6, \u5e76\u5bf9\u64cd\u4f5c\u5386\u53f2\u8fdb\u884c\u7f16\u7801\u7684. \u6bcf\u4e00\u4e2a\u65b0\u7684\u51fd\u6570\u9700\u8981\u4f60\u6765\u5b9e\u73b0\u4e24\u4e2a\u65b9\u6cd5: forward() - \u8fdb\u884c\u64cd\u4f5c\u7684\u4ee3\u7801. \u5982\u679c\u60a8\u6307\u5b9a\u9ed8\u8ba4\u503c, \u5219\u53ef\u4ee5\u6839\u636e\u9700\u8981\u4f7f\u7528\u4efb\u610f\u6570\u91cf\u7684\u53c2\u6570, \u5176\u4e2d\u4e00\u4e9b\u53c2\u6570\u662f\u53ef\u9009\u7684. \u53c2\u6570\u53ef\u63a5\u6536\u5404\u79cd\u7c7b\u578b\u7684 Python \u5bf9\u8c61. Variable \u53c2\u6570\u5728\u88ab\u8c03\u7528\u4e4b\u524d\u5c06\u88ab\u8f6c\u6362\u4e3a Tensor \u5bf9\u8c61, \u5e76\u4e14\u5b83\u4eec\u7684\u4f7f\u7528\u60c5\u51b5\u5c06\u4f1a\u88ab\u6ce8\u518c\u5230 graph (\u56fe) \u4e2d. \u8bf7\u6ce8\u610f, \u8fd9\u4e2a\u903b\u8f91\u4e0d\u4f1a\u904d\u5386 lists, dicts, \u548c\u4efb\u4f55\u5176\u5b83\u7684\u6570\u636e\u7ed3\u6784, \u53ea\u4f1a\u8003\u8651\u88ab\u8c03\u7528\u4e3a\u76f4\u63a5\u53c2\u6570\u7684\u53d8\u91cf. \u5982\u679c\u6709\u591a\u4e2a\u8f93\u51fa, \u5219\u53ef\u4ee5\u8003\u8651\u8fd4\u56de\u5355\u4e2a\u7684 Tensor \u7c7b\u683c\u5f0f\u7684\u8f93\u51fa, \u6216\u8005 Tensor \u7c7b\u7684 tuple \u7c7b\u683c\u5f0f\u8f93\u51fa. \u6b64\u5916, \u8bf7\u53c2\u9605 Function \u7c7b\u7684\u6587\u6863\u6765\u67e5\u627e\u53ea\u80fd\u4ece forward() \u8c03\u7528\u7684\u6709\u7528\u65b9\u6cd5\u7684\u63cf\u8ff0. backward() - \u8ba1\u7b97\u68af\u5ea6\u7684\u516c\u5f0f. \u5b83\u5c06\u88ab\u8d4b\u4e88\u4e0e\u8f93\u51fa\u4e00\u6837\u591a\u7684 Variable \u53c2\u6570, \u5176\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8868\u793a\u5bf9\u5e94\u68af\u5ea6\u7684\u8f93\u51fa. \u5b83\u5e94\u8be5\u8fd4\u56de\u4e0e\u8f93\u5165\u4e00\u6837\u591a\u7684 Variable , \u5176\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8868\u793a\u90fd\u5305\u542b\u5176\u76f8\u5e94\u8f93\u5165\u7684\u68af\u5ea6. \u5982\u679c\u8f93\u5165\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 (\u8bf7\u53c2\u9605 needs_input_grad \u5c5e\u6027), \u6216\u8005\u662f\u975e Variable \u5bf9\u8c61, \u5219\u53ef\u8fd4\u56de None \u7c7b. \u6b64\u5916, \u5982\u679c\u4f60\u5728 forward() \u65b9\u6cd5\u4e2d\u6709\u53ef\u9009\u7684\u53c2\u6570, \u5219\u53ef\u4ee5\u8fd4\u56de\u6bd4\u8f93\u5165\u66f4\u591a\u7684\u68af\u5ea6, \u53ea\u8981\u5b83\u4eec\u90fd\u662f None \u7c7b\u578b\u5373\u53ef. \u4e0b\u9762\u4f60\u53ef\u4ee5\u627e\u5230\u6765\u81ea torch.nn \u6a21\u5757\u7684 Linear \u51fd\u6570\u4ee3\u7801, \u4ee5\u53ca\u6ce8\u89e3 # \u7ee7\u627f\u81ea Function class LinearFunction(Function): # Note that both forward and backward are @staticmethods @staticmethod # bias is an optional argument def forward(ctx, input, weight, bias=None): ctx.save_for_backward(input, weight, bias) output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) return output # This function has only a single output, so it gets only one gradient @staticmethod def backward(ctx, grad_output): # This is a pattern that is very convenient - at the top of backward # unpack saved_tensors and initialize all gradients w.r.t. inputs to # None. Thanks to the fact that additional trailing Nones are # ignored, the return statement is simple even when the function has # optional inputs. input, weight, bias = ctx.saved_variables grad_input = grad_weight = grad_bias = None # These needs_input_grad checks are optional and there only to # improve efficiency. If you want to make your code simpler, you can # skip them. Returning gradients for inputs that don't require it is # not an error. if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0).squeeze(0) return grad_input, grad_weight, grad_bias \u73b0\u5728, \u4e3a\u4e86\u66f4\u65b9\u4fbf\u5730\u4f7f\u7528\u8fd9\u4e9b\u81ea\u5b9a\u4e49\u64cd\u4f5c, \u6211\u4eec\u63a8\u8350\u4f7f\u7528 apply \u65b9\u6cd5 linear = LinearFunction.apply \u5728\u8fd9\u91cc, \u6211\u4eec\u7ed9\u51fa\u4e86\u4e00\u4e2a\u7531\u975e\u53d8\u91cf\u53c2\u6570\u53c2\u6570\u5316\u7684\u51fd\u6570\u7684\u4f8b\u5b50 class MulConstant(Function): @staticmethod def forward(ctx, tensor, constant): # ctx is a context object that can be used to stash information # for backward computation ctx.constant = constant return tensor * constant @staticmethod def backward(ctx, grad_output): # We return as many input gradients as there were arguments. # Gradients of non-Tensor arguments to forward must be None. return grad_output * ctx.constant, None \u4f60\u53ef\u80fd\u60f3\u8981\u68c0\u6d4b\u4f60\u521a\u521a\u5b9e\u73b0\u7684 <cite>backward</cite> \u65b9\u6cd5\u662f\u5426\u6b63\u786e\u7684\u8ba1\u7b97\u4e86\u68af\u5ea6. \u4f60\u53ef\u4ee5\u4f7f\u7528\u5c0f\u800c\u6709\u9650\u7684\u5fae\u5206\u8fdb\u884c\u6570\u503c\u4f30\u8ba1 from torch.autograd import gradcheck # gradchek takes a tuple of tensor as input, check if your gradient # evaluated with these tensors are close enough to numerical # approximations and returns True if they all verify this condition. input = (Variable(torch.randn(20,20).double(), requires_grad=True), Variable(torch.randn(30,20).double(), requires_grad=True),) test = gradcheck(Linear.apply, input, eps=1e-6, atol=1e-4) print(test) \u6269\u5c55 torch.nn \u6a21\u5757 nn \u6a21\u5757\u6709\u4e24\u79cd\u7c7b\u578b\u7684\u63a5\u53e3 - modules \u548c their functional versions. \u4f60\u53ef\u4ee5\u7528\u4e24\u79cd\u65b9\u6cd5\u6269\u5c55\u5b83, \u4f46\u662f\u6211\u4eec\u63a8\u8350\u4f7f\u7528\u5404\u79cd\u5c42\u7684\u6a21\u5757, \u7528\u6765\u5b58\u653e\u4efb\u4f55 parameters(\u53c2\u6570) \u6216\u8005 buffers(\u7f13\u51b2), \u5e76\u4e14\u63a8\u8350\u4f7f\u7528\u4e00\u4e2a\u51fd\u6570\u5f62\u5f0f\u7684\u65e0\u53c2\u6570\u64cd\u4f5c, \u6bd4\u5982\u6fc0\u6d3b\u51fd\u6570, \u6c60\u5316\u7b49\u7b49. \u6dfb\u52a0\u64cd\u4f5c\u7684\u51fd\u6570\u7248\u672c\u5df2\u7ecf\u5728\u4e0a\u9762\u7684\u7ae0\u8282\u4e2d\u5b8c\u6574\u7684\u4ecb\u7ecd\u4e86. \u6dfb\u52a0 Module \u7c7b \u7531\u4e8e nn \u6a21\u5757\u5927\u91cf\u7684\u5229\u7528\u4e86 autograd \u6a21\u5757, \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684 Module \u7c7b\u9700\u8981\u5b9e\u73b0\u4e00\u4e2a Function \u7c7b, \u5b83\u4f1a\u6267\u884c\u5bf9\u5e94\u7684\u64cd\u4f5c\u5e76\u4e14\u8ba1\u7b97\u68af\u5ea6. \u4ece\u73b0\u5728\u5f00\u59cb, \u5047\u8bbe\u6211\u4eec\u60f3\u8981\u5b9e\u73b0\u4e00\u4e2a Linear \u6a21\u5757, \u5e76\u4e14\u6211\u4eec\u5177\u6709\u5982\u4e0a\u6240\u5217\u5b9e\u73b0\u7684\u529f\u80fd. \u6709\u5f88\u5c11\u7684\u4ee3\u7801\u9700\u8981\u6dfb\u52a0\u8fd9\u4e2a. \u73b0\u5728\u6709\u4e24\u4e2a\u51fd\u6570\u9700\u8981\u5b9e\u73b0: __init__ ( optional ) - \u63a5\u6536\u8bf8\u5982 kernel sizes (\u6838\u5927\u5c0f) , numbers of features (\u7279\u5f81\u6570\u91cf) \u7b49\u53c2\u6570, \u5e76\u521d\u59cb\u5316 parameters(\u53c2\u6570) \u548c buffers(\u7f13\u51b2\u533a). forward() - \u5b9e\u4f8b\u5316\u4e00\u4e2a Function \u7c7b, \u5e76\u4e14\u7528\u4e8e\u6267\u884c\u64cd\u4f5c. \u8fd9\u4e0e\u4e0a\u9762\u7684 functional wrapper (\u51fd\u6570\u7684\u5305\u88c5) \u975e\u5e38\u76f8\u4f3c. \u8fd9\u5c31\u662f Linear \u6a21\u5757\u7684\u5b9e\u73b0\u65b9\u5f0f class Linear(nn.Module): def __init__(self, input_features, output_features, bias=True): super(Linear, self).__init__() self.input_features = input_features self.output_features = output_features # nn.Parameter is a special kind of Variable, that will get # automatically registered as Module's parameter once it's assigned # as an attribute. Parameters and buffers need to be registered, or # they won't appear in .parameters() (doesn't apply to buffers), and # won't be converted when e.g. .cuda() is called. You can use # .register_buffer() to register buffers. # nn.Parameters can never be volatile and, different than Variables, # they require gradients by default. self.weight = nn.Parameter(torch.Tensor(output_features, input_features)) if bias: self.bias = nn.Parameter(torch.Tensor(output_features)) else: # You should always register all possible parameters, but the # optional ones can be None if you want. self.register_parameter('bias', None) # Not a very smart way to initialize weights self.weight.data.uniform_(-0.1, 0.1) if bias is not None: self.bias.data.uniform_(-0.1, 0.1) def forward(self, input): # See the autograd section for explanation of what happens here. return LinearFunction.apply(input, self.weight, self.bias) \u7f16\u5199\u81ea\u5b9a\u4e49\u7684 C \u6269\u5c55 \u73b0\u5728\u4f60\u53ef\u4ee5\u5728 GitHub \u4e2d\u627e\u5230\u4e00\u4e9b\u4f8b\u5b50.","title":"\u6269\u5c55 PyTorch"},{"location":"49/#pytorch","text":"\u5728\u672c\u6587\u4e2d, \u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u6269\u5c55 torch.nn , torch.autograd \u6a21\u5757, \u5e76\u4e14\u4f7f\u7528\u6211\u4eec\u7684 C \u5e93\u6765\u7f16\u5199\u81ea\u5b9a\u4e49\u7684 C \u6269\u5c55\u5de5\u5177.","title":"\u6269\u5c55 PyTorch"},{"location":"49/#torchautograd","text":"\u5c06\u64cd\u4f5c\u6dfb\u52a0\u5230 autograd \u6a21\u5757\u9700\u8981\u4e3a\u6bcf\u4e00\u4e2a\u64cd\u4f5c\u5b9e\u73b0\u4e00\u4e2a\u65b0\u7684 Function \u7c7b\u7684\u5b50\u7c7b. \u56de\u60f3\u4e00\u4e0b, Function \u51fd\u6570\u662f autograd \u6a21\u5757\u7528\u6765\u8ba1\u7b97\u7ed3\u679c\u548c\u68af\u5ea6, \u5e76\u5bf9\u64cd\u4f5c\u5386\u53f2\u8fdb\u884c\u7f16\u7801\u7684. \u6bcf\u4e00\u4e2a\u65b0\u7684\u51fd\u6570\u9700\u8981\u4f60\u6765\u5b9e\u73b0\u4e24\u4e2a\u65b9\u6cd5: forward() - \u8fdb\u884c\u64cd\u4f5c\u7684\u4ee3\u7801. \u5982\u679c\u60a8\u6307\u5b9a\u9ed8\u8ba4\u503c, \u5219\u53ef\u4ee5\u6839\u636e\u9700\u8981\u4f7f\u7528\u4efb\u610f\u6570\u91cf\u7684\u53c2\u6570, \u5176\u4e2d\u4e00\u4e9b\u53c2\u6570\u662f\u53ef\u9009\u7684. \u53c2\u6570\u53ef\u63a5\u6536\u5404\u79cd\u7c7b\u578b\u7684 Python \u5bf9\u8c61. Variable \u53c2\u6570\u5728\u88ab\u8c03\u7528\u4e4b\u524d\u5c06\u88ab\u8f6c\u6362\u4e3a Tensor \u5bf9\u8c61, \u5e76\u4e14\u5b83\u4eec\u7684\u4f7f\u7528\u60c5\u51b5\u5c06\u4f1a\u88ab\u6ce8\u518c\u5230 graph (\u56fe) \u4e2d. \u8bf7\u6ce8\u610f, \u8fd9\u4e2a\u903b\u8f91\u4e0d\u4f1a\u904d\u5386 lists, dicts, \u548c\u4efb\u4f55\u5176\u5b83\u7684\u6570\u636e\u7ed3\u6784, \u53ea\u4f1a\u8003\u8651\u88ab\u8c03\u7528\u4e3a\u76f4\u63a5\u53c2\u6570\u7684\u53d8\u91cf. \u5982\u679c\u6709\u591a\u4e2a\u8f93\u51fa, \u5219\u53ef\u4ee5\u8003\u8651\u8fd4\u56de\u5355\u4e2a\u7684 Tensor \u7c7b\u683c\u5f0f\u7684\u8f93\u51fa, \u6216\u8005 Tensor \u7c7b\u7684 tuple \u7c7b\u683c\u5f0f\u8f93\u51fa. \u6b64\u5916, \u8bf7\u53c2\u9605 Function \u7c7b\u7684\u6587\u6863\u6765\u67e5\u627e\u53ea\u80fd\u4ece forward() \u8c03\u7528\u7684\u6709\u7528\u65b9\u6cd5\u7684\u63cf\u8ff0. backward() - \u8ba1\u7b97\u68af\u5ea6\u7684\u516c\u5f0f. \u5b83\u5c06\u88ab\u8d4b\u4e88\u4e0e\u8f93\u51fa\u4e00\u6837\u591a\u7684 Variable \u53c2\u6570, \u5176\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8868\u793a\u5bf9\u5e94\u68af\u5ea6\u7684\u8f93\u51fa. \u5b83\u5e94\u8be5\u8fd4\u56de\u4e0e\u8f93\u5165\u4e00\u6837\u591a\u7684 Variable , \u5176\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8868\u793a\u90fd\u5305\u542b\u5176\u76f8\u5e94\u8f93\u5165\u7684\u68af\u5ea6. \u5982\u679c\u8f93\u5165\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 (\u8bf7\u53c2\u9605 needs_input_grad \u5c5e\u6027), \u6216\u8005\u662f\u975e Variable \u5bf9\u8c61, \u5219\u53ef\u8fd4\u56de None \u7c7b. \u6b64\u5916, \u5982\u679c\u4f60\u5728 forward() \u65b9\u6cd5\u4e2d\u6709\u53ef\u9009\u7684\u53c2\u6570, \u5219\u53ef\u4ee5\u8fd4\u56de\u6bd4\u8f93\u5165\u66f4\u591a\u7684\u68af\u5ea6, \u53ea\u8981\u5b83\u4eec\u90fd\u662f None \u7c7b\u578b\u5373\u53ef. \u4e0b\u9762\u4f60\u53ef\u4ee5\u627e\u5230\u6765\u81ea torch.nn \u6a21\u5757\u7684 Linear \u51fd\u6570\u4ee3\u7801, \u4ee5\u53ca\u6ce8\u89e3 # \u7ee7\u627f\u81ea Function class LinearFunction(Function): # Note that both forward and backward are @staticmethods @staticmethod # bias is an optional argument def forward(ctx, input, weight, bias=None): ctx.save_for_backward(input, weight, bias) output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) return output # This function has only a single output, so it gets only one gradient @staticmethod def backward(ctx, grad_output): # This is a pattern that is very convenient - at the top of backward # unpack saved_tensors and initialize all gradients w.r.t. inputs to # None. Thanks to the fact that additional trailing Nones are # ignored, the return statement is simple even when the function has # optional inputs. input, weight, bias = ctx.saved_variables grad_input = grad_weight = grad_bias = None # These needs_input_grad checks are optional and there only to # improve efficiency. If you want to make your code simpler, you can # skip them. Returning gradients for inputs that don't require it is # not an error. if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0).squeeze(0) return grad_input, grad_weight, grad_bias \u73b0\u5728, \u4e3a\u4e86\u66f4\u65b9\u4fbf\u5730\u4f7f\u7528\u8fd9\u4e9b\u81ea\u5b9a\u4e49\u64cd\u4f5c, \u6211\u4eec\u63a8\u8350\u4f7f\u7528 apply \u65b9\u6cd5 linear = LinearFunction.apply \u5728\u8fd9\u91cc, \u6211\u4eec\u7ed9\u51fa\u4e86\u4e00\u4e2a\u7531\u975e\u53d8\u91cf\u53c2\u6570\u53c2\u6570\u5316\u7684\u51fd\u6570\u7684\u4f8b\u5b50 class MulConstant(Function): @staticmethod def forward(ctx, tensor, constant): # ctx is a context object that can be used to stash information # for backward computation ctx.constant = constant return tensor * constant @staticmethod def backward(ctx, grad_output): # We return as many input gradients as there were arguments. # Gradients of non-Tensor arguments to forward must be None. return grad_output * ctx.constant, None \u4f60\u53ef\u80fd\u60f3\u8981\u68c0\u6d4b\u4f60\u521a\u521a\u5b9e\u73b0\u7684 <cite>backward</cite> \u65b9\u6cd5\u662f\u5426\u6b63\u786e\u7684\u8ba1\u7b97\u4e86\u68af\u5ea6. \u4f60\u53ef\u4ee5\u4f7f\u7528\u5c0f\u800c\u6709\u9650\u7684\u5fae\u5206\u8fdb\u884c\u6570\u503c\u4f30\u8ba1 from torch.autograd import gradcheck # gradchek takes a tuple of tensor as input, check if your gradient # evaluated with these tensors are close enough to numerical # approximations and returns True if they all verify this condition. input = (Variable(torch.randn(20,20).double(), requires_grad=True), Variable(torch.randn(30,20).double(), requires_grad=True),) test = gradcheck(Linear.apply, input, eps=1e-6, atol=1e-4) print(test)","title":"\u6269\u5c55 torch.autograd \u6a21\u5757"},{"location":"49/#torchnn","text":"nn \u6a21\u5757\u6709\u4e24\u79cd\u7c7b\u578b\u7684\u63a5\u53e3 - modules \u548c their functional versions. \u4f60\u53ef\u4ee5\u7528\u4e24\u79cd\u65b9\u6cd5\u6269\u5c55\u5b83, \u4f46\u662f\u6211\u4eec\u63a8\u8350\u4f7f\u7528\u5404\u79cd\u5c42\u7684\u6a21\u5757, \u7528\u6765\u5b58\u653e\u4efb\u4f55 parameters(\u53c2\u6570) \u6216\u8005 buffers(\u7f13\u51b2), \u5e76\u4e14\u63a8\u8350\u4f7f\u7528\u4e00\u4e2a\u51fd\u6570\u5f62\u5f0f\u7684\u65e0\u53c2\u6570\u64cd\u4f5c, \u6bd4\u5982\u6fc0\u6d3b\u51fd\u6570, \u6c60\u5316\u7b49\u7b49. \u6dfb\u52a0\u64cd\u4f5c\u7684\u51fd\u6570\u7248\u672c\u5df2\u7ecf\u5728\u4e0a\u9762\u7684\u7ae0\u8282\u4e2d\u5b8c\u6574\u7684\u4ecb\u7ecd\u4e86.","title":"\u6269\u5c55 torch.nn \u6a21\u5757"},{"location":"49/#module","text":"\u7531\u4e8e nn \u6a21\u5757\u5927\u91cf\u7684\u5229\u7528\u4e86 autograd \u6a21\u5757, \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684 Module \u7c7b\u9700\u8981\u5b9e\u73b0\u4e00\u4e2a Function \u7c7b, \u5b83\u4f1a\u6267\u884c\u5bf9\u5e94\u7684\u64cd\u4f5c\u5e76\u4e14\u8ba1\u7b97\u68af\u5ea6. \u4ece\u73b0\u5728\u5f00\u59cb, \u5047\u8bbe\u6211\u4eec\u60f3\u8981\u5b9e\u73b0\u4e00\u4e2a Linear \u6a21\u5757, \u5e76\u4e14\u6211\u4eec\u5177\u6709\u5982\u4e0a\u6240\u5217\u5b9e\u73b0\u7684\u529f\u80fd. \u6709\u5f88\u5c11\u7684\u4ee3\u7801\u9700\u8981\u6dfb\u52a0\u8fd9\u4e2a. \u73b0\u5728\u6709\u4e24\u4e2a\u51fd\u6570\u9700\u8981\u5b9e\u73b0: __init__ ( optional ) - \u63a5\u6536\u8bf8\u5982 kernel sizes (\u6838\u5927\u5c0f) , numbers of features (\u7279\u5f81\u6570\u91cf) \u7b49\u53c2\u6570, \u5e76\u521d\u59cb\u5316 parameters(\u53c2\u6570) \u548c buffers(\u7f13\u51b2\u533a). forward() - \u5b9e\u4f8b\u5316\u4e00\u4e2a Function \u7c7b, \u5e76\u4e14\u7528\u4e8e\u6267\u884c\u64cd\u4f5c. \u8fd9\u4e0e\u4e0a\u9762\u7684 functional wrapper (\u51fd\u6570\u7684\u5305\u88c5) \u975e\u5e38\u76f8\u4f3c. \u8fd9\u5c31\u662f Linear \u6a21\u5757\u7684\u5b9e\u73b0\u65b9\u5f0f class Linear(nn.Module): def __init__(self, input_features, output_features, bias=True): super(Linear, self).__init__() self.input_features = input_features self.output_features = output_features # nn.Parameter is a special kind of Variable, that will get # automatically registered as Module's parameter once it's assigned # as an attribute. Parameters and buffers need to be registered, or # they won't appear in .parameters() (doesn't apply to buffers), and # won't be converted when e.g. .cuda() is called. You can use # .register_buffer() to register buffers. # nn.Parameters can never be volatile and, different than Variables, # they require gradients by default. self.weight = nn.Parameter(torch.Tensor(output_features, input_features)) if bias: self.bias = nn.Parameter(torch.Tensor(output_features)) else: # You should always register all possible parameters, but the # optional ones can be None if you want. self.register_parameter('bias', None) # Not a very smart way to initialize weights self.weight.data.uniform_(-0.1, 0.1) if bias is not None: self.bias.data.uniform_(-0.1, 0.1) def forward(self, input): # See the autograd section for explanation of what happens here. return LinearFunction.apply(input, self.weight, self.bias)","title":"\u6dfb\u52a0 Module \u7c7b"},{"location":"49/#c","text":"\u73b0\u5728\u4f60\u53ef\u4ee5\u5728 GitHub \u4e2d\u627e\u5230\u4e00\u4e9b\u4f8b\u5b50.","title":"\u7f16\u5199\u81ea\u5b9a\u4e49\u7684 C \u6269\u5c55"},{"location":"5/","text":"\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668 \u5c31\u662f\u8fd9\u4e2a, \u4f60\u5df2\u7ecf\u770b\u5230\u4e86\u5982\u4f55\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc, \u8ba1\u7b97\u635f\u5931\u5e76\u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd. \u73b0\u5728\u4f60\u53ef\u80fd\u4f1a\u60f3, \u6570\u636e\u5462? \u4e00\u822c\u6765\u8bf4, \u5f53\u4f60\u4e0d\u5f97\u4e0d\u5904\u7406\u56fe\u50cf, \u6587\u672c, \u97f3\u9891\u6216\u8005\u89c6\u9891\u6570\u636e\u65f6, \u4f60\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6\u7684 Python \u5305\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u4e00\u4e2a numpy \u6570\u7ec4\u4e2d. \u7136\u540e\u4f60\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u6570\u7ec4\u8f6c\u6362\u6210\u4e00\u4e2a torch.*Tensor . \u5bf9\u4e8e\u56fe\u50cf, \u4f1a\u7528\u5230\u7684\u5305\u6709 Pillow, OpenCV . \u5bf9\u4e8e\u97f3\u9891, \u4f1a\u7528\u7684\u5305\u6709 scipy \u548c librosa. \u5bf9\u4e8e\u6587\u672c, \u539f\u59cb Python \u6216\u57fa\u4e8e Cython \u7684\u52a0\u8f7d, \u6216\u8005 NLTK \u548c Spacy \u90fd\u662f\u6709\u7528\u7684. \u7279\u522b\u662f\u5bf9\u4e8e vision , \u6211\u4eec\u5df2\u7ecf\u521b\u5efa\u4e86\u4e00\u4e2a\u53eb\u505a torchvision , \u5176\u4e2d\u6709\u5bf9\u666e\u901a\u6570\u636e\u96c6\u5982 Imagenet, CIFAR10, MNIST \u7b49\u548c\u7528\u4e8e\u56fe\u50cf\u6570\u636e\u7684\u8f6c\u6362\u5668, \u5373 torchvision.datasets \u548c torch.utils.data.DataLoader . \u8fd9\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u4fbf\u5229, \u907f\u514d\u4e86\u7f16\u5199\u91cd\u590d\u4ee3\u7801. \u5728\u672c\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u4f7f\u7528 CIFAR10 \u6570\u636e\u96c6. \u5b83\u6709: \u2018airplane\u2019, \u2018automobile\u2019, \u2018bird\u2019, \u2018cat\u2019, \u2018deer\u2019,\u2018dog\u2019, \u2018frog\u2019, \u2018horse\u2019, \u2018ship\u2019, \u2018truck\u2019 \u8fd9\u4e9b\u7c7b\u522b. CIFAR10 \u4e2d\u7684\u56fe\u50cf\u5927\u5c0f\u4e3a 3x32x32 , \u5373 32x32 \u50cf\u7d20\u7684 3 \u901a\u9053\u5f69\u8272\u56fe\u50cf. cifar10 \u8bad\u7ec3\u4e00\u4e2a\u56fe\u50cf\u5206\u7c7b\u5668 \u6211\u4eec\u5c06\u6309\u987a\u5e8f\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4: \u52a0\u8f7d CIFAR10 \u6d4b\u8bd5\u548c\u8bad\u7ec3\u6570\u636e\u96c6\u5e76\u89c4\u8303\u5316 torchvision \u5b9a\u4e49\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc \u5b9a\u4e49\u4e00\u4e2a\u635f\u5931\u51fd\u6570 \u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8bad\u7ec3\u7f51\u7edc \u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u6d4b\u8bd5\u7f51\u7edc 1. \u52a0\u8f7d\u5e76\u89c4\u8303\u5316 CIFAR10 \u4f7f\u7528 torchvision , \u52a0\u8f7d CIFAR10 \u975e\u5e38\u7b80\u5355. import torch import torchvision import torchvision.transforms as transforms torchvision \u6570\u636e\u96c6\u7684\u8f93\u51fa\u662f\u8303\u56f4 [0, 1] \u7684 PILImage \u56fe\u50cf. \u6211\u4eec\u5c06\u5b83\u4eec\u8f6c\u6362\u4e3a\u5f52\u4e00\u5316\u8303\u56f4\u662f[-1,1]\u7684\u5f20\u91cf transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') \u8ba9\u6211\u4eec\u5c55\u793a\u4e00\u4e9b\u8bad\u7ec3\u56fe\u50cf, \u53ea\u662f\u4e3a\u4e86\u597d\u73a9 (0.0). import matplotlib.pyplot as plt import numpy as np # \u5b9a\u4e49\u51fd\u6570\u6765\u663e\u793a\u56fe\u50cf def imshow(img): img = img / 2 + 0.5 # \u975e\u6807\u51c6\u5316 npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) # \u5f97\u5230\u4e00\u4e9b\u968f\u673a\u7684\u8bad\u7ec3\u56fe\u50cf dataiter = iter(trainloader) images, labels = dataiter.next() # \u663e\u793a\u56fe\u50cf imshow(torchvision.utils.make_grid(images)) # \u8f93\u51fa\u7c7b\u522b print(' '.join('%5s' % classes[labels[j]] for j in range(4))) 2. \u5b9a\u4e49\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc \u4ece\u795e\u7ecf\u7f51\u7edc\u90e8\u5206\u590d\u5236\u795e\u7ecf\u7f51\u7edc, \u5e76\u4fee\u6539\u5b83\u4ee5\u83b7\u53d6 3 \u901a\u9053\u56fe\u50cf(\u800c\u4e0d\u662f\u5b9a\u4e49\u7684 1 \u901a\u9053\u56fe\u50cf). from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() 3. \u5b9a\u4e49\u4e00\u4e2a\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668 \u6211\u4eec\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570( CrossEntropyLoss )\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d( SGD )\u4f18\u5316\u5668. import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) 4. \u8bad\u7ec3\u7f51\u7edc \u8fd9\u662f\u4e8b\u60c5\u5f00\u59cb\u53d8\u5f97\u6709\u8da3\u7684\u65f6\u5019. \u6211\u4eec\u53ea\u9700\u5faa\u73af\u904d\u5386\u6570\u636e\u8fed\u4ee3\u5668, \u5e76\u5c06\u8f93\u5165\u63d0\u4f9b\u7ed9\u7f51\u7edc\u548c\u4f18\u5316\u5668. for epoch in range(2): # \u5faa\u73af\u904d\u5386\u6570\u636e\u96c6\u591a\u6b21 running_loss = 0.0 for i, data in enumerate(trainloader, 0): # \u5f97\u5230\u8f93\u5165\u6570\u636e inputs, labels = data # \u5305\u88c5\u6570\u636e inputs, labels = Variable(inputs), Variable(labels) # \u68af\u5ea6\u6e05\u96f6 optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # \u6253\u5370\u4fe1\u606f running_loss += loss.data[0] if i % 2000 == 1999: # \u6bcf2000\u4e2a\u5c0f\u6279\u91cf\u6253\u5370\u4e00\u6b21 print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training') 5. \u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u6d4b\u8bd5\u7f51\u7edc \u6211\u4eec\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e862\u904d\u7f51\u7edc, \u4f46\u662f\u6211\u4eec\u9700\u8981\u68c0\u67e5\u7f51\u7edc\u662f\u5426\u5b66\u5230\u4e86\u4ec0\u4e48. \u6211\u4eec\u5c06\u901a\u8fc7\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u7684\u7c7b\u6807\u7b7e\u6765\u68c0\u67e5\u8fd9\u4e2a\u95ee\u9898, \u5e76\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u68c0\u67e5. \u5982\u679c\u9884\u6d4b\u662f\u6b63\u786e\u7684, \u6211\u4eec\u5c06\u6837\u672c\u6dfb\u52a0\u5230\u6b63\u786e\u9884\u6d4b\u7684\u5217\u8868\u4e2d. \u597d\u7684, \u7b2c\u4e00\u6b65. \u8ba9\u6211\u4eec\u663e\u793a\u6d4b\u8bd5\u96c6\u4e2d\u7684\u56fe\u50cf\u4ee5\u4fbf\u719f\u6089. dataiter = iter(testloader) images, labels = dataiter.next() # \u6253\u5370\u56fe\u50cf imshow(torchvision.utils.make_grid(images)) print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4))) \u597d\u7684, \u73b0\u5728\u8ba9\u6211\u4eec\u770b\u770b\u795e\u7ecf\u7f51\u7edc\u8ba4\u4e3a\u8fd9\u4e9b\u4f8b\u5b50\u662f\u4ec0\u4e48: outputs = net(Variable(images)) \u8f93\u51fa\u7684\u662f10\u4e2a\u7c7b\u522b\u7684\u80fd\u91cf. \u4e00\u4e2a\u7c7b\u522b\u7684\u80fd\u91cf\u8d8a\u9ad8, \u5219\u53ef\u4ee5\u7406\u89e3\u4e3a\u7f51\u7edc\u8ba4\u4e3a\u8d8a\u591a\u7684\u56fe\u50cf\u662f\u8be5\u7c7b\u522b\u7684. \u90a3\u4e48, \u8ba9\u6211\u4eec\u5f97\u5230\u6700\u9ad8\u80fd\u91cf\u7684\u7d22\u5f15: _, predicted = torch.max(outputs.data, 1) print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4))) \u7ed3\u679c\u770b\u8d77\u6765\u4e0d\u9519. \u8ba9\u6211\u4eec\u770b\u770b\u7f51\u7edc\u5982\u4f55\u5728\u6574\u4e2a\u6570\u636e\u96c6\u4e0a\u6267\u884c. correct = 0 total = 0 for data in testloader: images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum() print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) \u8bad\u7ec3\u7684\u51c6\u786e\u7387\u8fdc\u6bd4\u968f\u673a\u731c\u6d4b(\u51c6\u786e\u738710%)\u597d, \u8bc1\u660e\u7f51\u7edc\u786e\u5b9e\u5b66\u5230\u4e86\u4e1c\u897f. \u55ef, \u6211\u4eec\u6765\u770b\u770b\u54ea\u4e9b\u7c7b\u522b\u8868\u73b0\u826f\u597d, \u54ea\u4e9b\u7c7b\u522b\u8868\u73b0\u4e0d\u4f73: class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) for data in testloader: images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i] class_total[label] += 1 for i in range(10): print('Accuracy of %5s : %2d %%' % ( classes[i], 100 * class_correct[i] / class_total[i])) \u597d\u7684, \u63a5\u4e0b\u6765\u5462? \u6211\u4eec\u5982\u4f55\u5728 GPU \u4e0a\u8fd0\u884c\u8fd9\u4e9b\u795e\u7ecf\u7f51\u7edc? \u5728 GPU \u4e0a\u8bad\u7ec3 \u5c31\u50cf\u4f60\u5982\u4f55\u5c06\u4e00\u4e2a\u5f20\u91cf\u4f20\u9012\u7ed9GPU\u4e00\u6837, \u4f60\u5c06\u795e\u7ecf\u7f51\u7edc\u8f6c\u79fb\u5230GPU\u4e0a. \u8fd9\u5c06\u9012\u5f52\u904d\u5386\u6240\u6709\u6a21\u5757, \u5e76\u5c06\u5176\u53c2\u6570\u548c\u7f13\u51b2\u533a\u8f6c\u6362\u4e3aCUDA\u5f20\u91cf: net.cuda() \u8bf7\u8bb0\u4f4f, \u60a8\u5fc5\u987b\u5c06\u8f93\u5165\u548c\u76ee\u6807\u6bcf\u4e00\u6b65\u90fd\u53d1\u9001\u5230GPU: inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda()) \u5982\u679c\u53d1\u73b0\u5728 GPU \u4e0a\u5e76\u6ca1\u6709\u6bd4 CPU \u63d0\u901f\u5f88\u591a, \u5b9e\u9645\u4e0a\u662f\u56e0\u4e3a\u7f51\u7edc\u6bd4\u8f83\u5c0f, GPU \u6ca1\u6709\u5b8c\u5168\u53d1\u6325\u81ea\u5df1\u7684\u771f\u6b63\u5b9e\u529b. \u7ec3\u4e60: \u5c1d\u8bd5\u589e\u52a0\u7f51\u7edc\u7684\u5bbd\u5ea6(\u7b2c\u4e00\u4e2a nn.Conv2d \u7684\u53c2\u65702\u548c\u7b2c\u4e8c\u4e2a nn.Conv2d \u7684\u53c2\u65701 \u5b83\u4eec\u9700\u8981\u662f\u76f8\u540c\u7684\u6570\u5b57), \u770b\u770b\u4f60\u5f97\u5230\u4ec0\u4e48\u6837\u7684\u52a0\u901f. \u76ee\u6807\u8fbe\u6210 : \u6df1\u5165\u4e86\u89e3PyTorch\u7684\u5f20\u91cf\u5e93\u548c\u795e\u7ecf\u7f51\u7edc. \u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684\u795e\u7ecf\u7f51\u7edc\u6765\u5206\u7c7b\u56fe\u50cf. \u5728\u591a\u4e2aGPU\u4e0a\u8fdb\u884c\u8bad\u7ec3 \u5982\u679c\u4f60\u5e0c\u671b\u4f7f\u7528\u6240\u6709 GPU \u6765\u770b\u66f4\u591a\u7684 MASSIVE \u52a0\u901f, \u8bf7\u67e5\u770b\u53ef\u9009 \u53ef\u9009: \u6570\u636e\u5e76\u884c . \u6211\u4e0b\u4e00\u6b65\u53bb\u54ea\u91cc? \u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u73a9\u7535\u5b50\u6e38\u620f <cite>\u5728 imagenet \u4e0a\u57f9\u8bad\u6700\u5148\u8fdb\u7684 ResNet \u7f51\u7edc</cite> <cite>\u5229\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u8bad\u7ec3\u4eba\u8138\u751f\u6210\u5668</cite> <cite>\u4f7f\u7528 Recurrent LSTM \u7f51\u7edc\u8bad\u7ec3\u5355\u8bcd\u8bed\u8a00\u6a21\u578b</cite> <cite>\u66f4\u591a\u7684\u4f8b\u5b50</cite> <cite>\u66f4\u591a\u6559\u7a0b</cite> <cite>\u5728\u8bba\u575b\u4e0a\u8ba8\u8bba PyTorch</cite> <cite>\u4e0e Slack \u4e0a\u4e0e\u5176\u4ed6\u7528\u6237\u804a\u5929</cite> Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: cifar10_tutorial.py Download Jupyter notebook: cifar10_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668"},{"location":"5/#_1","text":"\u5c31\u662f\u8fd9\u4e2a, \u4f60\u5df2\u7ecf\u770b\u5230\u4e86\u5982\u4f55\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc, \u8ba1\u7b97\u635f\u5931\u5e76\u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd. \u73b0\u5728\u4f60\u53ef\u80fd\u4f1a\u60f3,","title":"\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668"},{"location":"5/#_2","text":"\u4e00\u822c\u6765\u8bf4, \u5f53\u4f60\u4e0d\u5f97\u4e0d\u5904\u7406\u56fe\u50cf, \u6587\u672c, \u97f3\u9891\u6216\u8005\u89c6\u9891\u6570\u636e\u65f6, \u4f60\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6\u7684 Python \u5305\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u4e00\u4e2a numpy \u6570\u7ec4\u4e2d. \u7136\u540e\u4f60\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u6570\u7ec4\u8f6c\u6362\u6210\u4e00\u4e2a torch.*Tensor . \u5bf9\u4e8e\u56fe\u50cf, \u4f1a\u7528\u5230\u7684\u5305\u6709 Pillow, OpenCV . \u5bf9\u4e8e\u97f3\u9891, \u4f1a\u7528\u7684\u5305\u6709 scipy \u548c librosa. \u5bf9\u4e8e\u6587\u672c, \u539f\u59cb Python \u6216\u57fa\u4e8e Cython \u7684\u52a0\u8f7d, \u6216\u8005 NLTK \u548c Spacy \u90fd\u662f\u6709\u7528\u7684. \u7279\u522b\u662f\u5bf9\u4e8e vision , \u6211\u4eec\u5df2\u7ecf\u521b\u5efa\u4e86\u4e00\u4e2a\u53eb\u505a torchvision , \u5176\u4e2d\u6709\u5bf9\u666e\u901a\u6570\u636e\u96c6\u5982 Imagenet, CIFAR10, MNIST \u7b49\u548c\u7528\u4e8e\u56fe\u50cf\u6570\u636e\u7684\u8f6c\u6362\u5668, \u5373 torchvision.datasets \u548c torch.utils.data.DataLoader . \u8fd9\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u4fbf\u5229, \u907f\u514d\u4e86\u7f16\u5199\u91cd\u590d\u4ee3\u7801. \u5728\u672c\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u4f7f\u7528 CIFAR10 \u6570\u636e\u96c6. \u5b83\u6709: \u2018airplane\u2019, \u2018automobile\u2019, \u2018bird\u2019, \u2018cat\u2019, \u2018deer\u2019,\u2018dog\u2019, \u2018frog\u2019, \u2018horse\u2019, \u2018ship\u2019, \u2018truck\u2019 \u8fd9\u4e9b\u7c7b\u522b. CIFAR10 \u4e2d\u7684\u56fe\u50cf\u5927\u5c0f\u4e3a 3x32x32 , \u5373 32x32 \u50cf\u7d20\u7684 3 \u901a\u9053\u5f69\u8272\u56fe\u50cf. cifar10","title":"\u6570\u636e\u5462?"},{"location":"5/#_3","text":"\u6211\u4eec\u5c06\u6309\u987a\u5e8f\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4: \u52a0\u8f7d CIFAR10 \u6d4b\u8bd5\u548c\u8bad\u7ec3\u6570\u636e\u96c6\u5e76\u89c4\u8303\u5316 torchvision \u5b9a\u4e49\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc \u5b9a\u4e49\u4e00\u4e2a\u635f\u5931\u51fd\u6570 \u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8bad\u7ec3\u7f51\u7edc \u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u6d4b\u8bd5\u7f51\u7edc","title":"\u8bad\u7ec3\u4e00\u4e2a\u56fe\u50cf\u5206\u7c7b\u5668"},{"location":"5/#146-cifar10","text":"\u4f7f\u7528 torchvision , \u52a0\u8f7d CIFAR10 \u975e\u5e38\u7b80\u5355. import torch import torchvision import torchvision.transforms as transforms torchvision \u6570\u636e\u96c6\u7684\u8f93\u51fa\u662f\u8303\u56f4 [0, 1] \u7684 PILImage \u56fe\u50cf. \u6211\u4eec\u5c06\u5b83\u4eec\u8f6c\u6362\u4e3a\u5f52\u4e00\u5316\u8303\u56f4\u662f[-1,1]\u7684\u5f20\u91cf transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') \u8ba9\u6211\u4eec\u5c55\u793a\u4e00\u4e9b\u8bad\u7ec3\u56fe\u50cf, \u53ea\u662f\u4e3a\u4e86\u597d\u73a9 (0.0). import matplotlib.pyplot as plt import numpy as np # \u5b9a\u4e49\u51fd\u6570\u6765\u663e\u793a\u56fe\u50cf def imshow(img): img = img / 2 + 0.5 # \u975e\u6807\u51c6\u5316 npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) # \u5f97\u5230\u4e00\u4e9b\u968f\u673a\u7684\u8bad\u7ec3\u56fe\u50cf dataiter = iter(trainloader) images, labels = dataiter.next() # \u663e\u793a\u56fe\u50cf imshow(torchvision.utils.make_grid(images)) # \u8f93\u51fa\u7c7b\u522b print(' '.join('%5s' % classes[labels[j]] for j in range(4)))","title":"1. \u52a0\u8f7d\u5e76\u89c4\u8303\u5316 CIFAR10"},{"location":"5/#246","text":"\u4ece\u795e\u7ecf\u7f51\u7edc\u90e8\u5206\u590d\u5236\u795e\u7ecf\u7f51\u7edc, \u5e76\u4fee\u6539\u5b83\u4ee5\u83b7\u53d6 3 \u901a\u9053\u56fe\u50cf(\u800c\u4e0d\u662f\u5b9a\u4e49\u7684 1 \u901a\u9053\u56fe\u50cf). from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net()","title":"2. \u5b9a\u4e49\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"},{"location":"5/#346","text":"\u6211\u4eec\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570( CrossEntropyLoss )\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d( SGD )\u4f18\u5316\u5668. import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)","title":"3. \u5b9a\u4e49\u4e00\u4e2a\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668"},{"location":"5/#446","text":"\u8fd9\u662f\u4e8b\u60c5\u5f00\u59cb\u53d8\u5f97\u6709\u8da3\u7684\u65f6\u5019. \u6211\u4eec\u53ea\u9700\u5faa\u73af\u904d\u5386\u6570\u636e\u8fed\u4ee3\u5668, \u5e76\u5c06\u8f93\u5165\u63d0\u4f9b\u7ed9\u7f51\u7edc\u548c\u4f18\u5316\u5668. for epoch in range(2): # \u5faa\u73af\u904d\u5386\u6570\u636e\u96c6\u591a\u6b21 running_loss = 0.0 for i, data in enumerate(trainloader, 0): # \u5f97\u5230\u8f93\u5165\u6570\u636e inputs, labels = data # \u5305\u88c5\u6570\u636e inputs, labels = Variable(inputs), Variable(labels) # \u68af\u5ea6\u6e05\u96f6 optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # \u6253\u5370\u4fe1\u606f running_loss += loss.data[0] if i % 2000 == 1999: # \u6bcf2000\u4e2a\u5c0f\u6279\u91cf\u6253\u5370\u4e00\u6b21 print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training')","title":"4. \u8bad\u7ec3\u7f51\u7edc"},{"location":"5/#546","text":"\u6211\u4eec\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e862\u904d\u7f51\u7edc, \u4f46\u662f\u6211\u4eec\u9700\u8981\u68c0\u67e5\u7f51\u7edc\u662f\u5426\u5b66\u5230\u4e86\u4ec0\u4e48. \u6211\u4eec\u5c06\u901a\u8fc7\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u7684\u7c7b\u6807\u7b7e\u6765\u68c0\u67e5\u8fd9\u4e2a\u95ee\u9898, \u5e76\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u68c0\u67e5. \u5982\u679c\u9884\u6d4b\u662f\u6b63\u786e\u7684, \u6211\u4eec\u5c06\u6837\u672c\u6dfb\u52a0\u5230\u6b63\u786e\u9884\u6d4b\u7684\u5217\u8868\u4e2d. \u597d\u7684, \u7b2c\u4e00\u6b65. \u8ba9\u6211\u4eec\u663e\u793a\u6d4b\u8bd5\u96c6\u4e2d\u7684\u56fe\u50cf\u4ee5\u4fbf\u719f\u6089. dataiter = iter(testloader) images, labels = dataiter.next() # \u6253\u5370\u56fe\u50cf imshow(torchvision.utils.make_grid(images)) print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4))) \u597d\u7684, \u73b0\u5728\u8ba9\u6211\u4eec\u770b\u770b\u795e\u7ecf\u7f51\u7edc\u8ba4\u4e3a\u8fd9\u4e9b\u4f8b\u5b50\u662f\u4ec0\u4e48: outputs = net(Variable(images)) \u8f93\u51fa\u7684\u662f10\u4e2a\u7c7b\u522b\u7684\u80fd\u91cf. \u4e00\u4e2a\u7c7b\u522b\u7684\u80fd\u91cf\u8d8a\u9ad8, \u5219\u53ef\u4ee5\u7406\u89e3\u4e3a\u7f51\u7edc\u8ba4\u4e3a\u8d8a\u591a\u7684\u56fe\u50cf\u662f\u8be5\u7c7b\u522b\u7684. \u90a3\u4e48, \u8ba9\u6211\u4eec\u5f97\u5230\u6700\u9ad8\u80fd\u91cf\u7684\u7d22\u5f15: _, predicted = torch.max(outputs.data, 1) print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4))) \u7ed3\u679c\u770b\u8d77\u6765\u4e0d\u9519. \u8ba9\u6211\u4eec\u770b\u770b\u7f51\u7edc\u5982\u4f55\u5728\u6574\u4e2a\u6570\u636e\u96c6\u4e0a\u6267\u884c. correct = 0 total = 0 for data in testloader: images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum() print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) \u8bad\u7ec3\u7684\u51c6\u786e\u7387\u8fdc\u6bd4\u968f\u673a\u731c\u6d4b(\u51c6\u786e\u738710%)\u597d, \u8bc1\u660e\u7f51\u7edc\u786e\u5b9e\u5b66\u5230\u4e86\u4e1c\u897f. \u55ef, \u6211\u4eec\u6765\u770b\u770b\u54ea\u4e9b\u7c7b\u522b\u8868\u73b0\u826f\u597d, \u54ea\u4e9b\u7c7b\u522b\u8868\u73b0\u4e0d\u4f73: class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) for data in testloader: images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i] class_total[label] += 1 for i in range(10): print('Accuracy of %5s : %2d %%' % ( classes[i], 100 * class_correct[i] / class_total[i])) \u597d\u7684, \u63a5\u4e0b\u6765\u5462? \u6211\u4eec\u5982\u4f55\u5728 GPU \u4e0a\u8fd0\u884c\u8fd9\u4e9b\u795e\u7ecf\u7f51\u7edc?","title":"5. \u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u6d4b\u8bd5\u7f51\u7edc"},{"location":"5/#gpu","text":"\u5c31\u50cf\u4f60\u5982\u4f55\u5c06\u4e00\u4e2a\u5f20\u91cf\u4f20\u9012\u7ed9GPU\u4e00\u6837, \u4f60\u5c06\u795e\u7ecf\u7f51\u7edc\u8f6c\u79fb\u5230GPU\u4e0a. \u8fd9\u5c06\u9012\u5f52\u904d\u5386\u6240\u6709\u6a21\u5757, \u5e76\u5c06\u5176\u53c2\u6570\u548c\u7f13\u51b2\u533a\u8f6c\u6362\u4e3aCUDA\u5f20\u91cf: net.cuda() \u8bf7\u8bb0\u4f4f, \u60a8\u5fc5\u987b\u5c06\u8f93\u5165\u548c\u76ee\u6807\u6bcf\u4e00\u6b65\u90fd\u53d1\u9001\u5230GPU: inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda()) \u5982\u679c\u53d1\u73b0\u5728 GPU \u4e0a\u5e76\u6ca1\u6709\u6bd4 CPU \u63d0\u901f\u5f88\u591a, \u5b9e\u9645\u4e0a\u662f\u56e0\u4e3a\u7f51\u7edc\u6bd4\u8f83\u5c0f, GPU \u6ca1\u6709\u5b8c\u5168\u53d1\u6325\u81ea\u5df1\u7684\u771f\u6b63\u5b9e\u529b. \u7ec3\u4e60: \u5c1d\u8bd5\u589e\u52a0\u7f51\u7edc\u7684\u5bbd\u5ea6(\u7b2c\u4e00\u4e2a nn.Conv2d \u7684\u53c2\u65702\u548c\u7b2c\u4e8c\u4e2a nn.Conv2d \u7684\u53c2\u65701 \u5b83\u4eec\u9700\u8981\u662f\u76f8\u540c\u7684\u6570\u5b57), \u770b\u770b\u4f60\u5f97\u5230\u4ec0\u4e48\u6837\u7684\u52a0\u901f. \u76ee\u6807\u8fbe\u6210 : \u6df1\u5165\u4e86\u89e3PyTorch\u7684\u5f20\u91cf\u5e93\u548c\u795e\u7ecf\u7f51\u7edc. \u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684\u795e\u7ecf\u7f51\u7edc\u6765\u5206\u7c7b\u56fe\u50cf.","title":"\u5728 GPU \u4e0a\u8bad\u7ec3"},{"location":"5/#gpu_1","text":"\u5982\u679c\u4f60\u5e0c\u671b\u4f7f\u7528\u6240\u6709 GPU \u6765\u770b\u66f4\u591a\u7684 MASSIVE \u52a0\u901f, \u8bf7\u67e5\u770b\u53ef\u9009 \u53ef\u9009: \u6570\u636e\u5e76\u884c .","title":"\u5728\u591a\u4e2aGPU\u4e0a\u8fdb\u884c\u8bad\u7ec3"},{"location":"5/#_4","text":"\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u73a9\u7535\u5b50\u6e38\u620f <cite>\u5728 imagenet \u4e0a\u57f9\u8bad\u6700\u5148\u8fdb\u7684 ResNet \u7f51\u7edc</cite> <cite>\u5229\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u8bad\u7ec3\u4eba\u8138\u751f\u6210\u5668</cite> <cite>\u4f7f\u7528 Recurrent LSTM \u7f51\u7edc\u8bad\u7ec3\u5355\u8bcd\u8bed\u8a00\u6a21\u578b</cite> <cite>\u66f4\u591a\u7684\u4f8b\u5b50</cite> <cite>\u66f4\u591a\u6559\u7a0b</cite> <cite>\u5728\u8bba\u575b\u4e0a\u8ba8\u8bba PyTorch</cite> <cite>\u4e0e Slack \u4e0a\u4e0e\u5176\u4ed6\u7528\u6237\u804a\u5929</cite> Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: cifar10_tutorial.py Download Jupyter notebook: cifar10_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u6211\u4e0b\u4e00\u6b65\u53bb\u54ea\u91cc?"},{"location":"50/","text":"\u591a\u8fdb\u7a0b\u7684\u6700\u4f73\u5b9e\u8df5 torch.multiprocessing \u662f Python \u4e2d multiprocessing \u6a21\u5757\u7684\u66ff\u4ee3. \u5b83\u652f\u6301\u5b8c\u5168\u76f8\u540c\u7684\u64cd\u4f5c, \u4f46\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86\u5b83\u7684\u529f\u80fd, \u4f7f\u5f97\u6240\u6709\u5f20\u91cf\u53ef\u4ee5\u901a\u8fc7 multiprocessing.Queue \u4f20\u8f93, \u5c06\u5176\u6570\u636e\u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58\u4e2d, \u5e76\u4e14\u53ea\u4f1a\u5411\u5176\u4ed6\u8fdb\u7a0b\u53d1\u9001\u4e00\u4e2a\u53e5\u67c4. Note \u5f53\u4e00\u4e2a Variable \u88ab\u53d1\u9001\u5230\u53e6\u4e00\u4e2a\u8fdb\u7a0b\u4e2d, Variable.data \u548c Variable.grad.data \u90fd\u5c06\u88ab\u5171\u4eab. \u8fd9\u91cc\u5141\u8bb8\u5b9e\u73b0\u5404\u79cd\u8bad\u7ec3\u65b9\u6cd5, \u4f8b\u5982 Hogwild, A3C, \u6216\u8005\u5176\u4ed6\u9700\u8981\u5f02\u6b65\u64cd\u4f5c\u7684\u65b9\u6cd5. \u5171\u4eab CUDA \u5411\u91cf \u53ea\u6709 Python 3 \u652f\u6301\u4f7f\u7528 spawn \u6216 forkserver \u542f\u52a8\u65b9\u6cd5\u5728\u8fdb\u7a0b\u4e2d\u5171\u4eab CUDA \u5411\u91cf. multiprocessing \u5728 Python 2 \u4f7f\u7528 fork \u53ea\u80fd\u521b\u5efa\u5b50\u8fdb\u7a0b, \u4f46\u662f\u5728 CUDA \u8fd0\u884c\u65f6\u4e0d\u88ab\u652f\u6301. Warning CUDA API \u8981\u6c42\u88ab\u5bfc\u51fa\u5230\u5176\u4ed6\u8fdb\u7a0b\u7684\u5206\u914d\u53ea\u8981\u88ab\u4f7f\u7528, \u5c31\u8981\u4e00\u76f4\u4fdd\u6301\u6709\u6548. \u60a8\u5e94\u8be5\u5c0f\u5fc3, \u786e\u4fdd\u60a8\u5171\u4eab\u7684CUDA\u5f20\u91cf\u53ea\u8981\u6709\u5fc5\u8981\u5c31\u4e0d\u8981\u8d85\u51fa\u8303\u56f4. \u8fd9\u4e0d\u662f\u5171\u4eab\u6a21\u578b\u53c2\u6570\u7684\u95ee\u9898, \u4f46\u4f20\u9012\u5176\u4ed6\u7c7b\u578b\u7684\u6570\u636e\u5e94\u8be5\u5c0f\u5fc3. \u6ce8\u610f, \u6b64\u9650\u5236\u4e0d\u9002\u7528\u4e8e\u5171\u4eab CPU \u5185\u5b58. \u53c2\u8003: \u4f7f\u7528 nn.DataParallel \u66ff\u4ee3 multiprocessing \u6700\u4f73\u5b9e\u8df5\u548c\u63d0\u793a \u907f\u514d\u548c\u62b5\u5236\u6b7b\u9501 \u5f53\u65b0\u8fdb\u7a0b\u88ab\u521b\u5efa\u65f6, \u53ef\u80fd\u4f1a\u53d1\u751f\u5f88\u591a\u9519\u8bef, \u6700\u5e38\u89c1\u7684\u539f\u56e0\u5c31\u662f\u540e\u53f0\u7ebf\u7a0b. \u5982\u679c\u6709\u4efb\u4f55\u7ebf\u7a0b\u6301\u6709\u9501\u6216\u5bfc\u5165\u6a21\u5757, \u5e76\u4e14 fork \u5df2\u88ab\u8c03\u7528, \u5219\u5b50\u8fdb\u7a0b\u5f88\u6709\u53ef\u80fd\u5c06\u4f1a\u5904\u4e8e\u6bc1\u574f\u7684\u72b6\u6001, \u5e76\u5bfc\u81f4\u6b7b\u9501\u6216\u5728\u5176\u4ed6\u5730\u65b9\u5931\u8d25. \u6ce8\u610f\u5373\u4f7f\u4f60\u81ea\u5df1\u6ca1\u6709\u8fd9\u6837\u505a, Python \u5185\u7f6e\u7684\u5e93\u4e5f\u4f1a\u8fd9\u6837\u505a - \u4e0d\u9700\u8981\u6bd4 multiprocessing \u770b\u5f97\u66f4\u8fdc. multiprocessing.Queue \u4e8b\u5b9e\u4e0a\u662f\u4e00\u4e2a\u975e\u5e38\u590d\u6742\u7684\u5e93, \u5b83\u53ef\u4ee5\u521b\u5efa\u591a\u4e2a\u7ebf\u7a0b, \u7528\u4e8e\u5e8f\u5217\u5316, \u53d1\u9001\u548c\u63a5\u6536\u5bf9\u8c61, \u4f46\u662f\u5b83\u4eec\u4e5f\u6709\u53ef\u80fd\u5f15\u8d77\u524d\u9762\u63d0\u5230\u7684\u95ee\u9898. \u5982\u679c\u4f60\u9047\u5230\u8fd9\u6837\u7684\u95ee\u9898, \u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528 multiprocessing.queues.SimpleQueue , \u5b83\u4e0d\u4f1a\u4f7f\u7528\u5176\u4ed6\u989d\u5916\u7684\u7ebf\u7a0b. \u6211\u4eec\u6b63\u5728\u7aed\u5c3d\u5168\u529b\u628a\u5b83\u8bbe\u8ba1\u5f97\u66f4\u7b80\u5355, \u5e76\u786e\u4fdd\u8fd9\u4e9b\u6b7b\u9501\u4e0d\u4f1a\u53d1\u751f, \u4f46\u6709\u4e9b\u4e8b\u60c5\u65e0\u6cd5\u63a7\u5236. \u5982\u679c\u6709\u4efb\u4f55\u95ee\u9898\u60a8\u4e00\u65f6\u65e0\u6cd5\u89e3\u51b3, \u8bf7\u5c1d\u8bd5\u5728\u8bba\u575b\u4e0a\u63d0\u51fa, \u6211\u4eec\u5c06\u770b\u770b\u662f\u5426\u53ef\u4ee5\u89e3\u51b3. \u91cd\u7528\u7ecf\u8fc7\u961f\u5217\u7684\u7f13\u51b2\u533a \u8bf7\u8bb0\u4f4f\u5f53\u6bcf\u6b21\u5c06 Tensor \u653e\u5165 multiprocessing.Queue , \u5b83\u5fc5\u987b\u88ab\u79fb\u81f3\u5171\u4eab\u5185\u5b58\u4e2d. \u5982\u679c\u5b83\u5df2\u7ecf\u88ab\u5171\u4eab, \u5b83\u662f\u4e00\u4e2a\u65e0\u6548\u64cd\u4f5c, \u5426\u5219\u4f1a\u4ea7\u751f\u4e00\u4e2a\u989d\u5916\u7684\u5185\u5b58\u526f\u672c, \u8fd9\u4f1a\u51cf\u7f13\u6574\u4e2a\u8fdb\u7a0b. \u5373\u4f7f\u4f60\u6709\u4e00\u4e2a\u8fdb\u7a0b\u6c60\u6765\u53d1\u9001\u6570\u636e\u5230\u4e00\u4e2a\u8fdb\u7a0b, \u4e5f\u5e94\u8be5\u5148\u628a\u5b83\u9001\u56de\u7f13\u51b2\u533a \u2014\u2014 \u8fd9\u51e0\u4e4e\u662f\u6ca1\u6709\u635f\u5931\u7684, \u5e76\u4e14\u5141\u8bb8\u4f60\u5728\u53d1\u9001\u4e0b\u4e00\u4e2a batch \u65f6\u907f\u514d\u4ea7\u751f\u526f\u672c. \u5f02\u6b65\u591a\u8fdb\u7a0b\u8bad\u7ec3 (\u4f8b\u5982 Hogwild) \u4f7f\u7528 torch.multiprocessing \u53ef\u4ee5\u5f02\u6b65\u5730\u8bad\u7ec3\u6a21\u578b, \u5176\u4e2d\u53c2\u6570\u53ef\u4ee5\u4e00\u76f4\u5171\u4eab, \u6216\u5b9a\u671f\u540c\u6b65. \u5bf9\u4e8e\u7b2c\u4e00\u79cd\u60c5\u51b5, \u6211\u4eec\u5efa\u8bae\u4f20\u8f93\u6574\u4e2a\u6a21\u578b\u5bf9\u8c61, \u800c\u5bf9\u4e8e\u7b2c\u4e8c\u79cd\u60c5\u51b5, \u6211\u4eec\u5efa\u8bae\u53ea\u4f20\u8f93 state_dict() . \u6211\u4eec\u5efa\u8bae\u4f7f\u7528 multiprocessing.Queue \u6765\u5728\u8fdb\u7a0b\u4e4b\u95f4\u4f20\u8f93\u5404\u79cd PyTorch \u5bf9\u8c61. \u4f8b\u5982, \u5f53\u4f7f\u7528 fork \u542f\u52a8\u65b9\u6cd5, \u6709\u53ef\u80fd\u4f1a\u7ee7\u627f\u5171\u4eab\u5185\u5b58\u4e2d\u7684\u5f20\u91cf\u548c\u5b58\u50a8\u91cf. \u4f46\u8fd9\u662f\u975e\u5e38\u5bb9\u6613\u51fa\u9519\u7684, \u5e94\u8c28\u614e\u4f7f\u7528, \u6700\u597d\u662f\u6210\u4e3a\u6df1\u5ea6\u7528\u6237\u4ee5\u540e, \u518d\u4f7f\u7528\u8fd9\u4e2a\u65b9\u6cd5. \u961f\u5217\u867d\u7136\u6709\u65f6\u662f\u4e00\u4e2a\u8f83\u4e0d\u4f18\u96c5\u7684\u89e3\u51b3\u65b9\u6848, \u4f46\u57fa\u672c\u4e0a\u80fd\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u6b63\u5e38\u5de5\u4f5c. Warning \u5f53\u4f7f\u7528\u5168\u5c40\u7684\u58f0\u660e\u65f6, \u4f60\u5e94\u8be5\u6ce8\u610f, \u56e0\u4e3a\u5b83\u4eec\u6ca1\u6709\u88ab if __name__ == '__main__' \u9650\u5236. \u5982\u679c\u4f7f\u7528\u4e0e fork \u4e0d\u540c\u7684\u542f\u52a8\u65b9\u6cd5, \u5b83\u4eec\u5c06\u5728\u6240\u6709\u5b50\u8fdb\u7a0b\u4e2d\u88ab\u6267\u884c. Hogwild \u4e00\u4e2a Hogwild \u7684\u5177\u4f53\u5b9e\u73b0\u53ef\u4ee5\u5728 examples repository \u4e2d\u627e\u5230. \u4e3a\u4e86\u5c55\u793a\u4ee3\u7801\u7684\u6574\u4f53\u7ed3\u6784, \u4e0b\u9762\u6709\u4e00\u4e2a\u5c0f\u4f8b\u5b50: import torch.multiprocessing as mp from model import MyModel def train(model): # Construct data_loader, optimizer, etc. for data, labels in data_loader: optimizer.zero_grad() loss_fn(model(data), labels).backward() optimizer.step() # This will update the shared parameters if __name__ == '__main__': num_processes = 4 model = MyModel() # NOTE: this is required for the ``fork`` method to work model.share_memory() processes = [] for rank in range(num_processes): p = mp.Process(target=train, args=(model,)) p.start() processes.append(p) for p in processes: p.join()","title":"\u591a\u8fdb\u7a0b\u7684\u6700\u4f73\u5b9e\u8df5"},{"location":"50/#_1","text":"torch.multiprocessing \u662f Python \u4e2d multiprocessing \u6a21\u5757\u7684\u66ff\u4ee3. \u5b83\u652f\u6301\u5b8c\u5168\u76f8\u540c\u7684\u64cd\u4f5c, \u4f46\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86\u5b83\u7684\u529f\u80fd, \u4f7f\u5f97\u6240\u6709\u5f20\u91cf\u53ef\u4ee5\u901a\u8fc7 multiprocessing.Queue \u4f20\u8f93, \u5c06\u5176\u6570\u636e\u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58\u4e2d, \u5e76\u4e14\u53ea\u4f1a\u5411\u5176\u4ed6\u8fdb\u7a0b\u53d1\u9001\u4e00\u4e2a\u53e5\u67c4. Note \u5f53\u4e00\u4e2a Variable \u88ab\u53d1\u9001\u5230\u53e6\u4e00\u4e2a\u8fdb\u7a0b\u4e2d, Variable.data \u548c Variable.grad.data \u90fd\u5c06\u88ab\u5171\u4eab. \u8fd9\u91cc\u5141\u8bb8\u5b9e\u73b0\u5404\u79cd\u8bad\u7ec3\u65b9\u6cd5, \u4f8b\u5982 Hogwild, A3C, \u6216\u8005\u5176\u4ed6\u9700\u8981\u5f02\u6b65\u64cd\u4f5c\u7684\u65b9\u6cd5.","title":"\u591a\u8fdb\u7a0b\u7684\u6700\u4f73\u5b9e\u8df5"},{"location":"50/#cuda","text":"\u53ea\u6709 Python 3 \u652f\u6301\u4f7f\u7528 spawn \u6216 forkserver \u542f\u52a8\u65b9\u6cd5\u5728\u8fdb\u7a0b\u4e2d\u5171\u4eab CUDA \u5411\u91cf. multiprocessing \u5728 Python 2 \u4f7f\u7528 fork \u53ea\u80fd\u521b\u5efa\u5b50\u8fdb\u7a0b, \u4f46\u662f\u5728 CUDA \u8fd0\u884c\u65f6\u4e0d\u88ab\u652f\u6301. Warning CUDA API \u8981\u6c42\u88ab\u5bfc\u51fa\u5230\u5176\u4ed6\u8fdb\u7a0b\u7684\u5206\u914d\u53ea\u8981\u88ab\u4f7f\u7528, \u5c31\u8981\u4e00\u76f4\u4fdd\u6301\u6709\u6548. \u60a8\u5e94\u8be5\u5c0f\u5fc3, \u786e\u4fdd\u60a8\u5171\u4eab\u7684CUDA\u5f20\u91cf\u53ea\u8981\u6709\u5fc5\u8981\u5c31\u4e0d\u8981\u8d85\u51fa\u8303\u56f4. \u8fd9\u4e0d\u662f\u5171\u4eab\u6a21\u578b\u53c2\u6570\u7684\u95ee\u9898, \u4f46\u4f20\u9012\u5176\u4ed6\u7c7b\u578b\u7684\u6570\u636e\u5e94\u8be5\u5c0f\u5fc3. \u6ce8\u610f, \u6b64\u9650\u5236\u4e0d\u9002\u7528\u4e8e\u5171\u4eab CPU \u5185\u5b58. \u53c2\u8003: \u4f7f\u7528 nn.DataParallel \u66ff\u4ee3 multiprocessing","title":"\u5171\u4eab CUDA \u5411\u91cf"},{"location":"50/#_2","text":"","title":"\u6700\u4f73\u5b9e\u8df5\u548c\u63d0\u793a"},{"location":"50/#_3","text":"\u5f53\u65b0\u8fdb\u7a0b\u88ab\u521b\u5efa\u65f6, \u53ef\u80fd\u4f1a\u53d1\u751f\u5f88\u591a\u9519\u8bef, \u6700\u5e38\u89c1\u7684\u539f\u56e0\u5c31\u662f\u540e\u53f0\u7ebf\u7a0b. \u5982\u679c\u6709\u4efb\u4f55\u7ebf\u7a0b\u6301\u6709\u9501\u6216\u5bfc\u5165\u6a21\u5757, \u5e76\u4e14 fork \u5df2\u88ab\u8c03\u7528, \u5219\u5b50\u8fdb\u7a0b\u5f88\u6709\u53ef\u80fd\u5c06\u4f1a\u5904\u4e8e\u6bc1\u574f\u7684\u72b6\u6001, \u5e76\u5bfc\u81f4\u6b7b\u9501\u6216\u5728\u5176\u4ed6\u5730\u65b9\u5931\u8d25. \u6ce8\u610f\u5373\u4f7f\u4f60\u81ea\u5df1\u6ca1\u6709\u8fd9\u6837\u505a, Python \u5185\u7f6e\u7684\u5e93\u4e5f\u4f1a\u8fd9\u6837\u505a - \u4e0d\u9700\u8981\u6bd4 multiprocessing \u770b\u5f97\u66f4\u8fdc. multiprocessing.Queue \u4e8b\u5b9e\u4e0a\u662f\u4e00\u4e2a\u975e\u5e38\u590d\u6742\u7684\u5e93, \u5b83\u53ef\u4ee5\u521b\u5efa\u591a\u4e2a\u7ebf\u7a0b, \u7528\u4e8e\u5e8f\u5217\u5316, \u53d1\u9001\u548c\u63a5\u6536\u5bf9\u8c61, \u4f46\u662f\u5b83\u4eec\u4e5f\u6709\u53ef\u80fd\u5f15\u8d77\u524d\u9762\u63d0\u5230\u7684\u95ee\u9898. \u5982\u679c\u4f60\u9047\u5230\u8fd9\u6837\u7684\u95ee\u9898, \u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528 multiprocessing.queues.SimpleQueue , \u5b83\u4e0d\u4f1a\u4f7f\u7528\u5176\u4ed6\u989d\u5916\u7684\u7ebf\u7a0b. \u6211\u4eec\u6b63\u5728\u7aed\u5c3d\u5168\u529b\u628a\u5b83\u8bbe\u8ba1\u5f97\u66f4\u7b80\u5355, \u5e76\u786e\u4fdd\u8fd9\u4e9b\u6b7b\u9501\u4e0d\u4f1a\u53d1\u751f, \u4f46\u6709\u4e9b\u4e8b\u60c5\u65e0\u6cd5\u63a7\u5236. \u5982\u679c\u6709\u4efb\u4f55\u95ee\u9898\u60a8\u4e00\u65f6\u65e0\u6cd5\u89e3\u51b3, \u8bf7\u5c1d\u8bd5\u5728\u8bba\u575b\u4e0a\u63d0\u51fa, \u6211\u4eec\u5c06\u770b\u770b\u662f\u5426\u53ef\u4ee5\u89e3\u51b3.","title":"\u907f\u514d\u548c\u62b5\u5236\u6b7b\u9501"},{"location":"50/#_4","text":"\u8bf7\u8bb0\u4f4f\u5f53\u6bcf\u6b21\u5c06 Tensor \u653e\u5165 multiprocessing.Queue , \u5b83\u5fc5\u987b\u88ab\u79fb\u81f3\u5171\u4eab\u5185\u5b58\u4e2d. \u5982\u679c\u5b83\u5df2\u7ecf\u88ab\u5171\u4eab, \u5b83\u662f\u4e00\u4e2a\u65e0\u6548\u64cd\u4f5c, \u5426\u5219\u4f1a\u4ea7\u751f\u4e00\u4e2a\u989d\u5916\u7684\u5185\u5b58\u526f\u672c, \u8fd9\u4f1a\u51cf\u7f13\u6574\u4e2a\u8fdb\u7a0b. \u5373\u4f7f\u4f60\u6709\u4e00\u4e2a\u8fdb\u7a0b\u6c60\u6765\u53d1\u9001\u6570\u636e\u5230\u4e00\u4e2a\u8fdb\u7a0b, \u4e5f\u5e94\u8be5\u5148\u628a\u5b83\u9001\u56de\u7f13\u51b2\u533a \u2014\u2014 \u8fd9\u51e0\u4e4e\u662f\u6ca1\u6709\u635f\u5931\u7684, \u5e76\u4e14\u5141\u8bb8\u4f60\u5728\u53d1\u9001\u4e0b\u4e00\u4e2a batch \u65f6\u907f\u514d\u4ea7\u751f\u526f\u672c.","title":"\u91cd\u7528\u7ecf\u8fc7\u961f\u5217\u7684\u7f13\u51b2\u533a"},{"location":"50/#hogwild","text":"\u4f7f\u7528 torch.multiprocessing \u53ef\u4ee5\u5f02\u6b65\u5730\u8bad\u7ec3\u6a21\u578b, \u5176\u4e2d\u53c2\u6570\u53ef\u4ee5\u4e00\u76f4\u5171\u4eab, \u6216\u5b9a\u671f\u540c\u6b65. \u5bf9\u4e8e\u7b2c\u4e00\u79cd\u60c5\u51b5, \u6211\u4eec\u5efa\u8bae\u4f20\u8f93\u6574\u4e2a\u6a21\u578b\u5bf9\u8c61, \u800c\u5bf9\u4e8e\u7b2c\u4e8c\u79cd\u60c5\u51b5, \u6211\u4eec\u5efa\u8bae\u53ea\u4f20\u8f93 state_dict() . \u6211\u4eec\u5efa\u8bae\u4f7f\u7528 multiprocessing.Queue \u6765\u5728\u8fdb\u7a0b\u4e4b\u95f4\u4f20\u8f93\u5404\u79cd PyTorch \u5bf9\u8c61. \u4f8b\u5982, \u5f53\u4f7f\u7528 fork \u542f\u52a8\u65b9\u6cd5, \u6709\u53ef\u80fd\u4f1a\u7ee7\u627f\u5171\u4eab\u5185\u5b58\u4e2d\u7684\u5f20\u91cf\u548c\u5b58\u50a8\u91cf. \u4f46\u8fd9\u662f\u975e\u5e38\u5bb9\u6613\u51fa\u9519\u7684, \u5e94\u8c28\u614e\u4f7f\u7528, \u6700\u597d\u662f\u6210\u4e3a\u6df1\u5ea6\u7528\u6237\u4ee5\u540e, \u518d\u4f7f\u7528\u8fd9\u4e2a\u65b9\u6cd5. \u961f\u5217\u867d\u7136\u6709\u65f6\u662f\u4e00\u4e2a\u8f83\u4e0d\u4f18\u96c5\u7684\u89e3\u51b3\u65b9\u6848, \u4f46\u57fa\u672c\u4e0a\u80fd\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u6b63\u5e38\u5de5\u4f5c. Warning \u5f53\u4f7f\u7528\u5168\u5c40\u7684\u58f0\u660e\u65f6, \u4f60\u5e94\u8be5\u6ce8\u610f, \u56e0\u4e3a\u5b83\u4eec\u6ca1\u6709\u88ab if __name__ == '__main__' \u9650\u5236. \u5982\u679c\u4f7f\u7528\u4e0e fork \u4e0d\u540c\u7684\u542f\u52a8\u65b9\u6cd5, \u5b83\u4eec\u5c06\u5728\u6240\u6709\u5b50\u8fdb\u7a0b\u4e2d\u88ab\u6267\u884c.","title":"\u5f02\u6b65\u591a\u8fdb\u7a0b\u8bad\u7ec3 (\u4f8b\u5982 Hogwild)"},{"location":"50/#hogwild_1","text":"\u4e00\u4e2a Hogwild \u7684\u5177\u4f53\u5b9e\u73b0\u53ef\u4ee5\u5728 examples repository \u4e2d\u627e\u5230. \u4e3a\u4e86\u5c55\u793a\u4ee3\u7801\u7684\u6574\u4f53\u7ed3\u6784, \u4e0b\u9762\u6709\u4e00\u4e2a\u5c0f\u4f8b\u5b50: import torch.multiprocessing as mp from model import MyModel def train(model): # Construct data_loader, optimizer, etc. for data, labels in data_loader: optimizer.zero_grad() loss_fn(model(data), labels).backward() optimizer.step() # This will update the shared parameters if __name__ == '__main__': num_processes = 4 model = MyModel() # NOTE: this is required for the ``fork`` method to work model.share_memory() processes = [] for rank in range(num_processes): p = mp.Process(target=train, args=(model,)) p.start() processes.append(p) for p in processes: p.join()","title":"Hogwild"},{"location":"51/","text":"\u5e8f\u5217\u5316\u8bed\u4e49 \u6700\u4f73\u5b9e\u8df5 \u4fdd\u5b58\u6a21\u578b\u7684\u63a8\u8350\u65b9\u6cd5 \u6709\u4e24\u79cd\u4e3b\u8981\u7684\u65b9\u6cd5\u53ef\u4ee5\u7528\u6765\u5e8f\u5217\u5316\u548c\u6062\u590d\u6a21\u578b. \u7b2c\u4e00\u79cd\u65b9\u6cd5 (\u63a8\u8350) , \u53ea\u4fdd\u5b58\u548c\u52a0\u8f7d\u6a21\u578b\u7684\u53c2\u6570: torch.save(the_model.state_dict(), PATH) \u7136\u540e: the_model = TheModelClass(*args, **kwargs) the_model.load_state_dict(torch.load(PATH)) \u7b2c\u4e8c\u79cd\u65b9\u6cd5, \u4fdd\u5b58\u548c\u52a0\u8f7d\u6574\u4e2a\u6a21\u578b: torch.save(the_model, PATH) \u7136\u540e: the_model = torch.load(PATH) \u4f46\u662f\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u5e8f\u5217\u5316\u7684\u6570\u636e\u4e0e\u7279\u5b9a\u7684\u7c7b\u548c\u56fa\u5b9a\u7684\u76ee\u5f55\u7ed3\u6784\u7ed1\u5b9a, \u6240\u4ee5\u5f53\u5b83\u88ab\u7528\u4e8e\u5176\u4ed6\u9879\u76ee\u4e2d, \u6216\u8005\u7ecf\u8fc7\u4e00\u4e9b\u91cd\u5927\u7684\u91cd\u6784\u4e4b\u540e, \u53ef\u80fd\u4f1a\u4ee5\u5404\u79cd\u5404\u6837\u7684\u65b9\u5f0f\u5d29\u6389.","title":"\u5e8f\u5217\u5316\u8bed\u4e49"},{"location":"51/#_1","text":"","title":"\u5e8f\u5217\u5316\u8bed\u4e49"},{"location":"51/#_2","text":"","title":"\u6700\u4f73\u5b9e\u8df5"},{"location":"51/#_3","text":"\u6709\u4e24\u79cd\u4e3b\u8981\u7684\u65b9\u6cd5\u53ef\u4ee5\u7528\u6765\u5e8f\u5217\u5316\u548c\u6062\u590d\u6a21\u578b. \u7b2c\u4e00\u79cd\u65b9\u6cd5 (\u63a8\u8350) , \u53ea\u4fdd\u5b58\u548c\u52a0\u8f7d\u6a21\u578b\u7684\u53c2\u6570: torch.save(the_model.state_dict(), PATH) \u7136\u540e: the_model = TheModelClass(*args, **kwargs) the_model.load_state_dict(torch.load(PATH)) \u7b2c\u4e8c\u79cd\u65b9\u6cd5, \u4fdd\u5b58\u548c\u52a0\u8f7d\u6574\u4e2a\u6a21\u578b: torch.save(the_model, PATH) \u7136\u540e: the_model = torch.load(PATH) \u4f46\u662f\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u5e8f\u5217\u5316\u7684\u6570\u636e\u4e0e\u7279\u5b9a\u7684\u7c7b\u548c\u56fa\u5b9a\u7684\u76ee\u5f55\u7ed3\u6784\u7ed1\u5b9a, \u6240\u4ee5\u5f53\u5b83\u88ab\u7528\u4e8e\u5176\u4ed6\u9879\u76ee\u4e2d, \u6216\u8005\u7ecf\u8fc7\u4e00\u4e9b\u91cd\u5927\u7684\u91cd\u6784\u4e4b\u540e, \u53ef\u80fd\u4f1a\u4ee5\u5404\u79cd\u5404\u6837\u7684\u65b9\u5f0f\u5d29\u6389.","title":"\u4fdd\u5b58\u6a21\u578b\u7684\u63a8\u8350\u65b9\u6cd5"},{"location":"52/","text":"Package \u53c2\u8003","title":"Package \u53c2\u8003"},{"location":"52/#package","text":"","title":"Package \u53c2\u8003"},{"location":"53/","text":"torch torch package \u5305\u542b\u4e86\u591a\u7ef4\u5f20\u91cf\u7684\u6570\u636e\u7ed3\u6784, \u4ee5\u53ca\u57fa\u4e8e\u5176\u4e0a\u7684\u591a\u79cd\u6570\u5b66\u64cd\u4f5c. \u6b64\u5916, \u5b83\u8fd8\u63d0\u4f9b\u4e86\u8bb8\u591a\u7528\u4e8e\u9ad8\u6548\u5e8f\u5217\u5316 Tensor \u548c\u4efb\u610f\u7c7b\u578b\u7684\u5b9e\u7528\u5de5\u5177\u5305, \u4ee5\u53ca\u4e00\u8d77\u5176\u5b83\u6709\u7528\u7684\u5b9e\u7528\u5de5\u5177\u5305. \u5b83\u6709\u4e00\u4e2a CUDA \u7684\u5bf9\u5e94\u5b9e\u73b0, \u5b83\u4f7f\u60a8\u80fd\u591f\u5728\u8ba1\u7b97\u80fd\u529b >=0.3 \u7684 NVIDIA GPU \u4e0a\u8fdb\u884c\u5f20\u91cf\u8fd0\u7b97. Tensors (\u5f20\u91cf) torch.is_tensor(obj) \u5982\u679c <cite>obj</cite> \u662f\u4e00\u4e2a pytorch tensor, \u5219\u8fd4\u56deTrue. Parameters: obj ( Object ) \u2013 \u7528\u4e8e\u6d4b\u8bd5\u7684\u5bf9\u8c61 torch.is_storage(obj) \u5982\u679c <cite>obj</cite> \u662f\u4e00\u4e2a pytorch storage object, \u5219\u8fd4\u56deTrue. Parameters: obj ( Object ) \u2013 \u7528\u4e8e\u6d4b\u8bd5\u7684\u5bf9\u8c61 torch.set_default_tensor_type(t) torch.numel(input) \u2192 int \u8fd4\u56de input Tensor \u4e2d\u7684\u5143\u7d20\u603b\u6570. Parameters: input ( Tensor ) \u2013 \u8f93\u5165\u7684 <cite>Tensor</cite> Example: >>> a = torch.randn(1,2,3,4,5) >>> torch.numel(a) 120 >>> a = torch.zeros(4,4) >>> torch.numel(a) 16 torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None) \u8bbe\u7f6e\u6253\u5370\u9009\u9879. \u4ece Numpy \u4e2d\u91c7\u96c6\u6570\u636e | Parameters: | precision \u2013 \u6d6e\u70b9\u8f93\u51fa\u7cbe\u5ea6\u7684\u4f4d\u6570 (\u9ed8\u8ba4\u503c\u4e3a 8). threshold \u2013 \u89e6\u53d1\u6c47\u603b\u663e\u793a\u800c\u4e0d\u662f\u5b8c\u5168\u663e\u793a(repr)\u7684\u6570\u7ec4\u5143\u7d20\u7684\u603b\u6570 (\u9ed8\u8ba4\u503c\u4e3a 1000). edgeitems \u2013 \u6bcf\u4e2a\u7ef4\u5ea6\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u603b\u7ed3\u7684\u6570\u7ec4\u9879\u6570 (\u9ed8\u8ba4\u503c\u4e3a 3). linewidth \u2013 \u63d2\u5165\u6362\u884c\u7b26\u7684\u6bcf\u884c\u5b57\u7b26\u6570 (\u9ed8\u8ba4\u503c\u4e3a 80). Thresholded matricies(\u9608\u503c\u77e9\u9635) \u5c06\u5ffd\u7565\u8fd9\u4e2a\u53c2\u6570. profile \u2013 \u7528\u4e8e\u6f02\u4eae\u683c\u5f0f\u7684\u6253\u5370. \u53ef\u4ee5\u7528\u4ee5\u4e0b\u4efb\u4f55\u9009\u9879\u6765\u8fdb\u884c\u8986\u76d6 (default, short, full) | | --- | --- | Creation Ops (\u521b\u5efa\u64cd\u4f5c) torch.eye(n, m=None, out=None) \u8fd4\u56de\u5bf9\u89d2\u7ebf\u4f4d\u7f6e\u5168\u4e3a1, \u5176\u5b83\u4f4d\u7f6e\u5168\u4e3a0\u7684\u4e8c\u7ef4 tensor. | Parameters: | n ( int ) \u2013 \u884c\u6570 m ( int , optional ) \u2013 \u5217\u6570. \u5982\u679c\u4e3a None,\u5219\u9ed8\u8ba4\u4e3a <cite>n</cite> out ( Tensor , optional ) \u2013 \u8f93\u51fa tensor | | --- | --- | | Returns: | \u4e00\u4e2a\u5bf9\u89d2\u7ebf\u4f4d\u7f6e\u5168\u4e3a1, \u5176\u5b83\u4f4d\u7f6e\u5168\u4e3a0\u7684\u4e8c\u7ef4 tensor. | | --- | --- | | Return type: | Tensor | | --- | --- | Example: >>> torch.eye(3) 1 0 0 0 1 0 0 0 1 [torch.FloatTensor of size 3x3] torch.from_numpy(ndarray) \u2192 Tensor \u4ece numpy.ndarray \u7c7b \u521b\u5efa\u4e00\u4e2a Tensor \u7c7b. \u8fd4\u56de tensor \u548c <cite>ndarray</cite> \u5171\u4eab\u76f8\u540c\u7684\u5185\u5b58. \u5bf9 tensor \u7684\u4fee\u6539\u5c06\u53cd\u6620\u5728 <cite>ndarray</cite> \u4e2d, \u53cd\u4e4b\u4ea6\u7136. \u8fd4\u56de tensor \u4e0d\u53ef\u8c03\u6574\u5927\u5c0f. Example: >>> a = numpy.array([1, 2, 3]) >>> t = torch.from_numpy(a) >>> t torch.LongTensor([1, 2, 3]) >>> t[0] = -1 >>> a array([-1, 2, 3]) torch.linspace(start, end, steps=100, out=None) \u2192 Tensor \u8fd4\u56de start \u548c end \u4e4b\u95f4\u7b49\u95f4\u9694 steps \u70b9\u7684\u4e00\u7ef4 Tensor. \u8f93\u51fa \u662f\u5c3a\u5bf8 steps \u4e3a\u4e00\u7ef4 tensor | Parameters: | start ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u8d77\u59cb\u503c end ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u7ed3\u675f\u503c steps ( int ) \u2013 \u5728 start \u548c end \u4e4b\u95f4\u7684\u6837\u672c\u6570 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c\u7684 <cite>Tensor</cite> | | --- | --- | Example: >>> torch.linspace(3, 10, steps=5) 3.0000 4.7500 6.5000 8.2500 10.0000 [torch.FloatTensor of size 5] >>> torch.linspace(-10, 10, steps=5) -10 -5 0 5 10 [torch.FloatTensor of size 5] >>> torch.linspace(start=-10, end=10, steps=5) -10 -5 0 5 10 [torch.FloatTensor of size 5] torch.logspace(start, end, steps=100, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5728 \\(10^{start}\\) \u548c \\(10^{end}\\) \u4e4b\u95f4\u7684\u5bf9\u6570\u95f4\u9694 steps \u70b9\u7684\u4e00\u7ef4 Tensor \u8f93\u51fa\u662f\u957f\u5ea6\u4e3a steps \u7684\u4e00\u7ef4 tensor | Parameters: | start ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u8d77\u59cb\u503c end ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u7ed3\u675f\u503c steps ( int ) \u2013 \u5728 start \u548c end \u4e4b\u95f4\u7684\u6837\u672c\u6570 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c Tensor | | --- | --- | Example: >>> torch.logspace(start=-10, end=10, steps=5) 1.0000e-10 1.0000e-05 1.0000e+00 1.0000e+05 1.0000e+10 [torch.FloatTensor of size 5] >>> torch.logspace(start=0.1, end=1.0, steps=5) 1.2589 2.1135 3.5481 5.9566 10.0000 [torch.FloatTensor of size 5] torch.ones(*sizes, out=None) \u2192 Tensor \u8fd4\u56de\u586b\u5145\u4e86\u6807\u91cf\u503c <cite>1</cite> \u7684 Tensor, \u5176\u5f62\u72b6\u7531\u53ef\u53d8\u53c2\u6570 sizes \u5b9a\u4e49. | Parameters: | sizes ( int... ) \u2013 \u4e00\u7ec4\u5b9a\u4e49\u8f93\u51fa Tensor \u5f62\u72b6\u7684\u6574\u6570 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c Tensor | | --- | --- | Example: >>> torch.ones(2, 3) 1 1 1 1 1 1 [torch.FloatTensor of size 2x3] >>> torch.ones(5) 1 1 1 1 1 [torch.FloatTensor of size 5] torch.ones_like(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u7528\u6807\u91cf\u503c <cite>1</cite> \u586b\u5145\u7684\u5f20\u91cf, \u5927\u5c0f\u4e0e input \u76f8\u540c. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u7684\u5927\u5c0f\u5c06\u51b3\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f. out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c Tensor | | --- | --- | Example: >>> input = torch.FloatTensor(2, 3) >>> torch.ones_like(input) 1 1 1 1 1 1 [torch.FloatTensor of size 2x3] torch.arange(start=0, end, step=1, out=None) \u2192 Tensor \u4ece <cite>start</cite> \u7528\u6b65\u957f\u4e3a step \u5f00\u59cb, \u95f4\u9694\u5728 [start, end) \u4e2d\u7684\u503c\u8fd4\u56de\u5927\u5c0f\u5c42\u6b21\u4e3a \\(floor((end - start) / step)\\) \u7684\u4e00\u7ef4 Tensor. | Parameters: | start ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u8d77\u59cb\u503c end ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u7ed3\u675f\u503c step ( float ) \u2013 \u6bcf\u5bf9\u76f8\u90bb\u70b9\u4e4b\u95f4\u7684\u95f4\u9694 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> torch.arange(5) 0 1 2 3 4 [torch.FloatTensor of size 5] >>> torch.arange(1, 4) 1 2 3 [torch.FloatTensor of size 3] >>> torch.arange(1, 2.5, 0.5) 1.0000 1.5000 2.0000 [torch.FloatTensor of size 3] torch.range(start, end, step=1, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5728 start \u5230 end \u5e76\u4e14\u6b65\u957f\u4e3a step \u7684\u533a\u95f4\u5185, \u5927\u5c0f\u4e3a \\(floor((end - start) / step) + 1\\) \u4e3a\u4e00\u7ef4 Tensor. step \u662f tensor \u4e2d\u4e24\u4e2a\u503c\u4e4b\u95f4\u7684\u5dee\u8ddd. \\(x_{i+1} = x_i + step\\) Warning \u6b64\u529f\u80fd\u5df2\u88ab\u5f03\u7528, \u4ee5\u652f\u6301 torch.arange() . | Parameters: | start ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u8d77\u59cb\u503c end ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u7ed3\u675f\u503c step ( float ) \u2013 \u6bcf\u5bf9\u76f8\u90bb\u70b9\u4e4b\u95f4\u7684\u95f4\u9694 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> torch.range(1, 4) 1 2 3 4 [torch.FloatTensor of size 4] >>> torch.range(1, 4, 0.5) 1.0000 1.5000 2.0000 2.5000 3.0000 3.5000 4.0000 [torch.FloatTensor of size 7] torch.zeros(*sizes, out=None) \u2192 Tensor \u8fd4\u56de\u586b\u5145\u4e86\u6807\u91cf\u503c\u4e3a <cite>0</cite> \u7684 Tensor, \u5176\u5f62\u72b6\u7531\u53ef\u53d8\u53c2\u91cf sizes \u5b9a\u4e49. | Parameters: | sizes ( int... ) \u2013 \u5b9a\u4e49\u8f93\u51fa Tensor \u5f62\u72b6\u7684\u4e00\u7ec4\u6574\u6570. out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c Tensor | | --- | --- | Example: >>> torch.zeros(2, 3) 0 0 0 0 0 0 [torch.FloatTensor of size 2x3] >>> torch.zeros(5) 0 0 0 0 0 [torch.FloatTensor of size 5] torch.zeros_like(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u7528\u6807\u91cf\u503c <cite>0</cite> \u586b\u5145\u7684 Tensor, \u5176\u5927\u5c0f\u4e0e input \u76f8\u540c. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u7684\u5927\u5c0f\u5c06\u51b3\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f. out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c Tensor | | --- | --- | Example: >>> input = torch.FloatTensor(2, 3) >>> torch.zeros_like(input) 0 0 0 0 0 0 [torch.FloatTensor of size 2x3] Indexing, Slicing, Joining, Mutating Ops (\u7d22\u5f15, \u5207\u7247, \u8fde\u63a5, \u6362\u4f4d) \u64cd\u4f5c torch.cat(seq, dim=0, out=None) \u2192 Tensor \u5728\u7ed9\u5b9a\u7ef4\u5ea6\u4e0a\u5bf9\u8f93\u5165\u7684\u5f20\u91cf\u5e8f\u5217 seq \u8fdb\u884c\u8fde\u63a5\u64cd\u4f5c. \u6240\u6709\u5f20\u91cf\u5fc5\u987b\u5177\u6709\u76f8\u540c\u7684\u5f62\u72b6(\u5728 cat \u7ef4\u5ea6\u4e2d\u9664\u5916) \u6216\u4e3a\u7a7a. torch.cat() \u53ef\u4ee5\u770b\u505a\u662f torch.split() \u548c torch.chunk() \u7684\u9006\u64cd\u4f5c. cat() \u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u4f8b\u5b50\u66f4\u597d\u5730\u7406\u89e3. | Parameters: | seq ( sequence of Tensors ) \u2013 \u53ef\u4ee5\u662f\u4efb\u4f55\u76f8\u540c\u7c7b\u578b\u7684 <cite>Tensor</cite> \u7684 Python \u5e8f\u5217. dim ( int , optional ) \u2013 tensors \u7ea7\u8054\u7684\u7ef4\u6570 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u53c2\u6570 | | --- | --- | Example: >>> x = torch.randn(2, 3) >>> x 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x3] >>> torch.cat((x, x, x), 0) 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 6x3] >>> torch.cat((x, x, x), 1) 0.5983 -0.0341 2.4918 0.5983 -0.0341 2.4918 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 1.5981 -0.5265 -0.8735 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x9] torch.chunk(tensor, chunks, dim=0) \u5728\u7ed9\u5b9a\u7ef4\u5ea6(\u8f74)\u4e0a\u5c06\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u5206\u5757\u5904\u7406. | Parameters: | tensor ( Tensor ) \u2013 \u5f85\u5206\u5757\u7684\u8f93\u5165\u5f20\u91cf. chunks ( int ) \u2013 \u8981\u8fd4\u56de\u7684\u5206\u5757\u7684\u4e2a\u6570. dim ( int ) \u2013 \u5207\u5206\u5f20\u91cf\u6240\u9700\u8981\u6cbf\u7740\u7684\u7ef4\u5ea6. | | --- | --- | torch.gather(input, dim, index, out=None) \u2192 Tensor \u6cbf\u7ed9\u5b9a\u8f74 <cite>dim</cite> ,\u5c06\u8f93\u5165\u7d22\u5f15\u5f20\u91cf <cite>index</cite> \u6307\u5b9a\u4f4d\u7f6e\u7684\u503c\u8fdb\u884c\u805a\u5408. \u5bf9\u4e00\u4e2a 3 \u7ef4\u5f20\u91cf,\u8f93\u51fa\u53ef\u4ee5\u5b9a\u4e49\u4e3a: out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0 out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1 out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 \u5982\u679c input \u662f size \u4e3a \\((x_0, x_1..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})\\) \u4e14 dim = i \u7684 n \u7ef4\u5f20\u91cf,\u5219 index \u5fc5\u987b\u662f\u5177\u6709 size \u4e3a \\((x_0, x_1, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})\\) \u7684 n \u7ef4\u5f20\u91cf,\u5176\u4e2d y >= 1 ,\u5e76\u4e14 out \u5c06\u4e0e index \u7684 size \u76f8\u540c. | Parameters: | input ( Tensor ) \u2013 \u6e90\u5f20\u91cf dim ( int ) \u2013 \u7d22\u5f15\u7684\u8f74 index ( LongTensor ) \u2013 \u805a\u5408\u5143\u7d20\u7684\u4e0b\u6807 out ( Tensor , optional ) \u2013 \u76ee\u6807\u5f20\u91cf | | --- | --- | Example: >>> t = torch.Tensor([[1,2],[3,4]]) >>> torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]])) 1 1 4 3 [torch.FloatTensor of size 2x2] torch.index_select(input, dim, index, out=None) \u2192 Tensor \u6cbf\u7740\u6307\u5b9a\u7ef4\u5ea6 dim \u5bf9\u8f93\u5165\u8fdb\u884c\u5207\u7247,\u53d6 index \u4e2d\u6307\u5b9a\u7684\u76f8\u5e94\u9879 ( index \u4e3a\u4e00\u4e2a <cite>LongTensor</cite> ),\u7136\u540e\u8fd4\u56de\u5230\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf. \u8fd4\u56de\u7684\u5f20\u91cf\u4e0e\u539f\u59cb\u5f20\u91cf <cite>Tensor</cite> \u6709\u76f8\u540c\u7684\u7ef4\u5ea6(\u5728\u6307\u5b9a\u8f74\u4e0a). Note \u8fd4\u56de\u7684\u5f20\u91cf\u4e0d\u4e0e\u539f\u59cb\u5f20\u91cf\u5171\u4eab\u5185\u5b58\u7a7a\u95f4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf dim ( int ) \u2013 \u7d22\u5f15\u7684\u8f74 index ( LongTensor ) \u2013 \u5305\u542b\u7d22\u5f15\u4e0b\u6807\u7684\u4e00\u7ef4\u5f20\u91cf out ( Tensor , optional ) \u2013 \u8f93\u51fa\u53c2\u6570/\u76ee\u6807\u5f20\u91cf | | --- | --- | Example: >>> x = torch.randn(3, 4) >>> x 1.2045 2.4084 0.4001 1.1372 0.5596 1.5677 0.6219 -0.7954 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 3x4] >>> indices = torch.LongTensor([0, 2]) >>> torch.index_select(x, 0, indices) 1.2045 2.4084 0.4001 1.1372 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 2x4] >>> torch.index_select(x, 1, indices) 1.2045 0.4001 0.5596 0.6219 1.3635 -0.5414 [torch.FloatTensor of size 3x2] torch.masked_select(input, mask, out=None) \u2192 Tensor \u6839\u636e\u63a9\u7801\u5f20\u91cf mask \u4e2d\u7684\u4e8c\u5143\u503c,\u53d6\u8f93\u5165\u5f20\u91cf\u4e2d\u7684\u6307\u5b9a\u9879 ( mask \u4e3a\u4e00\u4e2a <cite>ByteTensor</cite> ),\u5c06\u53d6\u503c\u8fd4\u56de\u5230\u4e00\u4e2a\u65b0\u7684\u4e00\u7ef4\u5f20\u91cf. \u5f20\u91cf mask \u4e0e input \u7684 shape \u6216\u7ef4\u5ea6\u4e0d\u9700\u8981\u76f8\u540c,\u4f46\u662f\u4ed6\u4eec\u5fc5\u987b\u662f broadcastable . Note \u8fd4\u56de\u7684\u5f20\u91cf\u4e0d\u4e0e\u539f\u59cb\u5f20\u91cf\u5171\u4eab\u5185\u5b58\u7a7a\u95f4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf mask ( ByteTensor ) \u2013 \u63a9\u7801\u5f20\u91cf,\u5305\u542b\u4e86\u4e8c\u5143\u7d22\u5f15\u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u53c2\u6570/\u76ee\u6807\u5f20\u91cf | | --- | --- | Example: >>> x = torch.randn(3, 4) >>> x 1.2045 2.4084 0.4001 1.1372 0.5596 1.5677 0.6219 -0.7954 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 3x4] >>> mask = x.ge(0.5) >>> mask 1 1 0 1 1 1 1 0 1 0 0 0 [torch.ByteTensor of size 3x4] >>> torch.masked_select(x, mask) 1.2045 2.4084 1.1372 0.5596 1.5677 0.6219 1.3635 [torch.FloatTensor of size 7] torch.nonzero(input, out=None) \u2192 LongTensor \u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u8f93\u5165 input \u4e2d\u975e\u96f6\u5143\u7d20\u7d22\u5f15\u7684\u5f20\u91cf. \u8f93\u51fa\u5f20\u91cf\u4e2d\u7684\u6bcf\u884c\u5305\u542b input \u4e2d\u975e\u96f6\u5143\u7d20\u7684\u7d22\u5f15. \u5982\u679c\u8f93\u5165\u5f20\u91cf input \u6709 <cite>n</cite> \u7ef4,\u5219\u8f93\u51fa\u7684\u7d22\u5f15\u5f20\u91cf out \u7684 size \u4e3a <cite>z x n</cite> , \u8fd9\u91cc <cite>z</cite> \u662f\u8f93\u5165\u5f20\u91cf input \u4e2d\u6240\u6709\u975e\u96f6\u5143\u7d20\u7684\u4e2a\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf/\u6e90\u5f20\u91cf out ( LongTensor__, optional ) \u2013 \u5305\u542b\u7d22\u5f15\u503c\u7684\u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> torch.nonzero(torch.Tensor([1, 1, 1, 0, 1])) 0 1 2 4 [torch.LongTensor of size 4x1] >>> torch.nonzero(torch.Tensor([[0.6, 0.0, 0.0, 0.0], ... [0.0, 0.4, 0.0, 0.0], ... [0.0, 0.0, 1.2, 0.0], ... [0.0, 0.0, 0.0,-0.4]])) 0 0 1 1 2 2 3 3 [torch.LongTensor of size 4x2] torch.split(tensor, split_size, dim=0) \u5c06\u8f93\u5165\u5f20\u91cf\u5206\u5272\u6210\u76f8\u7b49 size \u7684 chunks (\u5982\u679c\u53ef\u5206). \u5982\u679c\u6cbf\u6307\u5b9a\u7ef4\u7684\u5f20\u91cf\u5f62\u72b6\u5927\u5c0f\u4e0d\u80fd\u88ab split_size \u6574\u5206, \u5219\u6700\u540e\u4e00\u4e2a\u5206\u5757\u4f1a\u5c0f\u4e8e\u5176\u5b83\u5206\u5757. | Parameters: | tensor ( Tensor ) \u2013 \u5f85\u5206\u5272\u5f20\u91cf. split_size ( int ) \u2013 \u5355\u4e2a\u5206\u5757\u7684 size \u5927\u5c0f. dim ( int ) \u2013 \u6cbf\u7740\u6b64\u7ef4\u8fdb\u884c\u5206\u5272. | | --- | --- | torch.squeeze(input, dim=None, out=None) \u5c06 input \u5f20\u91cf size \u4e2d\u7684 <cite>1</cite> \u53bb\u9664\u5e76\u8fd4\u56de. \u5982\u679c <cite>input</cite> \u7684 shape \u5982 \\((A x 1 x B x C x 1 x D)\\) ,\u90a3\u4e48\u8f93\u51fa shape \u5c31\u4e3a: \\((A x B x C x D)\\) \u5f53\u7ed9\u5b9a dim \u65f6,\u90a3\u4e48\u6324\u538b\u64cd\u4f5c\u53ea\u5728\u7ed9\u5b9a\u7ef4\u5ea6\u4e0a.\u4f8b\u5982, <cite>input</cite> \u7684 shape \u4e3a: \\((A x 1 x B)\\) , <cite>squeeze(input, 0)</cite> \u5c06\u4f1a\u4fdd\u6301\u5f20\u91cf\u4e0d\u53d8,\u53ea\u6709\u7528 <cite>squeeze(input, 1)</cite> , shape \u4f1a\u53d8\u6210 \\((A x B)\\) . Note \u4f5c\u4e3a\u4e0a\u8ff0\u7684\u4e00\u4e2a\u4f8b\u5916,size \u4e3a 1 \u7684\u4e00\u7ef4\u5f20\u91cf\u4e0d\u4f1a\u6539\u53d8\u7ef4\u5ea6. Note \u8fd4\u56de\u5f20\u91cf\u4e0e\u8f93\u5165\u5f20\u91cf\u5171\u4eab\u5185\u5b58,\u6240\u4ee5\u6539\u53d8\u5176\u4e2d\u4e00\u4e2a\u7684\u5185\u5bb9\u4f1a\u6539\u53d8\u53e6\u4e00\u4e2a. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf dim ( int , optional ) \u2013 \u5982\u679c\u7ed9\u5b9a dim \u65f6,\u5219 <cite>input</cite> \u53ea\u4f1a\u5728\u7ed9\u5b9a\u7ef4\u5ea6\u6267\u884c\u6324\u538b out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> x = torch.zeros(2,1,2,1,2) >>> x.size() (2L, 1L, 2L, 1L, 2L) >>> y = torch.squeeze(x) >>> y.size() (2L, 2L, 2L) >>> y = torch.squeeze(x, 0) >>> y.size() (2L, 1L, 2L, 1L, 2L) >>> y = torch.squeeze(x, 1) >>> y.size() (2L, 2L, 1L, 2L) torch.stack(sequence, dim=0, out=None) \u6cbf\u7740\u4e00\u4e2a\u65b0\u7ef4\u5ea6\u5bf9\u8f93\u5165\u5f20\u91cf\u5e8f\u5217\u8fdb\u884c\u8fde\u63a5. \u5e8f\u5217\u4e2d\u6240\u6709\u7684\u5f20\u91cf\u90fd\u5e94\u8be5\u4e3a\u76f8\u540c size . | Parameters: | sequence ( Sequence ) \u2013 \u5f85\u8fde\u63a5\u7684\u5f20\u91cf\u5e8f\u5217. dim ( int ) \u2013 \u63d2\u5165\u7684\u7ef4\u5ea6.\u5fc5\u987b\u4ecb\u4e8e 0 \u4e0e\u5f85\u8fde\u63a5\u7684\u5f20\u91cf\u5e8f\u5217\u6570\uff08\u5305\u542b\uff09\u4e4b\u95f4. | | --- | --- | torch.t(input, out=None) \u2192 Tensor \u9884\u671f input \u4e3a\u4e00\u4e2a\u77e9\u9635 (2 \u7ef4\u5f20\u91cf), \u5e76\u8f6c\u7f6e 0, 1 \u7ef4. \u53ef\u4ee5\u88ab\u89c6\u4e3a\u51fd\u6570 <cite>transpose(input, 0, 1)</cite> \u7684\u7b80\u5199\u51fd\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> x = torch.randn(2, 3) >>> x 0.4834 0.6907 1.3417 -0.1300 0.5295 0.2321 [torch.FloatTensor of size 2x3] >>> torch.t(x) 0.4834 -0.1300 0.6907 0.5295 1.3417 0.2321 [torch.FloatTensor of size 3x2] torch.take(input, indices) \u2192 Tensor \u5728\u7ed9\u5b9a\u7684\u7d22\u5f15\u5904\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684 <cite>Tensor</cite> ,\u5176\u5143\u7d20\u4e3a input . \u8f93\u5165\u5f20\u91cf\u88ab\u770b\u4f5c\u662f\u4e00\u7ef4\u5f20\u91cf.\u7ed3\u679c\u4e0e\u7d22\u5f15\u5177\u6709\u76f8\u540c\u7684 shape . | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf indices ( LongTensor ) \u2013 \u8fdb\u5165 <cite>Tensor</cite> \u7684\u7d22\u5f15 | | --- | --- | Example: >>> src = torch.Tensor([[4, 3, 5], ... [6, 7, 8]]) >>> torch.take(src, torch.LongTensor([0, 2, 5])) 4 5 8 [torch.FloatTensor of size 3] torch.transpose(input, dim0, dim1, out=None) \u2192 Tensor \u8fd4\u56de\u8f93\u5165\u77e9\u9635 input \u7684\u8f6c\u7f6e.\u4ea4\u6362\u7ed9\u5b9a\u7ef4\u5ea6 dim0 \u548c dim1 . out \u5f20\u91cf\u4e0e input \u5f20\u91cf\u5171\u4eab\u5185\u5b58,\u6240\u4ee5\u6539\u53d8\u5176\u4e2d\u4e00\u4e2a\u4f1a\u5bfc\u81f4\u53e6\u5916\u4e00\u4e2a\u4e5f\u88ab\u4fee\u6539. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf dim0 ( int ) \u2013 \u8f6c\u7f6e\u7684\u7b2c\u4e00\u4e2a\u7ef4\u5ea6 dim1 ( int ) \u2013 \u8f6c\u7f6e\u7684\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6 | | --- | --- | Example: >>> x = torch.randn(2, 3) >>> x 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x3] >>> torch.transpose(x, 0, 1) 0.5983 1.5981 -0.0341 -0.5265 2.4918 -0.8735 [torch.FloatTensor of size 3x2] torch.unbind(tensor, dim=0) \u79fb\u9664\u4e00\u4e2a\u5f20\u91cf\u7684\u7ef4\u5ea6. \u79fb\u9664\u6307\u5b9a\u7ef4\u540e,\u8fd4\u56de\u4e00\u4e2a\u5143\u7ec4,\u5305\u542b\u4e86\u6cbf\u7740\u6307\u5b9a\u7ef4\u5207\u7247\u540e\u7684\u5404\u4e2a\u5207\u7247 (\u5df2\u7ecf\u6ca1\u6709\u4e86\u79fb\u9664\u7684\u7ef4\u5ea6). | Parameters: | tensor ( Tensor ) \u2013 \u8981\u6267\u884c unbind \u7684\u5f20\u91cf/\u8f93\u5165\u5f20\u91cf. dim ( int ) \u2013 \u8981\u79fb\u9664\u7684\u7ef4\u5ea6. | | --- | --- | torch.unsqueeze(input, dim, out=None) \u8fd4\u56de\u5728\u6307\u5b9a\u4f4d\u7f6e\u63d2\u5165\u7ef4\u5ea6 size \u4e3a 1 \u7684\u65b0\u5f20\u91cf. \u8fd4\u56de\u5f20\u91cf\u4e0e\u8f93\u5165\u5f20\u91cf\u5171\u4eab\u5185\u5b58,\u6240\u4ee5\u6539\u53d8\u5176\u4e2d\u4e00\u4e2a\u7684\u5185\u5bb9\u4f1a\u6539\u53d8\u53e6\u4e00\u4e2a. \u5982\u679c <cite>dim</cite> \u4e3a\u8d1f,\u5219\u5c06\u4f1a\u88ab\u8f6c\u5316 \\(dim + input.dim() + 1\\) . | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf dim ( int ) \u2013 \u63d2\u5165\u7ef4\u5ea6\u7684\u7d22\u5f15 out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example >>> x = torch.Tensor([1, 2, 3, 4]) >>> torch.unsqueeze(x, 0) 1 2 3 4 [torch.FloatTensor of size 1x4] >>> torch.unsqueeze(x, 1) 1 2 3 4 [torch.FloatTensor of size 4x1] Random sampling (\u968f\u673a\u91c7\u6837) torch.manual_seed(seed) \u8bbe\u7f6e\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50,\u5e76\u8fd4\u56de\u4e00\u4e2a <cite>torch._C.Generator</cite> \u5bf9\u8c61. Parameters: seed ( int or long ) \u2013 \u79cd\u5b50. torch.initial_seed() \u8fd4\u56de\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\u5b57\u7684\u521d\u59cb\u79cd\u5b50 (python <cite>long</cite>) . torch.get_rng_state() \u4ee5ByteTensor\u7684\u5f62\u5f0f\u8fd4\u56de\u968f\u673a\u6570\u53d1\u751f\u5668\u7684\u72b6\u6001. torch.set_rng_state(new_state) \u8bbe\u7f6e\u968f\u673a\u6570\u53d1\u751f\u5668\u7684\u53c2\u6570. Parameters: new_state ( torch.ByteTensor ) \u2013 \u7406\u60f3\u72b6\u6001 torch.``default_generator = torch.bernoulli(input, out=None) \u2192 Tensor \u4ece\u4f2f\u52aa\u5229\u5206\u5e03\u4e2d\u62bd\u53d6\u4e8c\u8fdb\u5236\u968f\u673a\u6570 (0 \u6216 1). The input \u5f20\u91cf\u5305\u542b\u7528\u4e8e\u62bd\u53d6\u4e8c\u8fdb\u5236\u968f\u673a\u6570\u7684\u6982\u7387. \u56e0\u6b64, input \u4e2d\u7684\u6240\u6709\u503c\u5fc5\u987b\u5728\u8fd9\u4e2a\u8303\u56f4\u5185: \\(0 &lt;= input_i &lt;= 1\\) \u6839\u636e input \u5f20\u91cf\u7b2c <cite>i</cite> \u4e2a\u6982\u7387\u503c, \u8f93\u51fa\u5f20\u91cf\u7684\u7b2c <cite>i</cite> \u4e2a\u5143\u7d20\u5c06\u53d6\u503c\u4e3a1. \u8fd4\u56de\u7684 out \u5f20\u91cf\u7684\u503c\u53ea\u6709 0 \u6216\u8005 1 \u5e76\u4e14\u5927\u5c0f\u4e0e input \u5f20\u91cf\u76f8\u540c. | Parameters: | input ( Tensor ) \u2013 \u4f2f\u52aa\u5229\u5206\u5e03\u7684\u6982\u7387\u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> a = torch.Tensor(3, 3).uniform_(0, 1) # generate a uniform random matrix with range [0, 1] >>> a 0.7544 0.8140 0.9842 0.5282 0.0595 0.6445 0.1925 0.9553 0.9732 [torch.FloatTensor of size 3x3] >>> torch.bernoulli(a) 1 1 1 0 0 1 0 1 1 [torch.FloatTensor of size 3x3] >>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1 >>> torch.bernoulli(a) 1 1 1 1 1 1 1 1 1 [torch.FloatTensor of size 3x3] >>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0 >>> torch.bernoulli(a) 0 0 0 0 0 0 0 0 0 [torch.FloatTensor of size 3x3] torch.multinomial(input, num_samples, replacement=False, out=None) \u2192 LongTensor \u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf, \u5176\u4e2d\u6bcf\u4e00\u884c\u5305\u542b\u5728 input \u5f20\u91cf\u5bf9\u5e94\u884c\u4e2d\u591a\u9879\u5f0f\u5206\u5e03\u53d6\u6837\u7684 num_samples \u7d22\u5f15. Note input \u7684\u6bcf\u884c\u503c\u4e0d\u9700\u8981\u603b\u548c\u4e3a 1 (\u6211\u4eec\u53ea\u4f7f\u7528\u8fd9\u4e9b\u503c\u4f5c\u4e3a\u6743\u91cd), \u4f46\u5fc5\u987b\u662f\u975e\u8d1f\u4e14\u975e\u96f6\u548c\u7684. \u53d6\u6837\u65f6\u4ece\u5de6\u5411\u53f3\u6392\u5217(\u7b2c\u4e00\u4e2a\u6837\u672c\u5728\u7b2c\u4e00\u5217). \u5982\u679c input \u662f\u4e00\u4e2a\u5411\u91cf, \u5219 out \u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a <cite>num_samples</cite> \u7684\u5411\u91cf. \u5982\u679c input \u662f\u4e00\u4e2a <cite>m</cite> \u884c\u7684\u77e9\u9635, \u5219 out \u662f\u4e00\u4e2a <cite>m \u00d7 n</cite> \u7684\u77e9\u9635. \u5982\u679c\u53c2\u6570 <cite>replacement</cite> \u662f True , \u5219\u53ef\u91cd\u590d\u53d6\u6837. \u5426\u5219, \u6837\u672c\u5728\u6bcf\u884c\u4e0d\u80fd\u88ab\u91cd\u590d\u53d6\u6837. \u53c2\u6570 num_samples \u5fc5\u987b\u5c0f\u4e8e input \u957f\u5ea6 (\u5982\u679c\u662f\u4e00\u4e2a\u77e9\u9635, \u5219\u662f input \u7684\u5217\u6570). | Parameters: | input ( Tensor ) \u2013 \u5305\u542b\u6982\u7387\u503c\u7684\u5f20\u91cf num_samples ( int ) \u2013 \u62bd\u53d6\u7684\u6837\u672c\u6570 replacement ( bool , optional ) \u2013 \u662f\u5426\u91cd\u590d\u62bd\u53d6\u6837\u672c out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: >>> weights = torch.Tensor([0, 10, 3, 0]) # create a Tensor of weights >>> torch.multinomial(weights, 4) 1 2 0 0 [torch.LongTensor of size 4] >>> torch.multinomial(weights, 4, replacement=True) 1 2 1 2 [torch.LongTensor of size 4] torch.normal() torch.normal(means, std, out=None) \u8fd4\u56de\u4e00\u4e2a\u968f\u673a\u6570\u5f20\u91cf, \u968f\u673a\u6570\u4ece\u7ed9\u5b9a\u5e73\u5747\u503c\u548c\u6807\u51c6\u5dee\u7684\u79bb\u6563\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6. \u53c2\u6570 means \u662f\u4e00\u4e2a\u5305\u542b\u6bcf\u4e2a\u8f93\u51fa\u5143\u7d20\u7684\u6b63\u6001\u5206\u5e03\u5747\u503c\u7684\u5f20\u91cf. \u53c2\u6570 std \u662f\u4e00\u4e2a\u5305\u542b\u6bcf\u4e2a\u8f93\u51fa\u5143\u7d20\u7684\u6b63\u6001\u5206\u5e03\u6807\u51c6\u5dee\u7684\u5f20\u91cf. \u5176\u4e2d means \u548c std \u7684\u5f62\u72b6\u4e0d\u9700\u8981\u5339\u914d, \u4f46\u662f\u6bcf\u4e2a\u5f20\u91cf\u4e2d\u7684\u5143\u7d20\u603b\u6570\u9700\u8981\u76f8\u540c. Note \u5f53\u5f62\u72b6\u4e0d\u5339\u914d\u65f6, means \u7684\u5f62\u72b6\u5c06\u4f5c\u4e3a\u8fd4\u56de\u8f93\u51fa\u5f20\u91cf\u7684\u5f62\u72b6. | Parameters: | means ( Tensor ) \u2013 \u5747\u503c std ( Tensor ) \u2013 \u6807\u51c6\u5dee out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1)) 1.5104 1.6955 2.4895 4.9185 4.9895 6.9155 7.3683 8.1836 8.7164 9.8916 [torch.FloatTensor of size 10] torch.normal(mean=0.0, std, out=None) \u529f\u80fd\u4e0e\u4e0a\u9762\u51fd\u6570\u7c7b\u4f3c, \u4f46\u6240\u6709\u88ab\u62bd\u53d6\u7684\u5143\u7d20\u5171\u4eab\u5747\u503c. | Parameters: | means ( float , optional ) \u2013 \u6240\u6709\u5206\u5e03\u7684\u5747\u503c std ( Tensor ) \u2013 \u6bcf\u4e2a\u5143\u7d20\u6807\u51c6\u5dee\u7684\u5f20\u91cf out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> torch.normal(mean=0.5, std=torch.arange(1, 6)) 0.5723 0.0871 -0.3783 -2.5689 10.7893 [torch.FloatTensor of size 5] torch.normal(means, std=1.0, out=None) \u529f\u80fd\u4e0e\u4e0a\u9762\u51fd\u6570\u7c7b\u4f3c, \u4f46\u6240\u6709\u88ab\u62bd\u53d6\u7684\u5143\u7d20\u5171\u4eab\u6807\u51c6\u5dee. | Parameters: | means ( Tensor ) \u2013 \u6bcf\u4e2a\u5143\u7d20\u5747\u503c\u7684\u5f20\u91cf std ( float , optional ) \u2013 \u6240\u6709\u5206\u5e03\u7684\u6807\u51c6\u5dee out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> torch.normal(means=torch.arange(1, 6)) 1.1681 2.8884 3.7718 2.5616 4.2500 [torch.FloatTensor of size 5] torch.rand(*sizes, out=None) \u2192 Tensor \u5728\u533a\u95f4 \\([0, 1)\\) \u4e2d, \u8fd4\u56de\u4e00\u4e2a\u586b\u5145\u4e86\u5747\u5300\u5206\u5e03\u7684\u968f\u673a\u6570\u7684\u5f20\u91cf. \u8fd9\u4e2a\u5f20\u91cf\u7684\u5f62\u72b6\u7531\u53ef\u53d8\u53c2\u6570 sizes \u6765\u5b9a\u4e49. | Parameters: | sizes ( int... ) \u2013 \u5b9a\u4e49\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u7684\u6574\u6570\u96c6. out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> torch.rand(4) 0.9193 0.3347 0.3232 0.7715 [torch.FloatTensor of size 4] >>> torch.rand(2, 3) 0.5010 0.5140 0.0719 0.1435 0.5636 0.0538 [torch.FloatTensor of size 2x3] torch.randn(*sizes, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u4ece\u6b63\u6001\u5206\u5e03\u4e2d\u586b\u5145\u968f\u673a\u6570\u7684\u5f20\u91cf, \u5176\u5747\u503c\u4e3a 0 , \u65b9\u5dee\u4e3a 1 . \u8fd9\u4e2a\u5f20\u91cf\u7684\u5f62\u72b6\u88ab\u53ef\u53d8\u53c2\u6570 sizes \u5b9a\u4e49. | Parameters: | sizes ( int... ) \u2013 \u5b9a\u4e49\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u7684\u6574\u6570\u96c6. out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> torch.randn(4) -0.1145 0.0094 -1.1717 0.9846 [torch.FloatTensor of size 4] >>> torch.randn(2, 3) 1.4339 0.3351 -1.0999 1.5458 -0.9643 -0.3558 [torch.FloatTensor of size 2x3] torch.randperm(n, out=None) \u2192 LongTensor \u8fd4\u56de\u4e00\u4e2a\u4ece 0 to n - 1 \u7684\u6574\u6570\u7684\u968f\u673a\u6392\u5217. Parameters: n ( int ) \u2013 \u4e0a\u9650 (\u552f\u4e00\u7684) Example: >>> torch.randperm(4) 2 1 3 0 [torch.LongTensor of size 4] In-place random sampling (\u76f4\u63a5\u968f\u673a\u91c7\u6837) \u5728Tensors\u6a21\u5757\u4e0a\u8fd8\u5b9a\u4e49\u4e86\u8bb8\u591a in-place \u968f\u673a\u91c7\u6837\u51fd\u6570,\u53ef\u4ee5\u70b9\u51fb\u53c2\u8003\u5b83\u4eec\u7684\u6587\u6863: torch.Tensor.bernoulli_() - \u662f torch.bernoulli() \u7684 in-place \u7248\u672c torch.Tensor.cauchy_() - \u4ece\u67ef\u897f\u5206\u5e03\u4e2d\u62bd\u53d6\u6570\u5b57 torch.Tensor.exponential_() - \u4ece\u6307\u6570\u5206\u5e03\u4e2d\u62bd\u53d6\u6570\u5b57 torch.Tensor.geometric_() - \u4ece\u51e0\u4f55\u5206\u5e03\u4e2d\u62bd\u53d6\u5143\u7d20 torch.Tensor.log_normal_() - \u5bf9\u6570\u6b63\u6001\u5206\u5e03\u4e2d\u7684\u6837\u672c torch.Tensor.normal_() - \u662f torch.normal() \u7684 in-place \u7248\u672c torch.Tensor.random_() - \u79bb\u6563\u5747\u5300\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u6570\u5b57 torch.Tensor.uniform_() - \u6b63\u6001\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u6570\u5b57 Serialization (\u5e8f\u5217\u5316) torch.save(obj, f, pickle_module=<module 'cPickle' from '/usr/lib64/python2.7/lib-dynload/cPickle.so'>, pickle_protocol=2) \u5c06\u4e00\u4e2a\u5bf9\u8c61\u4fdd\u5b58\u5230\u4e00\u4e2a\u78c1\u76d8\u6587\u4ef6\u4e2d. \u53e6\u89c1: \u4fdd\u5b58\u6a21\u578b\u7684\u63a8\u8350\u65b9\u6cd5 \u53c2\u6570: obj: \u8981\u4fdd\u5b58\u7684\u5bf9\u8c61 f: \u7c7b\u6587\u4ef6\u5bf9\u8c61 (\u5fc5\u987b\u5b9e\u73b0\u8fd4\u56de\u6587\u4ef6\u63cf\u8ff0\u7b26\u7684 fileno \u65b9\u6cd5) \u6216\u5305\u542b\u6587\u4ef6\u540d\u7684\u5b57\u7b26\u4e32 pickle_module: \u7528\u4e8e pickling \u5143\u6570\u636e\u548c\u5bf9\u8c61\u7684\u6a21\u5757 pickle_protocol: \u53ef\u4ee5\u6307\u5b9a\u6765\u8986\u76d6\u9ed8\u8ba4\u534f\u8bae torch.load(f, map_location=None, pickle_module=<module 'cPickle' from '/usr/lib64/python2.7/lib-dynload/cPickle.so'>) \u4ece\u78c1\u76d8\u6587\u4ef6\u4e2d\u52a0\u8f7d\u4e00\u4e2a\u7528 torch.save() \u4fdd\u5b58\u7684\u5bf9\u8c61. Func: <cite>torch.load</cite> \u4f7f\u7528 Python \u7684\u89e3\u5c01 (unpickling) \u8bbe\u65bd, \u4f46\u7279\u6b8a\u5bf9\u5f85\u5f20\u91cf\u4e0b\u7684\u5b58\u50a8 (storages). \u5b83\u4eec\u9996\u5148\u5728 CPU \u4e0a\u53cd\u5e8f\u5217\u5316, \u7136\u540e\u79fb\u52a8\u5230\u6240\u4fdd\u5b58\u7684\u8bbe\u5907\u4e0a. \u5982\u679c\u8fd9\u4e2a\u8fc7\u7a0b\u5931\u8d25\u4e86 (\u4f8b\u5982, \u56e0\u4e3a\u8fd0\u884c\u65f6\u7684\u7cfb\u7edf\u6ca1\u6709\u786e\u5b9a\u7684\u8bbe\u5907), \u5c06\u4f1a\u629b\u51fa\u5f02\u5e38. \u7136\u800c, \u4f7f\u7528 map_location \u53c2\u6570, \u5b58\u50a8\u53ef\u4ee5\u88ab\u52a8\u6001\u5730\u91cd\u65b0\u6620\u5c04\u5230\u53e6\u4e00\u7ec4\u8bbe\u5907\u4e0a. \u5982\u679c map_location \u662f\u53ef\u8c03\u7528\u5bf9\u8c61, \u5219\u5bf9\u4e8e\u6bcf\u4e2a\u5e8f\u5217\u5316\u5b58\u50a8, \u5b83\u90fd\u5c06\u4ee5\u4e24\u4e2a\u53c2\u6570\u8c03\u7528\u4e00\u6b21: storage \u548c location. \u53c2\u6570 storage \u662f\u9a7b\u7559\u5728 CPU \u4e0a\u7684\u5b58\u50a8\u7684\u521d\u59cb\u53cd\u5e8f\u5217\u5316. \u6bcf\u4e2a\u5e8f\u5217\u5316\u540e\u7684\u5b58\u50a8\u90fd\u6709\u4e00\u4e2a\u4e0e\u4e4b\u5173\u8054\u7684\u4f4d\u7f6e\u6807\u7b7e, \u5b83\u6807\u8bc6\u4e86\u4fdd\u5b58\u5b83\u7684\u8bbe\u5907, \u800c\u6b64\u6807\u7b7e\u662f\u4f20\u9012\u7ed9 map_location \u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570. \u5bf9\u4e8e CPU \u5f20\u91cf, \u5185\u5efa\u7684\u4f4d\u7f6e\u6807\u7b7e\u662f \u2018cpu\u2019, \u5bf9\u4e8e CUDA \u5f20\u91cf, \u5185\u5efa\u7684\u4f4d\u7f6e\u6807\u7b7e\u662f \u2018cuda:device_id\u2019 (\u4f8b\u5982 \u2018cuda:2\u2019). map_location \u8981\u4e48\u8fd4\u56de None , \u8981\u4e48\u8fd4\u56de\u4e00\u4e2a\u5b58\u50a8. \u5982\u679c map_location \u8fd4\u56de\u5b58\u50a8, \u5b83\u5c06\u7528\u4f5c\u5df2\u79fb\u52a8\u5230\u6b63\u786e\u8bbe\u5907\u4e0a\u7684, \u6700\u7ec8\u53cd\u5e8f\u5217\u5316\u7684\u5bf9\u8c61. \u5426\u5219, \u5982\u679c\u6ca1\u6709\u6307\u660e map_location, \u5373\u8fd4\u56de None, <cite>torch.load</cite> \u4f1a\u56de\u843d\u5230\u9ed8\u8ba4\u7684\u884c\u4e3a. \u5982\u679c map_location \u662f\u4e00\u4e2a\u5b57\u5178, \u5b83\u7528\u4e8e\u5c06\u51fa\u73b0\u5728\u6587\u4ef6 (\u952e) \u4e2d\u7684\u4f4d\u7f6e\u6807\u7b7e, \u91cd\u65b0\u6620\u5c04\u5230\u53e6\u4e00\u4e2a\u4f4d\u7f6e\u6807\u7b7e, \u5b83\u51fa\u73b0\u5728\u503c\u4e2d\u5e76\u6307\u660e\u5728\u54ea\u91cc\u5b58\u653e\u5b58\u50a8. \u7528\u6237\u6269\u5c55\u53ef\u4ee5\u4f7f\u7528 register_package \u6765\u6ce8\u518c\u4ed6\u4eec\u81ea\u5df1\u7684\u4f4d\u7f6e\u6807\u7b7e, \u4ee5\u53ca\u6807\u8bb0\u548c\u53cd\u5e8f\u5217\u5316\u65b9\u6cd5. \u53c2\u6570: f: \u4e00\u4e2a\u7c7b\u6587\u4ef6\u5bf9\u8c61 (\u5fc5\u987b\u5b9e\u73b0\u8fd4\u56de\u6587\u4ef6\u63cf\u8ff0\u7b26\u7684 fileno, \u4ee5\u53ca seek \u65b9\u6cd5), \u6216\u8005\u5305\u542b\u6587\u4ef6\u540d\u7684\u5b57\u7b26\u4e32. map_location: \u4e00\u4e2a\u51fd\u6570\u6216\u8005\u4e00\u4e2a\u6307\u660e\u5982\u4f55\u91cd\u65b0\u6620\u5c04\u5b58\u50a8\u4f4d\u7f6e\u7684\u5b57\u5178 pickle_module: \u7528\u4e8e\u89e3\u5c01 (unpickling) \u5143\u6570\u636e\u548c\u5bf9\u8c61\u7684\u6a21\u5757 (\u5fc5\u987b\u5339\u914d\u7528\u4e8e\u5e8f\u5217\u5316\u6587\u4ef6\u7684 pickle_module) \u793a\u4f8b: >>> torch.load('tensors.pt') # Load all tensors onto the CPU >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage) # Load all tensors onto GPU 1 >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1)) # Map tensors from GPU 1 to GPU 0 >>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'}) Parallelism (\u5e76\u884c\u5316) torch.get_num_threads() \u2192 int \u83b7\u5f97 OpenMP \u5e76\u884c\u5316\u64cd\u4f5c\u7684\u7ebf\u7a0b\u6570\u76ee torch.set_num_threads(int) \u8bbe\u7f6e OpenMP \u5e76\u884c\u5316\u64cd\u4f5c\u7684\u7ebf\u7a0b\u6570\u76ee Math operations (\u6570\u5b66\u64cd\u4f5c) Pointwise Ops (\u9010\u70b9\u64cd\u4f5c) torch.abs(input, out=None) \u2192 Tensor \u8ba1\u7b97\u7ed9\u5b9a input \u5f20\u91cf\u7684\u5143\u7d20\u7684\u7edd\u5bf9\u503c. Example: >>> torch.abs(torch.FloatTensor([-1, -2, 3])) FloatTensor([1, 2, 3]) torch.acos(input, out=None) \u2192 Tensor \u7528 input \u5143\u7d20\u7684\u53cd\u4f59\u5f26\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 the input <cite>Tensor</cite> out ( Tensor , optional ) \u2013 The result <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.acos(a) 2.2608 1.2956 1.1075 nan [torch.FloatTensor of size 4] torch.add() torch.add(input, value, out=None) \u5c06\u6807\u91cf\u503c value \u6dfb\u52a0\u5230\u8f93\u5165\u5f20\u91cf attr:<cite>input</cite> \u7684\u6bcf\u4e2a\u5143\u7d20\u5e76\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u7ed3\u679c\u5f20\u91cf. \\(out = tensor + value\\) \u5982\u679c\u8f93\u5165\u5f20\u91cf input \u662f FloatTensor \u6216\u8005 DoubleTensor \u7c7b\u578b, \u5219 value \u5fc5\u987b\u4e3a\u5b9e\u6570, \u5426\u5219\u4e3a\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> value ( Number ) \u2013 \u8981\u6dfb\u52a0\u5230 input \u6bcf\u4e2a\u5143\u7d20\u7684\u6570 out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 0.4050 -1.2227 1.8688 -0.4185 [torch.FloatTensor of size 4] >>> torch.add(a, 20) 20.4050 18.7773 21.8688 19.5815 [torch.FloatTensor of size 4] torch.add(input, value=1, other, out=None) \u5f20\u91cf other \u7684\u6bcf\u4e2a\u5143\u7d20\u4e58\u4ee5\u6807\u91cf\u503c value \u5e76\u52a0\u5230\u5f20\u91cf input \u4e0a, \u8fd4\u56de\u751f\u6210\u7684\u5f20\u91cf out . \u5f20\u91cf input \u7684\u5f62\u72b6\u4e0e\u5f20\u91cf other \u7684\u5f62\u72b6\u5fc5\u987b broadcastable . \\(out = input + (other * value)\\) \u5982\u679c\u5f20\u91cf other \u662f FloatTensor \u6216\u8005 DoubleTensor \u7c7b\u578b, \u5219 value \u5fc5\u987b\u4e3a\u5b9e\u6570, \u5426\u5219\u4e3a\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u7b2c\u4e00\u4e2a\u8f93\u5165 <cite>Tensor</cite> value ( Number ) \u2013 \u5f20\u91cf other \u7684\u6807\u91cf\u4e58\u6570 other ( Tensor ) \u2013 \u7b2c\u4e8c\u4e2a\u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> import torch >>> a = torch.randn(4) >>> a -0.9310 2.0330 0.0852 -0.2941 [torch.FloatTensor of size 4] >>> b = torch.randn(2, 2) >>> b 1.0663 0.2544 -0.1513 0.0749 [torch.FloatTensor of size 2x2] >>> torch.add(a, 10, b) 9.7322 4.5770 -1.4279 0.4552 [torch.FloatTensor of size 4] torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) \u2192 Tensor \u5c06\u5f20\u91cf tensor1 \u9010\u5143\u7d20\u9664\u4ee5\u5f20\u91cf tensor2 , \u7136\u540e\u4e58\u4ee5\u6807\u91cf\u503c value \u5e76\u52a0\u5230\u5f20\u91cf tensor \u4e0a. \u5f20\u91cf tensor , \u5f20\u91cf tensor1 , \u5f20\u91cf tensor2 \u7684\u5f62\u72b6\u5fc5\u987b broadcastable . \u5bf9\u4e8e\u7c7b\u578b\u4e3a <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7684\u5f20\u91cf\u8f93\u5165, value \u5fc5\u987b\u4e3a\u5b9e\u6570, \u5426\u5219\u4e3a\u6574\u6570. | Parameters: | tensor ( Tensor ) \u2013 \u5f20\u91cf, \u5bf9 <cite>tensor1 ./ tensor2</cite> \u8fdb\u884c\u76f8\u52a0 value ( Number__, optional ) \u2013 \u6807\u91cf, \u5bf9 <cite>tensor1 ./ tensor2</cite> \u8fdb\u884c\u76f8\u4e58 tensor1 ( Tensor ) \u2013 \u5206\u5b50\u5f20\u91cf, \u5373\u4f5c\u4e3a\u88ab\u9664\u6570 tensor2 ( Tensor ) \u2013 \u5206\u6bcd\u5f20\u91cf, \u5373\u4f5c\u4e3a\u9664\u6570 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> t = torch.randn(2, 3) >>> t1 = torch.randn(1, 6) >>> t2 = torch.randn(6, 1) >>> torch.addcdiv(t, 0.1, t1, t2) 0.0122 -0.0188 -0.2354 0.7396 -1.5721 1.2878 [torch.FloatTensor of size 2x3] torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) \u2192 Tensor \u5c06\u5f20\u91cf tensor1 \u9010\u5143\u7d20\u4e0e\u5f20\u91cf tensor2 \u76f8\u4e58, \u7136\u540e\u4e58\u4ee5\u6807\u91cf\u503c value \u5e76\u52a0\u5230\u5f20\u91cf tensor \u4e0a. \u5f20\u91cf tensor , \u5f20\u91cf tensor1 , \u5f20\u91cf tensor2 \u7684\u5f62\u72b6\u5fc5\u987b broadcastable . \u5bf9\u4e8e\u7c7b\u578b\u4e3a <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7684\u5f20\u91cf\u8f93\u5165, value \u5fc5\u987b\u4e3a\u5b9e\u6570, \u5426\u5219\u4e3a\u6574\u6570. :param tensor: \u5f20\u91cf, \u5bf9 <cite>tensor1 . tensor2</cite> \u8fdb\u884c\u76f8\u52a0 :type tensor: Tensor :param value: \u6807\u91cf, \u5bf9 <cite>tensor1 . tensor2</cite> \u8fdb\u884c\u76f8\u4e58 :type value: Number, optional :param tensor1: \u5f20\u91cf, \u4f5c\u4e3a\u4e58\u5b501 :type tensor1: Tensor :param tensor2: \u5f20\u91cf, \u4f5c\u4e3a\u4e58\u5b502 :type tensor2: Tensor :param out: \u8f93\u51fa\u5f20\u91cf :type out: Tensor, optional Example: >>> t = torch.randn(2, 3) >>> t1 = torch.randn(1, 6) >>> t2 = torch.randn(6, 1) >>> torch.addcmul(t, 0.1, t1, t2) 0.0122 -0.0188 -0.2354 0.7396 -1.5721 1.2878 [torch.FloatTensor of size 2x3] torch.asin(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684 <cite>Tensor</cite> , \u5176\u5143\u7d20\u4e3a\u5f20\u91cf input \u7684\u6bcf\u4e2a\u5143\u7d20\u7684\u53cd\u6b63\u5f26. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.asin(a) -0.6900 0.2752 0.4633 nan [torch.FloatTensor of size 4] torch.atan(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684 <cite>Tensor</cite> , \u5176\u5143\u7d20\u4e3a\u5f20\u91cf input \u7684\u6bcf\u4e2a\u5143\u7d20\u7684\u53cd\u6b63\u5207. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.atan(a) -0.5669 0.2653 0.4203 0.9196 [torch.FloatTensor of size 4] torch.atan2(input1, input2, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u8f93\u5165\u5f20\u91cf input1 \u548c\u8f93\u5165\u5f20\u91cf input2 \u5143\u7d20\u7684\u53cd\u6b63\u5207. \u8f93\u5165\u5f20\u91cf input1 \u7684\u5f62\u72b6\u548c\u8f93\u5165\u5f20\u91cf input2 \u7684\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . | Parameters: | input1 ( Tensor ) \u2013 \u7b2c\u4e00\u4e2a\u8f93\u5165 <cite>Tensor</cite> input2 ( Tensor ) \u2013 \u7b2c\u4e8c\u4e2a\u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.atan2(a, torch.randn(4)) -2.4167 2.9755 0.9363 1.6613 [torch.FloatTensor of size 4] torch.ceil(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u7684\u5143\u7d20\u5411\u4e0a\u53d6\u6574(\u53d6\u4e0d\u5c0f\u4e8e\u6bcf\u4e2a\u5143\u7d20\u7684\u6700\u5c0f\u6574\u6570). | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.ceil(a) 2 1 -0 -0 [torch.FloatTensor of size 4] torch.clamp(input, min, max, out=None) \u2192 Tensor \u5c06\u8f93\u5165\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u9650\u5236\u5728\u533a\u95f4 <cite>[min, max]</cite> \u4e2d\u5e76\u8fd4\u56de\u4e00\u4e2a\u7ed3\u679c\u5f20\u91cf. | min, if x_i < min y_i = | x_i, if min <= x_i <= max | max, if x_i > max \u5982\u679c\u8f93\u5165\u5f20\u91cf input \u7684\u7c7b\u578b <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite>, \u90a3\u4e48\u53c2\u6570 min \u548c max \u5fc5\u987b\u4e3a\u5b9e\u6570, \u5426\u5219\u4e3a\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> min ( Number ) \u2013 \u9650\u5236\u8303\u56f4\u4e0b\u9650 max ( Number ) \u2013 \u9650\u5236\u8303\u56f4\u4e0a\u9650 out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.clamp(a, min=-0.5, max=0.5) 0.5000 0.3912 -0.5000 -0.5000 [torch.FloatTensor of size 4] torch.clamp(input, *, min, out=None) \u2192 Tensor \u5f20\u91cf input \u7684\u6240\u6709\u5143\u7d20\u503c\u5927\u4e8e\u6216\u8005\u7b49\u4e8e min . \u5982\u679c\u5f20\u91cf input \u7684\u7c7b\u578b\u662f <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite>, \u5219 value \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> value ( Number ) \u2013 \u8f93\u51fa\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u6700\u5c0f\u503c out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.clamp(a, min=0.5) 1.3869 0.5000 0.5000 0.5000 [torch.FloatTensor of size 4] torch.clamp(input, *, max, out=None) \u2192 Tensor \u5f20\u91cf input \u7684\u6240\u6709\u5143\u7d20\u503c\u5c0f\u4e8e\u6216\u8005\u7b49\u4e8e max . \u5982\u679c\u5f20\u91cf input \u7684\u7c7b\u578b\u662f <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite>, \u5219 value \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> value ( Number ) \u2013 \u8f93\u51fa\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u6700\u5927\u503c out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.clamp(a, max=0.5) 0.5000 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] torch.cos(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u6bcf\u4e2a\u5143\u7d20\u7684\u4f59\u5f26. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.cos(a) 0.8041 0.9633 0.9018 0.2557 [torch.FloatTensor of size 4] torch.cosh(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u6bcf\u4e2a\u5143\u7d20\u7684\u53cc\u66f2\u4f59\u5f26. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.cosh(a) 1.2095 1.0372 1.1015 1.9917 [torch.FloatTensor of size 4] torch.div() torch.div(input, value, out=None) \u5c06\u5f20\u91cf input \u7684\u5143\u7d20\u9010\u4e00\u9664\u4ee5\u6807\u91cf\u503c value , \u5176\u7ed3\u679c\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\u8fd4\u56de. \\(out = tensor / value\\) \u5982\u679c\u5f20\u91cf input \u7684\u7c7b\u578b\u662f <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite>, \u5219\u6807\u91cf\u503c value \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> value ( Number ) \u2013 \u9664\u6570, \u88ab\u5f20\u91cf input \u7684\u5143\u7d20\u9664 out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(5) >>> a -0.6147 -1.1237 -0.1604 -0.6853 0.1063 [torch.FloatTensor of size 5] >>> torch.div(a, 0.5) -1.2294 -2.2474 -0.3208 -1.3706 0.2126 [torch.FloatTensor of size 5] torch.div(input, other, out=None) \u5f20\u91cf input \u7684\u5143\u7d20\u4e0e\u5f20\u91cf other \u7684\u5143\u7d20\u9010\u4e00\u76f8\u9664. \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u7ed3\u679c\u5f20\u91cf out . \u5f20\u91cf input \u4e0e\u5f20\u91cf other \u7684\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . \\(out_i = input_i / other_i\\) | Parameters: | input ( Tensor ) \u2013 \u5206\u5b50 <cite>Tensor</cite> (\u88ab\u9664\u6570) other ( Tensor ) \u2013 \u5206\u6bcd <cite>Tensor</cite> (\u9664\u6570) out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4,4) >>> a -0.1810 0.4017 0.2863 -0.1013 0.6183 2.0696 0.9012 -1.5933 0.5679 0.4743 -0.0117 -0.1266 -0.1213 0.9629 0.2682 1.5968 [torch.FloatTensor of size 4x4] >>> b = torch.randn(8, 2) >>> b 0.8774 0.7650 0.8866 1.4805 -0.6490 1.1172 1.4259 -0.8146 1.4633 -0.1228 0.4643 -0.6029 0.3492 1.5270 1.6103 -0.6291 [torch.FloatTensor of size 8x2] >>> torch.div(a, b) -0.2062 0.5251 0.3229 -0.0684 -0.9528 1.8525 0.6320 1.9559 0.3881 -3.8625 -0.0253 0.2099 -0.3473 0.6306 0.1666 -2.5381 [torch.FloatTensor of size 4x4] torch.erf(tensor, out=None) \u2192 Tensor \u8ba1\u7b97\u6bcf\u4e2a\u5143\u7d20\u7684\u8bef\u5dee\u51fd\u6570. Example: >>> torch.erf(torch.Tensor([0, -1., 10.])) torch.FloatTensor([0., -0.8427, 1.]) torch.erfinv(tensor, out=None) \u2192 Tensor \u8ba1\u7b97\u6bcf\u4e2a\u5143\u7d20\u7684\u53cd\u5411\u8bef\u5dee\u51fd\u6570. Example: >>> torch.erfinv(torch.Tensor([0, 0.5., -1.])) torch.FloatTensor([0., 0.4769, -inf]) torch.exp(tensor, out=None) \u2192 Tensor \u8ba1\u7b97\u6bcf\u4e2a\u5143\u7d20\u7684\u6307\u6570. Example: >>> torch.exp(torch.Tensor([0, math.log(2)])) torch.FloatTensor([1, 2]) torch.floor(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u7684\u5143\u7d20\u5411\u4e0b\u53d6\u6574(\u53d6\u4e0d\u5927\u4e8e\u6bcf\u4e2a\u5143\u7d20\u7684\u6700\u5927\u6574\u6570). | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.floor(a) 1 0 -1 -1 [torch.FloatTensor of size 4] torch.fmod(input, divisor, out=None) \u2192 Tensor \u8ba1\u7b97\u9664\u6cd5\u4f59\u6570. \u88ab\u9664\u6570\u548c\u9664\u6570\u53ef\u80fd\u540c\u65f6\u542b\u6709\u6574\u6570\u548c\u6d6e\u70b9\u6570. \u8fd9\u65f6\u4f59\u6570\u7684\u6b63\u8d1f\u4e0e\u88ab\u9664\u6570 <cite>tensor</cite> \u76f8\u540c. \u5f53\u9664\u6570 divisor \u662f\u4e00\u4e2a\u5f20\u91cf\u65f6r, \u5f20\u91cf input \u548c\u5f20\u91cf divisor \u7684\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . | Parameters: | input ( Tensor ) \u2013 \u88ab\u9664\u6570 divisor ( Tensor or float ) \u2013 \u9664\u6570. \u53ef\u80fd\u662f\u4e00\u4e2a\u6570\u6216\u8005\u662f\u4e00\u4e2a\u4e0e\u88ab\u9664\u6570\u76f8\u540c\u5f62\u72b6\u7684\u5f20\u91cf. out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> torch.fmod(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2) torch.FloatTensor([-1, -0, -1, 1, 0, 1]) >>> torch.fmod(torch.Tensor([1, 2, 3, 4, 5]), 1.5) torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5]) See also torch.remainder() , \u5176\u8ba1\u7b97\u7b49\u4ef7\u4e8e Python\u2019s <cite>%</cite> \u64cd\u4f5c\u7b26\u7684\u5143\u7d20\u4f59\u6570 torch.frac(tensor, out=None) \u2192 Tensor \u8ba1\u7b97\u5f20\u91cf <cite>tensor</cite> \u6bcf\u4e2a\u5143\u7d20\u7684\u5206\u6570\u90e8\u5206. Example: >>> torch.frac(torch.Tensor([1, 2.5, -3.2]) torch.FloatTensor([0, 0.5, -0.2]) torch.lerp(start, end, weight, out=None) \u57fa\u4e8e\u6807\u91cf\u503c weight : , \u5728\u5f20\u91cf start \u4e0e\u5f20\u91cf end \u4e4b\u95f4\u505a\u7ebf\u6027\u63d2\u503c \u5e76\u8fd4\u56de\u7ed3\u679c\u5f20\u91cf out . \\(out_i = start_i + weight * (end_i - start_i)\\) \u5f20\u91cf start \u548c\u5f20\u91cf end \u7684\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . | Parameters: | start ( Tensor ) \u2013 \u8d77\u59cb\u70b9 <cite>Tensor</cite> end ( Tensor ) \u2013 \u7ec8\u70b9 <cite>Tensor</cite> weight ( float ) \u2013 \u63d2\u503c\u516c\u5f0f\u7684\u6743\u91cd out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> start = torch.arange(1, 5) >>> end = torch.Tensor(4).fill_(10) >>> start 1 2 3 4 [torch.FloatTensor of size 4] >>> end 10 10 10 10 [torch.FloatTensor of size 4] >>> torch.lerp(start, end, 0.5) 5.5000 6.0000 6.5000 7.0000 [torch.FloatTensor of size 4] torch.log(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u81ea\u7136\u5bf9\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(5) >>> a -0.4183 0.3722 -0.3091 0.4149 0.5857 [torch.FloatTensor of size 5] >>> torch.log(a) nan -0.9883 nan -0.8797 -0.5349 [torch.FloatTensor of size 5] torch.log1p(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f(1 + input ) \u7684\u81ea\u7136\u5bf9\u6570. \\(y_i = log(x_i + 1)\\) Note \u5bf9\u4e8e\u8f83\u5c0f\u7684\u5f20\u91cf input \u7684\u503c, \u6b64\u51fd\u6570\u6bd4 torch.log() \u66f4\u7cbe\u786e. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(5) >>> a -0.4183 0.3722 -0.3091 0.4149 0.5857 [torch.FloatTensor of size 5] >>> torch.log1p(a) -0.5418 0.3164 -0.3697 0.3471 0.4611 [torch.FloatTensor of size 5] torch.mul() torch.mul(input, value, out=None) \u5c06\u8f93\u5165\u5f20\u91cf input \u7684\u6bcf\u4e2a\u5143\u7d20\u4e0e\u6807\u91cf\u503c value \u76f8\u4e58\u5e76\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u7ed3\u679c\u5f20\u91cf. \\(out = tensor * value\\) \u5982\u679c\u5f20\u91cf input \u7684\u7c7b\u578b\u4e3a <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, \u5219 value \u5e94\u8be5\u662f\u5b9e\u6570, \u5426\u5219\u4e3a\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> value ( Number ) \u2013 \u4e0e\u5f20\u91cf input \u6bcf\u4e2a\u5143\u7d20\u76f8\u4e58\u7684\u6570 out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(3) >>> a -0.9374 -0.5254 -0.6069 [torch.FloatTensor of size 3] >>> torch.mul(a, 100) -93.7411 -52.5374 -60.6908 [torch.FloatTensor of size 3] torch.mul(input, other, out=None) \u5f20\u91cf input \u7684\u5143\u7d20\u4e0e\u5f20\u91cf other \u7684\u5143\u7d20\u9010\u4e00\u76f8\u4e58. \u5176\u7ed3\u679c\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\u8fd4\u56de. \u5f20\u91cf input \u548c\u5f20\u91cf other \u7684\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . \\(out_i = input_i * other_i\\) | Parameters: | input ( Tensor ) \u2013 \u7b2c\u4e00\u4e2a\u4e58\u6570 <cite>Tensor</cite> other ( Tensor ) \u2013 \u7b2c\u4e8c\u4e2a\u4e58\u6570 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4,4) >>> a -0.7280 0.0598 -1.4327 -0.5825 -0.1427 -0.0690 0.0821 -0.3270 -0.9241 0.5110 0.4070 -1.1188 -0.8308 0.7426 -0.6240 -1.1582 [torch.FloatTensor of size 4x4] >>> b = torch.randn(2, 8) >>> b 0.0430 -1.0775 0.6015 1.1647 -0.6549 0.0308 -0.1670 1.0742 -1.2593 0.0292 -0.0849 0.4530 1.2404 -0.4659 -0.1840 0.5974 [torch.FloatTensor of size 2x8] >>> torch.mul(a, b) -0.0313 -0.0645 -0.8618 -0.6784 0.0934 -0.0021 -0.0137 -0.3513 1.1638 0.0149 -0.0346 -0.5068 -1.0304 -0.3460 0.1148 -0.6919 [torch.FloatTensor of size 4x4] torch.neg(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u7684\u5143\u7d20\u7684\u8d1f\u503c. \\(out = -1 * input\\) | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(5) >>> a -0.4430 1.1690 -0.8836 -0.4565 0.2968 [torch.FloatTensor of size 5] >>> torch.neg(a) 0.4430 -1.1690 0.8836 0.4565 -0.2968 [torch.FloatTensor of size 5] torch.pow() torch.pow(input, exponent, out=None) \u5bf9\u8f93\u5165\u5f20\u91cf input \u6309\u5143\u7d20\u6c42 exponent \u6b21\u5e42\u503c\u5e76\u8fd4\u56de\u7ed3\u679c\u5f20\u91cf(\u5176\u503c\u4f5c\u4e3a\u7ed3\u679c\u5f20\u91cf\u7684\u5143\u7d20). \u5e42\u503c exponent \u53ef\u4ee5\u662f\u4e00\u4e2a\u5355\u4e00\u7684\u6d6e\u70b9\u6570 float \u6216\u8005\u662f\u4e00\u4e2a\u4e0e\u5f20\u91cf input \u6709\u76f8\u540c\u5143\u7d20\u6570\u7684\u5f20\u91cf Tensor . \u5f53\u6307\u6570 exponent \u662f\u4e00\u4e2a\u6807\u91cf\u65f6, \u6267\u884c\u64cd\u4f5c: \\(out_i = x_i ^ {exponent}\\) \u5f53\u6307\u6570 exponent \u662f\u4e00\u4e2a\u5f20\u91cf, \u6267\u884c\u64cd\u4f5c: \\(out_i = x_i ^ {exponent_i}\\) \u5f53\u5e42\u503c exponent \u662f\u4e00\u4e2a\u5f20\u91cf, \u5f20\u91cf input \u548c\u5f20\u91cf exponent \u7684\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> exponent ( float or Tensor ) \u2013 \u6307\u6570 out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.5274 -0.8232 -2.1128 1.7558 [torch.FloatTensor of size 4] >>> torch.pow(a, 2) 0.2781 0.6776 4.4640 3.0829 [torch.FloatTensor of size 4] >>> exp = torch.arange(1, 5) >>> a = torch.arange(1, 5) >>> a 1 2 3 4 [torch.FloatTensor of size 4] >>> exp 1 2 3 4 [torch.FloatTensor of size 4] >>> torch.pow(a, exp) 1 4 27 256 [torch.FloatTensor of size 4] torch.pow(base, input, out=None) base \u662f\u4e00\u4e2a\u6807\u91cf\u6d6e\u70b9\u503c, input \u662f\u4e00\u4e2a\u5f20\u91cf. \u8fd4\u56de\u7684\u5f20\u91cf out \u7684\u5f62\u72b6\u4e0e\u5f20\u91cf input \u7684\u5f62\u72b6\u76f8\u540c. \u6267\u884c\u64cd\u4f5c: \\(out_i = base ^ {input_i}\\) | Parameters: | base ( float ) \u2013 \u5e42\u8fd0\u7b97\u7684\u5e95\u6570 input ( Tensor ) \u2013 \u6307\u6570 out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> exp = torch.arange(1, 5) >>> base = 2 >>> torch.pow(base, exp) 2 4 8 16 [torch.FloatTensor of size 4] torch.reciprocal(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684 <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u5012\u6570, i.e. \\(1.0 / x\\) | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.reciprocal(a) 0.7210 2.5565 -1.1583 -1.8289 [torch.FloatTensor of size 4] torch.remainder(input, divisor, out=None) \u2192 Tensor \u8ba1\u7b97\u5143\u7d20\u7684\u9664\u6cd5\u7684\u4f59\u6570. \u9664\u6570\u4e0e\u88ab\u9664\u6570\u53ef\u80fd\u540c\u65f6\u5305\u542b\u6574\u6570\u6216\u6d6e\u70b9\u6570. \u4f59\u6570\u4e0e\u9664\u6570\u6709\u76f8\u540c\u7684\u7b26\u53f7. \u5f53\u9664\u6570 divisor \u662f\u4e00\u4e2a\u5f20\u91cf, \u5f20\u91cf input \u7684\u5f62\u72b6\u548c\u5f20\u91cf divisor \u5f97\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . | Parameters: | input ( Tensor ) \u2013 \u88ab\u9664\u6570 divisor ( Tensor or float ) \u2013 \u9664\u6570. \u53ef\u80fd\u662f\u4e00\u4e2a\u6570\u6216\u8005\u53ef\u80fd\u662f\u4e00\u4e2a\u4e0e\u88ab\u9664\u6570\u5927\u5c0f\u76f8\u540c\u7684\u5f20\u91cf out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> torch.remainder(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2) torch.FloatTensor([1, 0, 1, 1, 0, 1]) >>> torch.remainder(torch.Tensor([1, 2, 3, 4, 5]), 1.5) torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5]) See also torch.fmod() \u540c\u6837\u8ba1\u7b97\u9664\u6cd5\u4f59\u6570, \u7b49\u6548\u4e8eC\u5e93\u51fd\u6570\u4e2d\u7684 fmod() torch.round(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u8f93\u5165\u5f20\u91cf\u7684\u5143\u7d20\u56db\u820d\u4e94\u5165\u5230\u6700\u8fd1\u7684\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] >>> torch.round(a) 1 1 -1 -0 [torch.FloatTensor of size 4] torch.rsqrt(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u5e73\u65b9\u6839\u7684\u5012\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] >>> torch.rsqrt(a) 0.9020 0.8636 nan nan [torch.FloatTensor of size 4] torch.sigmoid(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684sigmoid\u503c. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.4972 1.3512 0.1056 -0.2650 [torch.FloatTensor of size 4] >>> torch.sigmoid(a) 0.3782 0.7943 0.5264 0.4341 [torch.FloatTensor of size 4] torch.sign(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u7b26\u53f7. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.sign(a) -1 1 1 1 [torch.FloatTensor of size 4] torch.sin(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u6b63\u5f26. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.sin(a) -0.5944 0.2684 0.4322 0.9667 [torch.FloatTensor of size 4] torch.sinh(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u53cc\u66f2\u6b63\u5f26. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.sinh(a) -0.6804 0.2751 0.4619 1.7225 [torch.FloatTensor of size 4] torch.sqrt(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u5e73\u65b9\u6839. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] >>> torch.sqrt(a) 1.1086 1.1580 nan nan [torch.FloatTensor of size 4] torch.tan(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u6b63\u5207. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.tan(a) -0.7392 0.2786 0.4792 3.7801 [torch.FloatTensor of size 4] torch.tanh(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u53cc\u66f2\u6b63\u5207. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.tanh(a) -0.5625 0.2653 0.4193 0.8648 [torch.FloatTensor of size 4] torch.trunc(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u622a\u65ad\u6574\u6570\u503c (\u76f4\u63a5\u53bb\u9664\u5c0f\u6570\u90e8\u5206) . | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.4972 1.3512 0.1056 -0.2650 [torch.FloatTensor of size 4] >>> torch.trunc(a) -0 1 0 -0 [torch.FloatTensor of size 4] Reduction Ops (\u5f52\u7ea6\u64cd\u4f5c) torch.cumprod(input, dim, out=None) \u2192 Tensor \u8fd4\u56de\u5143\u7d20 input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u7684\u7d2f\u79ef\u79ef. \u4f8b\u5982, \u5982\u679c input \u662f\u4e00\u4e2aN\u5143\u5f20\u91cf, \u7ed3\u679c\u4e5f\u662f\u4e00\u4e2aN\u5143\u5f20\u91cf, \u5143\u7d20\u4e3a: \\(y_i = x_1 * x_2 * x_3 * ... * x_i\\) | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> dim ( int ) \u2013 \u8fdb\u884c\u64cd\u4f5c\u7684\u7ef4\u5ea6 out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(10) >>> a 1.1148 1.8423 1.4143 -0.4403 1.2859 -1.2514 -0.4748 1.1735 -1.6332 -0.4272 [torch.FloatTensor of size 10] >>> torch.cumprod(a, dim=0) 1.1148 2.0537 2.9045 -1.2788 -1.6444 2.0578 -0.9770 -1.1466 1.8726 -0.8000 [torch.FloatTensor of size 10] >>> a[5] = 0.0 >>> torch.cumprod(a, dim=0) 1.1148 2.0537 2.9045 -1.2788 -1.6444 -0.0000 0.0000 0.0000 -0.0000 0.0000 [torch.FloatTensor of size 10] torch.cumsum(input, dim, out=None) \u2192 Tensor \u8fd4\u56de\u5143\u7d20 input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u7684\u7d2f\u79ef\u548c. \u4f8b\u5982, \u5982\u679c input \u662f\u4e00\u4e2aN\u5143\u5f20\u91cf, \u7ed3\u679c\u5c06\u4e5f\u662f\u4e00\u4e2aN\u5143\u5f20\u91cf, \u5143\u7d20\u4e3a: \\(y_i = x_1 + x_2 + x_3 + ... + x_i\\) | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> dim ( int ) \u2013 \u8fdb\u884c\u64cd\u4f5c\u7684\u7ef4\u5ea6 out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(10) >>> a -0.6039 -0.2214 -0.3705 -0.0169 1.3415 -0.1230 0.9719 0.6081 -0.1286 1.0947 [torch.FloatTensor of size 10] >>> torch.cumsum(a, dim=0) -0.6039 -0.8253 -1.1958 -1.2127 0.1288 0.0058 0.9777 1.5858 1.4572 2.5519 [torch.FloatTensor of size 10] torch.dist(input, other, p=2) \u2192 float \u8fd4\u56de( input - other )\u7684p-\u8303\u6570 input \u548c other \u7684\u5f62\u72b6\u5fc5\u987b\u6ee1\u8db3 broadcastable . | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> other ( Tensor ) \u2013 \u53f3\u4fa7\u8f93\u5165 <cite>Tensor</cite> p ( float , optional ) \u2013 \u6240\u8ba1\u7b97\u7684\u8303\u6570. | | --- | --- | Example: >>> x = torch.randn(4) >>> x 0.2505 -0.4571 -0.3733 0.7807 [torch.FloatTensor of size 4] >>> y = torch.randn(4) >>> y 0.7782 -0.5185 1.4106 -2.4063 [torch.FloatTensor of size 4] >>> torch.dist(x, y, 3.5) 3.302832063224223 >>> torch.dist(x, y, 3) 3.3677282206393286 >>> torch.dist(x, y, 0) inf >>> torch.dist(x, y, 1) 5.560028076171875 torch.mean() torch.mean(input) \u2192 float \u8fd4\u56de\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u5747\u503c. Parameters: input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> Example: >>> a = torch.randn(1, 3) >>> a -0.2946 -0.9143 2.1809 [torch.FloatTensor of size 1x3] >>> torch.mean(a) 0.32398951053619385 torch.mean(input, dim, keepdim=False, out=None) \u2192 Tensor \u8fd4\u56de\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0a\u6bcf\u884c\u7684\u5747\u503c. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u5ea6 dim \u662f1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 out ( Tensor ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> a = torch.randn(4, 4) >>> a -1.2738 -0.3058 0.1230 -1.9615 0.8771 -0.5430 -0.9233 0.9879 1.4107 0.0317 -0.6823 0.2255 -1.3854 0.4953 -0.2160 0.2435 [torch.FloatTensor of size 4x4] >>> torch.mean(a, 1) -0.8545 0.0997 0.2464 -0.2157 [torch.FloatTensor of size 4] >>> torch.mean(a, 1, True) -0.8545 0.0997 0.2464 -0.2157 [torch.FloatTensor of size 4x1] torch.median() torch.median(input) \u2192 float \u8fd4\u56de\u8f93\u51fa\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u4e2d\u4f4d\u6570. Parameters: input ( Tensor ) \u2013 the input <cite>Tensor</cite> Example: >>> a = torch.randn(1, 3) >>> a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] >>> torch.median(a) -0.2085 torch.median(input, dim=-1, keepdim=False, values=None, indices=None) -> (Tensor, LongTensor) \u8fd4\u56de\u8f93\u51fa\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u7684\u4e2d\u4f4d\u6570. \u540c\u65f6\u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u4e2d\u4f4d\u6570\u7684\u7d22\u5f15 <cite>LongTensor</cite>. dim \u7684\u7f3a\u7701\u503c\u4e3a\u8f93\u5165\u5f20\u91cf input \u7684\u6700\u540e\u4e00\u7ef4. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u4e0e\u8f93\u5165\u5f20\u91cf input \u5f62\u72b6\u76f8\u540c, \u9664\u4e86\u7ef4\u6570 dim \u662f1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u6bd4\u8f93\u5165\u5f20\u91cf input \u5c11\u4e00\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u7559\u4e0e\u5426 values ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf indices ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf\u7d22\u5f15 | | --- | --- | Example: >>> a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] >>> a = torch.randn(4, 5) >>> a 0.4056 -0.3372 1.0973 -2.4884 0.4334 2.1336 0.3841 0.1404 -0.1821 -0.7646 -0.2403 1.3975 -2.0068 0.1298 0.0212 -1.5371 -0.7257 -0.4871 -0.2359 -1.1724 [torch.FloatTensor of size 4x5] >>> torch.median(a, 1) ( 0.4056 0.1404 0.0212 -0.7257 [torch.FloatTensor of size 4] , 0 2 4 1 [torch.LongTensor of size 4] ) torch.mode(input, dim=-1, keepdim=False, values=None, indices=None) -> (Tensor, LongTensor) \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u6570 dim \u4e0b\u6bcf\u884c\u5143\u7d20\u7684\u4f17\u6570\u503c. \u540c\u65f6\u4e5f\u8fd4\u56de\u4f17\u6570\u503c\u7684\u7d22\u5f15 <cite>LongTensor</cite>. \u7ef4\u5ea6 dim \u7684\u7f3a\u7701\u503c\u662f\u8f93\u5165\u5f20\u91cf input \u7684\u6700\u540e\u4e00\u7ef4. . \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u5ea6 dim \u662f1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. Note \u8fd9\u4e2a\u51fd\u6570\u81f3\u4eca\u6ca1\u6709\u4e3a torch.cuda.Tensor \u5b9a\u4e49. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 values ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf indices ( Tensor , optional ) \u2013 \u7ed3\u679c\u7d22\u5f15\u5f20\u91cf | | --- | --- | Example: >>> a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] >>> a = torch.randn(4, 5) >>> a 0.4056 -0.3372 1.0973 -2.4884 0.4334 2.1336 0.3841 0.1404 -0.1821 -0.7646 -0.2403 1.3975 -2.0068 0.1298 0.0212 -1.5371 -0.7257 -0.4871 -0.2359 -1.1724 [torch.FloatTensor of size 4x5] >>> torch.mode(a, 1) ( -2.4884 -0.7646 -2.0068 -1.5371 [torch.FloatTensor of size 4] , 3 4 2 0 [torch.LongTensor of size 4] ) torch.norm() torch.norm(input, p=2) \u2192 float \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u7684p-\u8303\u6570 | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> p ( float , optional ) \u2013 \u8303\u6570\u8ba1\u7b97\u4e2d\u7684\u5e42\u6307\u6570\u503c | | --- | --- | Example: >>> a = torch.randn(1, 3) >>> a -0.4376 -0.5328 0.9547 [torch.FloatTensor of size 1x3] >>> torch.norm(a, 3) 1.0338925067372466 torch.norm(input, p, dim, keepdim=False, out=None) \u2192 Tensor \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u5143\u7d20\u7684p-\u8303\u6570. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u975e\u7ef4\u5ea6 dim \u662f1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> p ( float ) \u2013 \u8303\u6570\u8ba1\u7b97\u4e2d\u7684\u5e42\u6307\u6570\u503c dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> a = torch.randn(4, 2) >>> a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] >>> torch.norm(a, 2, 1) 0.9585 0.7888 0.9077 0.6026 [torch.FloatTensor of size 4] >>> torch.norm(a, 0, 1, True) 2 2 2 2 [torch.FloatTensor of size 4x1] torch.prod() torch.prod(input) \u2192 float \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u4e58\u79ef. Parameters: input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> Example: >>> a = torch.randn(1, 3) >>> a 0.6170 0.3546 0.0253 [torch.FloatTensor of size 1x3] >>> torch.prod(a) 0.005537458061418483 torch.prod(input, dim, keepdim=False, out=None) \u2192 Tensor \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u5143\u7d20\u7684\u79ef. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u5ea6 dim \u662f1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> a = torch.randn(4, 2) >>> a 0.1598 -0.6884 -0.1831 -0.4412 -0.9925 -0.6244 -0.2416 -0.8080 [torch.FloatTensor of size 4x2] >>> torch.prod(a, 1) -0.1100 0.0808 0.6197 0.1952 [torch.FloatTensor of size 4] torch.std() torch.std(input, unbiased=True) \u2192 float \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u6807\u51c6\u5dee. \u5982\u679c unbiased \u662f False , \u90a3\u4e48\u6807\u51c6\u5dee\u5c06\u901a\u8fc7\u6709\u504f\u4f30\u8ba1\u8ba1\u7b97.\u5426\u5219, Bessel\u2019s correction \u5c06\u88ab\u4f7f\u7528. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> unbiased ( bool ) \u2013 \u662f\u5426\u4f7f\u7528\u65e0\u504f\u4f30\u8ba1 | | --- | --- | Example: >>> a = torch.randn(1, 3) >>> a -1.3063 1.4182 -0.3061 [torch.FloatTensor of size 1x3] >>> torch.std(a) 1.3782334731508061 torch.std(input, dim, keepdim=False, unbiased=True, out=None) \u2192 Tensor \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u5143\u7d20\u7684\u6807\u51c6\u5dee. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u5ea6 dim \u662f 1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. \u5982\u679c unbiased \u662f False , \u90a3\u4e48\u6807\u51c6\u5dee\u5c06\u901a\u8fc7\u6709\u504f\u4f30\u8ba1\u6765\u8ba1\u7b97. \u5426\u5219, Bessel\u2019s correction \u5c06\u88ab\u4f7f\u7528. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 unbiased ( bool ) \u2013 \u662f\u5426\u4f7f\u7528\u65e0\u504f\u4f30\u8ba1 out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> a = torch.randn(4, 4) >>> a 0.1889 -2.4856 0.0043 1.8169 -0.7701 -0.4682 -2.2410 0.4098 0.1919 -1.1856 -1.0361 0.9085 0.0173 1.0662 0.2143 -0.5576 [torch.FloatTensor of size 4x4] >>> torch.std(a, dim=1) 1.7756 1.1025 1.0045 0.6725 [torch.FloatTensor of size 4] torch.sum() torch.sum(input) \u2192 float \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u548c. Parameters: input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> Example: >>> a = torch.randn(1, 3) >>> a 0.6170 0.3546 0.0253 [torch.FloatTensor of size 1x3] >>> torch.sum(a) 0.9969287421554327 torch.sum(input, dim, keepdim=False, out=None) \u2192 Tensor \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u5143\u7d20\u7684\u548c. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u5ea6 dim \u662f 1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> a = torch.randn(4, 4) >>> a -0.4640 0.0609 0.1122 0.4784 -1.3063 1.6443 0.4714 -0.7396 -1.3561 -0.1959 1.0609 -1.9855 2.6833 0.5746 -0.5709 -0.4430 [torch.FloatTensor of size 4x4] >>> torch.sum(a, 1) 0.1874 0.0698 -2.4767 2.2440 [torch.FloatTensor of size 4] torch.var() torch.var(input, unbiased=True) \u2192 float \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u7684\u65b9\u5dee. \u5982\u679c unbiased \u662f False , \u65b9\u5dee\u7684\u8ba1\u7b97\u5c06\u901a\u8fc7\u6709\u504f\u4f30\u8ba1\u8ba1\u7b97. \u5426\u5219, Bessel\u2019s correction \u5c06\u4f1a\u88ab\u4f7f\u7528. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> unbiased ( bool ) \u2013 \u662f\u5426\u4f7f\u7528\u65e0\u504f\u4f30\u8ba1 | | --- | --- | Example: >>> a = torch.randn(1, 3) >>> a -1.3063 1.4182 -0.3061 [torch.FloatTensor of size 1x3] >>> torch.var(a) 1.899527506513334 torch.var(input, dim, keepdim=False, unbiased=True, out=None) \u2192 Tensor \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u7684\u65b9\u5dee. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u5ea6 dim \u662f 1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. \u5982\u679c unbiased \u662f False , \u65b9\u5dee\u7684\u8ba1\u7b97\u5c06\u901a\u8fc7\u6709\u504f\u4f30\u8ba1\u8ba1\u7b97. \u5426\u5219, Bessel\u2019s correction \u5c06\u4f1a\u88ab\u4f7f\u7528. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u7559\u4e0e\u5426 unbiased ( bool ) \u2013 \u662f\u5426\u4f7f\u7528\u65e0\u504f\u4f30\u8ba1 out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> a = torch.randn(4, 4) >>> a -1.2738 -0.3058 0.1230 -1.9615 0.8771 -0.5430 -0.9233 0.9879 1.4107 0.0317 -0.6823 0.2255 -1.3854 0.4953 -0.2160 0.2435 [torch.FloatTensor of size 4x4] >>> torch.var(a, 1) 0.8859 0.9509 0.7548 0.6949 [torch.FloatTensor of size 4] Comparison Ops (\u6bd4\u8f83\u64cd\u4f5c) torch.eq(input, other, out=None) \u2192 Tensor \u6bd4\u8f83\u5143\u7d20\u662f\u5426\u76f8\u7b49 \u7b2c\u4e8c\u4e2a\u5143\u7d20\u53ef\u4ee5\u662f\u4e00\u4e2a\u6570\u5b57\u6216 broadcastable \u4e3a\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570\u5f62\u72b6\u76f8\u540c\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 \u5f85\u6bd4\u8f83\u5f20\u91cf other ( Tensor or float ) \u2013 \u6bd4\u8f83\u5f20\u91cf\u6216\u6570 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf, \u987b\u4e3a ByteTensor \u7c7b\u578b\u6216\u4e0e input (Tensor) \u540c\u7c7b\u578b | | --- | --- | | Returns: | \u4e00\u4e2a torch.ByteTensor \u5f20\u91cf, \u5f85\u6bd4\u8f83\u548c\u8981\u6bd4\u8f83\u5f20\u91cf\u9010\u4f4d\u7f6e\u6bd4\u8f83, \u76f8\u7b49\u4e3a 1 , \u4e0d\u7b49\u4e3a 0 | | --- | --- | Example: >>> torch.eq(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 0 0 1 [torch.ByteTensor of size 2x2] torch.equal(tensor1, tensor2) \u2192 bool \u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u6709\u76f8\u540c\u7684\u5f62\u72b6\u548c\u5143\u7d20\u503c, \u5219\u8fd4\u56de True , \u5426\u5219 False . Example: >>> torch.equal(torch.Tensor([1, 2]), torch.Tensor([1, 2])) True torch.ge(input, other, out=None) \u2192 Tensor \u9010\u5143\u7d20\u6bd4\u8f83 input \u548c other , \u5373\u662f\u5426 input>=other . \u7b2c\u4e8c\u4e2a\u53c2\u6570\u53ef\u4ee5\u4e3a\u4e00\u4e2a\u6570\u6216\u5f62\u72b6\u53ef broadcastable \u4e3a\u548c\u7b2c\u4e00\u4e2a\u53c2\u6570\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 \u5f85\u5bf9\u6bd4\u7684\u5f20\u91cf other ( Tensor or float ) \u2013 \u5bf9\u6bd4\u7684\u5f20\u91cf\u6216 float \u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5fc5\u987b\u4e3a ByteTensor \u6216\u8005\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570 tensor \u76f8\u540c\u7c7b\u578b. | | --- | --- | | Returns: | \u4e00\u4e2a torch.ByteTensor \u5f20\u91cf, \u5305\u542b\u4e86\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u6bd4\u8f83\u7ed3\u679c(\u662f\u5426 input >= other ). | | --- | --- | | Return type: | \u5f20\u91cf | | --- | --- | Return type: \u5f20\u91cf Example: >>> torch.ge(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 1 0 1 [torch.ByteTensor of size 2x2] torch.gt(input, other, out=None) \u2192 Tensor \u9010\u5143\u7d20\u6bd4\u8f83 input \u548c other , \u5373\u662f\u5426 input>other \u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u6709\u76f8\u540c\u7684\u5f62\u72b6\u548c\u5143\u7d20\u503c, \u5219\u8fd4\u56de True ,\u5426\u5219 False . \u7b2c\u4e8c\u4e2a\u53c2\u6570\u53ef\u4ee5\u4e3a\u4e00\u4e2a\u6570\u6216\u5f62\u72b6\u53ef broadcastable \u4e3a\u548c\u7b2c\u4e00\u4e2a\u53c2\u6570\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 \u5f85\u5bf9\u6bd4\u7684\u5f20\u91cf other ( Tensor or float ) \u2013 \u5bf9\u6bd4\u7684\u5f20\u91cf\u6216 float \u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5fc5\u987b\u4e3a ByteTensor \u6216\u8005\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570 tensor \u76f8\u540c\u7c7b\u578b. | | --- | --- | | Returns: | \u4e00\u4e2a torch.ByteTensor \u5f20\u91cf, \u5305\u542b\u4e86\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u6bd4\u8f83\u7ed3\u679c(\u662f\u5426 input > other ). | | --- | --- | | Return type: | \u5f20\u91cf | | --- | --- | Return type: \u5f20\u91cf Example: >>> torch.gt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 1 0 0 [torch.ByteTensor of size 2x2] torch.kthvalue(input, k, dim=None, keepdim=False, out=None) -> (Tensor, LongTensor) \u53d6\u8f93\u5165\u5f20\u91cf input \u6307\u5b9a\u7ef4\u4e0a\u7b2c k \u4e2a\u6700\u5c0f\u503c. \u5982\u679c\u4e0d\u6307\u5b9a dim , \u5219\u9ed8\u8ba4\u4e3a input \u7684\u6700\u540e\u4e00\u7ef4. \u8fd4\u56de\u4e00\u4e2a\u5143\u7ec4 <cite>(values,indices)</cite> ,\u5176\u4e2d indices \u662f\u539f\u59cb\u8f93\u5165\u5f20\u91cf input \u4e2d\u6cbf dim \u7ef4\u7684\u7b2c k \u4e2a\u6700\u5c0f\u503c\u4e0b\u6807. \u5982\u679c keepdim \u4e3a True , values \u548c indices \u5f20\u91cf\u90fd\u548c input \u5927\u5c0f\u76f8\u540c, \u9664\u4e86\u5728\u6240\u6709\u503c\u90fd\u4e3a1\u7684 dim \u7ef4\u5ea6\u4e0a. \u5982\u679c keepdim \u4e3a False , dim \u88ab\u538b\u7f29. (\u53c2\u89c1 torch.squeeze() ), \u4f7f values \u548c indices \u4e24\u4e2a\u5f20\u91cf\u6bd4 input \u5f20\u91cf\u5c0f\u4e00\u4e2a\u7684\u7ef4\u5ea6. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> k ( int ) \u2013 \u7b2c k \u4e2a\u6700\u5c0f\u503c dim ( int , optional ) \u2013 \u6cbf\u7740\u6b64\u7ef4\u8fdb\u884c\u6392\u5e8f keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u6301\u7ef4\u5ea6 dim \u4e0d\u53d8 out ( tuple , optional ) \u2013 \u8f93\u51fa\u5143\u7ec4 ( Tensor, LongTensor ) \u53ef\u9009\u53c2\u6570(\u4f5c\u4e3a\u8f93\u51fa buffers ) | | --- | --- | Example: >>> x = torch.arange(1, 6) >>> x 1 2 3 4 5 [torch.FloatTensor of size 5] >>> torch.kthvalue(x, 4) ( 4 [torch.FloatTensor of size 1] , 3 [torch.LongTensor of size 1] ) >>> x=torch.arange(1,7).resize_(2,3) >>> x 1 2 3 4 5 6 [torch.FloatTensor of size 2x3] >>> torch.kthvalue(x,2,0,True) ( 4 5 6 [torch.FloatTensor of size 1x3] , 1 1 1 [torch.LongTensor of size 1x3] ) torch.le(input, other, out=None) \u2192 Tensor \u9010\u5143\u7d20\u6bd4\u8f83 input \u548c other , \u5373\u662f\u5426 input<=other \u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u6709\u76f8\u540c\u7684\u5f62\u72b6\u548c\u5143\u7d20\u503c, \u5219\u8fd4\u56de True ,\u5426\u5219 False . \u7b2c\u4e8c\u4e2a\u53c2\u6570\u53ef\u4ee5\u4e3a\u4e00\u4e2a\u6570\u6216\u5f62\u72b6\u53ef broadcastable \u4e3a\u548c\u7b2c\u4e00\u4e2a\u53c2\u6570\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 \u5f85\u5bf9\u6bd4\u7684\u5f20\u91cf other ( Tensor or float ) \u2013 \u5bf9\u6bd4\u7684\u5f20\u91cf\u6216 float \u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5fc5\u987b\u4e3a ByteTensor \u6216\u8005\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570 tensor \u76f8\u540c\u7c7b\u578b. | | --- | --- | | Returns: | \u4e00\u4e2a torch.ByteTensor \u5f20\u91cf, \u5305\u542b\u4e86\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u6bd4\u8f83\u7ed3\u679c(\u662f\u5426 input <= other ). | | --- | --- | | Return type: | \u5f20\u91cf | | --- | --- | Example: >>> torch.le(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 0 1 1 [torch.ByteTensor of size 2x2] torch.lt(input, other, out=None) \u2192 Tensor \u9010\u5143\u7d20\u6bd4\u8f83 input \u548c other , \u5373\u662f\u5426 input<other \u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u6709\u76f8\u540c\u7684\u5f62\u72b6\u548c\u5143\u7d20\u503c, \u5219\u8fd4\u56de True ,\u5426\u5219 False . \u7b2c\u4e8c\u4e2a\u53c2\u6570\u53ef\u4ee5\u4e3a\u4e00\u4e2a\u6570\u6216\u5f62\u72b6\u53ef broadcastable \u4e3a\u548c\u7b2c\u4e00\u4e2a\u53c2\u6570\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 \u5f85\u5bf9\u6bd4\u7684\u5f20\u91cf other ( Tensor or float ) \u2013 \u5bf9\u6bd4\u7684\u5f20\u91cf\u6216 float \u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5fc5\u987b\u4e3a ByteTensor \u6216\u8005\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570 tensor \u76f8\u540c\u7c7b\u578b. | | --- | --- | | Returns: | \u4e00\u4e2a torch.ByteTensor \u5f20\u91cf, \u5305\u542b\u4e86\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u6bd4\u8f83\u7ed3\u679c(\u662f\u5426 input < other ). | | --- | --- | | Return type: | \u5f20\u91cf | | --- | --- | Return type: \u5f20\u91cf Example: >>> torch.lt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 0 1 0 [torch.ByteTensor of size 2x2] torch.max() torch.max(input) \u2192 float \u8fd4\u56de\u8f93\u5165 input \u5f20\u91cf\u6240\u6709\u5143\u7d20\u7684\u6700\u5927\u503c. Parameters: input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> Example: >>> a = torch.randn(1, 3) >>> a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] >>> torch.max(a) 0.4729 torch.max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor) \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0a\u6bcf\u884c\u7684\u6700\u5927\u503c, \u5e76\u540c\u65f6\u8fd4\u56de\u6bcf\u4e2a\u6700\u5927\u503c\u7684\u4f4d\u7f6e\u7d22\u5f15. \u5982\u679c keepdim \u4e3a True , values \u548c indices \u5f20\u91cf\u90fd\u548c input \u5c3a\u5bf8\u76f8\u540c, \u9664\u4e86\u5728\u6240\u6709\u503c\u90fd\u4e3a 1 \u7684 dim \u7ef4\u5ea6\u4e0a. \u5982\u679c keepdim \u4e3a False , dim \u88ab\u538b\u7f29. (\u53c2\u89c1 torch.squeeze() ), \u4f7f values \u548c indices \u4e24\u4e2a\u5f20\u91cf\u6bd4 input \u5f20\u91cf\u5c0f\u4e00\u4e2a\u7684\u7ef4\u5ea6. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> k ( int ) \u2013 \u7b2c k \u4e2a\u6700\u5c0f\u503c dim ( int , optional ) \u2013 \u6cbf\u7740\u6b64\u7ef4\u8fdb\u884c\u6392\u5e8f keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u6301\u7ef4\u5ea6 dim \u4e0d\u53d8 out ( tuple , optional ) \u2013 \u8f93\u51fa\u5143\u7ec4 (max, max_indices) | | --- | --- | Example: >> a = torch.randn(4, 4) >> a 0.0692 0.3142 1.2513 -0.5428 0.9288 0.8552 -0.2073 0.6409 1.0695 -0.0101 -2.4507 -1.2230 0.7426 -0.7666 0.4862 -0.6628 torch.FloatTensor of size 4x4] >>> torch.max(a, 1) ( 1.2513 0.9288 1.0695 0.7426 [torch.FloatTensor of size 4] , 2 0 0 0 [torch.LongTensor of size 4] ) torch.max(input, other, out=None) \u2192 Tensor \u8f93\u5165 input \u6bcf\u4e00\u4e2a\u5143\u7d20\u548c\u5bf9\u5e94\u7684\u6bd4\u8f83\u5f20\u91cf other \u8fdb\u884c\u6bd4\u8f83, \u7559\u4e0b\u8f83\u5927\u7684\u5143\u7d20 <cite>max</cite>. \u8981\u6bd4\u8f83\u7684\u5f20\u91cf input \u4e0e\u6bd4\u8f83\u5f20\u91cf other \u4e0d\u5fc5\u5927\u5c0f\u4e00\u81f4, \u4f46\u5b83\u4eec\u4e00\u5b9a\u8981\u80fd broadcastable . | Parameters: | input ( Tensor ) \u2013 \u8981\u6bd4\u8f83\u5f20\u91cf <cite>Tensor</cite> other ( Tensor ) \u2013 \u6bd4\u8f83\u5f20\u91cf <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> b = torch.randn(4) >>> b 1.0067 -0.8010 0.6258 0.3627 [torch.FloatTensor of size 4] >>> torch.max(a, b) 1.3869 0.3912 0.6258 0.3627 [torch.FloatTensor of size 4] torch.min() torch.min(input) \u2192 float \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u6700\u5c0f\u503c. Parameters: input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> Example: >>> a = torch.randn(1, 3) >>> a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] >>> torch.min(a) -0.22663167119026184 torch.min(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor) \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u5143\u7d20\u7684\u6700\u5c0f\u503c. \u5176\u4e2d\u7b2c\u4e8c\u4e2a\u8fd4\u56de\u503c\u662f\u6bcf\u4e2a\u88ab\u627e\u51fa\u7684\u6700\u5c0f\u503c\u7684\u7d22\u5f15\u4f4d\u7f6e ( argmin ) . \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u6570 dim \u662f 1 . \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u6bd4\u8f93\u5165\u5f20\u91cf input \u5c11\u4e00\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 out ( tuple , optional ) \u2013 \u4e24\u4e2a\u8f93\u51fa\u5f20\u91cf\u7684\u7ed3\u679c\u5143\u7ec4 (min, min_indices) | | --- | --- | Example: >> a = torch.randn(4, 4) >> a 0.0692 0.3142 1.2513 -0.5428 0.9288 0.8552 -0.2073 0.6409 1.0695 -0.0101 -2.4507 -1.2230 0.7426 -0.7666 0.4862 -0.6628 torch.FloatTensor of size 4x4] >> torch.min(a, 1) 0.5428 0.2073 2.4507 0.7666 torch.FloatTensor of size 4] 3 2 2 1 torch.LongTensor of size 4] torch.min(input, other, out=None) \u2192 Tensor \u8f93\u5165 input \u6bcf\u4e00\u4e2a\u5143\u7d20\u548c\u5bf9\u5e94\u7684\u6bd4\u8f83\u5f20\u91cf other \u8fdb\u884c\u6bd4\u8f83, \u7559\u4e0b\u8f83\u5c0f\u7684\u5143\u7d20 <cite>min</cite> . \u8981\u6bd4\u8f83\u7684\u5f20\u91cf input \u4e0e\u6bd4\u8f83\u5f20\u91cf other \u4e0d\u5fc5\u5c3a\u5bf8\u4e00\u81f4, \u4f46\u5b83\u4eec\u4e00\u5b9a\u8981\u80fd\u5e7f\u64ad broadcastable . | Parameters: | input ( Tensor ) \u2013 \u7b2c\u4e00\u4e2a\u5f20\u91cf <cite>Tensor</cite> other ( Tensor ) \u2013 \u7b2c\u4e8c\u4e2a\u5f20\u91cf <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7684\u5f20\u91cf <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> b = torch.randn(4) >>> b 1.0067 -0.8010 0.6258 0.3627 [torch.FloatTensor of size 4] >>> torch.min(a, b) 1.0067 -0.8010 -0.8634 -0.5468 [torch.FloatTensor of size 4] torch.ne(input, other, out=None) \u2192 Tensor \u9010\u5143\u7d20\u6bd4\u8f83 input \u548c other , \u5373\u662f\u5426 tensor != other \u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u6709\u76f8\u540c\u7684\u5f62\u72b6\u548c\u5143\u7d20\u503c, \u5219\u8fd4\u56de True , \u5426\u5219 False . \u7b2c\u4e8c\u4e2a\u53c2\u6570\u53ef\u4ee5\u4e3a\u4e00\u4e2a\u6570\u6216\u5f62\u72b6\u5e7f\u64ad broadcastable \u4e3a\u548c\u7b2c\u4e00\u4e2a\u53c2\u6570\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 \u5f85\u5bf9\u6bd4\u7684\u5f20\u91cf other ( Tensor or float ) \u2013 \u5bf9\u6bd4\u7684\u5f20\u91cf\u6216 float \u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5fc5\u987b\u4e3a ByteTensor \u6216\u8005\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570 tensor \u76f8\u540c\u7c7b\u578b. | | --- | --- | | Returns: | \u4e00\u4e2a torch.ByteTensor \u5f20\u91cf, \u5305\u542b\u4e86\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u6bd4\u8f83\u7ed3\u679c (\u662f\u5426 input != other ) . | | --- | --- | | Return type: | \u5f20\u91cf | | --- | --- | \u8fd4\u56de\u7c7b\u578b: Tensor Example: >>> torch.ne(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 1 1 0 [torch.ByteTensor of size 2x2] torch.sort(input, dim=None, descending=False, out=None) -> (Tensor, LongTensor) \u5bf9\u8f93\u5165\u5f20\u91cf input \u6cbf\u7740\u6307\u5b9a\u7ef4\u6309\u5347\u5e8f\u6392\u5e8f. \u5982\u679c\u4e0d\u7ed9\u5b9a dim ,\u5219\u9ed8\u8ba4\u4e3a\u8f93\u5165\u7684\u6700\u540e\u4e00\u7ef4. \u5982\u679c\u6307\u5b9a\u53c2\u6570 descending \u4e3a True , \u5219\u6309\u964d\u5e8f\u6392\u5e8f. \u8fd4\u56de\u5143\u7ec4 (sorted_tensor, sorted_indices) , sorted_indices \u4e3a\u539f\u59cb\u8f93\u5165\u4e2d\u7684\u4e0b\u6807. | Parameters: | input ( Tensor ) \u2013 \u8981\u5bf9\u6bd4\u7684\u5f20\u91cf dim ( int , optional ) \u2013 \u6cbf\u7740\u6b64\u7ef4\u6392\u5e8f descending ( bool , optional ) \u2013 \u5e03\u5c14\u503c, \u63a7\u5236\u5347\u964d\u6392\u5e8f out ( tuple , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5fc5\u987b\u4e3a ByteTensor \u6216\u8005\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570 tensor \u76f8\u540c\u7c7b\u578b. | | --- | --- | Example: >>> x = torch.randn(3, 4) >>> sorted, indices = torch.sort(x) >>> sorted -1.6747 0.0610 0.1190 1.4137 -1.4782 0.7159 1.0341 1.3678 -0.3324 -0.0782 0.3518 0.4763 [torch.FloatTensor of size 3x4] >>> indices 0 1 3 2 2 1 0 3 3 1 0 2 [torch.LongTensor of size 3x4] >>> sorted, indices = torch.sort(x, 0) >>> sorted -1.6747 -0.0782 -1.4782 -0.3324 0.3518 0.0610 0.4763 0.1190 1.0341 0.7159 1.4137 1.3678 [torch.FloatTensor of size 3x4] >>> indices 0 2 1 2 2 0 2 0 1 1 0 1 [torch.LongTensor of size 3x4] torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -> (Tensor, LongTensor) \u6cbf\u7ed9\u5b9a dim \u7ef4\u5ea6\u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u4e2d k \u4e2a\u6700\u5927\u503c. \u5982\u679c\u4e0d\u6307\u5b9a dim , \u5219\u9ed8\u8ba4\u4e3a <cite>input</cite> \u7684\u6700\u540e\u4e00\u7ef4. \u5982\u679c\u4e3a largest \u4e3a False ,\u5219\u8fd4\u56de\u6700\u5c0f\u7684 <cite>k</cite> \u4e2a\u503c. \u8fd4\u56de\u4e00\u4e2a\u5143\u7ec4 <cite>(values, indices)</cite> , \u5176\u4e2d indices \u662f\u539f\u59cb\u8f93\u5165\u5f20\u91cf input \u4e2d\u6d4b\u5143\u7d20\u4e0b\u6807. \u5982\u679c\u8bbe\u5b9a\u5e03\u5c14\u503c sorted \u4e3a True , \u5c06\u4f1a\u786e\u4fdd\u8fd4\u56de\u7684 <cite>k</cite> \u4e2a\u503c\u88ab\u6392\u5e8f. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf k ( int ) \u2013 \u201ctop-k\u201d \u4e2d\u7684 k dim ( int , optional ) \u2013 \u6392\u5e8f\u7684\u7ef4 largest ( bool , optional ) \u2013 \u5e03\u5c14\u503c, \u63a7\u5236\u8fd4\u56de\u6700\u5927\u6216\u6700\u5c0f\u503c sorted ( bool , optional ) \u2013 \u5e03\u5c14\u503c, \u63a7\u5236\u8fd4\u56de\u503c\u662f\u5426\u6392\u5e8f out ( tuple , optional ) \u2013 \u53ef\u9009\u8f93\u51fa\u5f20\u91cf (Tensor, LongTensor) output buffers | | --- | --- | Example: >>> x = torch.arange(1, 6) >>> x 1 2 3 4 5 [torch.FloatTensor of size 5] >>> torch.topk(x, 3) ( 5 4 3 [torch.FloatTensor of size 3] , 4 3 2 [torch.LongTensor of size 3] ) >>> torch.topk(x, 3, 0, largest=False) ( 1 2 3 [torch.FloatTensor of size 3] , 0 1 2 [torch.LongTensor of size 3] ) Other Operations (\u5176\u5b83\u64cd\u4f5c) torch.cross(input, other, dim=-1, out=None) \u2192 Tensor \u8fd4\u56de\u6cbf\u7740\u7ef4\u5ea6 <cite>dim</cite> \u4e0a, \u4e24\u4e2a\u5f20\u91cf <cite>input</cite> \u548c <cite>other</cite> \u7684\u5411\u91cf\u79ef (\u53c9\u79ef), <cite>input</cite> \u548c <cite>other</cite> \u5fc5\u987b\u6709\u76f8\u540c\u7684\u5f62\u72b6, \u4e14\u6307\u5b9a\u7684 <cite>dim</cite> \u7ef4\u4e0a <cite>size</cite> \u5fc5\u987b\u4e3a 3. \u5982\u679c\u4e0d\u6307\u5b9a <cite>dim</cite>, \u5219\u9ed8\u8ba4\u4e3a\u7b2c\u4e00\u4e2a\u5c3a\u5ea6\u4e3a 3 \u7684\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> other ( Tensor ) \u2013 \u7b2c\u4e8c\u4e2a\u8f93\u5165 <cite>Tensor</cite> dim ( int , optional ) \u2013 \u6cbf\u7740\u6b64\u7ef4\u8fdb\u884c\u53c9\u79ef\u64cd\u4f5c. out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4, 3) >>> a -0.6652 -1.0116 -0.6857 0.2286 0.4446 -0.5272 0.0476 0.2321 1.9991 0.6199 1.1924 -0.9397 [torch.FloatTensor of size 4x3] >>> b = torch.randn(4, 3) >>> b -0.1042 -1.1156 0.1947 0.9947 0.1149 0.4701 -1.0108 0.8319 -0.0750 0.9045 -1.3754 1.0976 [torch.FloatTensor of size 4x3] >>> torch.cross(a, b, dim=1) -0.9619 0.2009 0.6367 0.2696 -0.6318 -0.4160 -1.6805 -2.0171 0.2741 0.0163 -1.5304 -1.9311 [torch.FloatTensor of size 4x3] >>> torch.cross(a, b) -0.9619 0.2009 0.6367 0.2696 -0.6318 -0.4160 -1.6805 -2.0171 0.2741 0.0163 -1.5304 -1.9311 [torch.FloatTensor of size 4x3] torch.diag(input, diagonal=0, out=None) \u2192 Tensor \u5982\u679c\u8f93\u5165\u662f\u4e00\u4e2a\u5411\u91cf( <cite>1D</cite> \u5f20\u91cf), \u5219\u8fd4\u56de\u4e00\u4e2a\u4ee5 <cite>input</cite> \u4e3a\u5bf9\u89d2\u7ebf\u5143\u7d20\u7684 <cite>2D</cite> \u65b9\u9635. \u5982\u679c\u8f93\u5165\u662f\u4e00\u4e2a\u77e9\u9635( <cite>2D</cite> \u5f20\u91cf), \u5219\u8fd4\u56de\u4e00\u4e2a\u5305\u542b <cite>input</cite> \u5bf9\u89d2\u7ebf\u5143\u7d20\u76841D\u5f20\u91cf. \u53c2\u6570 <cite>diagonal</cite> \u6307\u5b9a\u5bf9\u89d2\u7ebf: diagonal = 0, \u4e3b\u5bf9\u89d2\u7ebf. diagonal > 0, \u4e3b\u5bf9\u89d2\u7ebf\u4e4b\u4e0a. diagonal < 0, \u4e3b\u5bf9\u89d2\u7ebf\u4e4b\u4e0b. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> diagonal ( int , optional ) \u2013 \u6307\u5b9a\u5bf9\u89d2\u7ebf out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: \u83b7\u5f97\u4ee5 <cite>input</cite> \u4e3a\u5bf9\u89d2\u7ebf\u7684\u65b9\u9635: >>> a = torch.randn(3) >>> a 1.0480 -2.3405 -1.1138 [torch.FloatTensor of size 3] >>> torch.diag(a) 1.0480 0.0000 0.0000 0.0000 -2.3405 0.0000 0.0000 0.0000 -1.1138 [torch.FloatTensor of size 3x3] >>> torch.diag(a, 1) 0.0000 1.0480 0.0000 0.0000 0.0000 0.0000 -2.3405 0.0000 0.0000 0.0000 0.0000 -1.1138 0.0000 0.0000 0.0000 0.0000 [torch.FloatTensor of size 4x4] \u83b7\u5f97\u7ed9\u5b9a\u77e9\u9635\u7684\u7b2ck\u6761\u5bf9\u89d2\u7ebf: >>> a = torch.randn(3, 3) >>> a -1.5328 -1.3210 -1.5204 0.8596 0.0471 -0.2239 -0.6617 0.0146 -1.0817 [torch.FloatTensor of size 3x3] >>> torch.diag(a, 0) -1.5328 0.0471 -1.0817 [torch.FloatTensor of size 3] >>> torch.diag(a, 1) -1.3210 -0.2239 [torch.FloatTensor of size 2] torch.histc(input, bins=100, min=0, max=0, out=None) \u2192 Tensor \u8ba1\u7b97\u8f93\u5165\u5f20\u91cf\u7684\u76f4\u65b9\u56fe. \u4ee5 <cite>min</cite> \u548c <cite>max</cite> \u4e3a <cite>range</cite> \u8fb9\u754c, \u5c06\u5176\u5747\u5206\u6210 <cite>bins</cite> \u4e2a\u76f4\u6761, \u7136\u540e\u5c06\u6392\u5e8f\u597d\u7684\u6570\u636e\u5212\u5206\u5230\u5404\u4e2a\u76f4\u6761 <cite>(bins)</cite> \u4e2d. \u5982\u679c <cite>min</cite> \u548c <cite>max</cite> \u90fd\u4e3a 0, \u5219\u5229\u7528\u6570\u636e\u4e2d\u7684\u6700\u5927\u6700\u5c0f\u503c\u4f5c\u4e3a\u8fb9\u754c. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf bins ( int ) \u2013 \u76f4\u65b9\u56fe <cite>bins</cite> (\u76f4\u6761)\u7684\u4e2a\u6570(\u9ed8\u8ba4100\u4e2a) min ( int ) \u2013 <cite>range</cite> \u7684\u4e0b\u8fb9\u754c(\u5305\u542b) max ( int ) \u2013 <cite>range</cite> \u7684\u4e0a\u8fb9\u754c(\u5305\u542b) out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | | Returns: | \u76f4\u65b9\u56fe | | --- | --- | | Return type: | Tensor | | --- | --- | Example: >>> torch.histc(torch.FloatTensor([1, 2, 1]), bins=4, min=0, max=3) FloatTensor([0, 2, 1, 0]) torch.renorm(input, p, dim, maxnorm, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf, \u5305\u542b\u89c4\u8303\u5316\u540e\u7684\u5404\u4e2a\u5b50\u5f20\u91cf, \u4f7f\u5f97\u6cbf\u7740 <cite>dim</cite> \u7ef4\u5212\u5206\u7684\u5404\u5b50\u5f20\u91cf\u7684 <cite>p</cite> \u8303\u6570\u5c0f\u4e8e <cite>maxnorm</cite> Note \u5982\u679c p \u8303\u6570\u7684\u503c\u5c0f\u4e8e <cite>maxnorm</cite>, \u5219\u5f53\u524d\u5b50\u5f20\u91cf\u4e0d\u9700\u8981\u4fee\u6539. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> p ( float ) \u2013 \u8303\u6570\u7684 <cite>p</cite> dim ( int ) \u2013 \u6cbf\u7740\u6b64\u7ef4\u5207\u7247, \u5f97\u5230\u5f20\u91cf\u5b50\u96c6 maxnorm ( float ) \u2013 \u6bcf\u4e2a\u5b50\u5f20\u91cf\u7684\u8303\u6570\u7684\u6700\u5927\u503c out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> x = torch.ones(3, 3) >>> x[1].fill_(2) >>> x[2].fill_(3) >>> x 1 1 1 2 2 2 3 3 3 [torch.FloatTensor of size 3x3] >>> torch.renorm(x, 1, 0, 5) 1.0000 1.0000 1.0000 1.6667 1.6667 1.6667 1.6667 1.6667 1.6667 [torch.FloatTensor of size 3x3] torch.trace(input) \u2192 float \u8fd4\u56de\u8f93\u5165 2 \u7ef4\u77e9\u9635\u5bf9\u89d2\u7ebf\u5143\u7d20\u7684\u548c(\u8ff9). Example: >>> x = torch.arange(1, 10).view(3, 3) >>> x 1 2 3 4 5 6 7 8 9 [torch.FloatTensor of size 3x3] >>> torch.trace(x) 15.0 torch.tril(input, diagonal=0, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf, \u5305\u542b\u8f93\u5165\u77e9\u9635 ( <cite>2D</cite> \u5f20\u91cf)\u7684\u4e0b\u4e09\u89d2\u90e8\u5206, \u5176\u4f59\u90e8\u5206\u88ab\u8bbe\u4e3a 0. \u8fd9\u91cc\u6240\u8bf4\u7684\u4e0b\u4e09\u89d2\u90e8\u5206\u4e3a\u77e9\u9635\u6307\u5b9a\u5bf9\u89d2\u7ebf <cite>diagonal</cite> \u5728\u7ebf\u91cc\u7684\u548c\u4e0b\u9762\u7684\u5143\u7d20. \u53c2\u6570 <cite>diagonal</cite> \u63a7\u5236\u5bf9\u89d2\u7ebf. diagonal = 0, \u4e3b\u5bf9\u89d2\u7ebf. diagonal > 0, \u4e3b\u5bf9\u89d2\u7ebf\u4e4b\u4e0a. diagonal < 0, \u4e3b\u5bf9\u89d2\u7ebf\u4e4b\u4e0b. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> diagonal ( int , optional ) \u2013 \u6307\u5b9a\u5bf9\u89d2\u7ebf out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(3,3) >>> a 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.tril(a) 1.3225 0.0000 0.0000 -0.3052 -0.3111 0.0000 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.tril(a, diagonal=1) 1.3225 1.7304 0.0000 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.tril(a, diagonal=-1) 0.0000 0.0000 0.0000 -0.3052 0.0000 0.0000 1.2469 0.0064 0.0000 [torch.FloatTensor of size 3x3] torch.triu(input, diagonal=0, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf, \u5305\u542b\u8f93\u5165\u77e9\u9635 ( <cite>2D</cite> \u5f20\u91cf)\u7684\u4e0a\u4e09\u89d2\u90e8\u5206, \u5176\u4f59\u90e8\u5206\u88ab\u8bbe\u4e3a 0. \u8fd9\u91cc\u6240\u8bf4\u7684\u4e0b\u4e09\u89d2\u90e8\u5206\u4e3a\u77e9\u9635\u6307\u5b9a\u5bf9\u89d2\u7ebf <cite>diagonal</cite> \u5728\u7ebf\u91cc\u7684\u548c\u4e0a\u9762\u7684\u5143\u7d20. \u53c2\u6570 <cite>diagonal</cite> \u63a7\u5236\u5bf9\u89d2\u7ebf. diagonal = 0, \u4e3b\u5bf9\u89d2\u7ebf. diagonal > 0, \u4e3b\u5bf9\u89d2\u7ebf\u4e4b\u4e0a. diagonal < 0, \u4e3b\u5bf9\u89d2\u7ebf\u4e4b\u4e0b. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> diagonal ( int , optional ) \u2013 \u6307\u5b9a\u5bf9\u89d2\u7ebf out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(3,3) >>> a 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.triu(a) 1.3225 1.7304 1.4573 0.0000 -0.3111 -0.1809 0.0000 0.0000 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.triu(a, diagonal=1) 0.0000 1.7304 1.4573 0.0000 0.0000 -0.1809 0.0000 0.0000 0.0000 [torch.FloatTensor of size 3x3] >>> torch.triu(a, diagonal=-1) 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 0.0000 0.0064 -1.6250 [torch.FloatTensor of size 3x3] BLAS and LAPACK Operations (BLAS\u548cLAPACK\u64cd\u4f5c) torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) \u2192 Tensor \u6267\u884c\u4fdd\u5b58\u5728 batch1 \u548c batch2 \u4e2d\u7684\u77e9\u9635\u7684\u6279\u91cf\u70b9\u4e58, \u4f34\u968f\u7740\u4e00\u4e2a\u51cf\u5c11\u7684\u76f8\u52a0\u6b65\u9aa4 (\u6240\u6709\u7684\u77e9\u9635\u4e58\u6cd5\u6cbf\u7b2c\u4e00\u7ef4\u7d2f\u52a0). mat \u88ab\u76f8\u52a0\u5230\u6700\u7ec8\u7684\u7ed3\u679c\u4e2d. batch1 \u548c batch2 \u5fc5\u987b\u662f\u4e09\u7ef4\u7684\u5f20\u91cf, \u4e14\u6bcf\u4e2a\u5305\u542b\u76f8\u540c\u6570\u91cf\u7684\u77e9\u9635. \u5982\u679c batch1 \u662f\u4e00\u4e2a <cite>b x n x m</cite> \u7684\u5f20\u91cf, batch2 \u662f\u4e00\u4e2a <cite>b x m x p \u7684\u5f20\u91cf, \u90a3\u4e48 ::attr: mat</cite> \u5fc5\u987b\u662f broadcastable \u4e14\u662f\u4e00\u4e2a <cite>n x p</cite> \u7684\u5f20\u91cf, \u540c\u65f6 attr:<cite>out</cite> \u5c06\u662f\u4e00\u4e2a <cite>n x p</cite> \u7684\u5f20\u91cf. \u6362\u53e5\u8bdd\u8bf4, \\(res = (beta * M) + (alpha * sum(batch1_i @ batch2_i, i = 0, b))\\) \u5bf9\u4e8e <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7c7b\u578b\u7684\u8f93\u5165, \u53c2\u6570 <cite>beta</cite> \u548c <cite>alpha</cite> \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u4ed6\u4eec\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | beta ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e mat \u7684\u4e58\u5b50 (\u7cfb\u6570) mat ( Tensor ) \u2013 \u8981\u88ab\u76f8\u52a0\u7684\u77e9\u9635 alpha ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e <cite>batch1 @ batch2</cite> \u7684\u4e58\u5b50 batch1 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e00\u6279\u77e9\u9635 batch2 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e8c\u6279\u77e9\u9635 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7684\u5f20\u91cf\u7ed3\u679c | | --- | --- | Example: >>> M = torch.randn(3, 5) >>> batch1 = torch.randn(10, 3, 4) >>> batch2 = torch.randn(10, 4, 5) >>> torch.addbmm(M, batch1, batch2) -3.1162 11.0071 7.3102 0.1824 -7.6892 1.8265 6.0739 0.4589 -0.5641 -5.4283 -9.3387 -0.1794 -1.2318 -6.8841 -4.7239 [torch.FloatTensor of size 3x5] torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None) \u2192 Tensor \u6267\u884c\u77e9\u9635 mat1 \u548c mat2 \u7684\u76f8\u4e58. \u77e9\u9635 mat \u5c06\u4e0e\u76f8\u4e58\u7684\u6700\u7ec8\u8ba1\u7b97\u7ed3\u679c\u76f8\u52a0. \u5982\u679c mat1 \u662f\u4e00\u4e2a <cite>n x m</cite> \u7684\u5f20\u91cf, mat2 \u662f\u4e00\u4e2a <cite>m x p \u7684\u5f20\u91cf, \u90a3\u4e48 ::attr: mat</cite> \u5fc5\u987b\u662f broadcastable \u4e14\u662f\u4e00\u4e2a <cite>n x p</cite> \u7684\u5f20\u91cf, \u540c\u65f6 attr:<cite>out</cite> \u5c06\u662f\u4e00\u4e2a <cite>n x p</cite> \u7684\u5f20\u91cf. \u6362\u53e5\u8bdd\u8bf4, \\(out = (beta * M) + (alpha * mat1 @ mat2)\\) \u5bf9\u4e8e <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7c7b\u578b\u7684\u8f93\u5165, \u53c2\u6570 <cite>beta</cite> \u548c <cite>alpha</cite> \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u4ed6\u4eec\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | beta ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e:attr:[ ](#id1)mat \u7684\u4e58\u5b50 mat ( Tensor ) \u2013 \u8981\u88ab\u76f8\u52a0\u7684\u77e9\u9635 alpha ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e mat1 @ mat2 \u7684\u4e58\u5b50 mat1 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e00\u4e2a\u77e9\u9635 mat2 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e8c\u4e2a\u77e9\u9635 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c | | --- | --- | Example: >>> M = torch.randn(2, 3) >>> mat1 = torch.randn(2, 3) >>> mat2 = torch.randn(3, 3) >>> torch.addmm(M, mat1, mat2) -0.4095 -1.9703 1.3561 5.7674 -4.9760 2.7378 [torch.FloatTensor of size 2x3] torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None) \u2192 Tensor \u6267\u884c\u77e9\u9635 mat \u548c\u5411\u91cf vec \u7684\u76f8\u4e58. \u77e9\u9635 tensor \u5c06\u4e0e\u76f8\u4e58\u7684\u6700\u7ec8\u8ba1\u7b97\u7ed3\u679c\u76f8\u52a0. \u5982\u679c mat \u662f\u4e00\u4e2a <cite>n x m</cite> \u7684\u5f20\u91cf, vec \u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>m</cite> \u7684\u4e00\u7ef4\u5f20\u91cf, \u90a3\u4e48 : tensor \u5fc5\u987b\u662f broadcastable \u4e14\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>n</cite> \u7684\u4e00\u7ef4\u5f20\u91cf, \u540c\u65f6 attr:<cite>out</cite> \u5c06\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>n</cite> \u7684\u4e00\u7ef4\u5f20\u91cf. <cite>alpha</cite> \u548c <cite>beta</cite> \u5206\u522b\u662f <cite>mat * vec</cite> \u548c <cite>tensor</cite> \u7684\u7f29\u653e\u56e0\u5b50. \u6362\u53e5\u8bdd\u8bf4, \\(out = (beta * tensor) + (alpha * (mat @ vec2))\\) \u5bf9\u4e8e <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7c7b\u578b\u7684\u8f93\u5165, \u53c2\u6570 <cite>beta</cite> \u548c <cite>alpha</cite> \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u4ed6\u4eec\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | beta ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e tensor \u7684\u4e58\u5b50 tensor ( Tensor ) \u2013 \u8981\u88ab\u76f8\u52a0\u7684\u5411\u91cf alpha ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e <cite>mat @ vec</cite> \u7684\u4e58\u5b50 mat ( Tensor ) \u2013 \u8981\u88ab\u76f8\u4e58\u7684\u77e9\u9635 vec ( Tensor ) \u2013 \u8981\u88ab\u8981\u4e58\u7684\u5411\u91cf out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c | | --- | --- | Example: >>> M = torch.randn(2) >>> mat = torch.randn(2, 3) >>> vec = torch.randn(3) >>> torch.addmv(M, mat, vec) -2.0939 -2.2950 [torch.FloatTensor of size 2] torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None) \u2192 Tensor \u6267\u884c\u5411\u91cf:attr:<cite>vec1</cite> \u548c vec2 \u7684\u5916\u79ef, \u5e76\u628a\u5916\u79ef\u8ba1\u7b97\u7ed3\u679c\u4e0e\u77e9\u9635 mat \u76f8\u52a0. \u53ef\u9009\u503c beta \u548c alpha \u662f\u6807\u91cf, \u5206\u522b\u4e0e mat \u548c \\((vec1 \\otimes vec2)\\) \u76f8\u4e58. \u6362\u53e5\u8bdd\u8bf4, \\(out = (beta * mat) + (alpha * vec1 \\otimes vec2)\\) \u5982\u679c vec1 \u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>n</cite> \u7684\u5411\u91cf, vec2 \u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>m</cite> \u7684\u5411\u91cf, \u90a3\u4e48 mat \u5fc5\u987b\u662f broadcastable \u4e14\u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a <cite>n x m</cite> \u7684\u77e9\u9635, \u540c\u65f6 out \u5c06\u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a <cite>n x m</cite> \u7684\u77e9\u9635. \u5bf9\u4e8e <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7c7b\u578b\u7684\u8f93\u5165, \u53c2\u6570 <cite>beta</cite> \u548c <cite>alpha</cite> \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u4ed6\u4eec\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | beta ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e mat \u7684\u4e58\u5b50 mat ( Tensor ) \u2013 \u8981\u88ab\u76f8\u52a0\u7684\u77e9\u9635 alpha ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e vec1 \u548c vec2 \u5916\u79ef\u8ba1\u7b97\u7ed3\u679c\u7684\u4e58\u5b50 vec1 ( Tensor ) \u2013 \u5916\u79ef\u8ba1\u7b97\u7684\u7b2c\u4e00\u4e2a\u5411\u91cf vec2 ( Tensor ) \u2013 \u5916\u79ef\u8ba1\u7b97\u7684\u7b2c\u4e8c\u4e2a\u5411\u91cf out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c | | --- | --- | Example: >>> vec1 = torch.arange(1, 4) >>> vec2 = torch.arange(1, 3) >>> M = torch.zeros(3, 2) >>> torch.addr(M, vec1, vec2) 1 2 2 4 3 6 [torch.FloatTensor of size 3x2] torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) \u2192 Tensor \u6267\u884c\u4fdd\u5b58\u5728 batch1 \u548c batch2 \u4e2d\u7684\u77e9\u9635\u7684\u6279\u91cf\u70b9\u4e58. mat \u88ab\u76f8\u52a0\u5230\u6700\u7ec8\u7684\u7ed3\u679c\u4e2d. batch1 \u548c batch2 \u5fc5\u987b\u662f\u4e09\u7ef4\u7684\u5f20\u91cf, \u4e14\u6bcf\u4e2a\u5305\u542b\u76f8\u540c\u6570\u91cf\u7684\u77e9\u9635. \u5982\u679c batch1 \u662f\u4e00\u4e2a <cite>b x n x m</cite> \u7684\u5f20\u91cf, batch2 \u662f\u4e00\u4e2a <cite>b x m x p \u7684\u5f20\u91cf, \u90a3\u4e48 ::attr: mat</cite> \u5fc5\u987b\u662f broadcastable \u4e14\u662f\u4e00\u4e2a <cite>b x n x p</cite> \u7684\u5f20\u91cf, \u540c\u65f6 attr:<cite>out</cite> \u5c06\u662f\u4e00\u4e2a <cite>b x n x p</cite> \u7684\u5f20\u91cf. \u6362\u53e5\u8bdd\u8bf4, \\(res_i = (beta * M_i) + (alpha * batch1_i \\times batch2_i)\\) \u5bf9\u4e8e <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7c7b\u578b\u7684\u8f93\u5165, \u53c2\u6570 <cite>beta</cite> \u548c <cite>alpha</cite> \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u4ed6\u4eec\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | beta ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e mat \u7684\u4e58\u5b50 (\u7cfb\u6570) mat ( Tensor ) \u2013 \u8981\u88ab\u76f8\u52a0\u7684\u5f20\u91cf alpha ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e <cite>batch1 @ batch2</cite> \u7684\u4e58\u5b50 batch1 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e00\u6279\u77e9\u9635 batch2 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e8c\u6279\u77e9\u9635 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7684\u5f20\u91cf\u7ed3\u679c | | --- | --- | Example: >>> M = torch.randn(10, 3, 5) >>> batch1 = torch.randn(10, 3, 4) >>> batch2 = torch.randn(10, 4, 5) >>> torch.baddbmm(M, batch1, batch2).size() torch.Size([10, 3, 5]) torch.bmm(batch1, batch2, out=None) \u2192 Tensor \u6267\u884c\u4fdd\u5b58\u5728 batch1 \u548c batch2 \u4e2d\u7684\u77e9\u9635\u7684\u6279\u91cf\u70b9\u4e58. batch1 \u548c batch2 \u5fc5\u987b\u662f\u4e09\u7ef4\u7684\u5f20\u91cf, \u4e14\u6bcf\u4e2a\u5305\u542b\u76f8\u540c\u6570\u91cf\u7684\u77e9\u9635. \u5982\u679c batch1 \u662f\u4e00\u4e2a <cite>b x n x m</cite> \u7684\u5f20\u91cf, batch2 \u662f\u4e00\u4e2a <cite>b x m x p</cite> \u7684\u5f20\u91cf, out \u5c06\u662f\u4e00\u4e2a <cite>b x n x p</cite> \u7684\u5f20\u91cf. Note \u8fd9\u4e2a\u51fd\u6570\u4e0d\u80fd\u53c2\u8003 broadcast . \u5bf9\u4e8e\u5e7f\u64ad\u77e9\u9635\u76f8\u4e58, \u53c2\u89c1 torch.matmul() . | Parameters: | batch1 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e00\u6279\u77e9\u9635 batch2 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e8c\u6279\u77e9\u9635 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c | | --- | --- | Example: >>> batch1 = torch.randn(10, 3, 4) >>> batch2 = torch.randn(10, 4, 5) >>> res = torch.bmm(batch1, batch2) >>> res.size() torch.Size([10, 3, 5]) torch.btrifact(A, info=None, pivot=True) \u2192 Tensor, IntTensor \u6279\u91cf LU \u5206\u89e3. \u8fd4\u56de\u4e00\u4e2a\u5305\u542b LU \u5206\u89e3\u548c\u67a2\u8f74\u7684\u5143\u7ec4. \u5bf9\u4e8e\u6bcf\u4e2a minibatch \u793a\u4f8b, \u5982\u679c\u5206\u89e3\u6210\u529f, \u53ef\u9009\u53c2\u6570 <cite>info</cite> \u5c06\u63d0\u4f9b\u5206\u89e3\u4fe1\u606f. <cite>info</cite> \u7684\u503c\u6765\u81ea dgetrf, \u82e5\u662f\u975e\u96f6\u503c, \u5219\u8868\u793a\u6709\u9519\u8bef\u53d1\u751f. \u5982\u679c cuda \u88ab\u4f7f\u7528\u7684\u8bdd, \u5177\u4f53\u7684\u503c\u6765\u81ea cublas, \u5426\u5219\u6765\u81ea LAPACK. \u5982\u679c\u8bbe\u7f6e\u4e86 pivot, \u90a3\u4e48\u65cb\u8f6c\u64cd\u4f5c\u5c06\u88ab\u6267\u884c. Parameters: A ( Tensor ) \u2013 \u8981\u5206\u89e3\u7684\u5f20\u91cf. Example: >>> A = torch.randn(2, 3, 3) >>> A_LU = A.btrifact() torch.btrisolve(b, LU_data, LU_pivots) \u2192 Tensor \u6279\u91cf LU \u89e3. \u8fd4\u56de\u7ebf\u6027\u7cfb\u7edf Ax = b \u7684 LU \u89e3. | Parameters: | b ( Tensor ) \u2013 RHS tensor. LU_data ( Tensor ) \u2013 Pivoted LU factorization of A from btrifact. LU_pivots ( IntTensor ) \u2013 Pivots of the LU factorization. | | --- | --- | Example: >>> A = torch.randn(2, 3, 3) >>> b = torch.randn(2, 3) >>> A_LU = torch.btrifact(A) >>> x = b.btrisolve(*A_LU) >>> torch.norm(A.bmm(x.unsqueeze(2)) - b) 6.664001874625056e-08 torch.dot(tensor1, tensor2) \u2192 float \u8ba1\u7b97\u4e24\u4e2a\u5f20\u91cf\u7684\u70b9\u4e58 (\u5185\u79ef). Note \u8fd9\u4e2a\u51fd\u6570\u4e0d\u652f\u6301 broadcast . Example: >>> torch.dot(torch.Tensor([2, 3]), torch.Tensor([2, 1])) 7.0 torch.eig(a, eigenvectors=False, out=None) -> (Tensor, Tensor) \u8ba1\u7b97\u5b9e\u6570\u65b9\u9635\u7684\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf. | Parameters: | a ( Tensor ) \u2013 \u4e00\u4e2a\u8981\u88ab\u8ba1\u7b97\u7279\u5f81\u503c\u4e0e\u7279\u5f81\u5411\u91cf\u7684\u65b9\u9635 eigenvectors ( bool ) \u2013 \u82e5\u4e3a True , \u8868\u793a\u7279\u5f81\u503c\u4e0e\u7279\u5f81\u5411\u91cf\u90fd\u88ab\u8ba1\u7b97. \u5426\u5219, \u4ec5\u8ba1\u7b97\u7279\u5f81\u503c. out ( tuple , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | | Returns: | \u5305\u542b > * e ( Tensor ): a \u7684\u5de6\u7279\u5f81\u503c > * v ( Tensor ): \u5982\u679c eigenvectors \u4e3a True , \u8868\u793a a \u7684\u7279\u5f81\u5411\u91cf; \u5426\u5219\u662f\u4e00\u4e2a\u7a7a\u7684\u5f20\u91cf | | --- | --- | | Return type: | \u8fd4\u56de\u4e00\u4e2a\u5143\u7ec4, ( Tensor , Tensor ) | | --- | --- | torch.gels(B, A, out=None) \u2192 Tensor \u8ba1\u7b97\u79e9\u4e3a \\(m\\) \u7684\uff0c \u5927\u5c0f\u4e3a m x n \u7684\u77e9\u9635 \\(A\\) \u6700\u5c0f\u4e8c\u4e58\u548c\u6700\u5c0f\u8303\u6570\u95ee\u9898\u7684\u89e3 \u5982\u679c \\(m &gt;= n\\) , gels() \u6c42\u89e3\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898: \\[\\begin{array}{ll} \\mbox{minimize} & \\|AX-B\\|_F. \\end{array}\\] \u5982\u679c \\(m &lt; n\\) , gels() \u6c42\u89e3\u6700\u5c0f\u8303\u6570\u95ee\u9898: \\[\\begin{array}{ll} \\mbox{minimize} & \\|X\\|_F & \\mbox{subject to} & AX = B. \\end{array}\\] \u8fd4\u56de\u7684\u77e9\u9635 \\(X\\) \u7684\u5934 \\(n\\) \u884c\u5305\u542b\u89e3\u4fe1\u606f. \u5176\u4f59\u884c\u5305\u542b\u5269\u4f59\u4fe1\u606f: \u4ece\u7b2c \\(n\\) \u884c\u5f00\u59cb\u7684\u6bcf\u5217\u7684 euclidean \u8303\u6570, \u662f\u5bf9\u5e94\u5217\u7684\u5269\u4f59. | Parameters: | B ( Tensor ) \u2013 The matrix \\(B\\) A ( Tensor ) \u2013 The \\(m\\) by \\(n\\) matrix \\(A\\) out ( tuple , optional ) \u2013 Optional destination tensor | | --- | --- | | Returns: | tuple containing: > * X ( Tensor ): \u6700\u5c0f\u4e8c\u4e58\u89e3 > * qr ( Tensor ): QR \u5206\u89e3\u7684\u8be6\u7ec6\u4fe1\u606f | | --- | --- | | Return type: | ( Tensor , Tensor ) | | --- | --- | Note \u4e0d\u7ba1\u8f93\u5165\u77e9\u9635\u7684\u6b65\u957f\u5982\u4f55, \u8fd4\u56de\u6765\u7684\u77e9\u9635\u5c06\u603b\u662f\u88ab\u8f6c\u7f6e. \u4e5f\u5c31\u662f, \u4ed6\u4eec\u7684\u6b65\u957f\u662f <cite>(1, m)</cite> \u800c\u4e0d\u662f <cite>(m, 1)</cite>. Example: >>> A = torch.Tensor([[1, 1, 1], ... [2, 3, 4], ... [3, 5, 2], ... [4, 2, 5], ... [5, 4, 3]]) >>> B = torch.Tensor([[-10, -3], [ 12, 14], [ 14, 12], [ 16, 16], [ 18, 16]]) >>> X, _ = torch.gels(B, A) >>> X 2.0000 1.0000 1.0000 1.0000 1.0000 2.0000 [torch.FloatTensor of size 3x2] torch.geqrf(input, out=None) -> (Tensor, Tensor) \u8fd9\u662f\u76f4\u63a5\u8c03\u7528 LAPACK \u7684\u4f4e\u5c42\u51fd\u6570. \u901a\u5e38\u60a8\u5e94\u8be5\u4f7f\u7528 torch.qr() \u6765\u4ee3\u66ff\u4e4b. \u8ba1\u7b97 input \u7684 QR \u5206\u89e3, \u4f46\u4e0d\u6784\u9020 <cite>Q</cite> \u548c <cite>R</cite> \u4f5c\u4e3a\u663e\u793a\u5206\u5f00\u7684\u77e9\u9635. \u7136\u800c, \u8fd9\u6837\u76f4\u63a5\u8c03\u7528 LAPACK \u7684\u5e95\u5c42\u51fd\u6570 <cite>?geqrf</cite>, \u4f1a\u4ea7\u751f\u4e00\u8fde\u4e32\u7684 \u2018elementary reflectors\u2019. \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 LAPACK documentation . | Parameters: | input ( Tensor ) \u2013 the input matrix out ( tuple , optional ) \u2013 The result tuple of (Tensor, Tensor) | | --- | --- | torch.ger(vec1, vec2, out=None) \u2192 Tensor \u8ba1\u7b97 vec1 \u548c vec2 \u7684\u5916\u79ef. \u5982\u679c vec1 \u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>n</cite> \u7684\u5411\u91cf, vec2 \u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>m</cite> \u7684\u5411\u91cf, \u90a3\u4e48 out \u5fc5\u987b\u662f\u4e00\u4e2a <cite>n x m</cite> \u7684\u77e9\u9635. Note \u8fd9\u4e2a\u51fd\u6570\u4e0d\u652f\u6301 broadcast . | Parameters: | vec1 ( Tensor ) \u2013 1D input vector vec2 ( Tensor ) \u2013 1D input vector out ( Tensor , optional ) \u2013 optional output matrix | | --- | --- | Example: >>> v1 = torch.arange(1, 5) >>> v2 = torch.arange(1, 4) >>> torch.ger(v1, v2) 1 2 3 2 4 6 3 6 9 4 8 12 [torch.FloatTensor of size 4x3] torch.gesv(B, A, out=None) -> (Tensor, Tensor) <cite>X, LU = torch.gesv(B, A)</cite> , \u8be5\u51fd\u6570\u8fd4\u56de\u7ebf\u6027\u7cfb\u7edf \\(AX = B\\) \u7684\u89e3. <cite>LU</cite> \u5305\u542b <cite>A</cite> \u7684 LU \u5206\u89e3\u56e0\u5b50 <cite>L</cite> \u548c <cite>U</cite>. A \u5fc5\u987b\u662f\u65b9\u9635, \u4e14\u662f\u975e\u5947\u5f02\u7684 (2\u7ef4\u53ef\u9006\u5f20\u91cf). \u5982\u679c <cite>A</cite> \u662f\u4e00\u4e2a <cite>m x m</cite> \u77e9\u9635, <cite>B</cite> \u662f\u4e00\u4e2a <cite>m x k</cite> \u7684\u77e9\u9635, \u90a3\u4e48\u7ed3\u679c <cite>LU</cite> \u7684\u5927\u5c0f\u4e3a <cite>m x m</cite>, <cite>X</cite> \u7684\u5927\u5c0f\u4e3a <cite>m x k</cite> . Note Irrespective of the original strides, the returned matrices <cite>X</cite> and <cite>LU</cite> will be transposed, i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite>. | Parameters: | B ( Tensor ) \u2013 input matrix of <cite>m x k</cite> dimensions A ( Tensor ) \u2013 input square matrix of <cite>m x m</cite> dimensions out ( Tensor , optional ) \u2013 optional output matrix | | --- | --- | Example: >>> A = torch.Tensor([[6.80, -2.11, 5.66, 5.97, 8.23], ... [-6.05, -3.30, 5.36, -4.44, 1.08], ... [-0.45, 2.58, -2.70, 0.27, 9.04], ... [8.32, 2.71, 4.35, -7.17, 2.14], ... [-9.67, -5.14, -7.26, 6.08, -6.87]]).t() >>> B = torch.Tensor([[4.02, 6.19, -8.22, -7.57, -3.03], ... [-1.56, 4.00, -8.67, 1.75, 2.86], ... [9.81, -4.09, -4.57, -8.61, 8.99]]).t() >>> X, LU = torch.gesv(B, A) >>> torch.dist(B, torch.mm(A, X)) 9.250057093890353e-06 torch.inverse(input, out=None) \u2192 Tensor \u8ba1\u7b97\u65b9\u9635 input \u7684\u9006. Note Irrespective of the original strides, the returned matrix will be transposed, i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite> | Parameters: | input ( Tensor ) \u2013 the input 2D square <cite>Tensor</cite> out ( Tensor , optional ) \u2013 the optional output <cite>Tensor</cite> | | --- | --- | Example: >>> x = torch.rand(10, 10) >>> x 0.7800 0.2267 0.7855 0.9479 0.5914 0.7119 0.4437 0.9131 0.1289 0.1982 0.0045 0.0425 0.2229 0.4626 0.6210 0.0207 0.6338 0.7067 0.6381 0.8196 0.8350 0.7810 0.8526 0.9364 0.7504 0.2737 0.0694 0.5899 0.8516 0.3883 0.6280 0.6016 0.5357 0.2936 0.7827 0.2772 0.0744 0.2627 0.6326 0.9153 0.7897 0.0226 0.3102 0.0198 0.9415 0.9896 0.3528 0.9397 0.2074 0.6980 0.5235 0.6119 0.6522 0.3399 0.3205 0.5555 0.8454 0.3792 0.4927 0.6086 0.1048 0.0328 0.5734 0.6318 0.9802 0.4458 0.0979 0.3320 0.3701 0.0909 0.2616 0.3485 0.4370 0.5620 0.5291 0.8295 0.7693 0.1807 0.0650 0.8497 0.1655 0.2192 0.6913 0.0093 0.0178 0.3064 0.6715 0.5101 0.2561 0.3396 0.4370 0.4695 0.8333 0.1180 0.4266 0.4161 0.0699 0.4263 0.8865 0.2578 [torch.FloatTensor of size 10x10] >>> x = torch.rand(10, 10) >>> y = torch.inverse(x) >>> z = torch.mm(x, y) >>> z 1.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 1.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 1.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 1.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 1.0000 0.0000 0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 1.0000 -0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 1.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 1.0000 -0.0000 0.0000 -0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 -0.0000 1.0000 -0.0000 -0.0000 0.0000 -0.0000 -0.0000 -0.0000 0.0000 -0.0000 -0.0000 0.0000 1.0000 [torch.FloatTensor of size 10x10] >>> torch.max(torch.abs(z - torch.eye(10))) # Max nonzero 5.096662789583206e-07 torch.matmul(tensor1, tensor2, out=None) Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). For example, if tensor1 is a <cite>j x 1 x n x m</cite> Tensor and tensor2 is a <cite>k x m x p</cite> Tensor, out will be an <cite>j x k x n x p</cite> Tensor. Note The 1-dimensional dot product version of this function does not support an out parameter. | Parameters: | tensor1 ( Tensor ) \u2013 First tensor to be multiplied tensor2 ( Tensor ) \u2013 Second tensor to be multiplied out ( Tensor , optional ) \u2013 Output tensor | | --- | --- | torch.mm(mat1, mat2, out=None) \u2192 Tensor \u6267\u884c mat1 \u548c mat2 \u7684\u77e9\u9635\u4e58\u6cd5. \u5982\u679c mat1 \u662f\u4e00\u4e2a <cite>n x m</cite> \u5f20\u91cf, mat2 \u662f\u4e00\u4e2a <cite>m x p</cite> \u5f20\u91cf, out \u5c06\u662f\u4e00\u4e2a <cite>n x p</cite> \u5f20\u91cf. Note \u8fd9\u4e2a\u51fd\u6570\u4e0d\u652f\u6301 broadcast . \u8981\u4f7f\u7528\u652f\u6301\u5e7f\u64ad\u77e9\u9635\u4e58\u6cd5, \u53c2\u89c1 torch.matmul() . | Parameters: | mat1 ( Tensor ) \u2013 First matrix to be multiplied mat2 ( Tensor ) \u2013 Second matrix to be multiplied out ( Tensor , optional ) \u2013 Output tensor | | --- | --- | Example: >>> mat1 = torch.randn(2, 3) >>> mat2 = torch.randn(3, 3) >>> torch.mm(mat1, mat2) 0.0519 -0.3304 1.2232 4.3910 -5.1498 2.7571 [torch.FloatTensor of size 2x3] torch.mv(mat, vec, out=None) \u2192 Tensor \u6267\u884c\u77e9\u9635 mat \u4e0e\u5411\u91cf vec \u7684\u4e58\u6cd5\u64cd\u4f5c. \u5982\u679c mat \u662f\u4e00\u4e2a <cite>n x m</cite> \u5f20\u91cf, vec \u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a <cite>m</cite> \u7684\u4e00\u7ef4\u5f20\u91cf, out \u5c06\u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a <cite>n</cite> \u7684\u5f20\u91cf. Note \u8fd9\u4e2a\u51fd\u6570\u4e0d\u652f\u6301 broadcast . | Parameters: | mat ( Tensor ) \u2013 matrix to be multiplied vec ( Tensor ) \u2013 vector to be multiplied out ( Tensor , optional ) \u2013 Output tensor | | --- | --- | Example: >>> mat = torch.randn(2, 3) >>> vec = torch.randn(3) >>> torch.mv(mat, vec) -2.0939 -2.2950 [torch.FloatTensor of size 2] torch.orgqr() torch.ormqr() torch.potrf(a, out=None) potrf(a, upper, out=None) \u8ba1\u7b97\u534a\u6b63\u5b9a\u77e9\u9635 a : \u7684 Cholesky \u5206\u89e3. \u8fd4\u56de\u7ed3\u679c <cite>u</cite>, \u82e5 <cite>upper</cite> \u8bbe\u4e3a True \u6216\u672a\u63d0\u4f9b\u65f6, <cite>u</cite> \u662f\u4e00\u4e2a\u4e0a\u4e09\u89d2\u77e9\u9635, \u4f7f\u5f97 \\(a = u^T u\\) \u6210\u7acb; \u82e5 <cite>upper</cite> \u8bbe\u4e3a False , <cite>u</cite> \u662f\u4e00\u4e2a\u4e0b\u4e09\u89d2\u77e9\u9635, \u4f7f\u5f97 \\(a = u u^T\\) \u6210\u7acb. | Parameters: | a ( Tensor ) \u2013 the input 2D <cite>Tensor</cite>, a symmetric positive semidefinite matrix upper ( bool , optional ) \u2013 Return upper (default) or lower triangular matrix out ( Tensor , optional ) \u2013 A Tensor for u | | --- | --- | Example: >>> a = torch.randn(3,3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> u = torch.potrf(a) >>> a 2.3563 3.2318 -0.9406 3.2318 4.9557 -2.1618 -0.9406 -2.1618 2.2443 [torch.FloatTensor of size 3x3] >>> u 1.5350 2.1054 -0.6127 0.0000 0.7233 -1.2053 0.0000 0.0000 0.6451 [torch.FloatTensor of size 3x3] >>> torch.mm(u.t(),u) 2.3563 3.2318 -0.9406 3.2318 4.9557 -2.1618 -0.9406 -2.1618 2.2443 [torch.FloatTensor of size 3x3] torch.potri(u, out=None) potri(u, upper, out=None) \u7ed9\u5b9a\u4e00\u4e2a\u534a\u6b63\u5b9a\u77e9\u9635\u7684 Cholesky \u5206\u89e3\u56e0\u5b50 u , \u8ba1\u7b97\u8be5\u534a\u6b63\u5b9a\u77e9\u9635\u7684\u9006. \u8fd4\u56de\u77e9\u9635 <cite>inv</cite>, \u82e5 <cite>upper</cite> \u8bbe\u4e3a True \u6216\u4e3a\u63d0\u4f9b, <cite>u</cite> \u662f\u4e00\u4e2a\u4e0a\u4e09\u89d2\u77e9\u9635, \u4f7f\u5f97 \\(inv = (u^T u)^{-1}\\) \u6210\u7acb; \u82e5 <cite>upper</cite> \u8bbe\u4e3a False , <cite>u</cite> \u662f\u4e00\u4e2a\u4e0b\u4e09\u89d2\u77e9\u9635, \u4f7f\u5f97 \\(inv = (u u^T)^{-1}\\) \u6210\u7acb. | Parameters: | u ( Tensor ) \u2013 the input 2D <cite>Tensor</cite>, a upper or lower triangular Cholesky factor upper ( bool , optional ) \u2013 Flag if upper (default) or lower triangular matrix out ( Tensor , optional ) \u2013 A Tensor for inv | | --- | --- | Example: >>> a = torch.randn(3,3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> u = torch.potrf(a) >>> a 2.3563 3.2318 -0.9406 3.2318 4.9557 -2.1618 -0.9406 -2.1618 2.2443 [torch.FloatTensor of size 3x3] >>> torch.potri(u) 12.5724 -10.1765 -4.5333 -10.1765 8.5852 4.0047 -4.5333 4.0047 2.4031 [torch.FloatTensor of size 3x3] >>> a.inverse() 12.5723 -10.1765 -4.5333 -10.1765 8.5852 4.0047 -4.5333 4.0047 2.4031 [torch.FloatTensor of size 3x3] torch.potrs(b, u, out=None) potrs(b, u, upper, out=None) Solves a linear system of equations with a positive semidefinite matrix to be inverted given its given a Cholesky factor matrix u : returns matrix <cite>c</cite> If <cite>upper</cite> is True or not provided, <cite>u</cite> is and upper triangular such that \\(c = (u^T u)^{-1} b\\) . If <cite>upper</cite> is False , <cite>u</cite> is and lower triangular such that \\(c = (u u^T)^{-1} b\\) . Note <cite>b</cite> is always a 2D <cite>Tensor</cite>, use <cite>b.unsqueeze(1)</cite> to convert a vector. | Parameters: | b ( Tensor ) \u2013 the right hand side 2D <cite>Tensor</cite> u ( Tensor ) \u2013 the input 2D <cite>Tensor</cite>, a upper or lower triangular Cholesky factor upper ( bool , optional ) \u2013 Return upper (default) or lower triangular matrix out ( Tensor , optional ) \u2013 A Tensor for c | | --- | --- | Example: >>> a = torch.randn(3,3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> u = torch.potrf(a) >>> a 2.3563 3.2318 -0.9406 3.2318 4.9557 -2.1618 -0.9406 -2.1618 2.2443 [torch.FloatTensor of size 3x3] >>> b = torch.randn(3,2) >>> b -0.3119 -1.8224 -0.2798 0.1789 -0.3735 1.7451 [torch.FloatTensor of size 3x2] >>> torch.potrs(b,u) 0.6187 -32.6438 -0.7234 27.0703 -0.6039 13.1717 [torch.FloatTensor of size 3x2] >>> torch.mm(a.inverse(),b) 0.6187 -32.6436 -0.7234 27.0702 -0.6039 13.1717 [torch.FloatTensor of size 3x2] torch.pstrf(a, out=None) pstrf(a, upper, out=None) Computes the pivoted Cholesky decomposition of a positive semidefinite matrix a : returns matrices <cite>u</cite> and <cite>piv</cite>. If <cite>upper</cite> is True or not provided, <cite>u</cite> is and upper triangular such that \\(a = p^T u^T u p\\) , with <cite>p</cite> the permutation given by <cite>piv</cite>. If <cite>upper</cite> is False , <cite>u</cite> is and lower triangular such that \\(a = p^T u u^T p\\) . | Parameters: | a ( Tensor ) \u2013 the input 2D <cite>Tensor</cite> upper ( bool , optional ) \u2013 Return upper (default) or lower triangular matrix out ( tuple , optional ) \u2013 A tuple of u and piv Tensors | | --- | --- | Example: >>> a = torch.randn(3,3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> a 5.4417 -2.5280 1.3643 -2.5280 2.9689 -2.1368 1.3643 -2.1368 4.6116 [torch.FloatTensor of size 3x3] >>> u,piv = torch.pstrf(a) >>> u 2.3328 0.5848 -1.0837 0.0000 2.0663 -0.7274 0.0000 0.0000 1.1249 [torch.FloatTensor of size 3x3] >>> piv 0 2 1 [torch.IntTensor of size 3] >>> p = torch.eye(3).index_select(0,piv.long()).index_select(0,piv.long()).t() # make pivot permutation >>> torch.mm(torch.mm(p.t(),torch.mm(u.t(),u)),p) # reconstruct 5.4417 1.3643 -2.5280 1.3643 4.6116 -2.1368 -2.5280 -2.1368 2.9689 [torch.FloatTensor of size 3x3] torch.qr(input, out=None) -> (Tensor, Tensor) \u8ba1\u7b97\u77e9\u9635 input \u7684 QR \u5206\u89e3. \u8fd4\u56de\u77e9\u9635 <cite>q</cite> \u548c <cite>r</cite> \u4f7f\u5f97 \\(x = q * r\\) , \u4e14 <cite>q</cite> \u662f\u4e00\u4e2a \u6b63\u4ea4\u77e9\u9635, <cite>r</cite> \u662f\u4e00\u4e2a\u4e0a\u4e09\u89d2\u77e9\u9635. This returns the thin (reduced) QR factorization. Note \u5982\u679c\u77e9\u9635 <cite>input</cite> \u4e2d\u7684\u5143\u7d20\u592a\u5927, \u90a3\u4e48\u7cbe\u5ea6\u53ef\u80fd\u4f1a\u4e22\u5931. Note \u5c3d\u7ba1\u8be5\u51fd\u6570\u603b\u662f\u80fd\u7ed9\u60a8\u4e00\u4e2a\u6709\u6548\u7684\u5206\u89e3, \u4f46\u5728\u4e0d\u540c\u5e73\u53f0\u4e0a\u7ed3\u679c\u53ef\u80fd\u4e0d\u540c - \u53d6\u51b3\u4e8e\u8be5\u5e73\u53f0\u4e0a LAPACK \u7684\u5b9e\u73b0. Note Irrespective of the original strides, the returned matrix <cite>q</cite> will be transposed, i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite>. | Parameters: | input ( Tensor ) \u2013 the input 2D <cite>Tensor</cite> out ( tuple , optional ) \u2013 A tuple of Q and R Tensors | | --- | --- | Example: >>> a = torch.Tensor([[12, -51, 4], [6, 167, -68], [-4, 24, -41]]) >>> q, r = torch.qr(a) >>> q -0.8571 0.3943 0.3314 -0.4286 -0.9029 -0.0343 0.2857 -0.1714 0.9429 [torch.FloatTensor of size 3x3] >>> r -14.0000 -21.0000 14.0000 0.0000 -175.0000 70.0000 0.0000 0.0000 -35.0000 [torch.FloatTensor of size 3x3] >>> torch.mm(q, r).round() 12 -51 4 6 167 -68 -4 24 -41 [torch.FloatTensor of size 3x3] >>> torch.mm(q.t(), q).round() 1 -0 0 -0 1 0 0 0 1 [torch.FloatTensor of size 3x3] torch.svd(input, some=True, out=None) -> (Tensor, Tensor, Tensor) <cite>U, S, V = torch.svd(A)</cite> \u8fd4\u56de\u5927\u5c0f\u4e3a <cite>(n x m)</cite> \u7684\u5b9e\u77e9\u9635 <cite>A</cite> \u7684\u5947\u5f02\u503c\u5206\u89e3, \u4f7f\u5f97 \\(A = USV'*\\) . <cite>U</cite> \u7684\u5927\u5c0f\u4e3a <cite>n x n</cite> <cite>S</cite> \u7684\u5927\u5c0f\u4e3a n x m <cite>V</cite> \u7684\u5927\u5c0f\u4e3a <cite>m x m</cite>. some \u8868\u793a\u5c06\u88ab\u8ba1\u7b97\u7684\u5947\u5f02\u503c\u7684\u603b\u6570. \u5982\u679c <cite>some=True</cite>, \u5b83\u5c06\u8ba1\u7b97\u6307\u5b9a\u7684 some \u6570\u91cf\u4e2a\u5947\u5f02\u503c, \u5982\u679c <cite>some=False</cite>, \u5219\u8ba1\u7b97\u6240\u6709\u5947\u5f02\u503c. Note Irrespective of the original strides, the returned matrix <cite>U</cite> will be transposed, i.e. with strides <cite>(1, n)</cite> instead of <cite>(n, 1)</cite>. | Parameters: | input ( Tensor ) \u2013 the input 2D Tensor some ( bool , optional ) \u2013 controls the number of singular values to be computed out ( tuple , optional ) \u2013 the result tuple | | --- | --- | Example: >>> a = torch.Tensor([[8.79, 6.11, -9.15, 9.57, -3.49, 9.84], ... [9.93, 6.91, -7.93, 1.64, 4.02, 0.15], ... [9.83, 5.04, 4.86, 8.83, 9.80, -8.99], ... [5.45, -0.27, 4.85, 0.74, 10.00, -6.02], ... [3.16, 7.98, 3.01, 5.80, 4.27, -5.31]]).t() >>> a 8.7900 9.9300 9.8300 5.4500 3.1600 6.1100 6.9100 5.0400 -0.2700 7.9800 -9.1500 -7.9300 4.8600 4.8500 3.0100 9.5700 1.6400 8.8300 0.7400 5.8000 -3.4900 4.0200 9.8000 10.0000 4.2700 9.8400 0.1500 -8.9900 -6.0200 -5.3100 [torch.FloatTensor of size 6x5] >>> u, s, v = torch.svd(a) >>> u -0.5911 0.2632 0.3554 0.3143 0.2299 -0.3976 0.2438 -0.2224 -0.7535 -0.3636 -0.0335 -0.6003 -0.4508 0.2334 -0.3055 -0.4297 0.2362 -0.6859 0.3319 0.1649 -0.4697 -0.3509 0.3874 0.1587 -0.5183 0.2934 0.5763 -0.0209 0.3791 -0.6526 [torch.FloatTensor of size 6x5] >>> s 27.4687 22.6432 8.5584 5.9857 2.0149 [torch.FloatTensor of size 5] >>> v -0.2514 0.8148 -0.2606 0.3967 -0.2180 -0.3968 0.3587 0.7008 -0.4507 0.1402 -0.6922 -0.2489 -0.2208 0.2513 0.5891 -0.3662 -0.3686 0.3859 0.4342 -0.6265 -0.4076 -0.0980 -0.4932 -0.6227 -0.4396 [torch.FloatTensor of size 5x5] >>> torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t())) 8.934150226306685e-06 torch.symeig(input, eigenvectors=False, upper=True, out=None) -> (Tensor, Tensor) <cite>e, V = torch.symeig(input)</cite> \u8fd4\u56de\u5b9e\u5bf9\u79f0\u77e9\u9635 input \u7684\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf. <cite>input</cite> \u548c <cite>V</cite> \u662f <cite>m x m</cite> \u77e9\u9635, <cite>e</cite> \u662f\u4e00\u4e2a <cite>m</cite> \u7ef4\u7684\u5411\u91cf. \u8fd9\u4e2a\u51fd\u6570\u8ba1\u7b97\u77e9\u9635 <cite>input</cite> \u7684\u6240\u6709\u7279\u5f81\u503c (\u548c\u5411\u91cf), \u4f7f\u5f97 <cite>input = V diag(e) V\u2019</cite>. \u5e03\u5c14\u53c2\u6570 eigenvectors \u5b9a\u4e49\u4e86\u662f\u5426\u8ba1\u7b97\u7279\u5f81\u5411\u91cf. \u5982\u679c\u5b83\u4e3a False , \u90a3\u4e48\u53ea\u6709\u7279\u5f81\u503c\u4f1a\u88ab\u8ba1\u7b97. \u5982\u679c\u5b83\u4e3a True , \u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf\u90fd\u4f1a\u88ab\u8ba1\u7b97. \u7531\u4e8e\u8f93\u5165\u77e9\u9635 <cite>input</cite> \u88ab\u5047\u5b9a\u662f\u5bf9\u79f0\u7684, \u56e0\u6b64\u9ed8\u8ba4\u5730\u53ea\u6709\u5b83\u7684\u4e0a\u4e09\u89d2\u90e8\u5206\u4f1a\u88ab\u4f7f\u7528. \u5982\u679c upper \u662f False , \u90a3\u4e48\u5b83\u7684\u4e0b\u4e09\u89d2\u90e8\u5206\u4f1a\u88ab\u4f7f\u7528. Note: Irrespective of the original strides, the returned matrix <cite>V</cite> will be transposed, i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite>. | Parameters: | input ( Tensor ) \u2013 the input symmetric matrix eigenvectors ( boolean__, optional ) \u2013 controls whether eigenvectors have to be computed upper ( boolean__, optional ) \u2013 controls whether to consider upper-triangular or lower-triangular region out ( tuple , optional ) \u2013 The result tuple of (Tensor, Tensor) | | --- | --- | Examples: >>> a = torch.Tensor([[ 1.96, 0.00, 0.00, 0.00, 0.00], ... [-6.49, 3.80, 0.00, 0.00, 0.00], ... [-0.47, -6.39, 4.17, 0.00, 0.00], ... [-7.20, 1.50, -1.51, 5.70, 0.00], ... [-0.65, -6.34, 2.67, 1.80, -7.10]]).t() >>> e, v = torch.symeig(a, eigenvectors=True) >>> e -11.0656 -6.2287 0.8640 8.8655 16.0948 [torch.FloatTensor of size 5] >>> v -0.2981 -0.6075 0.4026 -0.3745 0.4896 -0.5078 -0.2880 -0.4066 -0.3572 -0.6053 -0.0816 -0.3843 -0.6600 0.5008 0.3991 -0.0036 -0.4467 0.4553 0.6204 -0.4564 -0.8041 0.4480 0.1725 0.3108 0.1622 [torch.FloatTensor of size 5x5] torch.trtrs()","title":"torch"},{"location":"53/#torch","text":"torch package \u5305\u542b\u4e86\u591a\u7ef4\u5f20\u91cf\u7684\u6570\u636e\u7ed3\u6784, \u4ee5\u53ca\u57fa\u4e8e\u5176\u4e0a\u7684\u591a\u79cd\u6570\u5b66\u64cd\u4f5c. \u6b64\u5916, \u5b83\u8fd8\u63d0\u4f9b\u4e86\u8bb8\u591a\u7528\u4e8e\u9ad8\u6548\u5e8f\u5217\u5316 Tensor \u548c\u4efb\u610f\u7c7b\u578b\u7684\u5b9e\u7528\u5de5\u5177\u5305, \u4ee5\u53ca\u4e00\u8d77\u5176\u5b83\u6709\u7528\u7684\u5b9e\u7528\u5de5\u5177\u5305. \u5b83\u6709\u4e00\u4e2a CUDA \u7684\u5bf9\u5e94\u5b9e\u73b0, \u5b83\u4f7f\u60a8\u80fd\u591f\u5728\u8ba1\u7b97\u80fd\u529b >=0.3 \u7684 NVIDIA GPU \u4e0a\u8fdb\u884c\u5f20\u91cf\u8fd0\u7b97.","title":"torch"},{"location":"53/#tensors","text":"torch.is_tensor(obj) \u5982\u679c <cite>obj</cite> \u662f\u4e00\u4e2a pytorch tensor, \u5219\u8fd4\u56deTrue. Parameters: obj ( Object ) \u2013 \u7528\u4e8e\u6d4b\u8bd5\u7684\u5bf9\u8c61 torch.is_storage(obj) \u5982\u679c <cite>obj</cite> \u662f\u4e00\u4e2a pytorch storage object, \u5219\u8fd4\u56deTrue. Parameters: obj ( Object ) \u2013 \u7528\u4e8e\u6d4b\u8bd5\u7684\u5bf9\u8c61 torch.set_default_tensor_type(t) torch.numel(input) \u2192 int \u8fd4\u56de input Tensor \u4e2d\u7684\u5143\u7d20\u603b\u6570. Parameters: input ( Tensor ) \u2013 \u8f93\u5165\u7684 <cite>Tensor</cite> Example: >>> a = torch.randn(1,2,3,4,5) >>> torch.numel(a) 120 >>> a = torch.zeros(4,4) >>> torch.numel(a) 16 torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None) \u8bbe\u7f6e\u6253\u5370\u9009\u9879. \u4ece Numpy \u4e2d\u91c7\u96c6\u6570\u636e | Parameters: | precision \u2013 \u6d6e\u70b9\u8f93\u51fa\u7cbe\u5ea6\u7684\u4f4d\u6570 (\u9ed8\u8ba4\u503c\u4e3a 8). threshold \u2013 \u89e6\u53d1\u6c47\u603b\u663e\u793a\u800c\u4e0d\u662f\u5b8c\u5168\u663e\u793a(repr)\u7684\u6570\u7ec4\u5143\u7d20\u7684\u603b\u6570 (\u9ed8\u8ba4\u503c\u4e3a 1000). edgeitems \u2013 \u6bcf\u4e2a\u7ef4\u5ea6\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u603b\u7ed3\u7684\u6570\u7ec4\u9879\u6570 (\u9ed8\u8ba4\u503c\u4e3a 3). linewidth \u2013 \u63d2\u5165\u6362\u884c\u7b26\u7684\u6bcf\u884c\u5b57\u7b26\u6570 (\u9ed8\u8ba4\u503c\u4e3a 80). Thresholded matricies(\u9608\u503c\u77e9\u9635) \u5c06\u5ffd\u7565\u8fd9\u4e2a\u53c2\u6570. profile \u2013 \u7528\u4e8e\u6f02\u4eae\u683c\u5f0f\u7684\u6253\u5370. \u53ef\u4ee5\u7528\u4ee5\u4e0b\u4efb\u4f55\u9009\u9879\u6765\u8fdb\u884c\u8986\u76d6 (default, short, full) | | --- | --- |","title":"Tensors (\u5f20\u91cf)"},{"location":"53/#creation-ops","text":"torch.eye(n, m=None, out=None) \u8fd4\u56de\u5bf9\u89d2\u7ebf\u4f4d\u7f6e\u5168\u4e3a1, \u5176\u5b83\u4f4d\u7f6e\u5168\u4e3a0\u7684\u4e8c\u7ef4 tensor. | Parameters: | n ( int ) \u2013 \u884c\u6570 m ( int , optional ) \u2013 \u5217\u6570. \u5982\u679c\u4e3a None,\u5219\u9ed8\u8ba4\u4e3a <cite>n</cite> out ( Tensor , optional ) \u2013 \u8f93\u51fa tensor | | --- | --- | | Returns: | \u4e00\u4e2a\u5bf9\u89d2\u7ebf\u4f4d\u7f6e\u5168\u4e3a1, \u5176\u5b83\u4f4d\u7f6e\u5168\u4e3a0\u7684\u4e8c\u7ef4 tensor. | | --- | --- | | Return type: | Tensor | | --- | --- | Example: >>> torch.eye(3) 1 0 0 0 1 0 0 0 1 [torch.FloatTensor of size 3x3] torch.from_numpy(ndarray) \u2192 Tensor \u4ece numpy.ndarray \u7c7b \u521b\u5efa\u4e00\u4e2a Tensor \u7c7b. \u8fd4\u56de tensor \u548c <cite>ndarray</cite> \u5171\u4eab\u76f8\u540c\u7684\u5185\u5b58. \u5bf9 tensor \u7684\u4fee\u6539\u5c06\u53cd\u6620\u5728 <cite>ndarray</cite> \u4e2d, \u53cd\u4e4b\u4ea6\u7136. \u8fd4\u56de tensor \u4e0d\u53ef\u8c03\u6574\u5927\u5c0f. Example: >>> a = numpy.array([1, 2, 3]) >>> t = torch.from_numpy(a) >>> t torch.LongTensor([1, 2, 3]) >>> t[0] = -1 >>> a array([-1, 2, 3]) torch.linspace(start, end, steps=100, out=None) \u2192 Tensor \u8fd4\u56de start \u548c end \u4e4b\u95f4\u7b49\u95f4\u9694 steps \u70b9\u7684\u4e00\u7ef4 Tensor. \u8f93\u51fa \u662f\u5c3a\u5bf8 steps \u4e3a\u4e00\u7ef4 tensor | Parameters: | start ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u8d77\u59cb\u503c end ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u7ed3\u675f\u503c steps ( int ) \u2013 \u5728 start \u548c end \u4e4b\u95f4\u7684\u6837\u672c\u6570 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c\u7684 <cite>Tensor</cite> | | --- | --- | Example: >>> torch.linspace(3, 10, steps=5) 3.0000 4.7500 6.5000 8.2500 10.0000 [torch.FloatTensor of size 5] >>> torch.linspace(-10, 10, steps=5) -10 -5 0 5 10 [torch.FloatTensor of size 5] >>> torch.linspace(start=-10, end=10, steps=5) -10 -5 0 5 10 [torch.FloatTensor of size 5] torch.logspace(start, end, steps=100, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5728 \\(10^{start}\\) \u548c \\(10^{end}\\) \u4e4b\u95f4\u7684\u5bf9\u6570\u95f4\u9694 steps \u70b9\u7684\u4e00\u7ef4 Tensor \u8f93\u51fa\u662f\u957f\u5ea6\u4e3a steps \u7684\u4e00\u7ef4 tensor | Parameters: | start ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u8d77\u59cb\u503c end ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u7ed3\u675f\u503c steps ( int ) \u2013 \u5728 start \u548c end \u4e4b\u95f4\u7684\u6837\u672c\u6570 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c Tensor | | --- | --- | Example: >>> torch.logspace(start=-10, end=10, steps=5) 1.0000e-10 1.0000e-05 1.0000e+00 1.0000e+05 1.0000e+10 [torch.FloatTensor of size 5] >>> torch.logspace(start=0.1, end=1.0, steps=5) 1.2589 2.1135 3.5481 5.9566 10.0000 [torch.FloatTensor of size 5] torch.ones(*sizes, out=None) \u2192 Tensor \u8fd4\u56de\u586b\u5145\u4e86\u6807\u91cf\u503c <cite>1</cite> \u7684 Tensor, \u5176\u5f62\u72b6\u7531\u53ef\u53d8\u53c2\u6570 sizes \u5b9a\u4e49. | Parameters: | sizes ( int... ) \u2013 \u4e00\u7ec4\u5b9a\u4e49\u8f93\u51fa Tensor \u5f62\u72b6\u7684\u6574\u6570 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c Tensor | | --- | --- | Example: >>> torch.ones(2, 3) 1 1 1 1 1 1 [torch.FloatTensor of size 2x3] >>> torch.ones(5) 1 1 1 1 1 [torch.FloatTensor of size 5] torch.ones_like(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u7528\u6807\u91cf\u503c <cite>1</cite> \u586b\u5145\u7684\u5f20\u91cf, \u5927\u5c0f\u4e0e input \u76f8\u540c. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u7684\u5927\u5c0f\u5c06\u51b3\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f. out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c Tensor | | --- | --- | Example: >>> input = torch.FloatTensor(2, 3) >>> torch.ones_like(input) 1 1 1 1 1 1 [torch.FloatTensor of size 2x3] torch.arange(start=0, end, step=1, out=None) \u2192 Tensor \u4ece <cite>start</cite> \u7528\u6b65\u957f\u4e3a step \u5f00\u59cb, \u95f4\u9694\u5728 [start, end) \u4e2d\u7684\u503c\u8fd4\u56de\u5927\u5c0f\u5c42\u6b21\u4e3a \\(floor((end - start) / step)\\) \u7684\u4e00\u7ef4 Tensor. | Parameters: | start ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u8d77\u59cb\u503c end ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u7ed3\u675f\u503c step ( float ) \u2013 \u6bcf\u5bf9\u76f8\u90bb\u70b9\u4e4b\u95f4\u7684\u95f4\u9694 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> torch.arange(5) 0 1 2 3 4 [torch.FloatTensor of size 5] >>> torch.arange(1, 4) 1 2 3 [torch.FloatTensor of size 3] >>> torch.arange(1, 2.5, 0.5) 1.0000 1.5000 2.0000 [torch.FloatTensor of size 3] torch.range(start, end, step=1, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5728 start \u5230 end \u5e76\u4e14\u6b65\u957f\u4e3a step \u7684\u533a\u95f4\u5185, \u5927\u5c0f\u4e3a \\(floor((end - start) / step) + 1\\) \u4e3a\u4e00\u7ef4 Tensor. step \u662f tensor \u4e2d\u4e24\u4e2a\u503c\u4e4b\u95f4\u7684\u5dee\u8ddd. \\(x_{i+1} = x_i + step\\) Warning \u6b64\u529f\u80fd\u5df2\u88ab\u5f03\u7528, \u4ee5\u652f\u6301 torch.arange() . | Parameters: | start ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u8d77\u59cb\u503c end ( float ) \u2013 \u70b9\u96c6\u5408\u7684\u7ed3\u675f\u503c step ( float ) \u2013 \u6bcf\u5bf9\u76f8\u90bb\u70b9\u4e4b\u95f4\u7684\u95f4\u9694 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> torch.range(1, 4) 1 2 3 4 [torch.FloatTensor of size 4] >>> torch.range(1, 4, 0.5) 1.0000 1.5000 2.0000 2.5000 3.0000 3.5000 4.0000 [torch.FloatTensor of size 7] torch.zeros(*sizes, out=None) \u2192 Tensor \u8fd4\u56de\u586b\u5145\u4e86\u6807\u91cf\u503c\u4e3a <cite>0</cite> \u7684 Tensor, \u5176\u5f62\u72b6\u7531\u53ef\u53d8\u53c2\u91cf sizes \u5b9a\u4e49. | Parameters: | sizes ( int... ) \u2013 \u5b9a\u4e49\u8f93\u51fa Tensor \u5f62\u72b6\u7684\u4e00\u7ec4\u6574\u6570. out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c Tensor | | --- | --- | Example: >>> torch.zeros(2, 3) 0 0 0 0 0 0 [torch.FloatTensor of size 2x3] >>> torch.zeros(5) 0 0 0 0 0 [torch.FloatTensor of size 5] torch.zeros_like(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u7528\u6807\u91cf\u503c <cite>0</cite> \u586b\u5145\u7684 Tensor, \u5176\u5927\u5c0f\u4e0e input \u76f8\u540c. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u7684\u5927\u5c0f\u5c06\u51b3\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f. out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c Tensor | | --- | --- | Example: >>> input = torch.FloatTensor(2, 3) >>> torch.zeros_like(input) 0 0 0 0 0 0 [torch.FloatTensor of size 2x3]","title":"Creation Ops (\u521b\u5efa\u64cd\u4f5c)"},{"location":"53/#indexing-slicing-joining-mutating-ops","text":"torch.cat(seq, dim=0, out=None) \u2192 Tensor \u5728\u7ed9\u5b9a\u7ef4\u5ea6\u4e0a\u5bf9\u8f93\u5165\u7684\u5f20\u91cf\u5e8f\u5217 seq \u8fdb\u884c\u8fde\u63a5\u64cd\u4f5c. \u6240\u6709\u5f20\u91cf\u5fc5\u987b\u5177\u6709\u76f8\u540c\u7684\u5f62\u72b6(\u5728 cat \u7ef4\u5ea6\u4e2d\u9664\u5916) \u6216\u4e3a\u7a7a. torch.cat() \u53ef\u4ee5\u770b\u505a\u662f torch.split() \u548c torch.chunk() \u7684\u9006\u64cd\u4f5c. cat() \u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u4f8b\u5b50\u66f4\u597d\u5730\u7406\u89e3. | Parameters: | seq ( sequence of Tensors ) \u2013 \u53ef\u4ee5\u662f\u4efb\u4f55\u76f8\u540c\u7c7b\u578b\u7684 <cite>Tensor</cite> \u7684 Python \u5e8f\u5217. dim ( int , optional ) \u2013 tensors \u7ea7\u8054\u7684\u7ef4\u6570 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u53c2\u6570 | | --- | --- | Example: >>> x = torch.randn(2, 3) >>> x 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x3] >>> torch.cat((x, x, x), 0) 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 6x3] >>> torch.cat((x, x, x), 1) 0.5983 -0.0341 2.4918 0.5983 -0.0341 2.4918 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 1.5981 -0.5265 -0.8735 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x9] torch.chunk(tensor, chunks, dim=0) \u5728\u7ed9\u5b9a\u7ef4\u5ea6(\u8f74)\u4e0a\u5c06\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u5206\u5757\u5904\u7406. | Parameters: | tensor ( Tensor ) \u2013 \u5f85\u5206\u5757\u7684\u8f93\u5165\u5f20\u91cf. chunks ( int ) \u2013 \u8981\u8fd4\u56de\u7684\u5206\u5757\u7684\u4e2a\u6570. dim ( int ) \u2013 \u5207\u5206\u5f20\u91cf\u6240\u9700\u8981\u6cbf\u7740\u7684\u7ef4\u5ea6. | | --- | --- | torch.gather(input, dim, index, out=None) \u2192 Tensor \u6cbf\u7ed9\u5b9a\u8f74 <cite>dim</cite> ,\u5c06\u8f93\u5165\u7d22\u5f15\u5f20\u91cf <cite>index</cite> \u6307\u5b9a\u4f4d\u7f6e\u7684\u503c\u8fdb\u884c\u805a\u5408. \u5bf9\u4e00\u4e2a 3 \u7ef4\u5f20\u91cf,\u8f93\u51fa\u53ef\u4ee5\u5b9a\u4e49\u4e3a: out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0 out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1 out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 \u5982\u679c input \u662f size \u4e3a \\((x_0, x_1..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})\\) \u4e14 dim = i \u7684 n \u7ef4\u5f20\u91cf,\u5219 index \u5fc5\u987b\u662f\u5177\u6709 size \u4e3a \\((x_0, x_1, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})\\) \u7684 n \u7ef4\u5f20\u91cf,\u5176\u4e2d y >= 1 ,\u5e76\u4e14 out \u5c06\u4e0e index \u7684 size \u76f8\u540c. | Parameters: | input ( Tensor ) \u2013 \u6e90\u5f20\u91cf dim ( int ) \u2013 \u7d22\u5f15\u7684\u8f74 index ( LongTensor ) \u2013 \u805a\u5408\u5143\u7d20\u7684\u4e0b\u6807 out ( Tensor , optional ) \u2013 \u76ee\u6807\u5f20\u91cf | | --- | --- | Example: >>> t = torch.Tensor([[1,2],[3,4]]) >>> torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]])) 1 1 4 3 [torch.FloatTensor of size 2x2] torch.index_select(input, dim, index, out=None) \u2192 Tensor \u6cbf\u7740\u6307\u5b9a\u7ef4\u5ea6 dim \u5bf9\u8f93\u5165\u8fdb\u884c\u5207\u7247,\u53d6 index \u4e2d\u6307\u5b9a\u7684\u76f8\u5e94\u9879 ( index \u4e3a\u4e00\u4e2a <cite>LongTensor</cite> ),\u7136\u540e\u8fd4\u56de\u5230\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf. \u8fd4\u56de\u7684\u5f20\u91cf\u4e0e\u539f\u59cb\u5f20\u91cf <cite>Tensor</cite> \u6709\u76f8\u540c\u7684\u7ef4\u5ea6(\u5728\u6307\u5b9a\u8f74\u4e0a). Note \u8fd4\u56de\u7684\u5f20\u91cf\u4e0d\u4e0e\u539f\u59cb\u5f20\u91cf\u5171\u4eab\u5185\u5b58\u7a7a\u95f4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf dim ( int ) \u2013 \u7d22\u5f15\u7684\u8f74 index ( LongTensor ) \u2013 \u5305\u542b\u7d22\u5f15\u4e0b\u6807\u7684\u4e00\u7ef4\u5f20\u91cf out ( Tensor , optional ) \u2013 \u8f93\u51fa\u53c2\u6570/\u76ee\u6807\u5f20\u91cf | | --- | --- | Example: >>> x = torch.randn(3, 4) >>> x 1.2045 2.4084 0.4001 1.1372 0.5596 1.5677 0.6219 -0.7954 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 3x4] >>> indices = torch.LongTensor([0, 2]) >>> torch.index_select(x, 0, indices) 1.2045 2.4084 0.4001 1.1372 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 2x4] >>> torch.index_select(x, 1, indices) 1.2045 0.4001 0.5596 0.6219 1.3635 -0.5414 [torch.FloatTensor of size 3x2] torch.masked_select(input, mask, out=None) \u2192 Tensor \u6839\u636e\u63a9\u7801\u5f20\u91cf mask \u4e2d\u7684\u4e8c\u5143\u503c,\u53d6\u8f93\u5165\u5f20\u91cf\u4e2d\u7684\u6307\u5b9a\u9879 ( mask \u4e3a\u4e00\u4e2a <cite>ByteTensor</cite> ),\u5c06\u53d6\u503c\u8fd4\u56de\u5230\u4e00\u4e2a\u65b0\u7684\u4e00\u7ef4\u5f20\u91cf. \u5f20\u91cf mask \u4e0e input \u7684 shape \u6216\u7ef4\u5ea6\u4e0d\u9700\u8981\u76f8\u540c,\u4f46\u662f\u4ed6\u4eec\u5fc5\u987b\u662f broadcastable . Note \u8fd4\u56de\u7684\u5f20\u91cf\u4e0d\u4e0e\u539f\u59cb\u5f20\u91cf\u5171\u4eab\u5185\u5b58\u7a7a\u95f4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf mask ( ByteTensor ) \u2013 \u63a9\u7801\u5f20\u91cf,\u5305\u542b\u4e86\u4e8c\u5143\u7d22\u5f15\u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u53c2\u6570/\u76ee\u6807\u5f20\u91cf | | --- | --- | Example: >>> x = torch.randn(3, 4) >>> x 1.2045 2.4084 0.4001 1.1372 0.5596 1.5677 0.6219 -0.7954 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 3x4] >>> mask = x.ge(0.5) >>> mask 1 1 0 1 1 1 1 0 1 0 0 0 [torch.ByteTensor of size 3x4] >>> torch.masked_select(x, mask) 1.2045 2.4084 1.1372 0.5596 1.5677 0.6219 1.3635 [torch.FloatTensor of size 7] torch.nonzero(input, out=None) \u2192 LongTensor \u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u8f93\u5165 input \u4e2d\u975e\u96f6\u5143\u7d20\u7d22\u5f15\u7684\u5f20\u91cf. \u8f93\u51fa\u5f20\u91cf\u4e2d\u7684\u6bcf\u884c\u5305\u542b input \u4e2d\u975e\u96f6\u5143\u7d20\u7684\u7d22\u5f15. \u5982\u679c\u8f93\u5165\u5f20\u91cf input \u6709 <cite>n</cite> \u7ef4,\u5219\u8f93\u51fa\u7684\u7d22\u5f15\u5f20\u91cf out \u7684 size \u4e3a <cite>z x n</cite> , \u8fd9\u91cc <cite>z</cite> \u662f\u8f93\u5165\u5f20\u91cf input \u4e2d\u6240\u6709\u975e\u96f6\u5143\u7d20\u7684\u4e2a\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf/\u6e90\u5f20\u91cf out ( LongTensor__, optional ) \u2013 \u5305\u542b\u7d22\u5f15\u503c\u7684\u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> torch.nonzero(torch.Tensor([1, 1, 1, 0, 1])) 0 1 2 4 [torch.LongTensor of size 4x1] >>> torch.nonzero(torch.Tensor([[0.6, 0.0, 0.0, 0.0], ... [0.0, 0.4, 0.0, 0.0], ... [0.0, 0.0, 1.2, 0.0], ... [0.0, 0.0, 0.0,-0.4]])) 0 0 1 1 2 2 3 3 [torch.LongTensor of size 4x2] torch.split(tensor, split_size, dim=0) \u5c06\u8f93\u5165\u5f20\u91cf\u5206\u5272\u6210\u76f8\u7b49 size \u7684 chunks (\u5982\u679c\u53ef\u5206). \u5982\u679c\u6cbf\u6307\u5b9a\u7ef4\u7684\u5f20\u91cf\u5f62\u72b6\u5927\u5c0f\u4e0d\u80fd\u88ab split_size \u6574\u5206, \u5219\u6700\u540e\u4e00\u4e2a\u5206\u5757\u4f1a\u5c0f\u4e8e\u5176\u5b83\u5206\u5757. | Parameters: | tensor ( Tensor ) \u2013 \u5f85\u5206\u5272\u5f20\u91cf. split_size ( int ) \u2013 \u5355\u4e2a\u5206\u5757\u7684 size \u5927\u5c0f. dim ( int ) \u2013 \u6cbf\u7740\u6b64\u7ef4\u8fdb\u884c\u5206\u5272. | | --- | --- | torch.squeeze(input, dim=None, out=None) \u5c06 input \u5f20\u91cf size \u4e2d\u7684 <cite>1</cite> \u53bb\u9664\u5e76\u8fd4\u56de. \u5982\u679c <cite>input</cite> \u7684 shape \u5982 \\((A x 1 x B x C x 1 x D)\\) ,\u90a3\u4e48\u8f93\u51fa shape \u5c31\u4e3a: \\((A x B x C x D)\\) \u5f53\u7ed9\u5b9a dim \u65f6,\u90a3\u4e48\u6324\u538b\u64cd\u4f5c\u53ea\u5728\u7ed9\u5b9a\u7ef4\u5ea6\u4e0a.\u4f8b\u5982, <cite>input</cite> \u7684 shape \u4e3a: \\((A x 1 x B)\\) , <cite>squeeze(input, 0)</cite> \u5c06\u4f1a\u4fdd\u6301\u5f20\u91cf\u4e0d\u53d8,\u53ea\u6709\u7528 <cite>squeeze(input, 1)</cite> , shape \u4f1a\u53d8\u6210 \\((A x B)\\) . Note \u4f5c\u4e3a\u4e0a\u8ff0\u7684\u4e00\u4e2a\u4f8b\u5916,size \u4e3a 1 \u7684\u4e00\u7ef4\u5f20\u91cf\u4e0d\u4f1a\u6539\u53d8\u7ef4\u5ea6. Note \u8fd4\u56de\u5f20\u91cf\u4e0e\u8f93\u5165\u5f20\u91cf\u5171\u4eab\u5185\u5b58,\u6240\u4ee5\u6539\u53d8\u5176\u4e2d\u4e00\u4e2a\u7684\u5185\u5bb9\u4f1a\u6539\u53d8\u53e6\u4e00\u4e2a. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf dim ( int , optional ) \u2013 \u5982\u679c\u7ed9\u5b9a dim \u65f6,\u5219 <cite>input</cite> \u53ea\u4f1a\u5728\u7ed9\u5b9a\u7ef4\u5ea6\u6267\u884c\u6324\u538b out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> x = torch.zeros(2,1,2,1,2) >>> x.size() (2L, 1L, 2L, 1L, 2L) >>> y = torch.squeeze(x) >>> y.size() (2L, 2L, 2L) >>> y = torch.squeeze(x, 0) >>> y.size() (2L, 1L, 2L, 1L, 2L) >>> y = torch.squeeze(x, 1) >>> y.size() (2L, 2L, 1L, 2L) torch.stack(sequence, dim=0, out=None) \u6cbf\u7740\u4e00\u4e2a\u65b0\u7ef4\u5ea6\u5bf9\u8f93\u5165\u5f20\u91cf\u5e8f\u5217\u8fdb\u884c\u8fde\u63a5. \u5e8f\u5217\u4e2d\u6240\u6709\u7684\u5f20\u91cf\u90fd\u5e94\u8be5\u4e3a\u76f8\u540c size . | Parameters: | sequence ( Sequence ) \u2013 \u5f85\u8fde\u63a5\u7684\u5f20\u91cf\u5e8f\u5217. dim ( int ) \u2013 \u63d2\u5165\u7684\u7ef4\u5ea6.\u5fc5\u987b\u4ecb\u4e8e 0 \u4e0e\u5f85\u8fde\u63a5\u7684\u5f20\u91cf\u5e8f\u5217\u6570\uff08\u5305\u542b\uff09\u4e4b\u95f4. | | --- | --- | torch.t(input, out=None) \u2192 Tensor \u9884\u671f input \u4e3a\u4e00\u4e2a\u77e9\u9635 (2 \u7ef4\u5f20\u91cf), \u5e76\u8f6c\u7f6e 0, 1 \u7ef4. \u53ef\u4ee5\u88ab\u89c6\u4e3a\u51fd\u6570 <cite>transpose(input, 0, 1)</cite> \u7684\u7b80\u5199\u51fd\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> x = torch.randn(2, 3) >>> x 0.4834 0.6907 1.3417 -0.1300 0.5295 0.2321 [torch.FloatTensor of size 2x3] >>> torch.t(x) 0.4834 -0.1300 0.6907 0.5295 1.3417 0.2321 [torch.FloatTensor of size 3x2] torch.take(input, indices) \u2192 Tensor \u5728\u7ed9\u5b9a\u7684\u7d22\u5f15\u5904\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684 <cite>Tensor</cite> ,\u5176\u5143\u7d20\u4e3a input . \u8f93\u5165\u5f20\u91cf\u88ab\u770b\u4f5c\u662f\u4e00\u7ef4\u5f20\u91cf.\u7ed3\u679c\u4e0e\u7d22\u5f15\u5177\u6709\u76f8\u540c\u7684 shape . | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf indices ( LongTensor ) \u2013 \u8fdb\u5165 <cite>Tensor</cite> \u7684\u7d22\u5f15 | | --- | --- | Example: >>> src = torch.Tensor([[4, 3, 5], ... [6, 7, 8]]) >>> torch.take(src, torch.LongTensor([0, 2, 5])) 4 5 8 [torch.FloatTensor of size 3] torch.transpose(input, dim0, dim1, out=None) \u2192 Tensor \u8fd4\u56de\u8f93\u5165\u77e9\u9635 input \u7684\u8f6c\u7f6e.\u4ea4\u6362\u7ed9\u5b9a\u7ef4\u5ea6 dim0 \u548c dim1 . out \u5f20\u91cf\u4e0e input \u5f20\u91cf\u5171\u4eab\u5185\u5b58,\u6240\u4ee5\u6539\u53d8\u5176\u4e2d\u4e00\u4e2a\u4f1a\u5bfc\u81f4\u53e6\u5916\u4e00\u4e2a\u4e5f\u88ab\u4fee\u6539. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf dim0 ( int ) \u2013 \u8f6c\u7f6e\u7684\u7b2c\u4e00\u4e2a\u7ef4\u5ea6 dim1 ( int ) \u2013 \u8f6c\u7f6e\u7684\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6 | | --- | --- | Example: >>> x = torch.randn(2, 3) >>> x 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x3] >>> torch.transpose(x, 0, 1) 0.5983 1.5981 -0.0341 -0.5265 2.4918 -0.8735 [torch.FloatTensor of size 3x2] torch.unbind(tensor, dim=0) \u79fb\u9664\u4e00\u4e2a\u5f20\u91cf\u7684\u7ef4\u5ea6. \u79fb\u9664\u6307\u5b9a\u7ef4\u540e,\u8fd4\u56de\u4e00\u4e2a\u5143\u7ec4,\u5305\u542b\u4e86\u6cbf\u7740\u6307\u5b9a\u7ef4\u5207\u7247\u540e\u7684\u5404\u4e2a\u5207\u7247 (\u5df2\u7ecf\u6ca1\u6709\u4e86\u79fb\u9664\u7684\u7ef4\u5ea6). | Parameters: | tensor ( Tensor ) \u2013 \u8981\u6267\u884c unbind \u7684\u5f20\u91cf/\u8f93\u5165\u5f20\u91cf. dim ( int ) \u2013 \u8981\u79fb\u9664\u7684\u7ef4\u5ea6. | | --- | --- | torch.unsqueeze(input, dim, out=None) \u8fd4\u56de\u5728\u6307\u5b9a\u4f4d\u7f6e\u63d2\u5165\u7ef4\u5ea6 size \u4e3a 1 \u7684\u65b0\u5f20\u91cf. \u8fd4\u56de\u5f20\u91cf\u4e0e\u8f93\u5165\u5f20\u91cf\u5171\u4eab\u5185\u5b58,\u6240\u4ee5\u6539\u53d8\u5176\u4e2d\u4e00\u4e2a\u7684\u5185\u5bb9\u4f1a\u6539\u53d8\u53e6\u4e00\u4e2a. \u5982\u679c <cite>dim</cite> \u4e3a\u8d1f,\u5219\u5c06\u4f1a\u88ab\u8f6c\u5316 \\(dim + input.dim() + 1\\) . | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf dim ( int ) \u2013 \u63d2\u5165\u7ef4\u5ea6\u7684\u7d22\u5f15 out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example >>> x = torch.Tensor([1, 2, 3, 4]) >>> torch.unsqueeze(x, 0) 1 2 3 4 [torch.FloatTensor of size 1x4] >>> torch.unsqueeze(x, 1) 1 2 3 4 [torch.FloatTensor of size 4x1]","title":"Indexing, Slicing, Joining, Mutating Ops (\u7d22\u5f15, \u5207\u7247, \u8fde\u63a5, \u6362\u4f4d) \u64cd\u4f5c"},{"location":"53/#random-sampling","text":"torch.manual_seed(seed) \u8bbe\u7f6e\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50,\u5e76\u8fd4\u56de\u4e00\u4e2a <cite>torch._C.Generator</cite> \u5bf9\u8c61. Parameters: seed ( int or long ) \u2013 \u79cd\u5b50. torch.initial_seed() \u8fd4\u56de\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\u5b57\u7684\u521d\u59cb\u79cd\u5b50 (python <cite>long</cite>) . torch.get_rng_state() \u4ee5ByteTensor\u7684\u5f62\u5f0f\u8fd4\u56de\u968f\u673a\u6570\u53d1\u751f\u5668\u7684\u72b6\u6001. torch.set_rng_state(new_state) \u8bbe\u7f6e\u968f\u673a\u6570\u53d1\u751f\u5668\u7684\u53c2\u6570. Parameters: new_state ( torch.ByteTensor ) \u2013 \u7406\u60f3\u72b6\u6001 torch.``default_generator = torch.bernoulli(input, out=None) \u2192 Tensor \u4ece\u4f2f\u52aa\u5229\u5206\u5e03\u4e2d\u62bd\u53d6\u4e8c\u8fdb\u5236\u968f\u673a\u6570 (0 \u6216 1). The input \u5f20\u91cf\u5305\u542b\u7528\u4e8e\u62bd\u53d6\u4e8c\u8fdb\u5236\u968f\u673a\u6570\u7684\u6982\u7387. \u56e0\u6b64, input \u4e2d\u7684\u6240\u6709\u503c\u5fc5\u987b\u5728\u8fd9\u4e2a\u8303\u56f4\u5185: \\(0 &lt;= input_i &lt;= 1\\) \u6839\u636e input \u5f20\u91cf\u7b2c <cite>i</cite> \u4e2a\u6982\u7387\u503c, \u8f93\u51fa\u5f20\u91cf\u7684\u7b2c <cite>i</cite> \u4e2a\u5143\u7d20\u5c06\u53d6\u503c\u4e3a1. \u8fd4\u56de\u7684 out \u5f20\u91cf\u7684\u503c\u53ea\u6709 0 \u6216\u8005 1 \u5e76\u4e14\u5927\u5c0f\u4e0e input \u5f20\u91cf\u76f8\u540c. | Parameters: | input ( Tensor ) \u2013 \u4f2f\u52aa\u5229\u5206\u5e03\u7684\u6982\u7387\u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> a = torch.Tensor(3, 3).uniform_(0, 1) # generate a uniform random matrix with range [0, 1] >>> a 0.7544 0.8140 0.9842 0.5282 0.0595 0.6445 0.1925 0.9553 0.9732 [torch.FloatTensor of size 3x3] >>> torch.bernoulli(a) 1 1 1 0 0 1 0 1 1 [torch.FloatTensor of size 3x3] >>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1 >>> torch.bernoulli(a) 1 1 1 1 1 1 1 1 1 [torch.FloatTensor of size 3x3] >>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0 >>> torch.bernoulli(a) 0 0 0 0 0 0 0 0 0 [torch.FloatTensor of size 3x3] torch.multinomial(input, num_samples, replacement=False, out=None) \u2192 LongTensor \u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf, \u5176\u4e2d\u6bcf\u4e00\u884c\u5305\u542b\u5728 input \u5f20\u91cf\u5bf9\u5e94\u884c\u4e2d\u591a\u9879\u5f0f\u5206\u5e03\u53d6\u6837\u7684 num_samples \u7d22\u5f15. Note input \u7684\u6bcf\u884c\u503c\u4e0d\u9700\u8981\u603b\u548c\u4e3a 1 (\u6211\u4eec\u53ea\u4f7f\u7528\u8fd9\u4e9b\u503c\u4f5c\u4e3a\u6743\u91cd), \u4f46\u5fc5\u987b\u662f\u975e\u8d1f\u4e14\u975e\u96f6\u548c\u7684. \u53d6\u6837\u65f6\u4ece\u5de6\u5411\u53f3\u6392\u5217(\u7b2c\u4e00\u4e2a\u6837\u672c\u5728\u7b2c\u4e00\u5217). \u5982\u679c input \u662f\u4e00\u4e2a\u5411\u91cf, \u5219 out \u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a <cite>num_samples</cite> \u7684\u5411\u91cf. \u5982\u679c input \u662f\u4e00\u4e2a <cite>m</cite> \u884c\u7684\u77e9\u9635, \u5219 out \u662f\u4e00\u4e2a <cite>m \u00d7 n</cite> \u7684\u77e9\u9635. \u5982\u679c\u53c2\u6570 <cite>replacement</cite> \u662f True , \u5219\u53ef\u91cd\u590d\u53d6\u6837. \u5426\u5219, \u6837\u672c\u5728\u6bcf\u884c\u4e0d\u80fd\u88ab\u91cd\u590d\u53d6\u6837. \u53c2\u6570 num_samples \u5fc5\u987b\u5c0f\u4e8e input \u957f\u5ea6 (\u5982\u679c\u662f\u4e00\u4e2a\u77e9\u9635, \u5219\u662f input \u7684\u5217\u6570). | Parameters: | input ( Tensor ) \u2013 \u5305\u542b\u6982\u7387\u503c\u7684\u5f20\u91cf num_samples ( int ) \u2013 \u62bd\u53d6\u7684\u6837\u672c\u6570 replacement ( bool , optional ) \u2013 \u662f\u5426\u91cd\u590d\u62bd\u53d6\u6837\u672c out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: >>> weights = torch.Tensor([0, 10, 3, 0]) # create a Tensor of weights >>> torch.multinomial(weights, 4) 1 2 0 0 [torch.LongTensor of size 4] >>> torch.multinomial(weights, 4, replacement=True) 1 2 1 2 [torch.LongTensor of size 4] torch.normal() torch.normal(means, std, out=None) \u8fd4\u56de\u4e00\u4e2a\u968f\u673a\u6570\u5f20\u91cf, \u968f\u673a\u6570\u4ece\u7ed9\u5b9a\u5e73\u5747\u503c\u548c\u6807\u51c6\u5dee\u7684\u79bb\u6563\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6. \u53c2\u6570 means \u662f\u4e00\u4e2a\u5305\u542b\u6bcf\u4e2a\u8f93\u51fa\u5143\u7d20\u7684\u6b63\u6001\u5206\u5e03\u5747\u503c\u7684\u5f20\u91cf. \u53c2\u6570 std \u662f\u4e00\u4e2a\u5305\u542b\u6bcf\u4e2a\u8f93\u51fa\u5143\u7d20\u7684\u6b63\u6001\u5206\u5e03\u6807\u51c6\u5dee\u7684\u5f20\u91cf. \u5176\u4e2d means \u548c std \u7684\u5f62\u72b6\u4e0d\u9700\u8981\u5339\u914d, \u4f46\u662f\u6bcf\u4e2a\u5f20\u91cf\u4e2d\u7684\u5143\u7d20\u603b\u6570\u9700\u8981\u76f8\u540c. Note \u5f53\u5f62\u72b6\u4e0d\u5339\u914d\u65f6, means \u7684\u5f62\u72b6\u5c06\u4f5c\u4e3a\u8fd4\u56de\u8f93\u51fa\u5f20\u91cf\u7684\u5f62\u72b6. | Parameters: | means ( Tensor ) \u2013 \u5747\u503c std ( Tensor ) \u2013 \u6807\u51c6\u5dee out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1)) 1.5104 1.6955 2.4895 4.9185 4.9895 6.9155 7.3683 8.1836 8.7164 9.8916 [torch.FloatTensor of size 10] torch.normal(mean=0.0, std, out=None) \u529f\u80fd\u4e0e\u4e0a\u9762\u51fd\u6570\u7c7b\u4f3c, \u4f46\u6240\u6709\u88ab\u62bd\u53d6\u7684\u5143\u7d20\u5171\u4eab\u5747\u503c. | Parameters: | means ( float , optional ) \u2013 \u6240\u6709\u5206\u5e03\u7684\u5747\u503c std ( Tensor ) \u2013 \u6bcf\u4e2a\u5143\u7d20\u6807\u51c6\u5dee\u7684\u5f20\u91cf out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> torch.normal(mean=0.5, std=torch.arange(1, 6)) 0.5723 0.0871 -0.3783 -2.5689 10.7893 [torch.FloatTensor of size 5] torch.normal(means, std=1.0, out=None) \u529f\u80fd\u4e0e\u4e0a\u9762\u51fd\u6570\u7c7b\u4f3c, \u4f46\u6240\u6709\u88ab\u62bd\u53d6\u7684\u5143\u7d20\u5171\u4eab\u6807\u51c6\u5dee. | Parameters: | means ( Tensor ) \u2013 \u6bcf\u4e2a\u5143\u7d20\u5747\u503c\u7684\u5f20\u91cf std ( float , optional ) \u2013 \u6240\u6709\u5206\u5e03\u7684\u6807\u51c6\u5dee out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> torch.normal(means=torch.arange(1, 6)) 1.1681 2.8884 3.7718 2.5616 4.2500 [torch.FloatTensor of size 5] torch.rand(*sizes, out=None) \u2192 Tensor \u5728\u533a\u95f4 \\([0, 1)\\) \u4e2d, \u8fd4\u56de\u4e00\u4e2a\u586b\u5145\u4e86\u5747\u5300\u5206\u5e03\u7684\u968f\u673a\u6570\u7684\u5f20\u91cf. \u8fd9\u4e2a\u5f20\u91cf\u7684\u5f62\u72b6\u7531\u53ef\u53d8\u53c2\u6570 sizes \u6765\u5b9a\u4e49. | Parameters: | sizes ( int... ) \u2013 \u5b9a\u4e49\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u7684\u6574\u6570\u96c6. out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> torch.rand(4) 0.9193 0.3347 0.3232 0.7715 [torch.FloatTensor of size 4] >>> torch.rand(2, 3) 0.5010 0.5140 0.0719 0.1435 0.5636 0.0538 [torch.FloatTensor of size 2x3] torch.randn(*sizes, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u4ece\u6b63\u6001\u5206\u5e03\u4e2d\u586b\u5145\u968f\u673a\u6570\u7684\u5f20\u91cf, \u5176\u5747\u503c\u4e3a 0 , \u65b9\u5dee\u4e3a 1 . \u8fd9\u4e2a\u5f20\u91cf\u7684\u5f62\u72b6\u88ab\u53ef\u53d8\u53c2\u6570 sizes \u5b9a\u4e49. | Parameters: | sizes ( int... ) \u2013 \u5b9a\u4e49\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u7684\u6574\u6570\u96c6. out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> torch.randn(4) -0.1145 0.0094 -1.1717 0.9846 [torch.FloatTensor of size 4] >>> torch.randn(2, 3) 1.4339 0.3351 -1.0999 1.5458 -0.9643 -0.3558 [torch.FloatTensor of size 2x3] torch.randperm(n, out=None) \u2192 LongTensor \u8fd4\u56de\u4e00\u4e2a\u4ece 0 to n - 1 \u7684\u6574\u6570\u7684\u968f\u673a\u6392\u5217. Parameters: n ( int ) \u2013 \u4e0a\u9650 (\u552f\u4e00\u7684) Example: >>> torch.randperm(4) 2 1 3 0 [torch.LongTensor of size 4]","title":"Random sampling (\u968f\u673a\u91c7\u6837)"},{"location":"53/#in-place-random-sampling","text":"\u5728Tensors\u6a21\u5757\u4e0a\u8fd8\u5b9a\u4e49\u4e86\u8bb8\u591a in-place \u968f\u673a\u91c7\u6837\u51fd\u6570,\u53ef\u4ee5\u70b9\u51fb\u53c2\u8003\u5b83\u4eec\u7684\u6587\u6863: torch.Tensor.bernoulli_() - \u662f torch.bernoulli() \u7684 in-place \u7248\u672c torch.Tensor.cauchy_() - \u4ece\u67ef\u897f\u5206\u5e03\u4e2d\u62bd\u53d6\u6570\u5b57 torch.Tensor.exponential_() - \u4ece\u6307\u6570\u5206\u5e03\u4e2d\u62bd\u53d6\u6570\u5b57 torch.Tensor.geometric_() - \u4ece\u51e0\u4f55\u5206\u5e03\u4e2d\u62bd\u53d6\u5143\u7d20 torch.Tensor.log_normal_() - \u5bf9\u6570\u6b63\u6001\u5206\u5e03\u4e2d\u7684\u6837\u672c torch.Tensor.normal_() - \u662f torch.normal() \u7684 in-place \u7248\u672c torch.Tensor.random_() - \u79bb\u6563\u5747\u5300\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u6570\u5b57 torch.Tensor.uniform_() - \u6b63\u6001\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u6570\u5b57","title":"In-place random sampling (\u76f4\u63a5\u968f\u673a\u91c7\u6837)"},{"location":"53/#serialization","text":"torch.save(obj, f, pickle_module=<module 'cPickle' from '/usr/lib64/python2.7/lib-dynload/cPickle.so'>, pickle_protocol=2) \u5c06\u4e00\u4e2a\u5bf9\u8c61\u4fdd\u5b58\u5230\u4e00\u4e2a\u78c1\u76d8\u6587\u4ef6\u4e2d. \u53e6\u89c1: \u4fdd\u5b58\u6a21\u578b\u7684\u63a8\u8350\u65b9\u6cd5 \u53c2\u6570: obj: \u8981\u4fdd\u5b58\u7684\u5bf9\u8c61 f: \u7c7b\u6587\u4ef6\u5bf9\u8c61 (\u5fc5\u987b\u5b9e\u73b0\u8fd4\u56de\u6587\u4ef6\u63cf\u8ff0\u7b26\u7684 fileno \u65b9\u6cd5) \u6216\u5305\u542b\u6587\u4ef6\u540d\u7684\u5b57\u7b26\u4e32 pickle_module: \u7528\u4e8e pickling \u5143\u6570\u636e\u548c\u5bf9\u8c61\u7684\u6a21\u5757 pickle_protocol: \u53ef\u4ee5\u6307\u5b9a\u6765\u8986\u76d6\u9ed8\u8ba4\u534f\u8bae torch.load(f, map_location=None, pickle_module=<module 'cPickle' from '/usr/lib64/python2.7/lib-dynload/cPickle.so'>) \u4ece\u78c1\u76d8\u6587\u4ef6\u4e2d\u52a0\u8f7d\u4e00\u4e2a\u7528 torch.save() \u4fdd\u5b58\u7684\u5bf9\u8c61. Func: <cite>torch.load</cite> \u4f7f\u7528 Python \u7684\u89e3\u5c01 (unpickling) \u8bbe\u65bd, \u4f46\u7279\u6b8a\u5bf9\u5f85\u5f20\u91cf\u4e0b\u7684\u5b58\u50a8 (storages). \u5b83\u4eec\u9996\u5148\u5728 CPU \u4e0a\u53cd\u5e8f\u5217\u5316, \u7136\u540e\u79fb\u52a8\u5230\u6240\u4fdd\u5b58\u7684\u8bbe\u5907\u4e0a. \u5982\u679c\u8fd9\u4e2a\u8fc7\u7a0b\u5931\u8d25\u4e86 (\u4f8b\u5982, \u56e0\u4e3a\u8fd0\u884c\u65f6\u7684\u7cfb\u7edf\u6ca1\u6709\u786e\u5b9a\u7684\u8bbe\u5907), \u5c06\u4f1a\u629b\u51fa\u5f02\u5e38. \u7136\u800c, \u4f7f\u7528 map_location \u53c2\u6570, \u5b58\u50a8\u53ef\u4ee5\u88ab\u52a8\u6001\u5730\u91cd\u65b0\u6620\u5c04\u5230\u53e6\u4e00\u7ec4\u8bbe\u5907\u4e0a. \u5982\u679c map_location \u662f\u53ef\u8c03\u7528\u5bf9\u8c61, \u5219\u5bf9\u4e8e\u6bcf\u4e2a\u5e8f\u5217\u5316\u5b58\u50a8, \u5b83\u90fd\u5c06\u4ee5\u4e24\u4e2a\u53c2\u6570\u8c03\u7528\u4e00\u6b21: storage \u548c location. \u53c2\u6570 storage \u662f\u9a7b\u7559\u5728 CPU \u4e0a\u7684\u5b58\u50a8\u7684\u521d\u59cb\u53cd\u5e8f\u5217\u5316. \u6bcf\u4e2a\u5e8f\u5217\u5316\u540e\u7684\u5b58\u50a8\u90fd\u6709\u4e00\u4e2a\u4e0e\u4e4b\u5173\u8054\u7684\u4f4d\u7f6e\u6807\u7b7e, \u5b83\u6807\u8bc6\u4e86\u4fdd\u5b58\u5b83\u7684\u8bbe\u5907, \u800c\u6b64\u6807\u7b7e\u662f\u4f20\u9012\u7ed9 map_location \u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570. \u5bf9\u4e8e CPU \u5f20\u91cf, \u5185\u5efa\u7684\u4f4d\u7f6e\u6807\u7b7e\u662f \u2018cpu\u2019, \u5bf9\u4e8e CUDA \u5f20\u91cf, \u5185\u5efa\u7684\u4f4d\u7f6e\u6807\u7b7e\u662f \u2018cuda:device_id\u2019 (\u4f8b\u5982 \u2018cuda:2\u2019). map_location \u8981\u4e48\u8fd4\u56de None , \u8981\u4e48\u8fd4\u56de\u4e00\u4e2a\u5b58\u50a8. \u5982\u679c map_location \u8fd4\u56de\u5b58\u50a8, \u5b83\u5c06\u7528\u4f5c\u5df2\u79fb\u52a8\u5230\u6b63\u786e\u8bbe\u5907\u4e0a\u7684, \u6700\u7ec8\u53cd\u5e8f\u5217\u5316\u7684\u5bf9\u8c61. \u5426\u5219, \u5982\u679c\u6ca1\u6709\u6307\u660e map_location, \u5373\u8fd4\u56de None, <cite>torch.load</cite> \u4f1a\u56de\u843d\u5230\u9ed8\u8ba4\u7684\u884c\u4e3a. \u5982\u679c map_location \u662f\u4e00\u4e2a\u5b57\u5178, \u5b83\u7528\u4e8e\u5c06\u51fa\u73b0\u5728\u6587\u4ef6 (\u952e) \u4e2d\u7684\u4f4d\u7f6e\u6807\u7b7e, \u91cd\u65b0\u6620\u5c04\u5230\u53e6\u4e00\u4e2a\u4f4d\u7f6e\u6807\u7b7e, \u5b83\u51fa\u73b0\u5728\u503c\u4e2d\u5e76\u6307\u660e\u5728\u54ea\u91cc\u5b58\u653e\u5b58\u50a8. \u7528\u6237\u6269\u5c55\u53ef\u4ee5\u4f7f\u7528 register_package \u6765\u6ce8\u518c\u4ed6\u4eec\u81ea\u5df1\u7684\u4f4d\u7f6e\u6807\u7b7e, \u4ee5\u53ca\u6807\u8bb0\u548c\u53cd\u5e8f\u5217\u5316\u65b9\u6cd5. \u53c2\u6570: f: \u4e00\u4e2a\u7c7b\u6587\u4ef6\u5bf9\u8c61 (\u5fc5\u987b\u5b9e\u73b0\u8fd4\u56de\u6587\u4ef6\u63cf\u8ff0\u7b26\u7684 fileno, \u4ee5\u53ca seek \u65b9\u6cd5), \u6216\u8005\u5305\u542b\u6587\u4ef6\u540d\u7684\u5b57\u7b26\u4e32. map_location: \u4e00\u4e2a\u51fd\u6570\u6216\u8005\u4e00\u4e2a\u6307\u660e\u5982\u4f55\u91cd\u65b0\u6620\u5c04\u5b58\u50a8\u4f4d\u7f6e\u7684\u5b57\u5178 pickle_module: \u7528\u4e8e\u89e3\u5c01 (unpickling) \u5143\u6570\u636e\u548c\u5bf9\u8c61\u7684\u6a21\u5757 (\u5fc5\u987b\u5339\u914d\u7528\u4e8e\u5e8f\u5217\u5316\u6587\u4ef6\u7684 pickle_module) \u793a\u4f8b: >>> torch.load('tensors.pt') # Load all tensors onto the CPU >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage) # Load all tensors onto GPU 1 >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1)) # Map tensors from GPU 1 to GPU 0 >>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})","title":"Serialization (\u5e8f\u5217\u5316)"},{"location":"53/#parallelism","text":"torch.get_num_threads() \u2192 int \u83b7\u5f97 OpenMP \u5e76\u884c\u5316\u64cd\u4f5c\u7684\u7ebf\u7a0b\u6570\u76ee torch.set_num_threads(int) \u8bbe\u7f6e OpenMP \u5e76\u884c\u5316\u64cd\u4f5c\u7684\u7ebf\u7a0b\u6570\u76ee","title":"Parallelism (\u5e76\u884c\u5316)"},{"location":"53/#math-operations","text":"","title":"Math operations (\u6570\u5b66\u64cd\u4f5c)"},{"location":"53/#pointwise-ops","text":"torch.abs(input, out=None) \u2192 Tensor \u8ba1\u7b97\u7ed9\u5b9a input \u5f20\u91cf\u7684\u5143\u7d20\u7684\u7edd\u5bf9\u503c. Example: >>> torch.abs(torch.FloatTensor([-1, -2, 3])) FloatTensor([1, 2, 3]) torch.acos(input, out=None) \u2192 Tensor \u7528 input \u5143\u7d20\u7684\u53cd\u4f59\u5f26\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 the input <cite>Tensor</cite> out ( Tensor , optional ) \u2013 The result <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.acos(a) 2.2608 1.2956 1.1075 nan [torch.FloatTensor of size 4] torch.add() torch.add(input, value, out=None) \u5c06\u6807\u91cf\u503c value \u6dfb\u52a0\u5230\u8f93\u5165\u5f20\u91cf attr:<cite>input</cite> \u7684\u6bcf\u4e2a\u5143\u7d20\u5e76\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u7ed3\u679c\u5f20\u91cf. \\(out = tensor + value\\) \u5982\u679c\u8f93\u5165\u5f20\u91cf input \u662f FloatTensor \u6216\u8005 DoubleTensor \u7c7b\u578b, \u5219 value \u5fc5\u987b\u4e3a\u5b9e\u6570, \u5426\u5219\u4e3a\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> value ( Number ) \u2013 \u8981\u6dfb\u52a0\u5230 input \u6bcf\u4e2a\u5143\u7d20\u7684\u6570 out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 0.4050 -1.2227 1.8688 -0.4185 [torch.FloatTensor of size 4] >>> torch.add(a, 20) 20.4050 18.7773 21.8688 19.5815 [torch.FloatTensor of size 4] torch.add(input, value=1, other, out=None) \u5f20\u91cf other \u7684\u6bcf\u4e2a\u5143\u7d20\u4e58\u4ee5\u6807\u91cf\u503c value \u5e76\u52a0\u5230\u5f20\u91cf input \u4e0a, \u8fd4\u56de\u751f\u6210\u7684\u5f20\u91cf out . \u5f20\u91cf input \u7684\u5f62\u72b6\u4e0e\u5f20\u91cf other \u7684\u5f62\u72b6\u5fc5\u987b broadcastable . \\(out = input + (other * value)\\) \u5982\u679c\u5f20\u91cf other \u662f FloatTensor \u6216\u8005 DoubleTensor \u7c7b\u578b, \u5219 value \u5fc5\u987b\u4e3a\u5b9e\u6570, \u5426\u5219\u4e3a\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u7b2c\u4e00\u4e2a\u8f93\u5165 <cite>Tensor</cite> value ( Number ) \u2013 \u5f20\u91cf other \u7684\u6807\u91cf\u4e58\u6570 other ( Tensor ) \u2013 \u7b2c\u4e8c\u4e2a\u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> import torch >>> a = torch.randn(4) >>> a -0.9310 2.0330 0.0852 -0.2941 [torch.FloatTensor of size 4] >>> b = torch.randn(2, 2) >>> b 1.0663 0.2544 -0.1513 0.0749 [torch.FloatTensor of size 2x2] >>> torch.add(a, 10, b) 9.7322 4.5770 -1.4279 0.4552 [torch.FloatTensor of size 4] torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) \u2192 Tensor \u5c06\u5f20\u91cf tensor1 \u9010\u5143\u7d20\u9664\u4ee5\u5f20\u91cf tensor2 , \u7136\u540e\u4e58\u4ee5\u6807\u91cf\u503c value \u5e76\u52a0\u5230\u5f20\u91cf tensor \u4e0a. \u5f20\u91cf tensor , \u5f20\u91cf tensor1 , \u5f20\u91cf tensor2 \u7684\u5f62\u72b6\u5fc5\u987b broadcastable . \u5bf9\u4e8e\u7c7b\u578b\u4e3a <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7684\u5f20\u91cf\u8f93\u5165, value \u5fc5\u987b\u4e3a\u5b9e\u6570, \u5426\u5219\u4e3a\u6574\u6570. | Parameters: | tensor ( Tensor ) \u2013 \u5f20\u91cf, \u5bf9 <cite>tensor1 ./ tensor2</cite> \u8fdb\u884c\u76f8\u52a0 value ( Number__, optional ) \u2013 \u6807\u91cf, \u5bf9 <cite>tensor1 ./ tensor2</cite> \u8fdb\u884c\u76f8\u4e58 tensor1 ( Tensor ) \u2013 \u5206\u5b50\u5f20\u91cf, \u5373\u4f5c\u4e3a\u88ab\u9664\u6570 tensor2 ( Tensor ) \u2013 \u5206\u6bcd\u5f20\u91cf, \u5373\u4f5c\u4e3a\u9664\u6570 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> t = torch.randn(2, 3) >>> t1 = torch.randn(1, 6) >>> t2 = torch.randn(6, 1) >>> torch.addcdiv(t, 0.1, t1, t2) 0.0122 -0.0188 -0.2354 0.7396 -1.5721 1.2878 [torch.FloatTensor of size 2x3] torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) \u2192 Tensor \u5c06\u5f20\u91cf tensor1 \u9010\u5143\u7d20\u4e0e\u5f20\u91cf tensor2 \u76f8\u4e58, \u7136\u540e\u4e58\u4ee5\u6807\u91cf\u503c value \u5e76\u52a0\u5230\u5f20\u91cf tensor \u4e0a. \u5f20\u91cf tensor , \u5f20\u91cf tensor1 , \u5f20\u91cf tensor2 \u7684\u5f62\u72b6\u5fc5\u987b broadcastable . \u5bf9\u4e8e\u7c7b\u578b\u4e3a <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7684\u5f20\u91cf\u8f93\u5165, value \u5fc5\u987b\u4e3a\u5b9e\u6570, \u5426\u5219\u4e3a\u6574\u6570. :param tensor: \u5f20\u91cf, \u5bf9 <cite>tensor1 . tensor2</cite> \u8fdb\u884c\u76f8\u52a0 :type tensor: Tensor :param value: \u6807\u91cf, \u5bf9 <cite>tensor1 . tensor2</cite> \u8fdb\u884c\u76f8\u4e58 :type value: Number, optional :param tensor1: \u5f20\u91cf, \u4f5c\u4e3a\u4e58\u5b501 :type tensor1: Tensor :param tensor2: \u5f20\u91cf, \u4f5c\u4e3a\u4e58\u5b502 :type tensor2: Tensor :param out: \u8f93\u51fa\u5f20\u91cf :type out: Tensor, optional Example: >>> t = torch.randn(2, 3) >>> t1 = torch.randn(1, 6) >>> t2 = torch.randn(6, 1) >>> torch.addcmul(t, 0.1, t1, t2) 0.0122 -0.0188 -0.2354 0.7396 -1.5721 1.2878 [torch.FloatTensor of size 2x3] torch.asin(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684 <cite>Tensor</cite> , \u5176\u5143\u7d20\u4e3a\u5f20\u91cf input \u7684\u6bcf\u4e2a\u5143\u7d20\u7684\u53cd\u6b63\u5f26. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.asin(a) -0.6900 0.2752 0.4633 nan [torch.FloatTensor of size 4] torch.atan(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684 <cite>Tensor</cite> , \u5176\u5143\u7d20\u4e3a\u5f20\u91cf input \u7684\u6bcf\u4e2a\u5143\u7d20\u7684\u53cd\u6b63\u5207. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.atan(a) -0.5669 0.2653 0.4203 0.9196 [torch.FloatTensor of size 4] torch.atan2(input1, input2, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u8f93\u5165\u5f20\u91cf input1 \u548c\u8f93\u5165\u5f20\u91cf input2 \u5143\u7d20\u7684\u53cd\u6b63\u5207. \u8f93\u5165\u5f20\u91cf input1 \u7684\u5f62\u72b6\u548c\u8f93\u5165\u5f20\u91cf input2 \u7684\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . | Parameters: | input1 ( Tensor ) \u2013 \u7b2c\u4e00\u4e2a\u8f93\u5165 <cite>Tensor</cite> input2 ( Tensor ) \u2013 \u7b2c\u4e8c\u4e2a\u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.atan2(a, torch.randn(4)) -2.4167 2.9755 0.9363 1.6613 [torch.FloatTensor of size 4] torch.ceil(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u7684\u5143\u7d20\u5411\u4e0a\u53d6\u6574(\u53d6\u4e0d\u5c0f\u4e8e\u6bcf\u4e2a\u5143\u7d20\u7684\u6700\u5c0f\u6574\u6570). | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.ceil(a) 2 1 -0 -0 [torch.FloatTensor of size 4] torch.clamp(input, min, max, out=None) \u2192 Tensor \u5c06\u8f93\u5165\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u9650\u5236\u5728\u533a\u95f4 <cite>[min, max]</cite> \u4e2d\u5e76\u8fd4\u56de\u4e00\u4e2a\u7ed3\u679c\u5f20\u91cf. | min, if x_i < min y_i = | x_i, if min <= x_i <= max | max, if x_i > max \u5982\u679c\u8f93\u5165\u5f20\u91cf input \u7684\u7c7b\u578b <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite>, \u90a3\u4e48\u53c2\u6570 min \u548c max \u5fc5\u987b\u4e3a\u5b9e\u6570, \u5426\u5219\u4e3a\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> min ( Number ) \u2013 \u9650\u5236\u8303\u56f4\u4e0b\u9650 max ( Number ) \u2013 \u9650\u5236\u8303\u56f4\u4e0a\u9650 out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.clamp(a, min=-0.5, max=0.5) 0.5000 0.3912 -0.5000 -0.5000 [torch.FloatTensor of size 4] torch.clamp(input, *, min, out=None) \u2192 Tensor \u5f20\u91cf input \u7684\u6240\u6709\u5143\u7d20\u503c\u5927\u4e8e\u6216\u8005\u7b49\u4e8e min . \u5982\u679c\u5f20\u91cf input \u7684\u7c7b\u578b\u662f <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite>, \u5219 value \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> value ( Number ) \u2013 \u8f93\u51fa\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u6700\u5c0f\u503c out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.clamp(a, min=0.5) 1.3869 0.5000 0.5000 0.5000 [torch.FloatTensor of size 4] torch.clamp(input, *, max, out=None) \u2192 Tensor \u5f20\u91cf input \u7684\u6240\u6709\u5143\u7d20\u503c\u5c0f\u4e8e\u6216\u8005\u7b49\u4e8e max . \u5982\u679c\u5f20\u91cf input \u7684\u7c7b\u578b\u662f <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite>, \u5219 value \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> value ( Number ) \u2013 \u8f93\u51fa\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u6700\u5927\u503c out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.clamp(a, max=0.5) 0.5000 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] torch.cos(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u6bcf\u4e2a\u5143\u7d20\u7684\u4f59\u5f26. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.cos(a) 0.8041 0.9633 0.9018 0.2557 [torch.FloatTensor of size 4] torch.cosh(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u6bcf\u4e2a\u5143\u7d20\u7684\u53cc\u66f2\u4f59\u5f26. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.cosh(a) 1.2095 1.0372 1.1015 1.9917 [torch.FloatTensor of size 4] torch.div() torch.div(input, value, out=None) \u5c06\u5f20\u91cf input \u7684\u5143\u7d20\u9010\u4e00\u9664\u4ee5\u6807\u91cf\u503c value , \u5176\u7ed3\u679c\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\u8fd4\u56de. \\(out = tensor / value\\) \u5982\u679c\u5f20\u91cf input \u7684\u7c7b\u578b\u662f <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite>, \u5219\u6807\u91cf\u503c value \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> value ( Number ) \u2013 \u9664\u6570, \u88ab\u5f20\u91cf input \u7684\u5143\u7d20\u9664 out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(5) >>> a -0.6147 -1.1237 -0.1604 -0.6853 0.1063 [torch.FloatTensor of size 5] >>> torch.div(a, 0.5) -1.2294 -2.2474 -0.3208 -1.3706 0.2126 [torch.FloatTensor of size 5] torch.div(input, other, out=None) \u5f20\u91cf input \u7684\u5143\u7d20\u4e0e\u5f20\u91cf other \u7684\u5143\u7d20\u9010\u4e00\u76f8\u9664. \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u7ed3\u679c\u5f20\u91cf out . \u5f20\u91cf input \u4e0e\u5f20\u91cf other \u7684\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . \\(out_i = input_i / other_i\\) | Parameters: | input ( Tensor ) \u2013 \u5206\u5b50 <cite>Tensor</cite> (\u88ab\u9664\u6570) other ( Tensor ) \u2013 \u5206\u6bcd <cite>Tensor</cite> (\u9664\u6570) out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4,4) >>> a -0.1810 0.4017 0.2863 -0.1013 0.6183 2.0696 0.9012 -1.5933 0.5679 0.4743 -0.0117 -0.1266 -0.1213 0.9629 0.2682 1.5968 [torch.FloatTensor of size 4x4] >>> b = torch.randn(8, 2) >>> b 0.8774 0.7650 0.8866 1.4805 -0.6490 1.1172 1.4259 -0.8146 1.4633 -0.1228 0.4643 -0.6029 0.3492 1.5270 1.6103 -0.6291 [torch.FloatTensor of size 8x2] >>> torch.div(a, b) -0.2062 0.5251 0.3229 -0.0684 -0.9528 1.8525 0.6320 1.9559 0.3881 -3.8625 -0.0253 0.2099 -0.3473 0.6306 0.1666 -2.5381 [torch.FloatTensor of size 4x4] torch.erf(tensor, out=None) \u2192 Tensor \u8ba1\u7b97\u6bcf\u4e2a\u5143\u7d20\u7684\u8bef\u5dee\u51fd\u6570. Example: >>> torch.erf(torch.Tensor([0, -1., 10.])) torch.FloatTensor([0., -0.8427, 1.]) torch.erfinv(tensor, out=None) \u2192 Tensor \u8ba1\u7b97\u6bcf\u4e2a\u5143\u7d20\u7684\u53cd\u5411\u8bef\u5dee\u51fd\u6570. Example: >>> torch.erfinv(torch.Tensor([0, 0.5., -1.])) torch.FloatTensor([0., 0.4769, -inf]) torch.exp(tensor, out=None) \u2192 Tensor \u8ba1\u7b97\u6bcf\u4e2a\u5143\u7d20\u7684\u6307\u6570. Example: >>> torch.exp(torch.Tensor([0, math.log(2)])) torch.FloatTensor([1, 2]) torch.floor(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u7684\u5143\u7d20\u5411\u4e0b\u53d6\u6574(\u53d6\u4e0d\u5927\u4e8e\u6bcf\u4e2a\u5143\u7d20\u7684\u6700\u5927\u6574\u6570). | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.floor(a) 1 0 -1 -1 [torch.FloatTensor of size 4] torch.fmod(input, divisor, out=None) \u2192 Tensor \u8ba1\u7b97\u9664\u6cd5\u4f59\u6570. \u88ab\u9664\u6570\u548c\u9664\u6570\u53ef\u80fd\u540c\u65f6\u542b\u6709\u6574\u6570\u548c\u6d6e\u70b9\u6570. \u8fd9\u65f6\u4f59\u6570\u7684\u6b63\u8d1f\u4e0e\u88ab\u9664\u6570 <cite>tensor</cite> \u76f8\u540c. \u5f53\u9664\u6570 divisor \u662f\u4e00\u4e2a\u5f20\u91cf\u65f6r, \u5f20\u91cf input \u548c\u5f20\u91cf divisor \u7684\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . | Parameters: | input ( Tensor ) \u2013 \u88ab\u9664\u6570 divisor ( Tensor or float ) \u2013 \u9664\u6570. \u53ef\u80fd\u662f\u4e00\u4e2a\u6570\u6216\u8005\u662f\u4e00\u4e2a\u4e0e\u88ab\u9664\u6570\u76f8\u540c\u5f62\u72b6\u7684\u5f20\u91cf. out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> torch.fmod(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2) torch.FloatTensor([-1, -0, -1, 1, 0, 1]) >>> torch.fmod(torch.Tensor([1, 2, 3, 4, 5]), 1.5) torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5]) See also torch.remainder() , \u5176\u8ba1\u7b97\u7b49\u4ef7\u4e8e Python\u2019s <cite>%</cite> \u64cd\u4f5c\u7b26\u7684\u5143\u7d20\u4f59\u6570 torch.frac(tensor, out=None) \u2192 Tensor \u8ba1\u7b97\u5f20\u91cf <cite>tensor</cite> \u6bcf\u4e2a\u5143\u7d20\u7684\u5206\u6570\u90e8\u5206. Example: >>> torch.frac(torch.Tensor([1, 2.5, -3.2]) torch.FloatTensor([0, 0.5, -0.2]) torch.lerp(start, end, weight, out=None) \u57fa\u4e8e\u6807\u91cf\u503c weight : , \u5728\u5f20\u91cf start \u4e0e\u5f20\u91cf end \u4e4b\u95f4\u505a\u7ebf\u6027\u63d2\u503c \u5e76\u8fd4\u56de\u7ed3\u679c\u5f20\u91cf out . \\(out_i = start_i + weight * (end_i - start_i)\\) \u5f20\u91cf start \u548c\u5f20\u91cf end \u7684\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . | Parameters: | start ( Tensor ) \u2013 \u8d77\u59cb\u70b9 <cite>Tensor</cite> end ( Tensor ) \u2013 \u7ec8\u70b9 <cite>Tensor</cite> weight ( float ) \u2013 \u63d2\u503c\u516c\u5f0f\u7684\u6743\u91cd out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> start = torch.arange(1, 5) >>> end = torch.Tensor(4).fill_(10) >>> start 1 2 3 4 [torch.FloatTensor of size 4] >>> end 10 10 10 10 [torch.FloatTensor of size 4] >>> torch.lerp(start, end, 0.5) 5.5000 6.0000 6.5000 7.0000 [torch.FloatTensor of size 4] torch.log(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u81ea\u7136\u5bf9\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(5) >>> a -0.4183 0.3722 -0.3091 0.4149 0.5857 [torch.FloatTensor of size 5] >>> torch.log(a) nan -0.9883 nan -0.8797 -0.5349 [torch.FloatTensor of size 5] torch.log1p(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f(1 + input ) \u7684\u81ea\u7136\u5bf9\u6570. \\(y_i = log(x_i + 1)\\) Note \u5bf9\u4e8e\u8f83\u5c0f\u7684\u5f20\u91cf input \u7684\u503c, \u6b64\u51fd\u6570\u6bd4 torch.log() \u66f4\u7cbe\u786e. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(5) >>> a -0.4183 0.3722 -0.3091 0.4149 0.5857 [torch.FloatTensor of size 5] >>> torch.log1p(a) -0.5418 0.3164 -0.3697 0.3471 0.4611 [torch.FloatTensor of size 5] torch.mul() torch.mul(input, value, out=None) \u5c06\u8f93\u5165\u5f20\u91cf input \u7684\u6bcf\u4e2a\u5143\u7d20\u4e0e\u6807\u91cf\u503c value \u76f8\u4e58\u5e76\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u7ed3\u679c\u5f20\u91cf. \\(out = tensor * value\\) \u5982\u679c\u5f20\u91cf input \u7684\u7c7b\u578b\u4e3a <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, \u5219 value \u5e94\u8be5\u662f\u5b9e\u6570, \u5426\u5219\u4e3a\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> value ( Number ) \u2013 \u4e0e\u5f20\u91cf input \u6bcf\u4e2a\u5143\u7d20\u76f8\u4e58\u7684\u6570 out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(3) >>> a -0.9374 -0.5254 -0.6069 [torch.FloatTensor of size 3] >>> torch.mul(a, 100) -93.7411 -52.5374 -60.6908 [torch.FloatTensor of size 3] torch.mul(input, other, out=None) \u5f20\u91cf input \u7684\u5143\u7d20\u4e0e\u5f20\u91cf other \u7684\u5143\u7d20\u9010\u4e00\u76f8\u4e58. \u5176\u7ed3\u679c\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\u8fd4\u56de. \u5f20\u91cf input \u548c\u5f20\u91cf other \u7684\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . \\(out_i = input_i * other_i\\) | Parameters: | input ( Tensor ) \u2013 \u7b2c\u4e00\u4e2a\u4e58\u6570 <cite>Tensor</cite> other ( Tensor ) \u2013 \u7b2c\u4e8c\u4e2a\u4e58\u6570 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4,4) >>> a -0.7280 0.0598 -1.4327 -0.5825 -0.1427 -0.0690 0.0821 -0.3270 -0.9241 0.5110 0.4070 -1.1188 -0.8308 0.7426 -0.6240 -1.1582 [torch.FloatTensor of size 4x4] >>> b = torch.randn(2, 8) >>> b 0.0430 -1.0775 0.6015 1.1647 -0.6549 0.0308 -0.1670 1.0742 -1.2593 0.0292 -0.0849 0.4530 1.2404 -0.4659 -0.1840 0.5974 [torch.FloatTensor of size 2x8] >>> torch.mul(a, b) -0.0313 -0.0645 -0.8618 -0.6784 0.0934 -0.0021 -0.0137 -0.3513 1.1638 0.0149 -0.0346 -0.5068 -1.0304 -0.3460 0.1148 -0.6919 [torch.FloatTensor of size 4x4] torch.neg(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u7684\u5143\u7d20\u7684\u8d1f\u503c. \\(out = -1 * input\\) | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(5) >>> a -0.4430 1.1690 -0.8836 -0.4565 0.2968 [torch.FloatTensor of size 5] >>> torch.neg(a) 0.4430 -1.1690 0.8836 0.4565 -0.2968 [torch.FloatTensor of size 5] torch.pow() torch.pow(input, exponent, out=None) \u5bf9\u8f93\u5165\u5f20\u91cf input \u6309\u5143\u7d20\u6c42 exponent \u6b21\u5e42\u503c\u5e76\u8fd4\u56de\u7ed3\u679c\u5f20\u91cf(\u5176\u503c\u4f5c\u4e3a\u7ed3\u679c\u5f20\u91cf\u7684\u5143\u7d20). \u5e42\u503c exponent \u53ef\u4ee5\u662f\u4e00\u4e2a\u5355\u4e00\u7684\u6d6e\u70b9\u6570 float \u6216\u8005\u662f\u4e00\u4e2a\u4e0e\u5f20\u91cf input \u6709\u76f8\u540c\u5143\u7d20\u6570\u7684\u5f20\u91cf Tensor . \u5f53\u6307\u6570 exponent \u662f\u4e00\u4e2a\u6807\u91cf\u65f6, \u6267\u884c\u64cd\u4f5c: \\(out_i = x_i ^ {exponent}\\) \u5f53\u6307\u6570 exponent \u662f\u4e00\u4e2a\u5f20\u91cf, \u6267\u884c\u64cd\u4f5c: \\(out_i = x_i ^ {exponent_i}\\) \u5f53\u5e42\u503c exponent \u662f\u4e00\u4e2a\u5f20\u91cf, \u5f20\u91cf input \u548c\u5f20\u91cf exponent \u7684\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> exponent ( float or Tensor ) \u2013 \u6307\u6570 out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.5274 -0.8232 -2.1128 1.7558 [torch.FloatTensor of size 4] >>> torch.pow(a, 2) 0.2781 0.6776 4.4640 3.0829 [torch.FloatTensor of size 4] >>> exp = torch.arange(1, 5) >>> a = torch.arange(1, 5) >>> a 1 2 3 4 [torch.FloatTensor of size 4] >>> exp 1 2 3 4 [torch.FloatTensor of size 4] >>> torch.pow(a, exp) 1 4 27 256 [torch.FloatTensor of size 4] torch.pow(base, input, out=None) base \u662f\u4e00\u4e2a\u6807\u91cf\u6d6e\u70b9\u503c, input \u662f\u4e00\u4e2a\u5f20\u91cf. \u8fd4\u56de\u7684\u5f20\u91cf out \u7684\u5f62\u72b6\u4e0e\u5f20\u91cf input \u7684\u5f62\u72b6\u76f8\u540c. \u6267\u884c\u64cd\u4f5c: \\(out_i = base ^ {input_i}\\) | Parameters: | base ( float ) \u2013 \u5e42\u8fd0\u7b97\u7684\u5e95\u6570 input ( Tensor ) \u2013 \u6307\u6570 out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> exp = torch.arange(1, 5) >>> base = 2 >>> torch.pow(base, exp) 2 4 8 16 [torch.FloatTensor of size 4] torch.reciprocal(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684 <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u5012\u6570, i.e. \\(1.0 / x\\) | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.reciprocal(a) 0.7210 2.5565 -1.1583 -1.8289 [torch.FloatTensor of size 4] torch.remainder(input, divisor, out=None) \u2192 Tensor \u8ba1\u7b97\u5143\u7d20\u7684\u9664\u6cd5\u7684\u4f59\u6570. \u9664\u6570\u4e0e\u88ab\u9664\u6570\u53ef\u80fd\u540c\u65f6\u5305\u542b\u6574\u6570\u6216\u6d6e\u70b9\u6570. \u4f59\u6570\u4e0e\u9664\u6570\u6709\u76f8\u540c\u7684\u7b26\u53f7. \u5f53\u9664\u6570 divisor \u662f\u4e00\u4e2a\u5f20\u91cf, \u5f20\u91cf input \u7684\u5f62\u72b6\u548c\u5f20\u91cf divisor \u5f97\u5f62\u72b6\u5fc5\u987b\u53ef broadcastable . | Parameters: | input ( Tensor ) \u2013 \u88ab\u9664\u6570 divisor ( Tensor or float ) \u2013 \u9664\u6570. \u53ef\u80fd\u662f\u4e00\u4e2a\u6570\u6216\u8005\u53ef\u80fd\u662f\u4e00\u4e2a\u4e0e\u88ab\u9664\u6570\u5927\u5c0f\u76f8\u540c\u7684\u5f20\u91cf out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> torch.remainder(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2) torch.FloatTensor([1, 0, 1, 1, 0, 1]) >>> torch.remainder(torch.Tensor([1, 2, 3, 4, 5]), 1.5) torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5]) See also torch.fmod() \u540c\u6837\u8ba1\u7b97\u9664\u6cd5\u4f59\u6570, \u7b49\u6548\u4e8eC\u5e93\u51fd\u6570\u4e2d\u7684 fmod() torch.round(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u8f93\u5165\u5f20\u91cf\u7684\u5143\u7d20\u56db\u820d\u4e94\u5165\u5230\u6700\u8fd1\u7684\u6574\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] >>> torch.round(a) 1 1 -1 -0 [torch.FloatTensor of size 4] torch.rsqrt(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u5e73\u65b9\u6839\u7684\u5012\u6570. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] >>> torch.rsqrt(a) 0.9020 0.8636 nan nan [torch.FloatTensor of size 4] torch.sigmoid(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684sigmoid\u503c. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.4972 1.3512 0.1056 -0.2650 [torch.FloatTensor of size 4] >>> torch.sigmoid(a) 0.3782 0.7943 0.5264 0.4341 [torch.FloatTensor of size 4] torch.sign(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u7b26\u53f7. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.sign(a) -1 1 1 1 [torch.FloatTensor of size 4] torch.sin(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u6b63\u5f26. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.sin(a) -0.5944 0.2684 0.4322 0.9667 [torch.FloatTensor of size 4] torch.sinh(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u53cc\u66f2\u6b63\u5f26. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.sinh(a) -0.6804 0.2751 0.4619 1.7225 [torch.FloatTensor of size 4] torch.sqrt(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u5e73\u65b9\u6839. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] >>> torch.sqrt(a) 1.1086 1.1580 nan nan [torch.FloatTensor of size 4] torch.tan(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u6b63\u5207. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.tan(a) -0.7392 0.2786 0.4792 3.7801 [torch.FloatTensor of size 4] torch.tanh(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u53cc\u66f2\u6b63\u5207. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.tanh(a) -0.5625 0.2653 0.4193 0.8648 [torch.FloatTensor of size 4] torch.trunc(input, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf <cite>Tensor</cite> , \u5176\u5143\u7d20\u662f\u5f20\u91cf input \u5143\u7d20\u7684\u622a\u65ad\u6574\u6570\u503c (\u76f4\u63a5\u53bb\u9664\u5c0f\u6570\u90e8\u5206) . | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a -0.4972 1.3512 0.1056 -0.2650 [torch.FloatTensor of size 4] >>> torch.trunc(a) -0 1 0 -0 [torch.FloatTensor of size 4]","title":"Pointwise Ops (\u9010\u70b9\u64cd\u4f5c)"},{"location":"53/#reduction-ops","text":"torch.cumprod(input, dim, out=None) \u2192 Tensor \u8fd4\u56de\u5143\u7d20 input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u7684\u7d2f\u79ef\u79ef. \u4f8b\u5982, \u5982\u679c input \u662f\u4e00\u4e2aN\u5143\u5f20\u91cf, \u7ed3\u679c\u4e5f\u662f\u4e00\u4e2aN\u5143\u5f20\u91cf, \u5143\u7d20\u4e3a: \\(y_i = x_1 * x_2 * x_3 * ... * x_i\\) | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> dim ( int ) \u2013 \u8fdb\u884c\u64cd\u4f5c\u7684\u7ef4\u5ea6 out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(10) >>> a 1.1148 1.8423 1.4143 -0.4403 1.2859 -1.2514 -0.4748 1.1735 -1.6332 -0.4272 [torch.FloatTensor of size 10] >>> torch.cumprod(a, dim=0) 1.1148 2.0537 2.9045 -1.2788 -1.6444 2.0578 -0.9770 -1.1466 1.8726 -0.8000 [torch.FloatTensor of size 10] >>> a[5] = 0.0 >>> torch.cumprod(a, dim=0) 1.1148 2.0537 2.9045 -1.2788 -1.6444 -0.0000 0.0000 0.0000 -0.0000 0.0000 [torch.FloatTensor of size 10] torch.cumsum(input, dim, out=None) \u2192 Tensor \u8fd4\u56de\u5143\u7d20 input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u7684\u7d2f\u79ef\u548c. \u4f8b\u5982, \u5982\u679c input \u662f\u4e00\u4e2aN\u5143\u5f20\u91cf, \u7ed3\u679c\u5c06\u4e5f\u662f\u4e00\u4e2aN\u5143\u5f20\u91cf, \u5143\u7d20\u4e3a: \\(y_i = x_1 + x_2 + x_3 + ... + x_i\\) | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> dim ( int ) \u2013 \u8fdb\u884c\u64cd\u4f5c\u7684\u7ef4\u5ea6 out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(10) >>> a -0.6039 -0.2214 -0.3705 -0.0169 1.3415 -0.1230 0.9719 0.6081 -0.1286 1.0947 [torch.FloatTensor of size 10] >>> torch.cumsum(a, dim=0) -0.6039 -0.8253 -1.1958 -1.2127 0.1288 0.0058 0.9777 1.5858 1.4572 2.5519 [torch.FloatTensor of size 10] torch.dist(input, other, p=2) \u2192 float \u8fd4\u56de( input - other )\u7684p-\u8303\u6570 input \u548c other \u7684\u5f62\u72b6\u5fc5\u987b\u6ee1\u8db3 broadcastable . | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> other ( Tensor ) \u2013 \u53f3\u4fa7\u8f93\u5165 <cite>Tensor</cite> p ( float , optional ) \u2013 \u6240\u8ba1\u7b97\u7684\u8303\u6570. | | --- | --- | Example: >>> x = torch.randn(4) >>> x 0.2505 -0.4571 -0.3733 0.7807 [torch.FloatTensor of size 4] >>> y = torch.randn(4) >>> y 0.7782 -0.5185 1.4106 -2.4063 [torch.FloatTensor of size 4] >>> torch.dist(x, y, 3.5) 3.302832063224223 >>> torch.dist(x, y, 3) 3.3677282206393286 >>> torch.dist(x, y, 0) inf >>> torch.dist(x, y, 1) 5.560028076171875 torch.mean() torch.mean(input) \u2192 float \u8fd4\u56de\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u5747\u503c. Parameters: input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> Example: >>> a = torch.randn(1, 3) >>> a -0.2946 -0.9143 2.1809 [torch.FloatTensor of size 1x3] >>> torch.mean(a) 0.32398951053619385 torch.mean(input, dim, keepdim=False, out=None) \u2192 Tensor \u8fd4\u56de\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0a\u6bcf\u884c\u7684\u5747\u503c. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u5ea6 dim \u662f1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 out ( Tensor ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | Example: >>> a = torch.randn(4, 4) >>> a -1.2738 -0.3058 0.1230 -1.9615 0.8771 -0.5430 -0.9233 0.9879 1.4107 0.0317 -0.6823 0.2255 -1.3854 0.4953 -0.2160 0.2435 [torch.FloatTensor of size 4x4] >>> torch.mean(a, 1) -0.8545 0.0997 0.2464 -0.2157 [torch.FloatTensor of size 4] >>> torch.mean(a, 1, True) -0.8545 0.0997 0.2464 -0.2157 [torch.FloatTensor of size 4x1] torch.median() torch.median(input) \u2192 float \u8fd4\u56de\u8f93\u51fa\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u4e2d\u4f4d\u6570. Parameters: input ( Tensor ) \u2013 the input <cite>Tensor</cite> Example: >>> a = torch.randn(1, 3) >>> a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] >>> torch.median(a) -0.2085 torch.median(input, dim=-1, keepdim=False, values=None, indices=None) -> (Tensor, LongTensor) \u8fd4\u56de\u8f93\u51fa\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u7684\u4e2d\u4f4d\u6570. \u540c\u65f6\u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u4e2d\u4f4d\u6570\u7684\u7d22\u5f15 <cite>LongTensor</cite>. dim \u7684\u7f3a\u7701\u503c\u4e3a\u8f93\u5165\u5f20\u91cf input \u7684\u6700\u540e\u4e00\u7ef4. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u4e0e\u8f93\u5165\u5f20\u91cf input \u5f62\u72b6\u76f8\u540c, \u9664\u4e86\u7ef4\u6570 dim \u662f1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u6bd4\u8f93\u5165\u5f20\u91cf input \u5c11\u4e00\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u7559\u4e0e\u5426 values ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf indices ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf\u7d22\u5f15 | | --- | --- | Example: >>> a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] >>> a = torch.randn(4, 5) >>> a 0.4056 -0.3372 1.0973 -2.4884 0.4334 2.1336 0.3841 0.1404 -0.1821 -0.7646 -0.2403 1.3975 -2.0068 0.1298 0.0212 -1.5371 -0.7257 -0.4871 -0.2359 -1.1724 [torch.FloatTensor of size 4x5] >>> torch.median(a, 1) ( 0.4056 0.1404 0.0212 -0.7257 [torch.FloatTensor of size 4] , 0 2 4 1 [torch.LongTensor of size 4] ) torch.mode(input, dim=-1, keepdim=False, values=None, indices=None) -> (Tensor, LongTensor) \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u6570 dim \u4e0b\u6bcf\u884c\u5143\u7d20\u7684\u4f17\u6570\u503c. \u540c\u65f6\u4e5f\u8fd4\u56de\u4f17\u6570\u503c\u7684\u7d22\u5f15 <cite>LongTensor</cite>. \u7ef4\u5ea6 dim \u7684\u7f3a\u7701\u503c\u662f\u8f93\u5165\u5f20\u91cf input \u7684\u6700\u540e\u4e00\u7ef4. . \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u5ea6 dim \u662f1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. Note \u8fd9\u4e2a\u51fd\u6570\u81f3\u4eca\u6ca1\u6709\u4e3a torch.cuda.Tensor \u5b9a\u4e49. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 values ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf indices ( Tensor , optional ) \u2013 \u7ed3\u679c\u7d22\u5f15\u5f20\u91cf | | --- | --- | Example: >>> a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] >>> a = torch.randn(4, 5) >>> a 0.4056 -0.3372 1.0973 -2.4884 0.4334 2.1336 0.3841 0.1404 -0.1821 -0.7646 -0.2403 1.3975 -2.0068 0.1298 0.0212 -1.5371 -0.7257 -0.4871 -0.2359 -1.1724 [torch.FloatTensor of size 4x5] >>> torch.mode(a, 1) ( -2.4884 -0.7646 -2.0068 -1.5371 [torch.FloatTensor of size 4] , 3 4 2 0 [torch.LongTensor of size 4] ) torch.norm() torch.norm(input, p=2) \u2192 float \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u7684p-\u8303\u6570 | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> p ( float , optional ) \u2013 \u8303\u6570\u8ba1\u7b97\u4e2d\u7684\u5e42\u6307\u6570\u503c | | --- | --- | Example: >>> a = torch.randn(1, 3) >>> a -0.4376 -0.5328 0.9547 [torch.FloatTensor of size 1x3] >>> torch.norm(a, 3) 1.0338925067372466 torch.norm(input, p, dim, keepdim=False, out=None) \u2192 Tensor \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u5143\u7d20\u7684p-\u8303\u6570. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u975e\u7ef4\u5ea6 dim \u662f1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> p ( float ) \u2013 \u8303\u6570\u8ba1\u7b97\u4e2d\u7684\u5e42\u6307\u6570\u503c dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> a = torch.randn(4, 2) >>> a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] >>> torch.norm(a, 2, 1) 0.9585 0.7888 0.9077 0.6026 [torch.FloatTensor of size 4] >>> torch.norm(a, 0, 1, True) 2 2 2 2 [torch.FloatTensor of size 4x1] torch.prod() torch.prod(input) \u2192 float \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u4e58\u79ef. Parameters: input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> Example: >>> a = torch.randn(1, 3) >>> a 0.6170 0.3546 0.0253 [torch.FloatTensor of size 1x3] >>> torch.prod(a) 0.005537458061418483 torch.prod(input, dim, keepdim=False, out=None) \u2192 Tensor \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u5143\u7d20\u7684\u79ef. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u5ea6 dim \u662f1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> a = torch.randn(4, 2) >>> a 0.1598 -0.6884 -0.1831 -0.4412 -0.9925 -0.6244 -0.2416 -0.8080 [torch.FloatTensor of size 4x2] >>> torch.prod(a, 1) -0.1100 0.0808 0.6197 0.1952 [torch.FloatTensor of size 4] torch.std() torch.std(input, unbiased=True) \u2192 float \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u6807\u51c6\u5dee. \u5982\u679c unbiased \u662f False , \u90a3\u4e48\u6807\u51c6\u5dee\u5c06\u901a\u8fc7\u6709\u504f\u4f30\u8ba1\u8ba1\u7b97.\u5426\u5219, Bessel\u2019s correction \u5c06\u88ab\u4f7f\u7528. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> unbiased ( bool ) \u2013 \u662f\u5426\u4f7f\u7528\u65e0\u504f\u4f30\u8ba1 | | --- | --- | Example: >>> a = torch.randn(1, 3) >>> a -1.3063 1.4182 -0.3061 [torch.FloatTensor of size 1x3] >>> torch.std(a) 1.3782334731508061 torch.std(input, dim, keepdim=False, unbiased=True, out=None) \u2192 Tensor \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u5143\u7d20\u7684\u6807\u51c6\u5dee. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u5ea6 dim \u662f 1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. \u5982\u679c unbiased \u662f False , \u90a3\u4e48\u6807\u51c6\u5dee\u5c06\u901a\u8fc7\u6709\u504f\u4f30\u8ba1\u6765\u8ba1\u7b97. \u5426\u5219, Bessel\u2019s correction \u5c06\u88ab\u4f7f\u7528. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 unbiased ( bool ) \u2013 \u662f\u5426\u4f7f\u7528\u65e0\u504f\u4f30\u8ba1 out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> a = torch.randn(4, 4) >>> a 0.1889 -2.4856 0.0043 1.8169 -0.7701 -0.4682 -2.2410 0.4098 0.1919 -1.1856 -1.0361 0.9085 0.0173 1.0662 0.2143 -0.5576 [torch.FloatTensor of size 4x4] >>> torch.std(a, dim=1) 1.7756 1.1025 1.0045 0.6725 [torch.FloatTensor of size 4] torch.sum() torch.sum(input) \u2192 float \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u548c. Parameters: input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> Example: >>> a = torch.randn(1, 3) >>> a 0.6170 0.3546 0.0253 [torch.FloatTensor of size 1x3] >>> torch.sum(a) 0.9969287421554327 torch.sum(input, dim, keepdim=False, out=None) \u2192 Tensor \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u5143\u7d20\u7684\u548c. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u5ea6 dim \u662f 1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> a = torch.randn(4, 4) >>> a -0.4640 0.0609 0.1122 0.4784 -1.3063 1.6443 0.4714 -0.7396 -1.3561 -0.1959 1.0609 -1.9855 2.6833 0.5746 -0.5709 -0.4430 [torch.FloatTensor of size 4x4] >>> torch.sum(a, 1) 0.1874 0.0698 -2.4767 2.2440 [torch.FloatTensor of size 4] torch.var() torch.var(input, unbiased=True) \u2192 float \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u7684\u65b9\u5dee. \u5982\u679c unbiased \u662f False , \u65b9\u5dee\u7684\u8ba1\u7b97\u5c06\u901a\u8fc7\u6709\u504f\u4f30\u8ba1\u8ba1\u7b97. \u5426\u5219, Bessel\u2019s correction \u5c06\u4f1a\u88ab\u4f7f\u7528. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> unbiased ( bool ) \u2013 \u662f\u5426\u4f7f\u7528\u65e0\u504f\u4f30\u8ba1 | | --- | --- | Example: >>> a = torch.randn(1, 3) >>> a -1.3063 1.4182 -0.3061 [torch.FloatTensor of size 1x3] >>> torch.var(a) 1.899527506513334 torch.var(input, dim, keepdim=False, unbiased=True, out=None) \u2192 Tensor \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u7684\u65b9\u5dee. \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u5ea6 dim \u662f 1. \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u51cf\u5c11\u4e00\u7ef4. \u5982\u679c unbiased \u662f False , \u65b9\u5dee\u7684\u8ba1\u7b97\u5c06\u901a\u8fc7\u6709\u504f\u4f30\u8ba1\u8ba1\u7b97. \u5426\u5219, Bessel\u2019s correction \u5c06\u4f1a\u88ab\u4f7f\u7528. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u7559\u4e0e\u5426 unbiased ( bool ) \u2013 \u662f\u5426\u4f7f\u7528\u65e0\u504f\u4f30\u8ba1 out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> a = torch.randn(4, 4) >>> a -1.2738 -0.3058 0.1230 -1.9615 0.8771 -0.5430 -0.9233 0.9879 1.4107 0.0317 -0.6823 0.2255 -1.3854 0.4953 -0.2160 0.2435 [torch.FloatTensor of size 4x4] >>> torch.var(a, 1) 0.8859 0.9509 0.7548 0.6949 [torch.FloatTensor of size 4]","title":"Reduction Ops (\u5f52\u7ea6\u64cd\u4f5c)"},{"location":"53/#comparison-ops","text":"torch.eq(input, other, out=None) \u2192 Tensor \u6bd4\u8f83\u5143\u7d20\u662f\u5426\u76f8\u7b49 \u7b2c\u4e8c\u4e2a\u5143\u7d20\u53ef\u4ee5\u662f\u4e00\u4e2a\u6570\u5b57\u6216 broadcastable \u4e3a\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570\u5f62\u72b6\u76f8\u540c\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 \u5f85\u6bd4\u8f83\u5f20\u91cf other ( Tensor or float ) \u2013 \u6bd4\u8f83\u5f20\u91cf\u6216\u6570 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf, \u987b\u4e3a ByteTensor \u7c7b\u578b\u6216\u4e0e input (Tensor) \u540c\u7c7b\u578b | | --- | --- | | Returns: | \u4e00\u4e2a torch.ByteTensor \u5f20\u91cf, \u5f85\u6bd4\u8f83\u548c\u8981\u6bd4\u8f83\u5f20\u91cf\u9010\u4f4d\u7f6e\u6bd4\u8f83, \u76f8\u7b49\u4e3a 1 , \u4e0d\u7b49\u4e3a 0 | | --- | --- | Example: >>> torch.eq(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 0 0 1 [torch.ByteTensor of size 2x2] torch.equal(tensor1, tensor2) \u2192 bool \u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u6709\u76f8\u540c\u7684\u5f62\u72b6\u548c\u5143\u7d20\u503c, \u5219\u8fd4\u56de True , \u5426\u5219 False . Example: >>> torch.equal(torch.Tensor([1, 2]), torch.Tensor([1, 2])) True torch.ge(input, other, out=None) \u2192 Tensor \u9010\u5143\u7d20\u6bd4\u8f83 input \u548c other , \u5373\u662f\u5426 input>=other . \u7b2c\u4e8c\u4e2a\u53c2\u6570\u53ef\u4ee5\u4e3a\u4e00\u4e2a\u6570\u6216\u5f62\u72b6\u53ef broadcastable \u4e3a\u548c\u7b2c\u4e00\u4e2a\u53c2\u6570\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 \u5f85\u5bf9\u6bd4\u7684\u5f20\u91cf other ( Tensor or float ) \u2013 \u5bf9\u6bd4\u7684\u5f20\u91cf\u6216 float \u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5fc5\u987b\u4e3a ByteTensor \u6216\u8005\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570 tensor \u76f8\u540c\u7c7b\u578b. | | --- | --- | | Returns: | \u4e00\u4e2a torch.ByteTensor \u5f20\u91cf, \u5305\u542b\u4e86\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u6bd4\u8f83\u7ed3\u679c(\u662f\u5426 input >= other ). | | --- | --- | | Return type: | \u5f20\u91cf | | --- | --- | Return type: \u5f20\u91cf Example: >>> torch.ge(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 1 0 1 [torch.ByteTensor of size 2x2] torch.gt(input, other, out=None) \u2192 Tensor \u9010\u5143\u7d20\u6bd4\u8f83 input \u548c other , \u5373\u662f\u5426 input>other \u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u6709\u76f8\u540c\u7684\u5f62\u72b6\u548c\u5143\u7d20\u503c, \u5219\u8fd4\u56de True ,\u5426\u5219 False . \u7b2c\u4e8c\u4e2a\u53c2\u6570\u53ef\u4ee5\u4e3a\u4e00\u4e2a\u6570\u6216\u5f62\u72b6\u53ef broadcastable \u4e3a\u548c\u7b2c\u4e00\u4e2a\u53c2\u6570\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 \u5f85\u5bf9\u6bd4\u7684\u5f20\u91cf other ( Tensor or float ) \u2013 \u5bf9\u6bd4\u7684\u5f20\u91cf\u6216 float \u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5fc5\u987b\u4e3a ByteTensor \u6216\u8005\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570 tensor \u76f8\u540c\u7c7b\u578b. | | --- | --- | | Returns: | \u4e00\u4e2a torch.ByteTensor \u5f20\u91cf, \u5305\u542b\u4e86\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u6bd4\u8f83\u7ed3\u679c(\u662f\u5426 input > other ). | | --- | --- | | Return type: | \u5f20\u91cf | | --- | --- | Return type: \u5f20\u91cf Example: >>> torch.gt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 1 0 0 [torch.ByteTensor of size 2x2] torch.kthvalue(input, k, dim=None, keepdim=False, out=None) -> (Tensor, LongTensor) \u53d6\u8f93\u5165\u5f20\u91cf input \u6307\u5b9a\u7ef4\u4e0a\u7b2c k \u4e2a\u6700\u5c0f\u503c. \u5982\u679c\u4e0d\u6307\u5b9a dim , \u5219\u9ed8\u8ba4\u4e3a input \u7684\u6700\u540e\u4e00\u7ef4. \u8fd4\u56de\u4e00\u4e2a\u5143\u7ec4 <cite>(values,indices)</cite> ,\u5176\u4e2d indices \u662f\u539f\u59cb\u8f93\u5165\u5f20\u91cf input \u4e2d\u6cbf dim \u7ef4\u7684\u7b2c k \u4e2a\u6700\u5c0f\u503c\u4e0b\u6807. \u5982\u679c keepdim \u4e3a True , values \u548c indices \u5f20\u91cf\u90fd\u548c input \u5927\u5c0f\u76f8\u540c, \u9664\u4e86\u5728\u6240\u6709\u503c\u90fd\u4e3a1\u7684 dim \u7ef4\u5ea6\u4e0a. \u5982\u679c keepdim \u4e3a False , dim \u88ab\u538b\u7f29. (\u53c2\u89c1 torch.squeeze() ), \u4f7f values \u548c indices \u4e24\u4e2a\u5f20\u91cf\u6bd4 input \u5f20\u91cf\u5c0f\u4e00\u4e2a\u7684\u7ef4\u5ea6. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> k ( int ) \u2013 \u7b2c k \u4e2a\u6700\u5c0f\u503c dim ( int , optional ) \u2013 \u6cbf\u7740\u6b64\u7ef4\u8fdb\u884c\u6392\u5e8f keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u6301\u7ef4\u5ea6 dim \u4e0d\u53d8 out ( tuple , optional ) \u2013 \u8f93\u51fa\u5143\u7ec4 ( Tensor, LongTensor ) \u53ef\u9009\u53c2\u6570(\u4f5c\u4e3a\u8f93\u51fa buffers ) | | --- | --- | Example: >>> x = torch.arange(1, 6) >>> x 1 2 3 4 5 [torch.FloatTensor of size 5] >>> torch.kthvalue(x, 4) ( 4 [torch.FloatTensor of size 1] , 3 [torch.LongTensor of size 1] ) >>> x=torch.arange(1,7).resize_(2,3) >>> x 1 2 3 4 5 6 [torch.FloatTensor of size 2x3] >>> torch.kthvalue(x,2,0,True) ( 4 5 6 [torch.FloatTensor of size 1x3] , 1 1 1 [torch.LongTensor of size 1x3] ) torch.le(input, other, out=None) \u2192 Tensor \u9010\u5143\u7d20\u6bd4\u8f83 input \u548c other , \u5373\u662f\u5426 input<=other \u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u6709\u76f8\u540c\u7684\u5f62\u72b6\u548c\u5143\u7d20\u503c, \u5219\u8fd4\u56de True ,\u5426\u5219 False . \u7b2c\u4e8c\u4e2a\u53c2\u6570\u53ef\u4ee5\u4e3a\u4e00\u4e2a\u6570\u6216\u5f62\u72b6\u53ef broadcastable \u4e3a\u548c\u7b2c\u4e00\u4e2a\u53c2\u6570\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 \u5f85\u5bf9\u6bd4\u7684\u5f20\u91cf other ( Tensor or float ) \u2013 \u5bf9\u6bd4\u7684\u5f20\u91cf\u6216 float \u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5fc5\u987b\u4e3a ByteTensor \u6216\u8005\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570 tensor \u76f8\u540c\u7c7b\u578b. | | --- | --- | | Returns: | \u4e00\u4e2a torch.ByteTensor \u5f20\u91cf, \u5305\u542b\u4e86\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u6bd4\u8f83\u7ed3\u679c(\u662f\u5426 input <= other ). | | --- | --- | | Return type: | \u5f20\u91cf | | --- | --- | Example: >>> torch.le(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 0 1 1 [torch.ByteTensor of size 2x2] torch.lt(input, other, out=None) \u2192 Tensor \u9010\u5143\u7d20\u6bd4\u8f83 input \u548c other , \u5373\u662f\u5426 input<other \u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u6709\u76f8\u540c\u7684\u5f62\u72b6\u548c\u5143\u7d20\u503c, \u5219\u8fd4\u56de True ,\u5426\u5219 False . \u7b2c\u4e8c\u4e2a\u53c2\u6570\u53ef\u4ee5\u4e3a\u4e00\u4e2a\u6570\u6216\u5f62\u72b6\u53ef broadcastable \u4e3a\u548c\u7b2c\u4e00\u4e2a\u53c2\u6570\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 \u5f85\u5bf9\u6bd4\u7684\u5f20\u91cf other ( Tensor or float ) \u2013 \u5bf9\u6bd4\u7684\u5f20\u91cf\u6216 float \u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5fc5\u987b\u4e3a ByteTensor \u6216\u8005\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570 tensor \u76f8\u540c\u7c7b\u578b. | | --- | --- | | Returns: | \u4e00\u4e2a torch.ByteTensor \u5f20\u91cf, \u5305\u542b\u4e86\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u6bd4\u8f83\u7ed3\u679c(\u662f\u5426 input < other ). | | --- | --- | | Return type: | \u5f20\u91cf | | --- | --- | Return type: \u5f20\u91cf Example: >>> torch.lt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 0 1 0 [torch.ByteTensor of size 2x2] torch.max() torch.max(input) \u2192 float \u8fd4\u56de\u8f93\u5165 input \u5f20\u91cf\u6240\u6709\u5143\u7d20\u7684\u6700\u5927\u503c. Parameters: input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> Example: >>> a = torch.randn(1, 3) >>> a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] >>> torch.max(a) 0.4729 torch.max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor) \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0a\u6bcf\u884c\u7684\u6700\u5927\u503c, \u5e76\u540c\u65f6\u8fd4\u56de\u6bcf\u4e2a\u6700\u5927\u503c\u7684\u4f4d\u7f6e\u7d22\u5f15. \u5982\u679c keepdim \u4e3a True , values \u548c indices \u5f20\u91cf\u90fd\u548c input \u5c3a\u5bf8\u76f8\u540c, \u9664\u4e86\u5728\u6240\u6709\u503c\u90fd\u4e3a 1 \u7684 dim \u7ef4\u5ea6\u4e0a. \u5982\u679c keepdim \u4e3a False , dim \u88ab\u538b\u7f29. (\u53c2\u89c1 torch.squeeze() ), \u4f7f values \u548c indices \u4e24\u4e2a\u5f20\u91cf\u6bd4 input \u5f20\u91cf\u5c0f\u4e00\u4e2a\u7684\u7ef4\u5ea6. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> k ( int ) \u2013 \u7b2c k \u4e2a\u6700\u5c0f\u503c dim ( int , optional ) \u2013 \u6cbf\u7740\u6b64\u7ef4\u8fdb\u884c\u6392\u5e8f keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u6301\u7ef4\u5ea6 dim \u4e0d\u53d8 out ( tuple , optional ) \u2013 \u8f93\u51fa\u5143\u7ec4 (max, max_indices) | | --- | --- | Example: >> a = torch.randn(4, 4) >> a 0.0692 0.3142 1.2513 -0.5428 0.9288 0.8552 -0.2073 0.6409 1.0695 -0.0101 -2.4507 -1.2230 0.7426 -0.7666 0.4862 -0.6628 torch.FloatTensor of size 4x4] >>> torch.max(a, 1) ( 1.2513 0.9288 1.0695 0.7426 [torch.FloatTensor of size 4] , 2 0 0 0 [torch.LongTensor of size 4] ) torch.max(input, other, out=None) \u2192 Tensor \u8f93\u5165 input \u6bcf\u4e00\u4e2a\u5143\u7d20\u548c\u5bf9\u5e94\u7684\u6bd4\u8f83\u5f20\u91cf other \u8fdb\u884c\u6bd4\u8f83, \u7559\u4e0b\u8f83\u5927\u7684\u5143\u7d20 <cite>max</cite>. \u8981\u6bd4\u8f83\u7684\u5f20\u91cf input \u4e0e\u6bd4\u8f83\u5f20\u91cf other \u4e0d\u5fc5\u5927\u5c0f\u4e00\u81f4, \u4f46\u5b83\u4eec\u4e00\u5b9a\u8981\u80fd broadcastable . | Parameters: | input ( Tensor ) \u2013 \u8981\u6bd4\u8f83\u5f20\u91cf <cite>Tensor</cite> other ( Tensor ) \u2013 \u6bd4\u8f83\u5f20\u91cf <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> b = torch.randn(4) >>> b 1.0067 -0.8010 0.6258 0.3627 [torch.FloatTensor of size 4] >>> torch.max(a, b) 1.3869 0.3912 0.6258 0.3627 [torch.FloatTensor of size 4] torch.min() torch.min(input) \u2192 float \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u6240\u6709\u5143\u7d20\u7684\u6700\u5c0f\u503c. Parameters: input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> Example: >>> a = torch.randn(1, 3) >>> a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] >>> torch.min(a) -0.22663167119026184 torch.min(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor) \u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u5728\u7ed9\u5b9a\u7ef4\u5ea6 dim \u4e0b\u6bcf\u884c\u5143\u7d20\u7684\u6700\u5c0f\u503c. \u5176\u4e2d\u7b2c\u4e8c\u4e2a\u8fd4\u56de\u503c\u662f\u6bcf\u4e2a\u88ab\u627e\u51fa\u7684\u6700\u5c0f\u503c\u7684\u7d22\u5f15\u4f4d\u7f6e ( argmin ) . \u5982\u679c keepdim \u662f True , \u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e0e\u8f93\u5165\u5f20\u91cf input \u76f8\u540c, \u9664\u4e86\u7ef4\u6570 dim \u662f 1 . \u53e6\u5916, dim \u88ab\u6324\u538b (\u53c2\u770b torch.squeeze() ), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u6bd4\u8f93\u5165\u5f20\u91cf input \u5c11\u4e00\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf <cite>Tensor</cite> dim ( int ) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6 keepdim ( bool ) \u2013 \u8f93\u51fa\u5f20\u91cf\u7684\u7ef4\u5ea6 dim \u4fdd\u6301\u4e0e\u5426 out ( tuple , optional ) \u2013 \u4e24\u4e2a\u8f93\u51fa\u5f20\u91cf\u7684\u7ed3\u679c\u5143\u7ec4 (min, min_indices) | | --- | --- | Example: >> a = torch.randn(4, 4) >> a 0.0692 0.3142 1.2513 -0.5428 0.9288 0.8552 -0.2073 0.6409 1.0695 -0.0101 -2.4507 -1.2230 0.7426 -0.7666 0.4862 -0.6628 torch.FloatTensor of size 4x4] >> torch.min(a, 1) 0.5428 0.2073 2.4507 0.7666 torch.FloatTensor of size 4] 3 2 2 1 torch.LongTensor of size 4] torch.min(input, other, out=None) \u2192 Tensor \u8f93\u5165 input \u6bcf\u4e00\u4e2a\u5143\u7d20\u548c\u5bf9\u5e94\u7684\u6bd4\u8f83\u5f20\u91cf other \u8fdb\u884c\u6bd4\u8f83, \u7559\u4e0b\u8f83\u5c0f\u7684\u5143\u7d20 <cite>min</cite> . \u8981\u6bd4\u8f83\u7684\u5f20\u91cf input \u4e0e\u6bd4\u8f83\u5f20\u91cf other \u4e0d\u5fc5\u5c3a\u5bf8\u4e00\u81f4, \u4f46\u5b83\u4eec\u4e00\u5b9a\u8981\u80fd\u5e7f\u64ad broadcastable . | Parameters: | input ( Tensor ) \u2013 \u7b2c\u4e00\u4e2a\u5f20\u91cf <cite>Tensor</cite> other ( Tensor ) \u2013 \u7b2c\u4e8c\u4e2a\u5f20\u91cf <cite>Tensor</cite> out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7684\u5f20\u91cf <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> b = torch.randn(4) >>> b 1.0067 -0.8010 0.6258 0.3627 [torch.FloatTensor of size 4] >>> torch.min(a, b) 1.0067 -0.8010 -0.8634 -0.5468 [torch.FloatTensor of size 4] torch.ne(input, other, out=None) \u2192 Tensor \u9010\u5143\u7d20\u6bd4\u8f83 input \u548c other , \u5373\u662f\u5426 tensor != other \u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u6709\u76f8\u540c\u7684\u5f62\u72b6\u548c\u5143\u7d20\u503c, \u5219\u8fd4\u56de True , \u5426\u5219 False . \u7b2c\u4e8c\u4e2a\u53c2\u6570\u53ef\u4ee5\u4e3a\u4e00\u4e2a\u6570\u6216\u5f62\u72b6\u5e7f\u64ad broadcastable \u4e3a\u548c\u7b2c\u4e00\u4e2a\u53c2\u6570\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf. | Parameters: | input ( Tensor ) \u2013 \u5f85\u5bf9\u6bd4\u7684\u5f20\u91cf other ( Tensor or float ) \u2013 \u5bf9\u6bd4\u7684\u5f20\u91cf\u6216 float \u503c out ( Tensor , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5fc5\u987b\u4e3a ByteTensor \u6216\u8005\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570 tensor \u76f8\u540c\u7c7b\u578b. | | --- | --- | | Returns: | \u4e00\u4e2a torch.ByteTensor \u5f20\u91cf, \u5305\u542b\u4e86\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u6bd4\u8f83\u7ed3\u679c (\u662f\u5426 input != other ) . | | --- | --- | | Return type: | \u5f20\u91cf | | --- | --- | \u8fd4\u56de\u7c7b\u578b: Tensor Example: >>> torch.ne(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 1 1 0 [torch.ByteTensor of size 2x2] torch.sort(input, dim=None, descending=False, out=None) -> (Tensor, LongTensor) \u5bf9\u8f93\u5165\u5f20\u91cf input \u6cbf\u7740\u6307\u5b9a\u7ef4\u6309\u5347\u5e8f\u6392\u5e8f. \u5982\u679c\u4e0d\u7ed9\u5b9a dim ,\u5219\u9ed8\u8ba4\u4e3a\u8f93\u5165\u7684\u6700\u540e\u4e00\u7ef4. \u5982\u679c\u6307\u5b9a\u53c2\u6570 descending \u4e3a True , \u5219\u6309\u964d\u5e8f\u6392\u5e8f. \u8fd4\u56de\u5143\u7ec4 (sorted_tensor, sorted_indices) , sorted_indices \u4e3a\u539f\u59cb\u8f93\u5165\u4e2d\u7684\u4e0b\u6807. | Parameters: | input ( Tensor ) \u2013 \u8981\u5bf9\u6bd4\u7684\u5f20\u91cf dim ( int , optional ) \u2013 \u6cbf\u7740\u6b64\u7ef4\u6392\u5e8f descending ( bool , optional ) \u2013 \u5e03\u5c14\u503c, \u63a7\u5236\u5347\u964d\u6392\u5e8f out ( tuple , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5fc5\u987b\u4e3a ByteTensor \u6216\u8005\u4e0e\u7b2c\u4e00\u4e2a\u53c2\u6570 tensor \u76f8\u540c\u7c7b\u578b. | | --- | --- | Example: >>> x = torch.randn(3, 4) >>> sorted, indices = torch.sort(x) >>> sorted -1.6747 0.0610 0.1190 1.4137 -1.4782 0.7159 1.0341 1.3678 -0.3324 -0.0782 0.3518 0.4763 [torch.FloatTensor of size 3x4] >>> indices 0 1 3 2 2 1 0 3 3 1 0 2 [torch.LongTensor of size 3x4] >>> sorted, indices = torch.sort(x, 0) >>> sorted -1.6747 -0.0782 -1.4782 -0.3324 0.3518 0.0610 0.4763 0.1190 1.0341 0.7159 1.4137 1.3678 [torch.FloatTensor of size 3x4] >>> indices 0 2 1 2 2 0 2 0 1 1 0 1 [torch.LongTensor of size 3x4] torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -> (Tensor, LongTensor) \u6cbf\u7ed9\u5b9a dim \u7ef4\u5ea6\u8fd4\u56de\u8f93\u5165\u5f20\u91cf input \u4e2d k \u4e2a\u6700\u5927\u503c. \u5982\u679c\u4e0d\u6307\u5b9a dim , \u5219\u9ed8\u8ba4\u4e3a <cite>input</cite> \u7684\u6700\u540e\u4e00\u7ef4. \u5982\u679c\u4e3a largest \u4e3a False ,\u5219\u8fd4\u56de\u6700\u5c0f\u7684 <cite>k</cite> \u4e2a\u503c. \u8fd4\u56de\u4e00\u4e2a\u5143\u7ec4 <cite>(values, indices)</cite> , \u5176\u4e2d indices \u662f\u539f\u59cb\u8f93\u5165\u5f20\u91cf input \u4e2d\u6d4b\u5143\u7d20\u4e0b\u6807. \u5982\u679c\u8bbe\u5b9a\u5e03\u5c14\u503c sorted \u4e3a True , \u5c06\u4f1a\u786e\u4fdd\u8fd4\u56de\u7684 <cite>k</cite> \u4e2a\u503c\u88ab\u6392\u5e8f. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf k ( int ) \u2013 \u201ctop-k\u201d \u4e2d\u7684 k dim ( int , optional ) \u2013 \u6392\u5e8f\u7684\u7ef4 largest ( bool , optional ) \u2013 \u5e03\u5c14\u503c, \u63a7\u5236\u8fd4\u56de\u6700\u5927\u6216\u6700\u5c0f\u503c sorted ( bool , optional ) \u2013 \u5e03\u5c14\u503c, \u63a7\u5236\u8fd4\u56de\u503c\u662f\u5426\u6392\u5e8f out ( tuple , optional ) \u2013 \u53ef\u9009\u8f93\u51fa\u5f20\u91cf (Tensor, LongTensor) output buffers | | --- | --- | Example: >>> x = torch.arange(1, 6) >>> x 1 2 3 4 5 [torch.FloatTensor of size 5] >>> torch.topk(x, 3) ( 5 4 3 [torch.FloatTensor of size 3] , 4 3 2 [torch.LongTensor of size 3] ) >>> torch.topk(x, 3, 0, largest=False) ( 1 2 3 [torch.FloatTensor of size 3] , 0 1 2 [torch.LongTensor of size 3] )","title":"Comparison Ops (\u6bd4\u8f83\u64cd\u4f5c)"},{"location":"53/#other-operations","text":"torch.cross(input, other, dim=-1, out=None) \u2192 Tensor \u8fd4\u56de\u6cbf\u7740\u7ef4\u5ea6 <cite>dim</cite> \u4e0a, \u4e24\u4e2a\u5f20\u91cf <cite>input</cite> \u548c <cite>other</cite> \u7684\u5411\u91cf\u79ef (\u53c9\u79ef), <cite>input</cite> \u548c <cite>other</cite> \u5fc5\u987b\u6709\u76f8\u540c\u7684\u5f62\u72b6, \u4e14\u6307\u5b9a\u7684 <cite>dim</cite> \u7ef4\u4e0a <cite>size</cite> \u5fc5\u987b\u4e3a 3. \u5982\u679c\u4e0d\u6307\u5b9a <cite>dim</cite>, \u5219\u9ed8\u8ba4\u4e3a\u7b2c\u4e00\u4e2a\u5c3a\u5ea6\u4e3a 3 \u7684\u7ef4. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> other ( Tensor ) \u2013 \u7b2c\u4e8c\u4e2a\u8f93\u5165 <cite>Tensor</cite> dim ( int , optional ) \u2013 \u6cbf\u7740\u6b64\u7ef4\u8fdb\u884c\u53c9\u79ef\u64cd\u4f5c. out ( Tensor , optional ) \u2013 \u7ed3\u679c <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(4, 3) >>> a -0.6652 -1.0116 -0.6857 0.2286 0.4446 -0.5272 0.0476 0.2321 1.9991 0.6199 1.1924 -0.9397 [torch.FloatTensor of size 4x3] >>> b = torch.randn(4, 3) >>> b -0.1042 -1.1156 0.1947 0.9947 0.1149 0.4701 -1.0108 0.8319 -0.0750 0.9045 -1.3754 1.0976 [torch.FloatTensor of size 4x3] >>> torch.cross(a, b, dim=1) -0.9619 0.2009 0.6367 0.2696 -0.6318 -0.4160 -1.6805 -2.0171 0.2741 0.0163 -1.5304 -1.9311 [torch.FloatTensor of size 4x3] >>> torch.cross(a, b) -0.9619 0.2009 0.6367 0.2696 -0.6318 -0.4160 -1.6805 -2.0171 0.2741 0.0163 -1.5304 -1.9311 [torch.FloatTensor of size 4x3] torch.diag(input, diagonal=0, out=None) \u2192 Tensor \u5982\u679c\u8f93\u5165\u662f\u4e00\u4e2a\u5411\u91cf( <cite>1D</cite> \u5f20\u91cf), \u5219\u8fd4\u56de\u4e00\u4e2a\u4ee5 <cite>input</cite> \u4e3a\u5bf9\u89d2\u7ebf\u5143\u7d20\u7684 <cite>2D</cite> \u65b9\u9635. \u5982\u679c\u8f93\u5165\u662f\u4e00\u4e2a\u77e9\u9635( <cite>2D</cite> \u5f20\u91cf), \u5219\u8fd4\u56de\u4e00\u4e2a\u5305\u542b <cite>input</cite> \u5bf9\u89d2\u7ebf\u5143\u7d20\u76841D\u5f20\u91cf. \u53c2\u6570 <cite>diagonal</cite> \u6307\u5b9a\u5bf9\u89d2\u7ebf: diagonal = 0, \u4e3b\u5bf9\u89d2\u7ebf. diagonal > 0, \u4e3b\u5bf9\u89d2\u7ebf\u4e4b\u4e0a. diagonal < 0, \u4e3b\u5bf9\u89d2\u7ebf\u4e4b\u4e0b. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> diagonal ( int , optional ) \u2013 \u6307\u5b9a\u5bf9\u89d2\u7ebf out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: \u83b7\u5f97\u4ee5 <cite>input</cite> \u4e3a\u5bf9\u89d2\u7ebf\u7684\u65b9\u9635: >>> a = torch.randn(3) >>> a 1.0480 -2.3405 -1.1138 [torch.FloatTensor of size 3] >>> torch.diag(a) 1.0480 0.0000 0.0000 0.0000 -2.3405 0.0000 0.0000 0.0000 -1.1138 [torch.FloatTensor of size 3x3] >>> torch.diag(a, 1) 0.0000 1.0480 0.0000 0.0000 0.0000 0.0000 -2.3405 0.0000 0.0000 0.0000 0.0000 -1.1138 0.0000 0.0000 0.0000 0.0000 [torch.FloatTensor of size 4x4] \u83b7\u5f97\u7ed9\u5b9a\u77e9\u9635\u7684\u7b2ck\u6761\u5bf9\u89d2\u7ebf: >>> a = torch.randn(3, 3) >>> a -1.5328 -1.3210 -1.5204 0.8596 0.0471 -0.2239 -0.6617 0.0146 -1.0817 [torch.FloatTensor of size 3x3] >>> torch.diag(a, 0) -1.5328 0.0471 -1.0817 [torch.FloatTensor of size 3] >>> torch.diag(a, 1) -1.3210 -0.2239 [torch.FloatTensor of size 2] torch.histc(input, bins=100, min=0, max=0, out=None) \u2192 Tensor \u8ba1\u7b97\u8f93\u5165\u5f20\u91cf\u7684\u76f4\u65b9\u56fe. \u4ee5 <cite>min</cite> \u548c <cite>max</cite> \u4e3a <cite>range</cite> \u8fb9\u754c, \u5c06\u5176\u5747\u5206\u6210 <cite>bins</cite> \u4e2a\u76f4\u6761, \u7136\u540e\u5c06\u6392\u5e8f\u597d\u7684\u6570\u636e\u5212\u5206\u5230\u5404\u4e2a\u76f4\u6761 <cite>(bins)</cite> \u4e2d. \u5982\u679c <cite>min</cite> \u548c <cite>max</cite> \u90fd\u4e3a 0, \u5219\u5229\u7528\u6570\u636e\u4e2d\u7684\u6700\u5927\u6700\u5c0f\u503c\u4f5c\u4e3a\u8fb9\u754c. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165\u5f20\u91cf bins ( int ) \u2013 \u76f4\u65b9\u56fe <cite>bins</cite> (\u76f4\u6761)\u7684\u4e2a\u6570(\u9ed8\u8ba4100\u4e2a) min ( int ) \u2013 <cite>range</cite> \u7684\u4e0b\u8fb9\u754c(\u5305\u542b) max ( int ) \u2013 <cite>range</cite> \u7684\u4e0a\u8fb9\u754c(\u5305\u542b) out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | | Returns: | \u76f4\u65b9\u56fe | | --- | --- | | Return type: | Tensor | | --- | --- | Example: >>> torch.histc(torch.FloatTensor([1, 2, 1]), bins=4, min=0, max=3) FloatTensor([0, 2, 1, 0]) torch.renorm(input, p, dim, maxnorm, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf, \u5305\u542b\u89c4\u8303\u5316\u540e\u7684\u5404\u4e2a\u5b50\u5f20\u91cf, \u4f7f\u5f97\u6cbf\u7740 <cite>dim</cite> \u7ef4\u5212\u5206\u7684\u5404\u5b50\u5f20\u91cf\u7684 <cite>p</cite> \u8303\u6570\u5c0f\u4e8e <cite>maxnorm</cite> Note \u5982\u679c p \u8303\u6570\u7684\u503c\u5c0f\u4e8e <cite>maxnorm</cite>, \u5219\u5f53\u524d\u5b50\u5f20\u91cf\u4e0d\u9700\u8981\u4fee\u6539. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> p ( float ) \u2013 \u8303\u6570\u7684 <cite>p</cite> dim ( int ) \u2013 \u6cbf\u7740\u6b64\u7ef4\u5207\u7247, \u5f97\u5230\u5f20\u91cf\u5b50\u96c6 maxnorm ( float ) \u2013 \u6bcf\u4e2a\u5b50\u5f20\u91cf\u7684\u8303\u6570\u7684\u6700\u5927\u503c out ( Tensor , optional ) \u2013 \u7ed3\u679c\u5f20\u91cf | | --- | --- | Example: >>> x = torch.ones(3, 3) >>> x[1].fill_(2) >>> x[2].fill_(3) >>> x 1 1 1 2 2 2 3 3 3 [torch.FloatTensor of size 3x3] >>> torch.renorm(x, 1, 0, 5) 1.0000 1.0000 1.0000 1.6667 1.6667 1.6667 1.6667 1.6667 1.6667 [torch.FloatTensor of size 3x3] torch.trace(input) \u2192 float \u8fd4\u56de\u8f93\u5165 2 \u7ef4\u77e9\u9635\u5bf9\u89d2\u7ebf\u5143\u7d20\u7684\u548c(\u8ff9). Example: >>> x = torch.arange(1, 10).view(3, 3) >>> x 1 2 3 4 5 6 7 8 9 [torch.FloatTensor of size 3x3] >>> torch.trace(x) 15.0 torch.tril(input, diagonal=0, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf, \u5305\u542b\u8f93\u5165\u77e9\u9635 ( <cite>2D</cite> \u5f20\u91cf)\u7684\u4e0b\u4e09\u89d2\u90e8\u5206, \u5176\u4f59\u90e8\u5206\u88ab\u8bbe\u4e3a 0. \u8fd9\u91cc\u6240\u8bf4\u7684\u4e0b\u4e09\u89d2\u90e8\u5206\u4e3a\u77e9\u9635\u6307\u5b9a\u5bf9\u89d2\u7ebf <cite>diagonal</cite> \u5728\u7ebf\u91cc\u7684\u548c\u4e0b\u9762\u7684\u5143\u7d20. \u53c2\u6570 <cite>diagonal</cite> \u63a7\u5236\u5bf9\u89d2\u7ebf. diagonal = 0, \u4e3b\u5bf9\u89d2\u7ebf. diagonal > 0, \u4e3b\u5bf9\u89d2\u7ebf\u4e4b\u4e0a. diagonal < 0, \u4e3b\u5bf9\u89d2\u7ebf\u4e4b\u4e0b. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> diagonal ( int , optional ) \u2013 \u6307\u5b9a\u5bf9\u89d2\u7ebf out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(3,3) >>> a 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.tril(a) 1.3225 0.0000 0.0000 -0.3052 -0.3111 0.0000 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.tril(a, diagonal=1) 1.3225 1.7304 0.0000 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.tril(a, diagonal=-1) 0.0000 0.0000 0.0000 -0.3052 0.0000 0.0000 1.2469 0.0064 0.0000 [torch.FloatTensor of size 3x3] torch.triu(input, diagonal=0, out=None) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf, \u5305\u542b\u8f93\u5165\u77e9\u9635 ( <cite>2D</cite> \u5f20\u91cf)\u7684\u4e0a\u4e09\u89d2\u90e8\u5206, \u5176\u4f59\u90e8\u5206\u88ab\u8bbe\u4e3a 0. \u8fd9\u91cc\u6240\u8bf4\u7684\u4e0b\u4e09\u89d2\u90e8\u5206\u4e3a\u77e9\u9635\u6307\u5b9a\u5bf9\u89d2\u7ebf <cite>diagonal</cite> \u5728\u7ebf\u91cc\u7684\u548c\u4e0a\u9762\u7684\u5143\u7d20. \u53c2\u6570 <cite>diagonal</cite> \u63a7\u5236\u5bf9\u89d2\u7ebf. diagonal = 0, \u4e3b\u5bf9\u89d2\u7ebf. diagonal > 0, \u4e3b\u5bf9\u89d2\u7ebf\u4e4b\u4e0a. diagonal < 0, \u4e3b\u5bf9\u89d2\u7ebf\u4e4b\u4e0b. | Parameters: | input ( Tensor ) \u2013 \u8f93\u5165 <cite>Tensor</cite> diagonal ( int , optional ) \u2013 \u6307\u5b9a\u5bf9\u89d2\u7ebf out ( Tensor , optional ) \u2013 \u8f93\u51fa <cite>Tensor</cite> | | --- | --- | Example: >>> a = torch.randn(3,3) >>> a 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.triu(a) 1.3225 1.7304 1.4573 0.0000 -0.3111 -0.1809 0.0000 0.0000 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.triu(a, diagonal=1) 0.0000 1.7304 1.4573 0.0000 0.0000 -0.1809 0.0000 0.0000 0.0000 [torch.FloatTensor of size 3x3] >>> torch.triu(a, diagonal=-1) 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 0.0000 0.0064 -1.6250 [torch.FloatTensor of size 3x3]","title":"Other Operations (\u5176\u5b83\u64cd\u4f5c)"},{"location":"53/#blas-and-lapack-operations-blaslapack","text":"torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) \u2192 Tensor \u6267\u884c\u4fdd\u5b58\u5728 batch1 \u548c batch2 \u4e2d\u7684\u77e9\u9635\u7684\u6279\u91cf\u70b9\u4e58, \u4f34\u968f\u7740\u4e00\u4e2a\u51cf\u5c11\u7684\u76f8\u52a0\u6b65\u9aa4 (\u6240\u6709\u7684\u77e9\u9635\u4e58\u6cd5\u6cbf\u7b2c\u4e00\u7ef4\u7d2f\u52a0). mat \u88ab\u76f8\u52a0\u5230\u6700\u7ec8\u7684\u7ed3\u679c\u4e2d. batch1 \u548c batch2 \u5fc5\u987b\u662f\u4e09\u7ef4\u7684\u5f20\u91cf, \u4e14\u6bcf\u4e2a\u5305\u542b\u76f8\u540c\u6570\u91cf\u7684\u77e9\u9635. \u5982\u679c batch1 \u662f\u4e00\u4e2a <cite>b x n x m</cite> \u7684\u5f20\u91cf, batch2 \u662f\u4e00\u4e2a <cite>b x m x p \u7684\u5f20\u91cf, \u90a3\u4e48 ::attr: mat</cite> \u5fc5\u987b\u662f broadcastable \u4e14\u662f\u4e00\u4e2a <cite>n x p</cite> \u7684\u5f20\u91cf, \u540c\u65f6 attr:<cite>out</cite> \u5c06\u662f\u4e00\u4e2a <cite>n x p</cite> \u7684\u5f20\u91cf. \u6362\u53e5\u8bdd\u8bf4, \\(res = (beta * M) + (alpha * sum(batch1_i @ batch2_i, i = 0, b))\\) \u5bf9\u4e8e <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7c7b\u578b\u7684\u8f93\u5165, \u53c2\u6570 <cite>beta</cite> \u548c <cite>alpha</cite> \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u4ed6\u4eec\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | beta ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e mat \u7684\u4e58\u5b50 (\u7cfb\u6570) mat ( Tensor ) \u2013 \u8981\u88ab\u76f8\u52a0\u7684\u77e9\u9635 alpha ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e <cite>batch1 @ batch2</cite> \u7684\u4e58\u5b50 batch1 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e00\u6279\u77e9\u9635 batch2 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e8c\u6279\u77e9\u9635 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7684\u5f20\u91cf\u7ed3\u679c | | --- | --- | Example: >>> M = torch.randn(3, 5) >>> batch1 = torch.randn(10, 3, 4) >>> batch2 = torch.randn(10, 4, 5) >>> torch.addbmm(M, batch1, batch2) -3.1162 11.0071 7.3102 0.1824 -7.6892 1.8265 6.0739 0.4589 -0.5641 -5.4283 -9.3387 -0.1794 -1.2318 -6.8841 -4.7239 [torch.FloatTensor of size 3x5] torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None) \u2192 Tensor \u6267\u884c\u77e9\u9635 mat1 \u548c mat2 \u7684\u76f8\u4e58. \u77e9\u9635 mat \u5c06\u4e0e\u76f8\u4e58\u7684\u6700\u7ec8\u8ba1\u7b97\u7ed3\u679c\u76f8\u52a0. \u5982\u679c mat1 \u662f\u4e00\u4e2a <cite>n x m</cite> \u7684\u5f20\u91cf, mat2 \u662f\u4e00\u4e2a <cite>m x p \u7684\u5f20\u91cf, \u90a3\u4e48 ::attr: mat</cite> \u5fc5\u987b\u662f broadcastable \u4e14\u662f\u4e00\u4e2a <cite>n x p</cite> \u7684\u5f20\u91cf, \u540c\u65f6 attr:<cite>out</cite> \u5c06\u662f\u4e00\u4e2a <cite>n x p</cite> \u7684\u5f20\u91cf. \u6362\u53e5\u8bdd\u8bf4, \\(out = (beta * M) + (alpha * mat1 @ mat2)\\) \u5bf9\u4e8e <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7c7b\u578b\u7684\u8f93\u5165, \u53c2\u6570 <cite>beta</cite> \u548c <cite>alpha</cite> \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u4ed6\u4eec\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | beta ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e:attr:[ ](#id1)mat \u7684\u4e58\u5b50 mat ( Tensor ) \u2013 \u8981\u88ab\u76f8\u52a0\u7684\u77e9\u9635 alpha ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e mat1 @ mat2 \u7684\u4e58\u5b50 mat1 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e00\u4e2a\u77e9\u9635 mat2 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e8c\u4e2a\u77e9\u9635 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c | | --- | --- | Example: >>> M = torch.randn(2, 3) >>> mat1 = torch.randn(2, 3) >>> mat2 = torch.randn(3, 3) >>> torch.addmm(M, mat1, mat2) -0.4095 -1.9703 1.3561 5.7674 -4.9760 2.7378 [torch.FloatTensor of size 2x3] torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None) \u2192 Tensor \u6267\u884c\u77e9\u9635 mat \u548c\u5411\u91cf vec \u7684\u76f8\u4e58. \u77e9\u9635 tensor \u5c06\u4e0e\u76f8\u4e58\u7684\u6700\u7ec8\u8ba1\u7b97\u7ed3\u679c\u76f8\u52a0. \u5982\u679c mat \u662f\u4e00\u4e2a <cite>n x m</cite> \u7684\u5f20\u91cf, vec \u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>m</cite> \u7684\u4e00\u7ef4\u5f20\u91cf, \u90a3\u4e48 : tensor \u5fc5\u987b\u662f broadcastable \u4e14\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>n</cite> \u7684\u4e00\u7ef4\u5f20\u91cf, \u540c\u65f6 attr:<cite>out</cite> \u5c06\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>n</cite> \u7684\u4e00\u7ef4\u5f20\u91cf. <cite>alpha</cite> \u548c <cite>beta</cite> \u5206\u522b\u662f <cite>mat * vec</cite> \u548c <cite>tensor</cite> \u7684\u7f29\u653e\u56e0\u5b50. \u6362\u53e5\u8bdd\u8bf4, \\(out = (beta * tensor) + (alpha * (mat @ vec2))\\) \u5bf9\u4e8e <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7c7b\u578b\u7684\u8f93\u5165, \u53c2\u6570 <cite>beta</cite> \u548c <cite>alpha</cite> \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u4ed6\u4eec\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | beta ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e tensor \u7684\u4e58\u5b50 tensor ( Tensor ) \u2013 \u8981\u88ab\u76f8\u52a0\u7684\u5411\u91cf alpha ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e <cite>mat @ vec</cite> \u7684\u4e58\u5b50 mat ( Tensor ) \u2013 \u8981\u88ab\u76f8\u4e58\u7684\u77e9\u9635 vec ( Tensor ) \u2013 \u8981\u88ab\u8981\u4e58\u7684\u5411\u91cf out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c | | --- | --- | Example: >>> M = torch.randn(2) >>> mat = torch.randn(2, 3) >>> vec = torch.randn(3) >>> torch.addmv(M, mat, vec) -2.0939 -2.2950 [torch.FloatTensor of size 2] torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None) \u2192 Tensor \u6267\u884c\u5411\u91cf:attr:<cite>vec1</cite> \u548c vec2 \u7684\u5916\u79ef, \u5e76\u628a\u5916\u79ef\u8ba1\u7b97\u7ed3\u679c\u4e0e\u77e9\u9635 mat \u76f8\u52a0. \u53ef\u9009\u503c beta \u548c alpha \u662f\u6807\u91cf, \u5206\u522b\u4e0e mat \u548c \\((vec1 \\otimes vec2)\\) \u76f8\u4e58. \u6362\u53e5\u8bdd\u8bf4, \\(out = (beta * mat) + (alpha * vec1 \\otimes vec2)\\) \u5982\u679c vec1 \u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>n</cite> \u7684\u5411\u91cf, vec2 \u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>m</cite> \u7684\u5411\u91cf, \u90a3\u4e48 mat \u5fc5\u987b\u662f broadcastable \u4e14\u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a <cite>n x m</cite> \u7684\u77e9\u9635, \u540c\u65f6 out \u5c06\u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a <cite>n x m</cite> \u7684\u77e9\u9635. \u5bf9\u4e8e <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7c7b\u578b\u7684\u8f93\u5165, \u53c2\u6570 <cite>beta</cite> \u548c <cite>alpha</cite> \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u4ed6\u4eec\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | beta ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e mat \u7684\u4e58\u5b50 mat ( Tensor ) \u2013 \u8981\u88ab\u76f8\u52a0\u7684\u77e9\u9635 alpha ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e vec1 \u548c vec2 \u5916\u79ef\u8ba1\u7b97\u7ed3\u679c\u7684\u4e58\u5b50 vec1 ( Tensor ) \u2013 \u5916\u79ef\u8ba1\u7b97\u7684\u7b2c\u4e00\u4e2a\u5411\u91cf vec2 ( Tensor ) \u2013 \u5916\u79ef\u8ba1\u7b97\u7684\u7b2c\u4e8c\u4e2a\u5411\u91cf out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c | | --- | --- | Example: >>> vec1 = torch.arange(1, 4) >>> vec2 = torch.arange(1, 3) >>> M = torch.zeros(3, 2) >>> torch.addr(M, vec1, vec2) 1 2 2 4 3 6 [torch.FloatTensor of size 3x2] torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) \u2192 Tensor \u6267\u884c\u4fdd\u5b58\u5728 batch1 \u548c batch2 \u4e2d\u7684\u77e9\u9635\u7684\u6279\u91cf\u70b9\u4e58. mat \u88ab\u76f8\u52a0\u5230\u6700\u7ec8\u7684\u7ed3\u679c\u4e2d. batch1 \u548c batch2 \u5fc5\u987b\u662f\u4e09\u7ef4\u7684\u5f20\u91cf, \u4e14\u6bcf\u4e2a\u5305\u542b\u76f8\u540c\u6570\u91cf\u7684\u77e9\u9635. \u5982\u679c batch1 \u662f\u4e00\u4e2a <cite>b x n x m</cite> \u7684\u5f20\u91cf, batch2 \u662f\u4e00\u4e2a <cite>b x m x p \u7684\u5f20\u91cf, \u90a3\u4e48 ::attr: mat</cite> \u5fc5\u987b\u662f broadcastable \u4e14\u662f\u4e00\u4e2a <cite>b x n x p</cite> \u7684\u5f20\u91cf, \u540c\u65f6 attr:<cite>out</cite> \u5c06\u662f\u4e00\u4e2a <cite>b x n x p</cite> \u7684\u5f20\u91cf. \u6362\u53e5\u8bdd\u8bf4, \\(res_i = (beta * M_i) + (alpha * batch1_i \\times batch2_i)\\) \u5bf9\u4e8e <cite>FloatTensor</cite> \u6216\u8005 <cite>DoubleTensor</cite> \u7c7b\u578b\u7684\u8f93\u5165, \u53c2\u6570 <cite>beta</cite> \u548c <cite>alpha</cite> \u5fc5\u987b\u662f\u5b9e\u6570, \u5426\u5219\u4ed6\u4eec\u5e94\u8be5\u662f\u6574\u6570. | Parameters: | beta ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e mat \u7684\u4e58\u5b50 (\u7cfb\u6570) mat ( Tensor ) \u2013 \u8981\u88ab\u76f8\u52a0\u7684\u5f20\u91cf alpha ( Number__, optional ) \u2013 \u4f5c\u7528\u4e8e <cite>batch1 @ batch2</cite> \u7684\u4e58\u5b50 batch1 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e00\u6279\u77e9\u9635 batch2 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e8c\u6279\u77e9\u9635 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7684\u5f20\u91cf\u7ed3\u679c | | --- | --- | Example: >>> M = torch.randn(10, 3, 5) >>> batch1 = torch.randn(10, 3, 4) >>> batch2 = torch.randn(10, 4, 5) >>> torch.baddbmm(M, batch1, batch2).size() torch.Size([10, 3, 5]) torch.bmm(batch1, batch2, out=None) \u2192 Tensor \u6267\u884c\u4fdd\u5b58\u5728 batch1 \u548c batch2 \u4e2d\u7684\u77e9\u9635\u7684\u6279\u91cf\u70b9\u4e58. batch1 \u548c batch2 \u5fc5\u987b\u662f\u4e09\u7ef4\u7684\u5f20\u91cf, \u4e14\u6bcf\u4e2a\u5305\u542b\u76f8\u540c\u6570\u91cf\u7684\u77e9\u9635. \u5982\u679c batch1 \u662f\u4e00\u4e2a <cite>b x n x m</cite> \u7684\u5f20\u91cf, batch2 \u662f\u4e00\u4e2a <cite>b x m x p</cite> \u7684\u5f20\u91cf, out \u5c06\u662f\u4e00\u4e2a <cite>b x n x p</cite> \u7684\u5f20\u91cf. Note \u8fd9\u4e2a\u51fd\u6570\u4e0d\u80fd\u53c2\u8003 broadcast . \u5bf9\u4e8e\u5e7f\u64ad\u77e9\u9635\u76f8\u4e58, \u53c2\u89c1 torch.matmul() . | Parameters: | batch1 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e00\u6279\u77e9\u9635 batch2 ( Tensor ) \u2013 \u8981\u76f8\u4e58\u7684\u7b2c\u4e8c\u6279\u77e9\u9635 out ( Tensor , optional ) \u2013 \u8f93\u51fa\u7ed3\u679c | | --- | --- | Example: >>> batch1 = torch.randn(10, 3, 4) >>> batch2 = torch.randn(10, 4, 5) >>> res = torch.bmm(batch1, batch2) >>> res.size() torch.Size([10, 3, 5]) torch.btrifact(A, info=None, pivot=True) \u2192 Tensor, IntTensor \u6279\u91cf LU \u5206\u89e3. \u8fd4\u56de\u4e00\u4e2a\u5305\u542b LU \u5206\u89e3\u548c\u67a2\u8f74\u7684\u5143\u7ec4. \u5bf9\u4e8e\u6bcf\u4e2a minibatch \u793a\u4f8b, \u5982\u679c\u5206\u89e3\u6210\u529f, \u53ef\u9009\u53c2\u6570 <cite>info</cite> \u5c06\u63d0\u4f9b\u5206\u89e3\u4fe1\u606f. <cite>info</cite> \u7684\u503c\u6765\u81ea dgetrf, \u82e5\u662f\u975e\u96f6\u503c, \u5219\u8868\u793a\u6709\u9519\u8bef\u53d1\u751f. \u5982\u679c cuda \u88ab\u4f7f\u7528\u7684\u8bdd, \u5177\u4f53\u7684\u503c\u6765\u81ea cublas, \u5426\u5219\u6765\u81ea LAPACK. \u5982\u679c\u8bbe\u7f6e\u4e86 pivot, \u90a3\u4e48\u65cb\u8f6c\u64cd\u4f5c\u5c06\u88ab\u6267\u884c. Parameters: A ( Tensor ) \u2013 \u8981\u5206\u89e3\u7684\u5f20\u91cf. Example: >>> A = torch.randn(2, 3, 3) >>> A_LU = A.btrifact() torch.btrisolve(b, LU_data, LU_pivots) \u2192 Tensor \u6279\u91cf LU \u89e3. \u8fd4\u56de\u7ebf\u6027\u7cfb\u7edf Ax = b \u7684 LU \u89e3. | Parameters: | b ( Tensor ) \u2013 RHS tensor. LU_data ( Tensor ) \u2013 Pivoted LU factorization of A from btrifact. LU_pivots ( IntTensor ) \u2013 Pivots of the LU factorization. | | --- | --- | Example: >>> A = torch.randn(2, 3, 3) >>> b = torch.randn(2, 3) >>> A_LU = torch.btrifact(A) >>> x = b.btrisolve(*A_LU) >>> torch.norm(A.bmm(x.unsqueeze(2)) - b) 6.664001874625056e-08 torch.dot(tensor1, tensor2) \u2192 float \u8ba1\u7b97\u4e24\u4e2a\u5f20\u91cf\u7684\u70b9\u4e58 (\u5185\u79ef). Note \u8fd9\u4e2a\u51fd\u6570\u4e0d\u652f\u6301 broadcast . Example: >>> torch.dot(torch.Tensor([2, 3]), torch.Tensor([2, 1])) 7.0 torch.eig(a, eigenvectors=False, out=None) -> (Tensor, Tensor) \u8ba1\u7b97\u5b9e\u6570\u65b9\u9635\u7684\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf. | Parameters: | a ( Tensor ) \u2013 \u4e00\u4e2a\u8981\u88ab\u8ba1\u7b97\u7279\u5f81\u503c\u4e0e\u7279\u5f81\u5411\u91cf\u7684\u65b9\u9635 eigenvectors ( bool ) \u2013 \u82e5\u4e3a True , \u8868\u793a\u7279\u5f81\u503c\u4e0e\u7279\u5f81\u5411\u91cf\u90fd\u88ab\u8ba1\u7b97. \u5426\u5219, \u4ec5\u8ba1\u7b97\u7279\u5f81\u503c. out ( tuple , optional ) \u2013 \u8f93\u51fa\u5f20\u91cf | | --- | --- | | Returns: | \u5305\u542b > * e ( Tensor ): a \u7684\u5de6\u7279\u5f81\u503c > * v ( Tensor ): \u5982\u679c eigenvectors \u4e3a True , \u8868\u793a a \u7684\u7279\u5f81\u5411\u91cf; \u5426\u5219\u662f\u4e00\u4e2a\u7a7a\u7684\u5f20\u91cf | | --- | --- | | Return type: | \u8fd4\u56de\u4e00\u4e2a\u5143\u7ec4, ( Tensor , Tensor ) | | --- | --- | torch.gels(B, A, out=None) \u2192 Tensor \u8ba1\u7b97\u79e9\u4e3a \\(m\\) \u7684\uff0c \u5927\u5c0f\u4e3a m x n \u7684\u77e9\u9635 \\(A\\) \u6700\u5c0f\u4e8c\u4e58\u548c\u6700\u5c0f\u8303\u6570\u95ee\u9898\u7684\u89e3 \u5982\u679c \\(m &gt;= n\\) , gels() \u6c42\u89e3\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898: \\[\\begin{array}{ll} \\mbox{minimize} & \\|AX-B\\|_F. \\end{array}\\] \u5982\u679c \\(m &lt; n\\) , gels() \u6c42\u89e3\u6700\u5c0f\u8303\u6570\u95ee\u9898: \\[\\begin{array}{ll} \\mbox{minimize} & \\|X\\|_F & \\mbox{subject to} & AX = B. \\end{array}\\] \u8fd4\u56de\u7684\u77e9\u9635 \\(X\\) \u7684\u5934 \\(n\\) \u884c\u5305\u542b\u89e3\u4fe1\u606f. \u5176\u4f59\u884c\u5305\u542b\u5269\u4f59\u4fe1\u606f: \u4ece\u7b2c \\(n\\) \u884c\u5f00\u59cb\u7684\u6bcf\u5217\u7684 euclidean \u8303\u6570, \u662f\u5bf9\u5e94\u5217\u7684\u5269\u4f59. | Parameters: | B ( Tensor ) \u2013 The matrix \\(B\\) A ( Tensor ) \u2013 The \\(m\\) by \\(n\\) matrix \\(A\\) out ( tuple , optional ) \u2013 Optional destination tensor | | --- | --- | | Returns: | tuple containing: > * X ( Tensor ): \u6700\u5c0f\u4e8c\u4e58\u89e3 > * qr ( Tensor ): QR \u5206\u89e3\u7684\u8be6\u7ec6\u4fe1\u606f | | --- | --- | | Return type: | ( Tensor , Tensor ) | | --- | --- | Note \u4e0d\u7ba1\u8f93\u5165\u77e9\u9635\u7684\u6b65\u957f\u5982\u4f55, \u8fd4\u56de\u6765\u7684\u77e9\u9635\u5c06\u603b\u662f\u88ab\u8f6c\u7f6e. \u4e5f\u5c31\u662f, \u4ed6\u4eec\u7684\u6b65\u957f\u662f <cite>(1, m)</cite> \u800c\u4e0d\u662f <cite>(m, 1)</cite>. Example: >>> A = torch.Tensor([[1, 1, 1], ... [2, 3, 4], ... [3, 5, 2], ... [4, 2, 5], ... [5, 4, 3]]) >>> B = torch.Tensor([[-10, -3], [ 12, 14], [ 14, 12], [ 16, 16], [ 18, 16]]) >>> X, _ = torch.gels(B, A) >>> X 2.0000 1.0000 1.0000 1.0000 1.0000 2.0000 [torch.FloatTensor of size 3x2] torch.geqrf(input, out=None) -> (Tensor, Tensor) \u8fd9\u662f\u76f4\u63a5\u8c03\u7528 LAPACK \u7684\u4f4e\u5c42\u51fd\u6570. \u901a\u5e38\u60a8\u5e94\u8be5\u4f7f\u7528 torch.qr() \u6765\u4ee3\u66ff\u4e4b. \u8ba1\u7b97 input \u7684 QR \u5206\u89e3, \u4f46\u4e0d\u6784\u9020 <cite>Q</cite> \u548c <cite>R</cite> \u4f5c\u4e3a\u663e\u793a\u5206\u5f00\u7684\u77e9\u9635. \u7136\u800c, \u8fd9\u6837\u76f4\u63a5\u8c03\u7528 LAPACK \u7684\u5e95\u5c42\u51fd\u6570 <cite>?geqrf</cite>, \u4f1a\u4ea7\u751f\u4e00\u8fde\u4e32\u7684 \u2018elementary reflectors\u2019. \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 LAPACK documentation . | Parameters: | input ( Tensor ) \u2013 the input matrix out ( tuple , optional ) \u2013 The result tuple of (Tensor, Tensor) | | --- | --- | torch.ger(vec1, vec2, out=None) \u2192 Tensor \u8ba1\u7b97 vec1 \u548c vec2 \u7684\u5916\u79ef. \u5982\u679c vec1 \u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>n</cite> \u7684\u5411\u91cf, vec2 \u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>m</cite> \u7684\u5411\u91cf, \u90a3\u4e48 out \u5fc5\u987b\u662f\u4e00\u4e2a <cite>n x m</cite> \u7684\u77e9\u9635. Note \u8fd9\u4e2a\u51fd\u6570\u4e0d\u652f\u6301 broadcast . | Parameters: | vec1 ( Tensor ) \u2013 1D input vector vec2 ( Tensor ) \u2013 1D input vector out ( Tensor , optional ) \u2013 optional output matrix | | --- | --- | Example: >>> v1 = torch.arange(1, 5) >>> v2 = torch.arange(1, 4) >>> torch.ger(v1, v2) 1 2 3 2 4 6 3 6 9 4 8 12 [torch.FloatTensor of size 4x3] torch.gesv(B, A, out=None) -> (Tensor, Tensor) <cite>X, LU = torch.gesv(B, A)</cite> , \u8be5\u51fd\u6570\u8fd4\u56de\u7ebf\u6027\u7cfb\u7edf \\(AX = B\\) \u7684\u89e3. <cite>LU</cite> \u5305\u542b <cite>A</cite> \u7684 LU \u5206\u89e3\u56e0\u5b50 <cite>L</cite> \u548c <cite>U</cite>. A \u5fc5\u987b\u662f\u65b9\u9635, \u4e14\u662f\u975e\u5947\u5f02\u7684 (2\u7ef4\u53ef\u9006\u5f20\u91cf). \u5982\u679c <cite>A</cite> \u662f\u4e00\u4e2a <cite>m x m</cite> \u77e9\u9635, <cite>B</cite> \u662f\u4e00\u4e2a <cite>m x k</cite> \u7684\u77e9\u9635, \u90a3\u4e48\u7ed3\u679c <cite>LU</cite> \u7684\u5927\u5c0f\u4e3a <cite>m x m</cite>, <cite>X</cite> \u7684\u5927\u5c0f\u4e3a <cite>m x k</cite> . Note Irrespective of the original strides, the returned matrices <cite>X</cite> and <cite>LU</cite> will be transposed, i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite>. | Parameters: | B ( Tensor ) \u2013 input matrix of <cite>m x k</cite> dimensions A ( Tensor ) \u2013 input square matrix of <cite>m x m</cite> dimensions out ( Tensor , optional ) \u2013 optional output matrix | | --- | --- | Example: >>> A = torch.Tensor([[6.80, -2.11, 5.66, 5.97, 8.23], ... [-6.05, -3.30, 5.36, -4.44, 1.08], ... [-0.45, 2.58, -2.70, 0.27, 9.04], ... [8.32, 2.71, 4.35, -7.17, 2.14], ... [-9.67, -5.14, -7.26, 6.08, -6.87]]).t() >>> B = torch.Tensor([[4.02, 6.19, -8.22, -7.57, -3.03], ... [-1.56, 4.00, -8.67, 1.75, 2.86], ... [9.81, -4.09, -4.57, -8.61, 8.99]]).t() >>> X, LU = torch.gesv(B, A) >>> torch.dist(B, torch.mm(A, X)) 9.250057093890353e-06 torch.inverse(input, out=None) \u2192 Tensor \u8ba1\u7b97\u65b9\u9635 input \u7684\u9006. Note Irrespective of the original strides, the returned matrix will be transposed, i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite> | Parameters: | input ( Tensor ) \u2013 the input 2D square <cite>Tensor</cite> out ( Tensor , optional ) \u2013 the optional output <cite>Tensor</cite> | | --- | --- | Example: >>> x = torch.rand(10, 10) >>> x 0.7800 0.2267 0.7855 0.9479 0.5914 0.7119 0.4437 0.9131 0.1289 0.1982 0.0045 0.0425 0.2229 0.4626 0.6210 0.0207 0.6338 0.7067 0.6381 0.8196 0.8350 0.7810 0.8526 0.9364 0.7504 0.2737 0.0694 0.5899 0.8516 0.3883 0.6280 0.6016 0.5357 0.2936 0.7827 0.2772 0.0744 0.2627 0.6326 0.9153 0.7897 0.0226 0.3102 0.0198 0.9415 0.9896 0.3528 0.9397 0.2074 0.6980 0.5235 0.6119 0.6522 0.3399 0.3205 0.5555 0.8454 0.3792 0.4927 0.6086 0.1048 0.0328 0.5734 0.6318 0.9802 0.4458 0.0979 0.3320 0.3701 0.0909 0.2616 0.3485 0.4370 0.5620 0.5291 0.8295 0.7693 0.1807 0.0650 0.8497 0.1655 0.2192 0.6913 0.0093 0.0178 0.3064 0.6715 0.5101 0.2561 0.3396 0.4370 0.4695 0.8333 0.1180 0.4266 0.4161 0.0699 0.4263 0.8865 0.2578 [torch.FloatTensor of size 10x10] >>> x = torch.rand(10, 10) >>> y = torch.inverse(x) >>> z = torch.mm(x, y) >>> z 1.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 1.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 1.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 1.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 1.0000 0.0000 0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 1.0000 -0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 1.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 1.0000 -0.0000 0.0000 -0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 -0.0000 1.0000 -0.0000 -0.0000 0.0000 -0.0000 -0.0000 -0.0000 0.0000 -0.0000 -0.0000 0.0000 1.0000 [torch.FloatTensor of size 10x10] >>> torch.max(torch.abs(z - torch.eye(10))) # Max nonzero 5.096662789583206e-07 torch.matmul(tensor1, tensor2, out=None) Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). For example, if tensor1 is a <cite>j x 1 x n x m</cite> Tensor and tensor2 is a <cite>k x m x p</cite> Tensor, out will be an <cite>j x k x n x p</cite> Tensor. Note The 1-dimensional dot product version of this function does not support an out parameter. | Parameters: | tensor1 ( Tensor ) \u2013 First tensor to be multiplied tensor2 ( Tensor ) \u2013 Second tensor to be multiplied out ( Tensor , optional ) \u2013 Output tensor | | --- | --- | torch.mm(mat1, mat2, out=None) \u2192 Tensor \u6267\u884c mat1 \u548c mat2 \u7684\u77e9\u9635\u4e58\u6cd5. \u5982\u679c mat1 \u662f\u4e00\u4e2a <cite>n x m</cite> \u5f20\u91cf, mat2 \u662f\u4e00\u4e2a <cite>m x p</cite> \u5f20\u91cf, out \u5c06\u662f\u4e00\u4e2a <cite>n x p</cite> \u5f20\u91cf. Note \u8fd9\u4e2a\u51fd\u6570\u4e0d\u652f\u6301 broadcast . \u8981\u4f7f\u7528\u652f\u6301\u5e7f\u64ad\u77e9\u9635\u4e58\u6cd5, \u53c2\u89c1 torch.matmul() . | Parameters: | mat1 ( Tensor ) \u2013 First matrix to be multiplied mat2 ( Tensor ) \u2013 Second matrix to be multiplied out ( Tensor , optional ) \u2013 Output tensor | | --- | --- | Example: >>> mat1 = torch.randn(2, 3) >>> mat2 = torch.randn(3, 3) >>> torch.mm(mat1, mat2) 0.0519 -0.3304 1.2232 4.3910 -5.1498 2.7571 [torch.FloatTensor of size 2x3] torch.mv(mat, vec, out=None) \u2192 Tensor \u6267\u884c\u77e9\u9635 mat \u4e0e\u5411\u91cf vec \u7684\u4e58\u6cd5\u64cd\u4f5c. \u5982\u679c mat \u662f\u4e00\u4e2a <cite>n x m</cite> \u5f20\u91cf, vec \u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a <cite>m</cite> \u7684\u4e00\u7ef4\u5f20\u91cf, out \u5c06\u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a <cite>n</cite> \u7684\u5f20\u91cf. Note \u8fd9\u4e2a\u51fd\u6570\u4e0d\u652f\u6301 broadcast . | Parameters: | mat ( Tensor ) \u2013 matrix to be multiplied vec ( Tensor ) \u2013 vector to be multiplied out ( Tensor , optional ) \u2013 Output tensor | | --- | --- | Example: >>> mat = torch.randn(2, 3) >>> vec = torch.randn(3) >>> torch.mv(mat, vec) -2.0939 -2.2950 [torch.FloatTensor of size 2] torch.orgqr() torch.ormqr() torch.potrf(a, out=None) potrf(a, upper, out=None) \u8ba1\u7b97\u534a\u6b63\u5b9a\u77e9\u9635 a : \u7684 Cholesky \u5206\u89e3. \u8fd4\u56de\u7ed3\u679c <cite>u</cite>, \u82e5 <cite>upper</cite> \u8bbe\u4e3a True \u6216\u672a\u63d0\u4f9b\u65f6, <cite>u</cite> \u662f\u4e00\u4e2a\u4e0a\u4e09\u89d2\u77e9\u9635, \u4f7f\u5f97 \\(a = u^T u\\) \u6210\u7acb; \u82e5 <cite>upper</cite> \u8bbe\u4e3a False , <cite>u</cite> \u662f\u4e00\u4e2a\u4e0b\u4e09\u89d2\u77e9\u9635, \u4f7f\u5f97 \\(a = u u^T\\) \u6210\u7acb. | Parameters: | a ( Tensor ) \u2013 the input 2D <cite>Tensor</cite>, a symmetric positive semidefinite matrix upper ( bool , optional ) \u2013 Return upper (default) or lower triangular matrix out ( Tensor , optional ) \u2013 A Tensor for u | | --- | --- | Example: >>> a = torch.randn(3,3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> u = torch.potrf(a) >>> a 2.3563 3.2318 -0.9406 3.2318 4.9557 -2.1618 -0.9406 -2.1618 2.2443 [torch.FloatTensor of size 3x3] >>> u 1.5350 2.1054 -0.6127 0.0000 0.7233 -1.2053 0.0000 0.0000 0.6451 [torch.FloatTensor of size 3x3] >>> torch.mm(u.t(),u) 2.3563 3.2318 -0.9406 3.2318 4.9557 -2.1618 -0.9406 -2.1618 2.2443 [torch.FloatTensor of size 3x3] torch.potri(u, out=None) potri(u, upper, out=None) \u7ed9\u5b9a\u4e00\u4e2a\u534a\u6b63\u5b9a\u77e9\u9635\u7684 Cholesky \u5206\u89e3\u56e0\u5b50 u , \u8ba1\u7b97\u8be5\u534a\u6b63\u5b9a\u77e9\u9635\u7684\u9006. \u8fd4\u56de\u77e9\u9635 <cite>inv</cite>, \u82e5 <cite>upper</cite> \u8bbe\u4e3a True \u6216\u4e3a\u63d0\u4f9b, <cite>u</cite> \u662f\u4e00\u4e2a\u4e0a\u4e09\u89d2\u77e9\u9635, \u4f7f\u5f97 \\(inv = (u^T u)^{-1}\\) \u6210\u7acb; \u82e5 <cite>upper</cite> \u8bbe\u4e3a False , <cite>u</cite> \u662f\u4e00\u4e2a\u4e0b\u4e09\u89d2\u77e9\u9635, \u4f7f\u5f97 \\(inv = (u u^T)^{-1}\\) \u6210\u7acb. | Parameters: | u ( Tensor ) \u2013 the input 2D <cite>Tensor</cite>, a upper or lower triangular Cholesky factor upper ( bool , optional ) \u2013 Flag if upper (default) or lower triangular matrix out ( Tensor , optional ) \u2013 A Tensor for inv | | --- | --- | Example: >>> a = torch.randn(3,3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> u = torch.potrf(a) >>> a 2.3563 3.2318 -0.9406 3.2318 4.9557 -2.1618 -0.9406 -2.1618 2.2443 [torch.FloatTensor of size 3x3] >>> torch.potri(u) 12.5724 -10.1765 -4.5333 -10.1765 8.5852 4.0047 -4.5333 4.0047 2.4031 [torch.FloatTensor of size 3x3] >>> a.inverse() 12.5723 -10.1765 -4.5333 -10.1765 8.5852 4.0047 -4.5333 4.0047 2.4031 [torch.FloatTensor of size 3x3] torch.potrs(b, u, out=None) potrs(b, u, upper, out=None) Solves a linear system of equations with a positive semidefinite matrix to be inverted given its given a Cholesky factor matrix u : returns matrix <cite>c</cite> If <cite>upper</cite> is True or not provided, <cite>u</cite> is and upper triangular such that \\(c = (u^T u)^{-1} b\\) . If <cite>upper</cite> is False , <cite>u</cite> is and lower triangular such that \\(c = (u u^T)^{-1} b\\) . Note <cite>b</cite> is always a 2D <cite>Tensor</cite>, use <cite>b.unsqueeze(1)</cite> to convert a vector. | Parameters: | b ( Tensor ) \u2013 the right hand side 2D <cite>Tensor</cite> u ( Tensor ) \u2013 the input 2D <cite>Tensor</cite>, a upper or lower triangular Cholesky factor upper ( bool , optional ) \u2013 Return upper (default) or lower triangular matrix out ( Tensor , optional ) \u2013 A Tensor for c | | --- | --- | Example: >>> a = torch.randn(3,3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> u = torch.potrf(a) >>> a 2.3563 3.2318 -0.9406 3.2318 4.9557 -2.1618 -0.9406 -2.1618 2.2443 [torch.FloatTensor of size 3x3] >>> b = torch.randn(3,2) >>> b -0.3119 -1.8224 -0.2798 0.1789 -0.3735 1.7451 [torch.FloatTensor of size 3x2] >>> torch.potrs(b,u) 0.6187 -32.6438 -0.7234 27.0703 -0.6039 13.1717 [torch.FloatTensor of size 3x2] >>> torch.mm(a.inverse(),b) 0.6187 -32.6436 -0.7234 27.0702 -0.6039 13.1717 [torch.FloatTensor of size 3x2] torch.pstrf(a, out=None) pstrf(a, upper, out=None) Computes the pivoted Cholesky decomposition of a positive semidefinite matrix a : returns matrices <cite>u</cite> and <cite>piv</cite>. If <cite>upper</cite> is True or not provided, <cite>u</cite> is and upper triangular such that \\(a = p^T u^T u p\\) , with <cite>p</cite> the permutation given by <cite>piv</cite>. If <cite>upper</cite> is False , <cite>u</cite> is and lower triangular such that \\(a = p^T u u^T p\\) . | Parameters: | a ( Tensor ) \u2013 the input 2D <cite>Tensor</cite> upper ( bool , optional ) \u2013 Return upper (default) or lower triangular matrix out ( tuple , optional ) \u2013 A tuple of u and piv Tensors | | --- | --- | Example: >>> a = torch.randn(3,3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> a 5.4417 -2.5280 1.3643 -2.5280 2.9689 -2.1368 1.3643 -2.1368 4.6116 [torch.FloatTensor of size 3x3] >>> u,piv = torch.pstrf(a) >>> u 2.3328 0.5848 -1.0837 0.0000 2.0663 -0.7274 0.0000 0.0000 1.1249 [torch.FloatTensor of size 3x3] >>> piv 0 2 1 [torch.IntTensor of size 3] >>> p = torch.eye(3).index_select(0,piv.long()).index_select(0,piv.long()).t() # make pivot permutation >>> torch.mm(torch.mm(p.t(),torch.mm(u.t(),u)),p) # reconstruct 5.4417 1.3643 -2.5280 1.3643 4.6116 -2.1368 -2.5280 -2.1368 2.9689 [torch.FloatTensor of size 3x3] torch.qr(input, out=None) -> (Tensor, Tensor) \u8ba1\u7b97\u77e9\u9635 input \u7684 QR \u5206\u89e3. \u8fd4\u56de\u77e9\u9635 <cite>q</cite> \u548c <cite>r</cite> \u4f7f\u5f97 \\(x = q * r\\) , \u4e14 <cite>q</cite> \u662f\u4e00\u4e2a \u6b63\u4ea4\u77e9\u9635, <cite>r</cite> \u662f\u4e00\u4e2a\u4e0a\u4e09\u89d2\u77e9\u9635. This returns the thin (reduced) QR factorization. Note \u5982\u679c\u77e9\u9635 <cite>input</cite> \u4e2d\u7684\u5143\u7d20\u592a\u5927, \u90a3\u4e48\u7cbe\u5ea6\u53ef\u80fd\u4f1a\u4e22\u5931. Note \u5c3d\u7ba1\u8be5\u51fd\u6570\u603b\u662f\u80fd\u7ed9\u60a8\u4e00\u4e2a\u6709\u6548\u7684\u5206\u89e3, \u4f46\u5728\u4e0d\u540c\u5e73\u53f0\u4e0a\u7ed3\u679c\u53ef\u80fd\u4e0d\u540c - \u53d6\u51b3\u4e8e\u8be5\u5e73\u53f0\u4e0a LAPACK \u7684\u5b9e\u73b0. Note Irrespective of the original strides, the returned matrix <cite>q</cite> will be transposed, i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite>. | Parameters: | input ( Tensor ) \u2013 the input 2D <cite>Tensor</cite> out ( tuple , optional ) \u2013 A tuple of Q and R Tensors | | --- | --- | Example: >>> a = torch.Tensor([[12, -51, 4], [6, 167, -68], [-4, 24, -41]]) >>> q, r = torch.qr(a) >>> q -0.8571 0.3943 0.3314 -0.4286 -0.9029 -0.0343 0.2857 -0.1714 0.9429 [torch.FloatTensor of size 3x3] >>> r -14.0000 -21.0000 14.0000 0.0000 -175.0000 70.0000 0.0000 0.0000 -35.0000 [torch.FloatTensor of size 3x3] >>> torch.mm(q, r).round() 12 -51 4 6 167 -68 -4 24 -41 [torch.FloatTensor of size 3x3] >>> torch.mm(q.t(), q).round() 1 -0 0 -0 1 0 0 0 1 [torch.FloatTensor of size 3x3] torch.svd(input, some=True, out=None) -> (Tensor, Tensor, Tensor) <cite>U, S, V = torch.svd(A)</cite> \u8fd4\u56de\u5927\u5c0f\u4e3a <cite>(n x m)</cite> \u7684\u5b9e\u77e9\u9635 <cite>A</cite> \u7684\u5947\u5f02\u503c\u5206\u89e3, \u4f7f\u5f97 \\(A = USV'*\\) . <cite>U</cite> \u7684\u5927\u5c0f\u4e3a <cite>n x n</cite> <cite>S</cite> \u7684\u5927\u5c0f\u4e3a n x m <cite>V</cite> \u7684\u5927\u5c0f\u4e3a <cite>m x m</cite>. some \u8868\u793a\u5c06\u88ab\u8ba1\u7b97\u7684\u5947\u5f02\u503c\u7684\u603b\u6570. \u5982\u679c <cite>some=True</cite>, \u5b83\u5c06\u8ba1\u7b97\u6307\u5b9a\u7684 some \u6570\u91cf\u4e2a\u5947\u5f02\u503c, \u5982\u679c <cite>some=False</cite>, \u5219\u8ba1\u7b97\u6240\u6709\u5947\u5f02\u503c. Note Irrespective of the original strides, the returned matrix <cite>U</cite> will be transposed, i.e. with strides <cite>(1, n)</cite> instead of <cite>(n, 1)</cite>. | Parameters: | input ( Tensor ) \u2013 the input 2D Tensor some ( bool , optional ) \u2013 controls the number of singular values to be computed out ( tuple , optional ) \u2013 the result tuple | | --- | --- | Example: >>> a = torch.Tensor([[8.79, 6.11, -9.15, 9.57, -3.49, 9.84], ... [9.93, 6.91, -7.93, 1.64, 4.02, 0.15], ... [9.83, 5.04, 4.86, 8.83, 9.80, -8.99], ... [5.45, -0.27, 4.85, 0.74, 10.00, -6.02], ... [3.16, 7.98, 3.01, 5.80, 4.27, -5.31]]).t() >>> a 8.7900 9.9300 9.8300 5.4500 3.1600 6.1100 6.9100 5.0400 -0.2700 7.9800 -9.1500 -7.9300 4.8600 4.8500 3.0100 9.5700 1.6400 8.8300 0.7400 5.8000 -3.4900 4.0200 9.8000 10.0000 4.2700 9.8400 0.1500 -8.9900 -6.0200 -5.3100 [torch.FloatTensor of size 6x5] >>> u, s, v = torch.svd(a) >>> u -0.5911 0.2632 0.3554 0.3143 0.2299 -0.3976 0.2438 -0.2224 -0.7535 -0.3636 -0.0335 -0.6003 -0.4508 0.2334 -0.3055 -0.4297 0.2362 -0.6859 0.3319 0.1649 -0.4697 -0.3509 0.3874 0.1587 -0.5183 0.2934 0.5763 -0.0209 0.3791 -0.6526 [torch.FloatTensor of size 6x5] >>> s 27.4687 22.6432 8.5584 5.9857 2.0149 [torch.FloatTensor of size 5] >>> v -0.2514 0.8148 -0.2606 0.3967 -0.2180 -0.3968 0.3587 0.7008 -0.4507 0.1402 -0.6922 -0.2489 -0.2208 0.2513 0.5891 -0.3662 -0.3686 0.3859 0.4342 -0.6265 -0.4076 -0.0980 -0.4932 -0.6227 -0.4396 [torch.FloatTensor of size 5x5] >>> torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t())) 8.934150226306685e-06 torch.symeig(input, eigenvectors=False, upper=True, out=None) -> (Tensor, Tensor) <cite>e, V = torch.symeig(input)</cite> \u8fd4\u56de\u5b9e\u5bf9\u79f0\u77e9\u9635 input \u7684\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf. <cite>input</cite> \u548c <cite>V</cite> \u662f <cite>m x m</cite> \u77e9\u9635, <cite>e</cite> \u662f\u4e00\u4e2a <cite>m</cite> \u7ef4\u7684\u5411\u91cf. \u8fd9\u4e2a\u51fd\u6570\u8ba1\u7b97\u77e9\u9635 <cite>input</cite> \u7684\u6240\u6709\u7279\u5f81\u503c (\u548c\u5411\u91cf), \u4f7f\u5f97 <cite>input = V diag(e) V\u2019</cite>. \u5e03\u5c14\u53c2\u6570 eigenvectors \u5b9a\u4e49\u4e86\u662f\u5426\u8ba1\u7b97\u7279\u5f81\u5411\u91cf. \u5982\u679c\u5b83\u4e3a False , \u90a3\u4e48\u53ea\u6709\u7279\u5f81\u503c\u4f1a\u88ab\u8ba1\u7b97. \u5982\u679c\u5b83\u4e3a True , \u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf\u90fd\u4f1a\u88ab\u8ba1\u7b97. \u7531\u4e8e\u8f93\u5165\u77e9\u9635 <cite>input</cite> \u88ab\u5047\u5b9a\u662f\u5bf9\u79f0\u7684, \u56e0\u6b64\u9ed8\u8ba4\u5730\u53ea\u6709\u5b83\u7684\u4e0a\u4e09\u89d2\u90e8\u5206\u4f1a\u88ab\u4f7f\u7528. \u5982\u679c upper \u662f False , \u90a3\u4e48\u5b83\u7684\u4e0b\u4e09\u89d2\u90e8\u5206\u4f1a\u88ab\u4f7f\u7528. Note: Irrespective of the original strides, the returned matrix <cite>V</cite> will be transposed, i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite>. | Parameters: | input ( Tensor ) \u2013 the input symmetric matrix eigenvectors ( boolean__, optional ) \u2013 controls whether eigenvectors have to be computed upper ( boolean__, optional ) \u2013 controls whether to consider upper-triangular or lower-triangular region out ( tuple , optional ) \u2013 The result tuple of (Tensor, Tensor) | | --- | --- | Examples: >>> a = torch.Tensor([[ 1.96, 0.00, 0.00, 0.00, 0.00], ... [-6.49, 3.80, 0.00, 0.00, 0.00], ... [-0.47, -6.39, 4.17, 0.00, 0.00], ... [-7.20, 1.50, -1.51, 5.70, 0.00], ... [-0.65, -6.34, 2.67, 1.80, -7.10]]).t() >>> e, v = torch.symeig(a, eigenvectors=True) >>> e -11.0656 -6.2287 0.8640 8.8655 16.0948 [torch.FloatTensor of size 5] >>> v -0.2981 -0.6075 0.4026 -0.3745 0.4896 -0.5078 -0.2880 -0.4066 -0.3572 -0.6053 -0.0816 -0.3843 -0.6600 0.5008 0.3991 -0.0036 -0.4467 0.4553 0.6204 -0.4564 -0.8041 0.4480 0.1725 0.3108 0.1622 [torch.FloatTensor of size 5x5] torch.trtrs()","title":"BLAS and LAPACK Operations (BLAS\u548cLAPACK\u64cd\u4f5c)"},{"location":"54/","text":"torch.Tensor torch.Tensor \u662f\u4e00\u79cd\u5305\u542b\u5355\u4e00\u6570\u636e\u7c7b\u578b\u5143\u7d20\u7684\u591a\u7ef4\u77e9\u9635. Torch \u5b9a\u4e49\u4e86\u4e03\u79cd CPU tensor \u7c7b\u578b\u548c\u516b\u79cd GPU tensor \u7c7b\u578b: Data type CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point torch.HalfTensor torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.Tensor \u662f\u9ed8\u8ba4\u7684 tensor \u7c7b\u578b( torch.FloatTensor )\u7684\u7b80\u79f0. \u4e00\u4e2a tensor \u5bf9\u8c61\u53ef\u4ee5\u4ece Python \u7684 list \u6216\u8005\u5e8f\u5217(sequence)\u6784\u5efa: >>> torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) 1 2 3 4 5 6 [torch.FloatTensor of size 2x3] \u4e00\u4e2a\u7a7a\u7684 tensor \u5bf9\u8c61\u53ef\u4ee5\u901a\u8fc7\u6240\u6307\u5b9a\u7684\u5927\u5c0f\u6765\u6784\u5efa: >>> torch.IntTensor(2, 4).zero_() 0 0 0 0 0 0 0 0 [torch.IntTensor of size 2x4] \u53ef\u4ee5\u901a\u8fc7 Python \u7684\u7d22\u5f15\u548c\u5207\u7247\u65b9\u5f0f\u6765\u83b7\u53d6\u6216\u4fee\u6539 tensor \u5bf9\u8c61\u7684\u5185\u5bb9: >>> x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) >>> print(x[1][2]) 6.0 >>> x[0][1] = 8 >>> print(x) 1 8 3 4 5 6 [torch.FloatTensor of size 2x3] \u6bcf\u4e00\u4e2a tensor \u5bf9\u8c61\u90fd\u6709\u4e00\u4e2a\u76f8\u5e94\u7684 torch.Storage \u7528\u6765\u4fdd\u5b58\u6570\u636e. tensor \u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b58\u50a8\u7684\u591a\u7ef4\u7684, \u6709 \u8de8\u5ea6(strided) \u7684\u89c6\u56fe, \u5e76\u4e14\u5728\u89c6\u56fe\u4e0a\u5b9a\u4e49\u4e86\u6570\u503c\u8fd0\u7b97. Note \u4f1a\u6539\u53d8 tensor \u5bf9\u8c61\u7684\u51fd\u6570\u65b9\u6cd5\u540d, \u5176\u4f7f\u7528\u4e86\u4e00\u4e2a\u4e0b\u5212\u7ebf\u540e\u7f00\u4f5c\u4e3a\u6807\u8bc6. \u6bd4\u5982, torch.FloatTensor.abs_() \u4f1a\u5728\u539f\u5730(in-place)\u8ba1\u7b97\u7edd\u5bf9\u503c\u5e76\u8fd4\u56de\u6539\u53d8\u540e\u7684 tensor. \u800c torch.FloatTensor.abs() \u4f1a\u5728\u4e00\u4e2a\u65b0\u5efa\u7684 tensor \u4e2d\u8ba1\u7b97\u7ed3\u679c. class torch.Tensor class torch.Tensor(*sizes) class torch.Tensor(size) class torch.Tensor(sequence) class torch.Tensor(ndarray) class torch.Tensor(tensor) class torch.Tensor(storage) \u53ef\u4ee5\u901a\u8fc7\u63d0\u4f9b\u5927\u5c0f\u6216\u8005\u6570\u636e\u6765\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 tensor \u5bf9\u8c61. \u5982\u679c\u6ca1\u6709\u63d0\u4f9b\u53c2\u6570, \u5c06\u8fd4\u56de\u4e00\u4e2a\u7a7a\u7684\u96f6\u7ef4\u7684 tensor. \u5982\u679c\u63d0\u4f9b\u4e86 numpy.ndarray , torch.Tensor , \u6216\u8005 torch.Storage \u4f5c\u4e3a\u53c2\u6570, \u5176\u5c06\u8fd4\u56de\u4e00\u4e2a\u4e0e\u53c2\u6570\u5171\u4eab\u6570\u636e\u7684 tensor \u5bf9\u8c61. \u5982\u679c\u63d0\u4f9b\u4e00\u4e2a Python \u5e8f\u5217(sequence)\u4f5c\u4e3a\u53c2\u6570, \u5c06\u8fd4\u56de\u4ece\u5e8f\u5217\u7684\u526f\u672c\u4e2d\u521b\u5efa\u7684\u4e00\u4e2a\u65b0\u7684 tensor \u5bf9\u8c61. abs() \u2192 Tensor \u8bf7\u67e5\u770b torch.abs() abs_() \u2192 Tensor abs() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f acos() \u2192 Tensor \u8bf7\u67e5\u770b torch.acos() acos_() \u2192 Tensor acos() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f add(value) \u8bf7\u67e5\u770b torch.add() add_(value) add() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f addbmm(beta=1, mat, alpha=1, batch1, batch2) \u2192 Tensor \u8bf7\u67e5\u770b torch.addbmm() addbmm_(beta=1, mat, alpha=1, batch1, batch2) \u2192 Tensor addbmm() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f addcdiv(value=1, tensor1, tensor2) \u2192 Tensor \u8bf7\u67e5\u770b torch.addcdiv() addcdiv_(value=1, tensor1, tensor2) \u2192 Tensor addcdiv() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f addcmul(value=1, tensor1, tensor2) \u2192 Tensor \u8bf7\u67e5\u770b torch.addcmul() addcmul_(value=1, tensor1, tensor2) \u2192 Tensor addcmul() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f addmm(beta=1, mat, alpha=1, mat1, mat2) \u2192 Tensor \u8bf7\u67e5\u770b torch.addmm() addmm_(beta=1, mat, alpha=1, mat1, mat2) \u2192 Tensor addmm() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f addmv(beta=1, tensor, alpha=1, mat, vec) \u2192 Tensor \u8bf7\u67e5\u770b torch.addmv() addmv_(beta=1, tensor, alpha=1, mat, vec) \u2192 Tensor addmv() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f addr(beta=1, alpha=1, vec1, vec2) \u2192 Tensor \u8bf7\u67e5\u770b torch.addr() addr_(beta=1, alpha=1, vec1, vec2) \u2192 Tensor addr() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f apply_(callable) \u2192 Tensor \u5c06\u51fd\u6570 callable \u4f5c\u7528\u4e8e tensor \u7684\u6bcf\u4e00\u4e2a\u5143\u7d20, \u5e76\u5c06\u6bcf\u4e2a\u5143\u7d20\u7528 callable \u7684\u8fd4\u56de\u503c\u66ff\u6362. Note \u8be5\u51fd\u6570\u53ea\u80fd\u5728 CPU tensor \u4e2d\u4f7f\u7528, \u5e76\u4e14\u4e0d\u5e94\u8be5\u7528\u5728\u6709\u8f83\u9ad8\u6027\u80fd\u7684\u8981\u6c42\u7684\u4ee3\u7801\u5757\u4e2d. asin() \u2192 Tensor \u8bf7\u67e5\u770b torch.asin() asin_() \u2192 Tensor asin() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f atan() \u2192 Tensor \u8bf7\u67e5\u770b torch.atan() atan2(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.atan2() atan2_(other) \u2192 Tensor atan2() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f atan_() \u2192 Tensor atan() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f baddbmm(beta=1, alpha=1, batch1, batch2) \u2192 Tensor \u8bf7\u67e5\u770b torch.baddbmm() baddbmm_(beta=1, alpha=1, batch1, batch2) \u2192 Tensor baddbmm() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f bernoulli() \u2192 Tensor \u8bf7\u67e5\u770b torch.bernoulli() bernoulli_() \u2192 Tensor bernoulli() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f bmm(batch2) \u2192 Tensor \u8bf7\u67e5\u770b torch.bmm() byte() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a byte \u7c7b\u578b cauchy_(median=0, sigma=1, *, generator=None) \u2192 Tensor \u7528\u67ef\u897f\u5206\u5e03\u5f97\u5230\u7684\u6570\u503c\u6765\u586b\u5145 tensor \u4e2d\u7684\u5143\u7d20: \\[P(x) = \\dfrac{1}{\\pi} \\dfrac{\\sigma}{(x - median)^2 + \\sigma^2}\\] ceil() \u2192 Tensor \u8bf7\u67e5\u770b torch.ceil() ceil_() \u2192 Tensor ceil() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f char() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a char \u7c7b\u578b chunk(n_chunks, dim=0) \u5c06 tensor \u5206\u89e3\u6210 tensor \u5143\u7ec4. \u8bf7\u67e5\u770b torch.chunk() . clamp(min, max) \u2192 Tensor \u8bf7\u67e5\u770b torch.clamp() clamp_(min, max) \u2192 Tensor clamp() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f clone() \u2192 Tensor \u8fd4\u56de\u4e0e\u539f tensor \u5177\u6709\u76f8\u540c\u5927\u5c0f\u548c\u6570\u636e\u7c7b\u578b\u7684 tensor. contiguous() \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5185\u5b58\u8fde\u7eed\u7684\u6709\u76f8\u540c\u6570\u636e\u7684 tensor, \u5982\u679c\u539f tensor \u5185\u5b58\u8fde\u7eed\u5219\u8fd4\u56de\u539f tensor. copy_(src, async=False, broadcast=True) \u2192 Tensor \u5c06 src \u4e2d\u7684\u5143\u7d20\u590d\u5236\u5230\u8fd9\u4e2a tensor \u4e2d\u5e76\u8fd4\u56de\u8fd9\u4e2a tensor \u5982\u679c broadcast \u662f True, \u6e90 tensor \u4e00\u5b9a\u548c\u8fd9\u4e2a tensor broadcastable . \u53e6\u5916, \u6e90 tensor \u7684\u5143\u7d20\u6570\u91cf\u5e94\u8be5\u548c\u8fd9\u4e2a tensor \u7684\u5143\u7d20\u4e2a\u6570\u4e00\u81f4. \u6e90 tensor \u53ef\u4ee5\u662f\u53e6\u4e00\u79cd\u6570\u636e\u7c7b\u578b, \u6216\u8005\u5728\u522b\u7684\u7684\u8bbe\u5907\u4e0a. | Parameters: | src ( Tensor ) \u2013 \u88ab\u590d\u5236\u7684\u6e90 tensor async ( bool ) \u2013 \u5982\u679c\u503c\u4e3a True \u5e76\u4e14\u8fd9\u4e2a\u590d\u5236\u64cd\u4f5c\u5728 CPU \u548c GPU \u4e4b\u95f4\u8fdb\u884c, \u5219\u62f7\u8d1d\u7684\u526f\u672c\u4e0e\u6e90\u4fe1\u606f\u53ef\u80fd\u4f1a\u51fa\u73b0\u5f02\u6b65(asynchronously). \u5bf9\u4e8e\u5176\u4ed6\u7c7b\u578b\u7684\u590d\u5236\u64cd\u4f5c, \u8fd9\u4e2a\u53c2\u6570\u4e0d\u8d77\u4f5c\u7528. broadcast ( bool ) \u2013 \u5982\u679c\u503c\u4e3a True , src \u5c06\u5e7f\u64ad\u57fa\u7840\u7684 tensor \u7684\u5f62\u72b6. | | --- | --- | cos() \u2192 Tensor \u8bf7\u67e5\u770b torch.cos() cos_() \u2192 Tensor cos() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f cosh() \u2192 Tensor \u8bf7\u67e5\u770b torch.cosh() cosh_() \u2192 Tensor cosh() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f cpu() \u5982\u679c\u6b64 tensor \u4e0d\u5728CPU\u4e0a, \u5219\u8fd4\u56de\u6b64 tensor \u7684CPU\u526f\u672c cross(other, dim=-1) \u2192 Tensor \u8bf7\u67e5\u770b torch.cross() cuda(device=None, async=False) \u8fd4\u56de\u6b64\u5bf9\u8c61\u5728 CUDA \u5185\u5b58\u4e2d\u7684\u4e00\u4e2a\u526f\u672c . \u5982\u679c\u6b64\u5bf9\u8c61\u5df2\u7ecf\u5728 CUDA \u5185\u5b58\u4e2d\u5e76\u4e14\u5728\u6b63\u786e\u7684\u8bbe\u5907\u4e0a , \u90a3\u4e48\u4e0d\u4f1a\u6267\u884c\u590d\u5236\u64cd\u4f5c , \u76f4\u63a5\u8fd4\u56de\u539f\u5bf9\u8c61 . | Parameters: | device ( int ) \u2013 \u76ee\u6807 GPU \u7684 id . \u9ed8\u8ba4\u503c\u662f\u5f53\u524d\u8bbe\u5907 . async ( bool ) \u2013 \u5982\u679c\u4e3a True \u5e76\u4e14\u6e90\u4f4d\u4e8e\u9501\u5b9a\u5185\u5b58\u4e2d , \u5219\u526f\u672c\u76f8\u5bf9\u4e8e\u4e3b\u673a\u662f\u5f02\u6b65\u7684 . \u5426\u5219\u6b64\u53c2\u6570\u4e0d\u8d77\u6548\u679c . | | --- | --- | cumprod(dim) \u2192 Tensor \u8bf7\u67e5\u770b torch.cumprod() cumsum(dim) \u2192 Tensor \u8bf7\u67e5\u770b torch.cumsum() data_ptr() \u2192 int \u8fd4\u56de tensor \u7b2c\u4e00\u4e2a\u5143\u7d20\u7684\u5730\u5740. diag(diagonal=0) \u2192 Tensor \u8bf7\u67e5\u770b torch.diag() dim() \u2192 int \u8fd4\u56de tensor \u7684\u7ef4\u6570. dist(other, p=2) \u2192 float \u8bf7\u67e5\u770b torch.dist() div(value) \u8bf7\u67e5\u770b torch.div() div_(value) div() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f dot(tensor2) \u2192 float \u8bf7\u67e5\u770b torch.dot() double() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a double \u7c7b\u578b eig(eigenvectors=False) -> (Tensor, Tensor) \u8bf7\u67e5\u770b torch.eig() element_size() \u2192 int \u8fd4\u56de\u5355\u4e2a\u5143\u7d20\u7684\u5b57\u8282\u5927\u5c0f. Example >>> torch.FloatTensor().element_size() 4 >>> torch.ByteTensor().element_size() 1 eq(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.eq() eq_(other) \u2192 Tensor eq() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f equal(other) \u2192 bool \u8bf7\u67e5\u770b torch.equal() erf() \u2192 Tensor \u8bf7\u67e5\u770b torch.erf() erf_() erfinv() \u2192 Tensor \u8bf7\u67e5\u770b torch.erfinv() erfinv_() exp() \u2192 Tensor \u8bf7\u67e5\u770b torch.exp() exp_() \u2192 Tensor exp() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f expand(*sizes) \u2192 Tensor \u8fd4\u56de tensor \u5355\u4e2a\u7ef4\u5ea6\u6269\u5c55\u5230\u5927\u7684\u4e00\u4e2a\u65b0\u7684\u89c6\u56fe. \u4f20\u9012 -1 \u4f5c\u4e3a\u4e00\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f, \u8868\u793a\u8fd9\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f\u4e0d\u505a\u6539\u53d8. Tensor \u4e5f\u53ef\u4ee5\u6269\u5c55\u5230\u4e00\u4e2a\u5f88\u5927\u7684\u7ef4\u6570, \u65b0\u6dfb\u52a0\u7684\u7ef4\u5ea6\u5c06\u653e\u5728\u524d\u9762. (\u5bf9\u4e8e\u65b0\u7684\u7ef4\u5ea6, \u5927\u5c0f\u4e0d\u80fd\u8bbe\u7f6e\u4e3a -1 .) \u6269\u5c55\u4e00\u4e2a tensor \u4e0d\u662f\u5206\u914d\u4e00\u4e2a\u65b0\u7684\u5185\u5b58, \u800c\u53ea\u662f\u5728\u8fd9\u4e2a\u5b58\u5728\u7684 tensor \u4e0a, \u901a\u8fc7\u8bbe\u7f6e stride \u4e3a 0, \u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u67d0\u4e2a\u7ef4\u5ea6\u4ece 1 \u6269\u5c55\u5230\u5f88\u5927\u7684\u89c6\u56fe. \u4efb\u4f55\u5927\u5c0f\u4e3a 1 \u7684\u7ef4\u5ea6, \u5728\u4e0d\u7528\u91cd\u65b0\u5206\u914d\u5185\u5b58\u7684\u60c5\u51b5\u4e0b, \u53ef\u4ee5\u6269\u5c55\u5230\u968f\u610f\u4efb\u4f55\u4e00\u4e2a\u503c. Parameters: *sizes ( torch.Size or int... ) \u2013 \u671f\u671b\u6269\u5c55\u7684\u5927\u5c0f Example >>> x = torch.Tensor([[1], [2], [3]]) >>> x.size() torch.Size([3, 1]) >>> x.expand(3, 4) 1 1 1 1 2 2 2 2 3 3 3 3 [torch.FloatTensor of size 3x4] >>> x.expand(-1, 4) # -1 means not changing the size of that dimension 1 1 1 1 2 2 2 2 3 3 3 3 [torch.FloatTensor of size 3x4] expand_as(tensor) \u5c06\u6b64 tensor \u5c55\u5f00\u4e3a\u6307\u5b9a tensor \u7684\u5927\u5c0f. \u8fd9\u76f8\u5f53\u4e8e: self.expand(tensor.size()) exponential_(lambd=1, *, generator=None) \u2192 Tensor \u5c06\u8be5 tensor \u7528\u6307\u6570\u5206\u5e03\u5f97\u5230\u7684\u5143\u7d20\u586b\u5145: \\[P(x) = \\lambda e^{-\\lambda x}\\] fill_(value) \u2192 Tensor \u5c06\u8be5 tensor \u7528\u6307\u5b9a\u7684\u6570\u503c\u586b\u5145. float() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a float \u7c7b\u578b floor() \u2192 Tensor \u8bf7\u67e5\u770b torch.floor() floor_() \u2192 Tensor floor() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f fmod(divisor) \u2192 Tensor \u8bf7\u67e5\u770b torch.fmod() fmod_(divisor) \u2192 Tensor fmod() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f frac() \u2192 Tensor \u8bf7\u67e5\u770b torch.frac() frac_() \u2192 Tensor frac() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f gather(dim, index) \u2192 Tensor \u8bf7\u67e5\u770b torch.gather() ge(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.ge() ge_(other) \u2192 Tensor ge() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f gels(A) \u2192 Tensor \u8bf7\u67e5\u770b torch.gels() geometric_(p, *, generator=None) \u2192 Tensor \u5c06\u8be5 tensor \u7528\u51e0\u4f55\u5206\u5e03\u5f97\u5230\u7684\u5143\u7d20\u586b\u5145: \\[P(X=k) = (1 - p)^{k - 1} p\\] geqrf() -> (Tensor, Tensor) \u8bf7\u67e5\u770b torch.geqrf() ger(vec2) \u2192 Tensor \u8bf7\u67e5\u770b torch.ger() gesv(A) \u2192 Tensor, Tensor \u8bf7\u67e5\u770b torch.gesv() gt(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.gt() gt_(other) \u2192 Tensor gt() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f half() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a half-precision float \u7c7b\u578b histc(bins=100, min=0, max=0) \u2192 Tensor \u8bf7\u67e5\u770b torch.histc() index(m) \u2192 Tensor \u7528\u4e00\u4e2a\u4e8c\u8fdb\u5236\u7684\u63a9\u7801\u6216\u6cbf\u7740\u4e00\u4e2a\u7ed9\u5b9a\u7684\u7ef4\u5ea6\u4ece tensor \u4e2d\u9009\u53d6\u5143\u7d20. tensor.index(m) \u7b49\u540c\u4e8e tensor[m] . Parameters: m ( int or ByteTensor or slice ) \u2013 \u7528\u6765\u9009\u53d6\u5143\u7d20\u7684\u7ef4\u5ea6\u6216\u63a9\u7801 index_add_(dim, index, tensor) \u2192 Tensor \u6309\u53c2\u6570 index \u7ed9\u51fa\u7684\u7d22\u5f15\u5e8f\u5217, \u5c06\u53c2\u6570 tensor \u4e2d\u7684\u5143\u7d20\u52a0\u5230\u539f\u6765\u7684 tensor \u4e2d. \u53c2\u6570 tensor \u7684\u5c3a\u5bf8\u5fc5\u987b\u4e25\u683c\u5730\u4e0e\u539f tensor \u5339\u914d, \u5426\u5219\u4f1a\u53d1\u751f\u9519\u8bef. | Parameters: | dim ( int ) \u2013 \u7d22\u5f15 index \u6240\u6307\u5411\u7684\u7ef4\u5ea6 index ( LongTensor ) \u2013 \u4ece\u53c2\u6570 tensor \u4e2d\u9009\u53d6\u6570\u636e\u7684\u7d22\u5f15\u5e8f\u5217 tensor ( Tensor ) \u2013 \u5305\u542b\u9700\u8981\u76f8\u52a0\u7684\u5143\u7d20\u7684 tensor | | --- | --- | Example >>> x = torch.Tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]]) >>> t = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> index = torch.LongTensor([0, 2, 1]) >>> x.index_add_(0, index, t) >>> x 2 3 4 8 9 10 5 6 7 [torch.FloatTensor of size 3x3] index_copy_(dim, index, tensor) \u2192 Tensor \u6309\u53c2\u6570 index \u7ed9\u51fa\u7684\u7d22\u5f15\u5e8f\u5217, \u5c06\u53c2\u6570 tensor \u4e2d\u7684\u5143\u7d20\u590d\u5236\u5230\u539f\u6765\u7684 tensor \u4e2d. \u53c2\u6570 tensor \u7684\u5c3a\u5bf8\u5fc5\u987b\u4e25\u683c\u5730\u4e0e\u539f tensor \u5339\u914d, \u5426\u5219\u4f1a\u53d1\u751f\u9519\u8bef. | Parameters: | dim ( int ) \u2013 \u7d22\u5f15 index \u6240\u6307\u5411\u7684\u7ef4\u5ea6 index ( LongTensor ) \u2013 \u4ece\u53c2\u6570 tensor \u4e2d\u9009\u53d6\u6570\u636e\u7684\u7d22\u5f15\u5e8f\u5217 tensor ( Tensor ) \u2013 \u5305\u542b\u9700\u8981\u590d\u5236\u7684\u5143\u7d20\u7684 tensor | | --- | --- | Example >>> x = torch.Tensor(3, 3) >>> t = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> index = torch.LongTensor([0, 2, 1]) >>> x.index_copy_(0, index, t) >>> x 1 2 3 7 8 9 4 5 6 [torch.FloatTensor of size 3x3] index_fill_(dim, index, val) \u2192 Tensor \u6309\u53c2\u6570 index \u7ed9\u51fa\u7684\u7d22\u5f15\u5e8f\u5217, \u5c06\u539f tensor \u4e2d\u7684\u5143\u7d20\u7528 val \u586b\u5145. | Parameters: | dim ( int ) \u2013 \u7d22\u5f15 index \u6240\u6307\u5411\u7684\u7ef4\u5ea6 index ( LongTensor ) \u2013 \u4ece\u53c2\u6570 val \u4e2d\u9009\u53d6\u6570\u636e\u7684\u7d22\u5f15\u5e8f\u5217 val ( float ) \u2013 \u7528\u6765\u586b\u5145\u7684\u503c | | --- | --- | Example >>> x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> index = torch.LongTensor([0, 2]) >>> x.index_fill_(1, index, -1) >>> x -1 2 -1 -1 5 -1 -1 8 -1 [torch.FloatTensor of size 3x3] index_select(dim, index) \u2192 Tensor \u8bf7\u67e5\u770b torch.index_select() int() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a int \u7c7b\u578b inverse() \u2192 Tensor \u8bf7\u67e5\u770b torch.inverse() is_contiguous() \u2192 bool \u4ee5 C \u8bed\u8a00\u7684\u5185\u5b58\u6a21\u578b\u4e3a\u539f\u5219, \u5982\u679c\u8be5 tensor \u5728\u5185\u5982\u679c\u8be5 tensor \u5728\u5185\u5b58\u4e2d\u8fde\u7eed\u7684, \u5219\u8fd4\u56de True. is_cuda is_pinned() \u5982\u679c tensor \u9a7b\u7559\u5728\u56fa\u5b9a\u5185\u5b58\u4e2d, \u5219\u8fd4\u56de true is_set_to(tensor) \u2192 bool \u5982\u679c\u6b64\u5bf9\u8c61\u4ece Torch C API \u5f15\u7528\u7684 THTensor \u5bf9\u8c61\u4e0e\u53c2\u6570 tensor \u5f15\u7528\u7684\u5bf9\u8c61\u4e00\u81f4, \u5219\u8fd4\u56deTrue. is_signed() kthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor) \u8bf7\u67e5\u770b torch.kthvalue() le(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.le() le_(other) \u2192 Tensor le() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f lerp(start, end, weight) \u8bf7\u67e5\u770b torch.lerp() lerp_(start, end, weight) lerp() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f log() \u2192 Tensor \u8bf7\u67e5\u770b torch.log() log1p() \u2192 Tensor \u8bf7\u67e5\u770b torch.log1p() log1p_() \u2192 Tensor log1p() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f log_() \u2192 Tensor log() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f log_normal_(mean=1, std=2, *, generator=None) \u5c06\u8be5 tensor \u7528\u5747\u503c\u4e3a mean (\u00b5), \u6807\u51c6\u5dee\u4e3a std (\u03c3) \u7684\u5bf9\u6570\u6b63\u6001\u5206\u5e03\u5f97\u5230\u7684\u5143\u7d20\u586b\u5145. \u8981\u6ce8\u610f mean \u548c stdv \u662f\u57fa\u672c\u6b63\u6001\u5206\u5e03\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee, \u4e0d\u662f\u8fd4\u56de\u7684\u5206\u5e03: \\[P(x) = \\dfrac{1}{x \\sigma \\sqrt{2\\pi}} e^{-\\dfrac{(\\ln x - \\mu)^2}{2\\sigma^2}}\\] long() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a long \u7c7b\u578b lt(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.lt() lt_(other) \u2192 Tensor lt() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f map_(tensor, callable) \u5c06 callable \u4f5c\u7528\u4e8e\u672c tensor \u548c\u53c2\u6570 tensor \u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20, \u5e76\u5c06\u7ed3\u679c\u5b58\u653e\u5728\u672c tensor \u4e2d. \u672c tensor \u548c\u53c2\u6570 tensor \u90fd\u5fc5\u987b\u662f broadcastable . callable \u5e94\u8be5\u6709\u4e0b\u5217\u6807\u5fd7: ```py def callable(a, b) -> number ``` masked_scatter_(mask, source) \u590d\u5236 source \u7684\u5143\u7d20\u5230\u672c tensor \u88ab:attr: mask \u4e2d\u503c\u4e3a 1 \u7684\u5143\u7d20\u6807\u8bb0\u7684\u4f4d\u7f6e\u4e2d. mask \u7684\u5f62\u72b6\u548c\u672c tensor \u7684\u5f62\u72b6\u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u7684 ( broadcastable ). source \u4e2d\u5143\u7d20\u7684\u4e2a\u6570\u6700\u5c11\u4e3a mask \u4e2d\u503c\u4e3a1\u7684\u5143\u7d20\u7684\u4e2a\u6570. | Parameters: | mask ( ByteTensor ) \u2013 \u4e8c\u8fdb\u5236\u63a9\u7801 source ( Tensor ) \u2013 \u590d\u5236\u7684\u6e90 tensor | | --- | --- | Note mask \u4f5c\u7528\u4e8e self \u81ea\u8eab\u7684 tensor, \u800c\u4e0d\u662f\u53c2\u6570 source \u7684 tensor. masked_fill_(mask, value) \u5c06\u672c tensor \u88ab mask \u4e2d\u503c\u4e3a 1 \u7684\u5143\u7d20\u6807\u8bb0\u7684\u4f4d\u7f6e, \u7528 value \u586b\u5145. mask \u7684\u5f62\u72b6\u548c\u672c tensor \u7684\u5f62\u72b6\u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u7684 ( broadcastable ). Fills elements of this tensor with value where mask is one. | Parameters: | mask ( ByteTensor ) \u2013 \u4e8c\u8fdb\u5236\u63a9\u7801 value ( float ) \u2013 \u7528\u6765\u586b\u5145\u7684\u503c | | --- | --- | masked_select(mask) \u2192 Tensor \u8bf7\u67e5\u770b torch.masked_select() matmul(other) \u4e24\u4e2a tensor \u7684\u77e9\u9635\u4e58\u79ef. \u8bf7\u67e5\u770b torch.matmul() . max(dim=None, keepdim=False) -> float or (Tensor, Tensor) \u8bf7\u67e5\u770b torch.max() mean(dim=None, keepdim=False) -> float or (Tensor, Tensor) \u8bf7\u67e5\u770b torch.mean() median(dim=None, keepdim=False) -> (Tensor, LongTensor) \u8bf7\u67e5\u770b torch.median() min(dim=None, keepdim=False) -> float or (Tensor, Tensor) \u8bf7\u67e5\u770b torch.min() mm(mat2) \u2192 Tensor \u8bf7\u67e5\u770b torch.mm() mode(dim=None, keepdim=False) -> (Tensor, LongTensor) \u8bf7\u67e5\u770b torch.mode() mul(value) \u2192 Tensor \u8bf7\u67e5\u770b torch.mul() mul_(value) mul() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f multinomial(num_samples, replacement=False, *, generator=None) \u8bf7\u67e5\u770b torch.multinomial() mv(vec) \u2192 Tensor \u8bf7\u67e5\u770b torch.mv() narrow(dimension, start, length) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u672c tensor \u7ecf\u8fc7\u7f29\u5c0f\u540e\u7684 tensor. \u7ef4\u5ea6 dim \u7f29\u5c0f\u8303\u56f4\u662f start \u5230 start + length . \u539f tensor \u4e0e\u8fd4\u56de\u7684 tensor \u5171\u4eab\u76f8\u540c\u7684\u5e95\u5c42\u5b58\u50a8. | Parameters: | dimension ( int ) \u2013 \u9700\u8981\u7f29\u5c0f\u7684\u7ef4\u5ea6 start ( int ) \u2013 \u8d77\u59cb\u7ef4\u5ea6 length ( int ) \u2013 | | --- | --- | Example >>> x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> x.narrow(0, 0, 2) 1 2 3 4 5 6 [torch.FloatTensor of size 2x3] >>> x.narrow(1, 1, 2) 2 3 5 6 8 9 [torch.FloatTensor of size 3x2] ndimension() \u2192 int dim() \u7684\u53e6\u4e00\u79cd\u8868\u793a ne(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.ne() ne_(other) \u2192 Tensor ne() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f neg() \u2192 Tensor \u8bf7\u67e5\u770b torch.neg() neg_() \u2192 Tensor neg() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f nelement() \u2192 int numel() \u7684\u53e6\u4e00\u79cd\u8868\u793a new(*args, **kwargs) \u6784\u9020\u76f8\u540c\u6570\u636e\u7c7b\u578b\u7684\u65b0 tensor. nonzero() \u2192 LongTensor \u8bf7\u67e5\u770b torch.nonzero() norm(p=2, dim=None, keepdim=False) \u2192 float \u8bf7\u67e5\u770b torch.norm() normal_(mean=0, std=1, *, generator=None) \u5c06 tensor \u7528\u5747\u503c\u4e3a mean \u548c\u6807\u51c6\u5dee\u4e3a :attr:[ ](#id1)std \u7684\u6b63\u6001\u5206\u5e03\u586b\u5145. numel() \u2192 int \u8bf7\u67e5\u770b torch.numel() numpy() \u2192 ndarray \u5c06\u8be5 tensor \u4ee5 NumPy ndarray \u7684\u5f62\u5f0f\u8fd4\u56de. \u4e24\u8005\u5171\u4eab\u76f8\u540c\u7684\u5e95\u5c42\u5b58\u50a8. \u539f tensor \u7684\u6539\u53d8\u4f1a\u5f71\u54cd\u5230 ndarray , \u53cd\u4e4b\u4e5f\u4e00\u6837. orgqr(input2) \u2192 Tensor \u8bf7\u67e5\u770b torch.orgqr() ormqr(input2, input3, left=True, transpose=False) \u2192 Tensor \u8bf7\u67e5\u770b torch.ormqr() permute(*dims) \u6392\u5217\u8be5 tensor \u7684\u5c3a\u5bf8. Parameters: *dims ( int... ) \u2013 \u6309\u6240\u671f\u671b\u7684\u7ef4\u6570\u6392\u5e8f Example >>> x = torch.randn(2, 3, 5) >>> x.size() torch.Size([2, 3, 5]) >>> x.permute(2, 0, 1).size() torch.Size([5, 2, 3]) pin_memory() \u5982\u679c tensor \u5c1a\u672a\u56fa\u5b9a, \u5219\u5c06 tensor \u590d\u5236\u5230\u56fa\u5b9a\u5185\u5b58. potrf(upper=True) \u2192 Tensor \u8bf7\u67e5\u770b torch.potrf() potri(upper=True) \u2192 Tensor \u8bf7\u67e5\u770b torch.potri() potrs(input2, upper=True) \u2192 Tensor \u8bf7\u67e5\u770b torch.potrs() pow(exponent) \u8bf7\u67e5\u770b torch.pow() pow_(exponent) pow() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f prod(dim=None, keepdim=False) \u2192 float \u8bf7\u67e5\u770b torch.prod() pstrf(upper=True, tol=-1) -> (Tensor, IntTensor) \u8bf7\u67e5\u770b torch.pstrf() put_(indices, tensor, accumulate=False) \u2192 Tensor \u590d\u5236 tensor \u5185\u7684\u5143\u7d20\u5230 indices \u6307\u5b9a\u7684\u4f4d\u7f6e. \u4e3a\u4e86\u8fbe\u5230\u7d22\u5f15\u7684\u76ee\u7684, self tensor \u88ab\u5f53\u505a\u4e00\u7ef4 (1D) \u7684 tensor. \u5982\u679c accumulate \u662f True , tensor \u5185\u7684\u5143\u7d20\u7d2f\u52a0\u5230 self \u4e2d. \u5982\u679c accumulate \u662f False , \u5728\u7d22\u5f15\u5305\u542b\u91cd\u590d\u7684\u503c\u65f6, \u884c\u4e3a\u672a\u5b9a\u4e49. | Parameters: | indices ( LongTensor ) \u2013 self \u7684\u7d22\u5f15 tensor ( Tensor ) \u2013 \u5305\u542b\u9700\u8981\u590d\u5236\u503c\u7684 tensor accumulate ( bool ) \u2013 \u5982\u679c\u662f True, \u5143\u7d20\u7d2f\u52a0\u5230 self | | --- | --- | Example: >>> src = torch.Tensor([[4, 3, 5], ... [6, 7, 8]]) >>> src.put_(torch.LongTensor([1, 3]), torch.Tensor([9, 10])) 4 9 5 10 7 8 [torch.FloatTensor of size 2x3] qr() -> (Tensor, Tensor) \u8bf7\u67e5\u770b torch.qr() random_(from=0, to=None, *, generator=None) \u5c06 tensor \u7528\u5728 [from, to - 1] \u4e0a\u7684\u79bb\u6563\u5747\u5300\u5206\u5e03\u8fdb\u884c\u586b\u5145. \u5982\u679c\u6ca1\u6709\u7279\u522b\u8bf4\u660e, \u586b\u5165\u7684\u503c\u7531\u672c tensor \u7684\u6570\u636e\u7c7b\u578b\u9650\u5b9a\u8303\u56f4. \u4f46\u662f, \u5bf9\u4e8e\u6d6e\u70b9\u7c7b\u578b (floating point types), \u5982\u679c\u6ca1\u6709\u7279\u522b\u8bf4\u660e, \u53d6\u503c\u8303\u56f4\u662f 0, 2^mantissa , \u4ee5\u786e\u4fdd\u6bcf\u4e2a\u6570\u90fd\u662f\u53ef\u8868\u793a\u7684. \u4f8b\u5982, <cite>torch.DoubleTensor(1).random_()</cite> \u5c06\u5747\u5300\u5206\u5e03\u5728[0, 2^53]. reciprocal() \u2192 Tensor \u8bf7\u67e5\u770b torch.reciprocal() reciprocal_() \u2192 Tensor reciprocal() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f remainder(divisor) \u2192 Tensor \u8bf7\u67e5\u770b torch.remainder() remainder_(divisor) \u2192 Tensor remainder() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f renorm(p, dim, maxnorm) \u2192 Tensor \u8bf7\u67e5\u770b torch.renorm() renorm_(p, dim, maxnorm) \u2192 Tensor renorm() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f repeat(*sizes) \u6cbf\u7740\u6307\u5b9a\u7684\u5c3a\u5bf8\u91cd\u590d tensor. \u548c expand() \u4e0d\u540c, \u8fd9\u4e2a\u51fd\u6570\u590d\u5236 tensor \u7684\u6570\u636e. Parameters: *sizes ( torch.Size or int... ) \u2013 \u6cbf\u6bcf\u4e2a\u7ef4\u5ea6\u91cd\u590d tensor \u7684\u6b21\u6570 Example >>> x = torch.Tensor([1, 2, 3]) >>> x.repeat(4, 2) 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 [torch.FloatTensor of size 4x6] >>> x.repeat(4, 2, 1).size() torch.Size([4, 2, 3]) resize_(*sizes) \u5c06 tensor \u7684\u5927\u5c0f\u8c03\u6574\u4e3a\u6307\u5b9a\u7684\u5927\u5c0f. \u5982\u679c\u5143\u7d20\u4e2a\u6570\u6bd4\u5f53\u524d\u7684\u5185\u5b58\u5927\u5c0f\u5927, \u5c31\u5c06\u5e95\u5c42\u5b58\u50a8\u5927\u5c0f\u8c03\u6574\u4e3a\u4e0e\u65b0\u5143\u7d20\u6570\u76ee\u4e00\u81f4\u7684\u5927\u5c0f. \u5982\u679c\u5143\u7d20\u4e2a\u6570\u6bd4\u5f53\u524d\u5185\u5b58\u5c0f, \u5219\u5e95\u5c42\u5b58\u50a8\u4e0d\u4f1a\u88ab\u6539\u53d8. \u539f\u6765tensor\u4e2d\u88ab\u4fdd\u5b58\u4e0b\u6765\u7684\u5143\u7d20\u5c06\u4fdd\u6301\u4e0d\u53d8, \u4f46\u65b0\u5185\u5b58\u5c06\u4e0d\u4f1a\u88ab\u521d\u59cb\u5316. Parameters: sizes ( torch.Size or int... ) \u2013 \u671f\u671b\u7684\u5927\u5c0f Example >>> x = torch.Tensor([[1, 2], [3, 4], [5, 6]]) >>> x.resize_(2, 2) >>> x 1 2 3 4 [torch.FloatTensor of size 2x2] resize_as_(tensor) \u5c06\u672c tensor \u7684\u5927\u5c0f\u8c03\u6574\u4e3a\u53c2\u6570 tensor \u7684\u5927\u5c0f. \u7b49\u6548\u4e8e: self.resize_(tensor.size()) round() \u2192 Tensor \u8bf7\u67e5\u770b torch.round() round_() \u2192 Tensor round() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f rsqrt() \u2192 Tensor \u8bf7\u67e5\u770b torch.rsqrt() rsqrt_() \u2192 Tensor rsqrt() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f scatter_(dim, index, src) \u2192 Tensor \u5c06 src \u4e2d\u7684\u6240\u6709\u503c\u6309\u7167 index \u786e\u5b9a\u7684\u7d22\u5f15\u987a\u5e8f\u5199\u5165\u672c tensor \u4e2d. \u7ed9\u5b9a\u7684 dim \u58f0\u660e\u7d22\u5f15\u7684\u7ef4\u5ea6, dim \u6309\u7167 gather() \u4e2d\u7684\u63cf\u8ff0\u7684\u89c4\u5219\u6765\u786e\u5b9a. \u6ce8\u610f, \u5173\u4e8e gather, index \u7684\u503c\u5fc5\u987b\u662f <cite>0</cite> \u5230 <cite>(self.size(dim) -1)</cite> \u533a\u95f4, \u800c\u4e14, \u5c5e\u4e8e\u540c\u4e00\u7ef4\u5ea6\u7684\u4e00\u884c\u7684\u503c\u5fc5\u987b\u662f\u552f\u4e00\u7684. | Parameters: | dim ( int ) \u2013 \u7d22\u5f15\u7684\u8f74\u5411 index ( LongTensor ) \u2013 \u6563\u5c04\u5143\u7d20\u7684\u7d22\u5f15\u6307\u6570 src ( Tensor or float ) \u2013 \u6563\u5c04\u7684\u6e90\u5143\u7d20 | | --- | --- | Example: >>> x = torch.rand(2, 5) >>> x 0.4319 0.6500 0.4080 0.8760 0.2355 0.2609 0.4711 0.8486 0.8573 0.1029 [torch.FloatTensor of size 2x5] >>> torch.zeros(3, 5).scatter_(0, torch.LongTensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x) 0.4319 0.4711 0.8486 0.8760 0.2355 0.0000 0.6500 0.0000 0.8573 0.0000 0.2609 0.0000 0.4080 0.0000 0.1029 [torch.FloatTensor of size 3x5] >>> z = torch.zeros(2, 4).scatter_(1, torch.LongTensor([[2], [3]]), 1.23) >>> z 0.0000 0.0000 1.2300 0.0000 0.0000 0.0000 0.0000 1.2300 [torch.FloatTensor of size 2x4] select(dim, index) \u2192 Tensor or number \u6cbf\u7740 dim \u7ed9\u5b9a\u7684\u7ef4\u5ea6, \u6309\u7167 index \u5207\u7247. \u5982\u679c\u8fd9\u4e2a tensor \u662f\u4e00\u7ef4\u7684, \u8fd4\u56de\u4e00\u4e2a\u6570\u5b57. \u5426\u5219, \u8fd4\u56de\u4e00\u4e2a\u7ed9\u5b9a\u7ef4\u5ea6\u5df2\u7ecf\u88ab\u79fb\u9664\u7684 tensor. | Parameters: | dim ( int ) \u2013 \u5207\u7247\u7684\u7ef4\u5ea6 index ( int ) \u2013 \u7528\u6765\u9009\u53d6\u7684\u7d22\u5f15 | | --- | --- | Note select \u7b49\u6548\u4e8e\u5207\u7247. \u4f8b\u5982, `tensor.select(0, index) () \u7b49\u6548\u4e8e tensor[index] \u548c tensor.select(2, index) \u7b49\u6548\u4e8e tensor[:,:,index]`. set_(source=None, storage_offset=0, size=None, stride=None) \u8bbe\u7f6e\u5e95\u5c42\u5b58\u50a8, \u5927\u5c0f, \u548c\u6b65\u957f. \u5982\u679c source \u662f\u4e00\u4e2a tensor \u5bf9\u8c61, \u672c tensor \u548c\u8be5 tensor \u5171\u4eab\u5e95\u5c42\u5b58\u50a8, \u5e76\u4e14\u5927\u5c0f\u548c\u6b65\u957f\u4e00\u6837. \u5728\u5176\u4e2d\u4e00\u4e2a tensor \u4e2d\u6539\u53d8\u5143\u7d20, \u4f1a\u97f3\u54cd\u5230\u53e6\u4e00\u4e2a tensor. \u5982\u679c source \u662f\u4e00\u4e2a Storage , \u5219\u5c06\u8bbe\u7f6e\u5e95\u5c42\u5185\u5b58, \u504f\u79fb\u91cf, \u5927\u5c0f\u548c\u6b65\u957f. | Parameters: | source ( Tensor or Storage ) \u2013 \u7528\u5230\u7684 tensor \u6216 storage storage_offset ( int ) \u2013 storage \u7684\u504f\u79fb\u91cf size ( torch.Size ) \u2013 \u671f\u671b\u7684\u5927\u5c0f. \u9ed8\u8ba4\u4e3a\u6e90 tensor \u7684\u5927\u5c0f. stride ( tuple ) \u2013 \u671f\u671b\u7684\u6b65\u957f. \u9ed8\u8ba4\u4e3a C \u76f8\u90bb\u5185\u5b58\u7684\u6b65\u957f. | | --- | --- | share_memory_() \u5c06\u5e95\u5c42\u5b58\u50a8\u79fb\u5230\u5171\u4eab\u5185\u5b58. \u5982\u679c\u5e95\u5c42\u5b58\u50a8\u5df2\u7ecf\u5728\u5171\u4eab\u5185\u5b58\u548cCUDA tensor \u4e2d, \u5219\u8fd9\u662f\u65e0\u64cd\u4f5c. \u5171\u4eab\u5185\u5b58\u4e2d\u7684 tensor \u4e0d\u80fd\u8c03\u6574\u5927\u5c0f. short() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a short \u7c7b\u578b sigmoid() \u2192 Tensor \u8bf7\u67e5\u770b torch.sigmoid() sigmoid_() \u2192 Tensor sigmoid() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f sign() \u2192 Tensor \u8bf7\u67e5\u770b torch.sign() sign_() \u2192 Tensor sign() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f sin() \u2192 Tensor \u8bf7\u67e5\u770b torch.sin() sin_() \u2192 Tensor sin() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f sinh() \u2192 Tensor \u8bf7\u67e5\u770b torch.sinh() sinh_() \u2192 Tensor sinh() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f size() \u2192 torch.Size \u8fd4\u56de tensor \u7684\u5927\u5c0f. \u8fd4\u56de\u7684\u503c\u662f tuple \u7684\u5b50\u7c7b. Example >>> torch.Tensor(3, 4, 5).size() torch.Size([3, 4, 5]) sort(dim=None, descending=False) -> (Tensor, LongTensor) \u8bf7\u67e5\u770b torch.sort() split(split_size, dim=0) \u5c06 tensor \u5206\u89e3\u6210 tensor \u5143\u7ec4. See torch.split() . sqrt() \u2192 Tensor \u8bf7\u67e5\u770b torch.sqrt() sqrt_() \u2192 Tensor sqrt() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f squeeze(dim=None) \u8bf7\u67e5\u770b torch.squeeze() squeeze_(dim=None) squeeze() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f std(dim=None, unbiased=True, keepdim=False) \u2192 float \u8bf7\u67e5\u770b torch.std() storage() \u2192 torch.Storage \u8fd4\u56de\u5e95\u5c42\u5b58\u50a8 storage_offset() \u2192 int \u6309\u7167\u50a8\u5b58\u5143\u7d20\u4e2a\u6570\u7684\u504f\u79fb\u8fd4\u56de tensor \u5728\u5e95\u5c42\u5b58\u50a8\u4e2d\u7684\u504f\u79fb\u91cf(\u4e0d\u662f\u6309\u7167\u5b57\u8282\u8ba1\u7b97). Example >>> x = torch.Tensor([1, 2, 3, 4, 5]) >>> x.storage_offset() 0 >>> x[3:].storage_offset() 3 classmethod storage_type () stride(dim) \u2192 tuple or int \u8fd4\u56de tesnor \u7684\u6b65\u957f. \u6b65\u957f\u662f\u6307\u6309\u7167 dim \u6307\u5b9a\u7684\u7ef4\u5ea6, \u4ece\u4e00\u4e2a\u5143\u7d20\u5230\u4e0b\u4e00\u4e2a\u5143\u7d20\u9700\u8981\u8df3\u8dc3\u7684\u8ddd\u79bb. \u5f53\u6ca1\u6709\u6307\u5b9a\u7ef4\u5ea6, \u4f1a\u8ba1\u7b97\u6240\u6709\u7ef4\u5ea6\u7684\u6b65\u957f, \u5e76\u8fd4\u56de\u4e00\u4e2a tuple. \u5f53\u7ed9\u5b9a\u7ef4\u5ea6\u65f6, \u8fd4\u56de\u8fd9\u4e2a\u7ef4\u5ea6\u7684\u6b65\u957f. Parameters: dim ( int ) \u2013 \u671f\u671b\u7684\u9700\u8981\u8ba1\u7b97\u6b65\u957f\u7684\u7ef4\u5ea6. Example >>> x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) >>> x.stride() (5, 1) >>>x.stride(0) 5 >>> x.stride(-1) 1 sub(value, other) \u2192 Tensor \u4ece tensor \u4e2d\u62bd\u53d6\u4e00\u4e2a\u6807\u91cf\u6216\u5f20\u91cf. \u5982\u679c value \u548c other \u90fd\u662f\u7ed9\u5b9a\u7684, \u5219\u5728\u4f7f\u7528\u4e4b\u524d other \u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u4f1a\u88ab :attr: value \u7f29\u653e. \u5982\u679c other \u662f\u4e00\u4e2atensor, other \u7684\u5f62\u72b6\u5fc5\u987b\u4e8e\u57fa\u7840 tensor \u7684\u5f62\u72b6\u662f\u53ef\u5e7f\u64ad\u7684 ( broadcastable ). sub_(x) \u2192 Tensor sub() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f sum(dim=None, keepdim=False) \u2192 float \u8bf7\u67e5\u770b torch.sum() svd(some=True) -> (Tensor, Tensor, Tensor) \u8bf7\u67e5\u770b torch.svd() symeig(eigenvectors=False, upper=True) -> (Tensor, Tensor) \u8bf7\u67e5\u770b torch.symeig() t() \u2192 Tensor \u8bf7\u67e5\u770b torch.t() t_() \u2192 Tensor t() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f take(indices) \u2192 Tensor \u8bf7\u67e5\u770b torch.take() tan() tan_() \u2192 Tensor tan() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f tanh() \u2192 Tensor \u8bf7\u67e5\u770b torch.tanh() tanh_() \u2192 Tensor tanh() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f tolist() \u8fd4\u56de\u6b64 tensor \u7684\u5d4c\u5957\u5217\u8868\u8868\u793a. topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor) \u8bf7\u67e5\u770b torch.topk() trace() \u2192 float \u8bf7\u67e5\u770b torch.trace() transpose(dim0, dim1) \u2192 Tensor \u8bf7\u67e5\u770b torch.transpose() transpose_(dim0, dim1) \u2192 Tensor transpose() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f tril(k=0) \u2192 Tensor \u8bf7\u67e5\u770b torch.tril() tril_(k=0) \u2192 Tensor tril() triu(k=0) \u2192 Tensor \u8bf7\u67e5\u770b torch.triu() triu_(k=0) \u2192 Tensor triu() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f trtrs(A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor) \u8bf7\u67e5\u770b torch.trtrs() trunc() \u2192 Tensor \u8bf7\u67e5\u770b torch.trunc() trunc_() \u2192 Tensor trunc() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f type(new_type=None, async=False) \u5982\u679c\u6ca1\u6709\u6307\u5b9a <cite>new_type</cite> \u5219\u8fd4\u56de\u8be5\u7c7b\u578b , \u5426\u5219\u5c06\u6b64\u5bf9\u8c61\u8f6c\u6362\u4e3a\u6307\u5b9a\u7c7b\u578b . \u5982\u679c\u5df2\u7ecf\u662f\u6b63\u786e\u7684\u7c7b\u578b , \u5219\u4e0d\u6267\u884c\u590d\u5236\u5e76\u76f4\u63a5\u8fd4\u56de\u539f\u5bf9\u8c61 . | Parameters: | new_type ( type or string ) \u2013 \u671f\u671b\u7684\u7c7b\u578b async ( bool ) \u2013 \u5982\u679c\u4e3a True , \u5e76\u4e14\u6e90\u5728\u9501\u5b9a\u5185\u5b58\u4e2d\u800c\u76ee\u6807\u5728GPU\u4e2d , \u5219\u526f\u672c\u5c06\u4e0e\u4e3b\u673a\u5f02\u6b65\u6267\u884c , \u53cd\u4e4b\u4ea6\u7136 . \u5426\u5219\u6b64\u53c2\u6570\u4e0d\u8d77\u6548\u679c . | | --- | --- | type_as(tensor) \u5c06\u6b64\u5f20\u91cf\u8f6c\u6362\u4e3a\u7ed9\u5b9a tensor \u7684\u7c7b\u578b. \u5982\u679c tensor \u5df2\u7ecf\u662f\u6b63\u786e\u7684\u7c7b\u578b, \u8fd9\u662f\u4e00\u4e2a\u65e0\u7528\u7684\u64cd\u4f5c. \u8fd9\u76f8\u5f53\u4e8e: self.type(tensor.type()) Params: tensor (Tensor): tensor \u5177\u6709\u6240\u9700\u7684\u7c7b\u578b unfold(dim, size, step) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5728 dim \u7ef4\u5ea6\u4e0a\u5305\u542b\u6240\u6709 size \u5927\u5c0f\u5207\u7247\u7684 tensor. step \u8bf4\u660e\u4e24\u4e2a\u5207\u7247\u4e4b\u95f4\u7684\u6b65\u957f. \u5982\u679c <cite>sizedim</cite> \u662f\u539ftensor\u5728 dim \u7ef4\u5ea6\u539f\u6765\u7684\u5927\u5c0f, \u5219\u8fd4\u56de\u7684 tensor \u5728 <cite>dim</cite> \u7ef4\u5ea6\u7684\u5927\u5c0f\u662f <cite>(sizedim - size) / step + 1</cite> \u4e00\u4e2a\u989d\u5916\u7684\u5207\u7247\u5927\u5c0f\u7684\u7ef4\u5ea6\u5df2\u7ecf\u6dfb\u52a0\u5728\u8fd4\u56de\u7684 tensor \u4e2d. | Parameters: | dim ( int ) \u2013 \u9700\u8981\u5c55\u5f00\u7684\u7ef4\u5ea6 size ( int ) \u2013 \u6bcf\u4e00\u4e2a\u5206\u7247\u9700\u8981\u5c55\u5f00\u7684\u5927\u5c0f step ( int ) \u2013 \u76f8\u90bb\u5206\u7247\u4e4b\u95f4\u7684\u6b65\u957f | | --- | --- | Example: >>> x = torch.arange(1, 8) >>> x 1 2 3 4 5 6 7 [torch.FloatTensor of size 7] >>> x.unfold(0, 2, 1) 1 2 2 3 3 4 4 5 5 6 6 7 [torch.FloatTensor of size 6x2] >>> x.unfold(0, 2, 2) 1 2 3 4 5 6 [torch.FloatTensor of size 3x2] uniform_(from=0, to=1) \u2192 Tensor \u5c06 tensor \u7528\u4ece\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u6837\u5f97\u5230\u7684\u503c\u586b\u5145: unsqueeze(dim) \u8bf7\u67e5\u770b torch.unsqueeze() unsqueeze_(dim) unsqueeze() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f var(dim=None, unbiased=True, keepdim=False) \u2192 float \u8bf7\u67e5\u770b torch.var() view(*args) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u6709\u76f8\u540c\u6570\u636e\u4f46\u5927\u5c0f\u4e0d\u540c\u7684\u65b0\u7684 tensor. \u8fd4\u56de\u7684 tensor \u4e0e\u539f tensor \u5171\u4eab\u76f8\u540c\u7684\u6570\u636e, \u4e00\u5b9a\u6709\u76f8\u540c\u6570\u76ee\u7684\u5143\u7d20, \u4f46\u5927\u5c0f\u4e0d\u540c. \u4e00\u4e2a tensor \u5fc5\u987b\u662f\u8fde\u7eed\u7684 ( contiguous() ) \u624d\u80fd\u88ab\u67e5\u770b. Parameters: args ( torch.Size or int... ) \u2013 \u671f\u671b\u7684\u5927\u5c0f Example >>> x = torch.randn(4, 4) >>> x.size() torch.Size([4, 4]) >>> y = x.view(16) >>> y.size() torch.Size([16]) >>> z = x.view(-1, 8) # the size -1 is inferred from other dimensions >>> z.size() torch.Size([2, 8]) view_as(tensor) \u5c06\u8be5 tensor \u4f5c\u4e3a\u6307\u5b9a\u7684 tensor \u8fd4\u56de\u67e5\u770b. \u8fd9\u76f8\u5f53\u4e8e: self.view(tensor.size()) zero_() \u75280\u586b\u5145\u8be5 tensor. class torch.ByteTensor \u4e0b\u9762\u8fd9\u4e9b\u51fd\u6570\u65b9\u6cd5\u53ea\u5b58\u5728\u4e8e torch.ByteTensor . all() \u2192 bool \u5982\u679c tensor \u91cc\u7684\u6240\u6709\u5143\u7d20\u90fd\u662f\u975e\u96f6\u7684, \u5219\u8fd4\u56de True, \u5426\u5728\u8fd4\u56de False. any() \u2192 bool \u5982\u679c tensor \u91cc\u7684\u5b58\u5728\u5143\u7d20\u662f\u975e\u96f6\u7684, \u5219\u8fd4\u56de True, \u5426\u5728\u8fd4\u56de False.","title":"torch.Tensor"},{"location":"54/#torchtensor","text":"torch.Tensor \u662f\u4e00\u79cd\u5305\u542b\u5355\u4e00\u6570\u636e\u7c7b\u578b\u5143\u7d20\u7684\u591a\u7ef4\u77e9\u9635. Torch \u5b9a\u4e49\u4e86\u4e03\u79cd CPU tensor \u7c7b\u578b\u548c\u516b\u79cd GPU tensor \u7c7b\u578b: Data type CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point torch.HalfTensor torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.Tensor \u662f\u9ed8\u8ba4\u7684 tensor \u7c7b\u578b( torch.FloatTensor )\u7684\u7b80\u79f0. \u4e00\u4e2a tensor \u5bf9\u8c61\u53ef\u4ee5\u4ece Python \u7684 list \u6216\u8005\u5e8f\u5217(sequence)\u6784\u5efa: >>> torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) 1 2 3 4 5 6 [torch.FloatTensor of size 2x3] \u4e00\u4e2a\u7a7a\u7684 tensor \u5bf9\u8c61\u53ef\u4ee5\u901a\u8fc7\u6240\u6307\u5b9a\u7684\u5927\u5c0f\u6765\u6784\u5efa: >>> torch.IntTensor(2, 4).zero_() 0 0 0 0 0 0 0 0 [torch.IntTensor of size 2x4] \u53ef\u4ee5\u901a\u8fc7 Python \u7684\u7d22\u5f15\u548c\u5207\u7247\u65b9\u5f0f\u6765\u83b7\u53d6\u6216\u4fee\u6539 tensor \u5bf9\u8c61\u7684\u5185\u5bb9: >>> x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) >>> print(x[1][2]) 6.0 >>> x[0][1] = 8 >>> print(x) 1 8 3 4 5 6 [torch.FloatTensor of size 2x3] \u6bcf\u4e00\u4e2a tensor \u5bf9\u8c61\u90fd\u6709\u4e00\u4e2a\u76f8\u5e94\u7684 torch.Storage \u7528\u6765\u4fdd\u5b58\u6570\u636e. tensor \u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b58\u50a8\u7684\u591a\u7ef4\u7684, \u6709 \u8de8\u5ea6(strided) \u7684\u89c6\u56fe, \u5e76\u4e14\u5728\u89c6\u56fe\u4e0a\u5b9a\u4e49\u4e86\u6570\u503c\u8fd0\u7b97. Note \u4f1a\u6539\u53d8 tensor \u5bf9\u8c61\u7684\u51fd\u6570\u65b9\u6cd5\u540d, \u5176\u4f7f\u7528\u4e86\u4e00\u4e2a\u4e0b\u5212\u7ebf\u540e\u7f00\u4f5c\u4e3a\u6807\u8bc6. \u6bd4\u5982, torch.FloatTensor.abs_() \u4f1a\u5728\u539f\u5730(in-place)\u8ba1\u7b97\u7edd\u5bf9\u503c\u5e76\u8fd4\u56de\u6539\u53d8\u540e\u7684 tensor. \u800c torch.FloatTensor.abs() \u4f1a\u5728\u4e00\u4e2a\u65b0\u5efa\u7684 tensor \u4e2d\u8ba1\u7b97\u7ed3\u679c. class torch.Tensor class torch.Tensor(*sizes) class torch.Tensor(size) class torch.Tensor(sequence) class torch.Tensor(ndarray) class torch.Tensor(tensor) class torch.Tensor(storage) \u53ef\u4ee5\u901a\u8fc7\u63d0\u4f9b\u5927\u5c0f\u6216\u8005\u6570\u636e\u6765\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 tensor \u5bf9\u8c61. \u5982\u679c\u6ca1\u6709\u63d0\u4f9b\u53c2\u6570, \u5c06\u8fd4\u56de\u4e00\u4e2a\u7a7a\u7684\u96f6\u7ef4\u7684 tensor. \u5982\u679c\u63d0\u4f9b\u4e86 numpy.ndarray , torch.Tensor , \u6216\u8005 torch.Storage \u4f5c\u4e3a\u53c2\u6570, \u5176\u5c06\u8fd4\u56de\u4e00\u4e2a\u4e0e\u53c2\u6570\u5171\u4eab\u6570\u636e\u7684 tensor \u5bf9\u8c61. \u5982\u679c\u63d0\u4f9b\u4e00\u4e2a Python \u5e8f\u5217(sequence)\u4f5c\u4e3a\u53c2\u6570, \u5c06\u8fd4\u56de\u4ece\u5e8f\u5217\u7684\u526f\u672c\u4e2d\u521b\u5efa\u7684\u4e00\u4e2a\u65b0\u7684 tensor \u5bf9\u8c61. abs() \u2192 Tensor \u8bf7\u67e5\u770b torch.abs() abs_() \u2192 Tensor abs() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f acos() \u2192 Tensor \u8bf7\u67e5\u770b torch.acos() acos_() \u2192 Tensor acos() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f add(value) \u8bf7\u67e5\u770b torch.add() add_(value) add() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f addbmm(beta=1, mat, alpha=1, batch1, batch2) \u2192 Tensor \u8bf7\u67e5\u770b torch.addbmm() addbmm_(beta=1, mat, alpha=1, batch1, batch2) \u2192 Tensor addbmm() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f addcdiv(value=1, tensor1, tensor2) \u2192 Tensor \u8bf7\u67e5\u770b torch.addcdiv() addcdiv_(value=1, tensor1, tensor2) \u2192 Tensor addcdiv() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f addcmul(value=1, tensor1, tensor2) \u2192 Tensor \u8bf7\u67e5\u770b torch.addcmul() addcmul_(value=1, tensor1, tensor2) \u2192 Tensor addcmul() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f addmm(beta=1, mat, alpha=1, mat1, mat2) \u2192 Tensor \u8bf7\u67e5\u770b torch.addmm() addmm_(beta=1, mat, alpha=1, mat1, mat2) \u2192 Tensor addmm() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f addmv(beta=1, tensor, alpha=1, mat, vec) \u2192 Tensor \u8bf7\u67e5\u770b torch.addmv() addmv_(beta=1, tensor, alpha=1, mat, vec) \u2192 Tensor addmv() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f addr(beta=1, alpha=1, vec1, vec2) \u2192 Tensor \u8bf7\u67e5\u770b torch.addr() addr_(beta=1, alpha=1, vec1, vec2) \u2192 Tensor addr() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f apply_(callable) \u2192 Tensor \u5c06\u51fd\u6570 callable \u4f5c\u7528\u4e8e tensor \u7684\u6bcf\u4e00\u4e2a\u5143\u7d20, \u5e76\u5c06\u6bcf\u4e2a\u5143\u7d20\u7528 callable \u7684\u8fd4\u56de\u503c\u66ff\u6362. Note \u8be5\u51fd\u6570\u53ea\u80fd\u5728 CPU tensor \u4e2d\u4f7f\u7528, \u5e76\u4e14\u4e0d\u5e94\u8be5\u7528\u5728\u6709\u8f83\u9ad8\u6027\u80fd\u7684\u8981\u6c42\u7684\u4ee3\u7801\u5757\u4e2d. asin() \u2192 Tensor \u8bf7\u67e5\u770b torch.asin() asin_() \u2192 Tensor asin() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f atan() \u2192 Tensor \u8bf7\u67e5\u770b torch.atan() atan2(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.atan2() atan2_(other) \u2192 Tensor atan2() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f atan_() \u2192 Tensor atan() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f baddbmm(beta=1, alpha=1, batch1, batch2) \u2192 Tensor \u8bf7\u67e5\u770b torch.baddbmm() baddbmm_(beta=1, alpha=1, batch1, batch2) \u2192 Tensor baddbmm() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f bernoulli() \u2192 Tensor \u8bf7\u67e5\u770b torch.bernoulli() bernoulli_() \u2192 Tensor bernoulli() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f bmm(batch2) \u2192 Tensor \u8bf7\u67e5\u770b torch.bmm() byte() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a byte \u7c7b\u578b cauchy_(median=0, sigma=1, *, generator=None) \u2192 Tensor \u7528\u67ef\u897f\u5206\u5e03\u5f97\u5230\u7684\u6570\u503c\u6765\u586b\u5145 tensor \u4e2d\u7684\u5143\u7d20: \\[P(x) = \\dfrac{1}{\\pi} \\dfrac{\\sigma}{(x - median)^2 + \\sigma^2}\\] ceil() \u2192 Tensor \u8bf7\u67e5\u770b torch.ceil() ceil_() \u2192 Tensor ceil() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f char() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a char \u7c7b\u578b chunk(n_chunks, dim=0) \u5c06 tensor \u5206\u89e3\u6210 tensor \u5143\u7ec4. \u8bf7\u67e5\u770b torch.chunk() . clamp(min, max) \u2192 Tensor \u8bf7\u67e5\u770b torch.clamp() clamp_(min, max) \u2192 Tensor clamp() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f clone() \u2192 Tensor \u8fd4\u56de\u4e0e\u539f tensor \u5177\u6709\u76f8\u540c\u5927\u5c0f\u548c\u6570\u636e\u7c7b\u578b\u7684 tensor. contiguous() \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5185\u5b58\u8fde\u7eed\u7684\u6709\u76f8\u540c\u6570\u636e\u7684 tensor, \u5982\u679c\u539f tensor \u5185\u5b58\u8fde\u7eed\u5219\u8fd4\u56de\u539f tensor. copy_(src, async=False, broadcast=True) \u2192 Tensor \u5c06 src \u4e2d\u7684\u5143\u7d20\u590d\u5236\u5230\u8fd9\u4e2a tensor \u4e2d\u5e76\u8fd4\u56de\u8fd9\u4e2a tensor \u5982\u679c broadcast \u662f True, \u6e90 tensor \u4e00\u5b9a\u548c\u8fd9\u4e2a tensor broadcastable . \u53e6\u5916, \u6e90 tensor \u7684\u5143\u7d20\u6570\u91cf\u5e94\u8be5\u548c\u8fd9\u4e2a tensor \u7684\u5143\u7d20\u4e2a\u6570\u4e00\u81f4. \u6e90 tensor \u53ef\u4ee5\u662f\u53e6\u4e00\u79cd\u6570\u636e\u7c7b\u578b, \u6216\u8005\u5728\u522b\u7684\u7684\u8bbe\u5907\u4e0a. | Parameters: | src ( Tensor ) \u2013 \u88ab\u590d\u5236\u7684\u6e90 tensor async ( bool ) \u2013 \u5982\u679c\u503c\u4e3a True \u5e76\u4e14\u8fd9\u4e2a\u590d\u5236\u64cd\u4f5c\u5728 CPU \u548c GPU \u4e4b\u95f4\u8fdb\u884c, \u5219\u62f7\u8d1d\u7684\u526f\u672c\u4e0e\u6e90\u4fe1\u606f\u53ef\u80fd\u4f1a\u51fa\u73b0\u5f02\u6b65(asynchronously). \u5bf9\u4e8e\u5176\u4ed6\u7c7b\u578b\u7684\u590d\u5236\u64cd\u4f5c, \u8fd9\u4e2a\u53c2\u6570\u4e0d\u8d77\u4f5c\u7528. broadcast ( bool ) \u2013 \u5982\u679c\u503c\u4e3a True , src \u5c06\u5e7f\u64ad\u57fa\u7840\u7684 tensor \u7684\u5f62\u72b6. | | --- | --- | cos() \u2192 Tensor \u8bf7\u67e5\u770b torch.cos() cos_() \u2192 Tensor cos() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f cosh() \u2192 Tensor \u8bf7\u67e5\u770b torch.cosh() cosh_() \u2192 Tensor cosh() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f cpu() \u5982\u679c\u6b64 tensor \u4e0d\u5728CPU\u4e0a, \u5219\u8fd4\u56de\u6b64 tensor \u7684CPU\u526f\u672c cross(other, dim=-1) \u2192 Tensor \u8bf7\u67e5\u770b torch.cross() cuda(device=None, async=False) \u8fd4\u56de\u6b64\u5bf9\u8c61\u5728 CUDA \u5185\u5b58\u4e2d\u7684\u4e00\u4e2a\u526f\u672c . \u5982\u679c\u6b64\u5bf9\u8c61\u5df2\u7ecf\u5728 CUDA \u5185\u5b58\u4e2d\u5e76\u4e14\u5728\u6b63\u786e\u7684\u8bbe\u5907\u4e0a , \u90a3\u4e48\u4e0d\u4f1a\u6267\u884c\u590d\u5236\u64cd\u4f5c , \u76f4\u63a5\u8fd4\u56de\u539f\u5bf9\u8c61 . | Parameters: | device ( int ) \u2013 \u76ee\u6807 GPU \u7684 id . \u9ed8\u8ba4\u503c\u662f\u5f53\u524d\u8bbe\u5907 . async ( bool ) \u2013 \u5982\u679c\u4e3a True \u5e76\u4e14\u6e90\u4f4d\u4e8e\u9501\u5b9a\u5185\u5b58\u4e2d , \u5219\u526f\u672c\u76f8\u5bf9\u4e8e\u4e3b\u673a\u662f\u5f02\u6b65\u7684 . \u5426\u5219\u6b64\u53c2\u6570\u4e0d\u8d77\u6548\u679c . | | --- | --- | cumprod(dim) \u2192 Tensor \u8bf7\u67e5\u770b torch.cumprod() cumsum(dim) \u2192 Tensor \u8bf7\u67e5\u770b torch.cumsum() data_ptr() \u2192 int \u8fd4\u56de tensor \u7b2c\u4e00\u4e2a\u5143\u7d20\u7684\u5730\u5740. diag(diagonal=0) \u2192 Tensor \u8bf7\u67e5\u770b torch.diag() dim() \u2192 int \u8fd4\u56de tensor \u7684\u7ef4\u6570. dist(other, p=2) \u2192 float \u8bf7\u67e5\u770b torch.dist() div(value) \u8bf7\u67e5\u770b torch.div() div_(value) div() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f dot(tensor2) \u2192 float \u8bf7\u67e5\u770b torch.dot() double() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a double \u7c7b\u578b eig(eigenvectors=False) -> (Tensor, Tensor) \u8bf7\u67e5\u770b torch.eig() element_size() \u2192 int \u8fd4\u56de\u5355\u4e2a\u5143\u7d20\u7684\u5b57\u8282\u5927\u5c0f. Example >>> torch.FloatTensor().element_size() 4 >>> torch.ByteTensor().element_size() 1 eq(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.eq() eq_(other) \u2192 Tensor eq() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f equal(other) \u2192 bool \u8bf7\u67e5\u770b torch.equal() erf() \u2192 Tensor \u8bf7\u67e5\u770b torch.erf() erf_() erfinv() \u2192 Tensor \u8bf7\u67e5\u770b torch.erfinv() erfinv_() exp() \u2192 Tensor \u8bf7\u67e5\u770b torch.exp() exp_() \u2192 Tensor exp() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f expand(*sizes) \u2192 Tensor \u8fd4\u56de tensor \u5355\u4e2a\u7ef4\u5ea6\u6269\u5c55\u5230\u5927\u7684\u4e00\u4e2a\u65b0\u7684\u89c6\u56fe. \u4f20\u9012 -1 \u4f5c\u4e3a\u4e00\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f, \u8868\u793a\u8fd9\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f\u4e0d\u505a\u6539\u53d8. Tensor \u4e5f\u53ef\u4ee5\u6269\u5c55\u5230\u4e00\u4e2a\u5f88\u5927\u7684\u7ef4\u6570, \u65b0\u6dfb\u52a0\u7684\u7ef4\u5ea6\u5c06\u653e\u5728\u524d\u9762. (\u5bf9\u4e8e\u65b0\u7684\u7ef4\u5ea6, \u5927\u5c0f\u4e0d\u80fd\u8bbe\u7f6e\u4e3a -1 .) \u6269\u5c55\u4e00\u4e2a tensor \u4e0d\u662f\u5206\u914d\u4e00\u4e2a\u65b0\u7684\u5185\u5b58, \u800c\u53ea\u662f\u5728\u8fd9\u4e2a\u5b58\u5728\u7684 tensor \u4e0a, \u901a\u8fc7\u8bbe\u7f6e stride \u4e3a 0, \u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u67d0\u4e2a\u7ef4\u5ea6\u4ece 1 \u6269\u5c55\u5230\u5f88\u5927\u7684\u89c6\u56fe. \u4efb\u4f55\u5927\u5c0f\u4e3a 1 \u7684\u7ef4\u5ea6, \u5728\u4e0d\u7528\u91cd\u65b0\u5206\u914d\u5185\u5b58\u7684\u60c5\u51b5\u4e0b, \u53ef\u4ee5\u6269\u5c55\u5230\u968f\u610f\u4efb\u4f55\u4e00\u4e2a\u503c. Parameters: *sizes ( torch.Size or int... ) \u2013 \u671f\u671b\u6269\u5c55\u7684\u5927\u5c0f Example >>> x = torch.Tensor([[1], [2], [3]]) >>> x.size() torch.Size([3, 1]) >>> x.expand(3, 4) 1 1 1 1 2 2 2 2 3 3 3 3 [torch.FloatTensor of size 3x4] >>> x.expand(-1, 4) # -1 means not changing the size of that dimension 1 1 1 1 2 2 2 2 3 3 3 3 [torch.FloatTensor of size 3x4] expand_as(tensor) \u5c06\u6b64 tensor \u5c55\u5f00\u4e3a\u6307\u5b9a tensor \u7684\u5927\u5c0f. \u8fd9\u76f8\u5f53\u4e8e: self.expand(tensor.size()) exponential_(lambd=1, *, generator=None) \u2192 Tensor \u5c06\u8be5 tensor \u7528\u6307\u6570\u5206\u5e03\u5f97\u5230\u7684\u5143\u7d20\u586b\u5145: \\[P(x) = \\lambda e^{-\\lambda x}\\] fill_(value) \u2192 Tensor \u5c06\u8be5 tensor \u7528\u6307\u5b9a\u7684\u6570\u503c\u586b\u5145. float() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a float \u7c7b\u578b floor() \u2192 Tensor \u8bf7\u67e5\u770b torch.floor() floor_() \u2192 Tensor floor() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f fmod(divisor) \u2192 Tensor \u8bf7\u67e5\u770b torch.fmod() fmod_(divisor) \u2192 Tensor fmod() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f frac() \u2192 Tensor \u8bf7\u67e5\u770b torch.frac() frac_() \u2192 Tensor frac() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f gather(dim, index) \u2192 Tensor \u8bf7\u67e5\u770b torch.gather() ge(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.ge() ge_(other) \u2192 Tensor ge() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f gels(A) \u2192 Tensor \u8bf7\u67e5\u770b torch.gels() geometric_(p, *, generator=None) \u2192 Tensor \u5c06\u8be5 tensor \u7528\u51e0\u4f55\u5206\u5e03\u5f97\u5230\u7684\u5143\u7d20\u586b\u5145: \\[P(X=k) = (1 - p)^{k - 1} p\\] geqrf() -> (Tensor, Tensor) \u8bf7\u67e5\u770b torch.geqrf() ger(vec2) \u2192 Tensor \u8bf7\u67e5\u770b torch.ger() gesv(A) \u2192 Tensor, Tensor \u8bf7\u67e5\u770b torch.gesv() gt(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.gt() gt_(other) \u2192 Tensor gt() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f half() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a half-precision float \u7c7b\u578b histc(bins=100, min=0, max=0) \u2192 Tensor \u8bf7\u67e5\u770b torch.histc() index(m) \u2192 Tensor \u7528\u4e00\u4e2a\u4e8c\u8fdb\u5236\u7684\u63a9\u7801\u6216\u6cbf\u7740\u4e00\u4e2a\u7ed9\u5b9a\u7684\u7ef4\u5ea6\u4ece tensor \u4e2d\u9009\u53d6\u5143\u7d20. tensor.index(m) \u7b49\u540c\u4e8e tensor[m] . Parameters: m ( int or ByteTensor or slice ) \u2013 \u7528\u6765\u9009\u53d6\u5143\u7d20\u7684\u7ef4\u5ea6\u6216\u63a9\u7801 index_add_(dim, index, tensor) \u2192 Tensor \u6309\u53c2\u6570 index \u7ed9\u51fa\u7684\u7d22\u5f15\u5e8f\u5217, \u5c06\u53c2\u6570 tensor \u4e2d\u7684\u5143\u7d20\u52a0\u5230\u539f\u6765\u7684 tensor \u4e2d. \u53c2\u6570 tensor \u7684\u5c3a\u5bf8\u5fc5\u987b\u4e25\u683c\u5730\u4e0e\u539f tensor \u5339\u914d, \u5426\u5219\u4f1a\u53d1\u751f\u9519\u8bef. | Parameters: | dim ( int ) \u2013 \u7d22\u5f15 index \u6240\u6307\u5411\u7684\u7ef4\u5ea6 index ( LongTensor ) \u2013 \u4ece\u53c2\u6570 tensor \u4e2d\u9009\u53d6\u6570\u636e\u7684\u7d22\u5f15\u5e8f\u5217 tensor ( Tensor ) \u2013 \u5305\u542b\u9700\u8981\u76f8\u52a0\u7684\u5143\u7d20\u7684 tensor | | --- | --- | Example >>> x = torch.Tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]]) >>> t = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> index = torch.LongTensor([0, 2, 1]) >>> x.index_add_(0, index, t) >>> x 2 3 4 8 9 10 5 6 7 [torch.FloatTensor of size 3x3] index_copy_(dim, index, tensor) \u2192 Tensor \u6309\u53c2\u6570 index \u7ed9\u51fa\u7684\u7d22\u5f15\u5e8f\u5217, \u5c06\u53c2\u6570 tensor \u4e2d\u7684\u5143\u7d20\u590d\u5236\u5230\u539f\u6765\u7684 tensor \u4e2d. \u53c2\u6570 tensor \u7684\u5c3a\u5bf8\u5fc5\u987b\u4e25\u683c\u5730\u4e0e\u539f tensor \u5339\u914d, \u5426\u5219\u4f1a\u53d1\u751f\u9519\u8bef. | Parameters: | dim ( int ) \u2013 \u7d22\u5f15 index \u6240\u6307\u5411\u7684\u7ef4\u5ea6 index ( LongTensor ) \u2013 \u4ece\u53c2\u6570 tensor \u4e2d\u9009\u53d6\u6570\u636e\u7684\u7d22\u5f15\u5e8f\u5217 tensor ( Tensor ) \u2013 \u5305\u542b\u9700\u8981\u590d\u5236\u7684\u5143\u7d20\u7684 tensor | | --- | --- | Example >>> x = torch.Tensor(3, 3) >>> t = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> index = torch.LongTensor([0, 2, 1]) >>> x.index_copy_(0, index, t) >>> x 1 2 3 7 8 9 4 5 6 [torch.FloatTensor of size 3x3] index_fill_(dim, index, val) \u2192 Tensor \u6309\u53c2\u6570 index \u7ed9\u51fa\u7684\u7d22\u5f15\u5e8f\u5217, \u5c06\u539f tensor \u4e2d\u7684\u5143\u7d20\u7528 val \u586b\u5145. | Parameters: | dim ( int ) \u2013 \u7d22\u5f15 index \u6240\u6307\u5411\u7684\u7ef4\u5ea6 index ( LongTensor ) \u2013 \u4ece\u53c2\u6570 val \u4e2d\u9009\u53d6\u6570\u636e\u7684\u7d22\u5f15\u5e8f\u5217 val ( float ) \u2013 \u7528\u6765\u586b\u5145\u7684\u503c | | --- | --- | Example >>> x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> index = torch.LongTensor([0, 2]) >>> x.index_fill_(1, index, -1) >>> x -1 2 -1 -1 5 -1 -1 8 -1 [torch.FloatTensor of size 3x3] index_select(dim, index) \u2192 Tensor \u8bf7\u67e5\u770b torch.index_select() int() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a int \u7c7b\u578b inverse() \u2192 Tensor \u8bf7\u67e5\u770b torch.inverse() is_contiguous() \u2192 bool \u4ee5 C \u8bed\u8a00\u7684\u5185\u5b58\u6a21\u578b\u4e3a\u539f\u5219, \u5982\u679c\u8be5 tensor \u5728\u5185\u5982\u679c\u8be5 tensor \u5728\u5185\u5b58\u4e2d\u8fde\u7eed\u7684, \u5219\u8fd4\u56de True. is_cuda is_pinned() \u5982\u679c tensor \u9a7b\u7559\u5728\u56fa\u5b9a\u5185\u5b58\u4e2d, \u5219\u8fd4\u56de true is_set_to(tensor) \u2192 bool \u5982\u679c\u6b64\u5bf9\u8c61\u4ece Torch C API \u5f15\u7528\u7684 THTensor \u5bf9\u8c61\u4e0e\u53c2\u6570 tensor \u5f15\u7528\u7684\u5bf9\u8c61\u4e00\u81f4, \u5219\u8fd4\u56deTrue. is_signed() kthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor) \u8bf7\u67e5\u770b torch.kthvalue() le(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.le() le_(other) \u2192 Tensor le() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f lerp(start, end, weight) \u8bf7\u67e5\u770b torch.lerp() lerp_(start, end, weight) lerp() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f log() \u2192 Tensor \u8bf7\u67e5\u770b torch.log() log1p() \u2192 Tensor \u8bf7\u67e5\u770b torch.log1p() log1p_() \u2192 Tensor log1p() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f log_() \u2192 Tensor log() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f log_normal_(mean=1, std=2, *, generator=None) \u5c06\u8be5 tensor \u7528\u5747\u503c\u4e3a mean (\u00b5), \u6807\u51c6\u5dee\u4e3a std (\u03c3) \u7684\u5bf9\u6570\u6b63\u6001\u5206\u5e03\u5f97\u5230\u7684\u5143\u7d20\u586b\u5145. \u8981\u6ce8\u610f mean \u548c stdv \u662f\u57fa\u672c\u6b63\u6001\u5206\u5e03\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee, \u4e0d\u662f\u8fd4\u56de\u7684\u5206\u5e03: \\[P(x) = \\dfrac{1}{x \\sigma \\sqrt{2\\pi}} e^{-\\dfrac{(\\ln x - \\mu)^2}{2\\sigma^2}}\\] long() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a long \u7c7b\u578b lt(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.lt() lt_(other) \u2192 Tensor lt() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f map_(tensor, callable) \u5c06 callable \u4f5c\u7528\u4e8e\u672c tensor \u548c\u53c2\u6570 tensor \u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20, \u5e76\u5c06\u7ed3\u679c\u5b58\u653e\u5728\u672c tensor \u4e2d. \u672c tensor \u548c\u53c2\u6570 tensor \u90fd\u5fc5\u987b\u662f broadcastable . callable \u5e94\u8be5\u6709\u4e0b\u5217\u6807\u5fd7: ```py def callable(a, b) -> number ``` masked_scatter_(mask, source) \u590d\u5236 source \u7684\u5143\u7d20\u5230\u672c tensor \u88ab:attr: mask \u4e2d\u503c\u4e3a 1 \u7684\u5143\u7d20\u6807\u8bb0\u7684\u4f4d\u7f6e\u4e2d. mask \u7684\u5f62\u72b6\u548c\u672c tensor \u7684\u5f62\u72b6\u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u7684 ( broadcastable ). source \u4e2d\u5143\u7d20\u7684\u4e2a\u6570\u6700\u5c11\u4e3a mask \u4e2d\u503c\u4e3a1\u7684\u5143\u7d20\u7684\u4e2a\u6570. | Parameters: | mask ( ByteTensor ) \u2013 \u4e8c\u8fdb\u5236\u63a9\u7801 source ( Tensor ) \u2013 \u590d\u5236\u7684\u6e90 tensor | | --- | --- | Note mask \u4f5c\u7528\u4e8e self \u81ea\u8eab\u7684 tensor, \u800c\u4e0d\u662f\u53c2\u6570 source \u7684 tensor. masked_fill_(mask, value) \u5c06\u672c tensor \u88ab mask \u4e2d\u503c\u4e3a 1 \u7684\u5143\u7d20\u6807\u8bb0\u7684\u4f4d\u7f6e, \u7528 value \u586b\u5145. mask \u7684\u5f62\u72b6\u548c\u672c tensor \u7684\u5f62\u72b6\u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u7684 ( broadcastable ). Fills elements of this tensor with value where mask is one. | Parameters: | mask ( ByteTensor ) \u2013 \u4e8c\u8fdb\u5236\u63a9\u7801 value ( float ) \u2013 \u7528\u6765\u586b\u5145\u7684\u503c | | --- | --- | masked_select(mask) \u2192 Tensor \u8bf7\u67e5\u770b torch.masked_select() matmul(other) \u4e24\u4e2a tensor \u7684\u77e9\u9635\u4e58\u79ef. \u8bf7\u67e5\u770b torch.matmul() . max(dim=None, keepdim=False) -> float or (Tensor, Tensor) \u8bf7\u67e5\u770b torch.max() mean(dim=None, keepdim=False) -> float or (Tensor, Tensor) \u8bf7\u67e5\u770b torch.mean() median(dim=None, keepdim=False) -> (Tensor, LongTensor) \u8bf7\u67e5\u770b torch.median() min(dim=None, keepdim=False) -> float or (Tensor, Tensor) \u8bf7\u67e5\u770b torch.min() mm(mat2) \u2192 Tensor \u8bf7\u67e5\u770b torch.mm() mode(dim=None, keepdim=False) -> (Tensor, LongTensor) \u8bf7\u67e5\u770b torch.mode() mul(value) \u2192 Tensor \u8bf7\u67e5\u770b torch.mul() mul_(value) mul() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f multinomial(num_samples, replacement=False, *, generator=None) \u8bf7\u67e5\u770b torch.multinomial() mv(vec) \u2192 Tensor \u8bf7\u67e5\u770b torch.mv() narrow(dimension, start, length) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u672c tensor \u7ecf\u8fc7\u7f29\u5c0f\u540e\u7684 tensor. \u7ef4\u5ea6 dim \u7f29\u5c0f\u8303\u56f4\u662f start \u5230 start + length . \u539f tensor \u4e0e\u8fd4\u56de\u7684 tensor \u5171\u4eab\u76f8\u540c\u7684\u5e95\u5c42\u5b58\u50a8. | Parameters: | dimension ( int ) \u2013 \u9700\u8981\u7f29\u5c0f\u7684\u7ef4\u5ea6 start ( int ) \u2013 \u8d77\u59cb\u7ef4\u5ea6 length ( int ) \u2013 | | --- | --- | Example >>> x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> x.narrow(0, 0, 2) 1 2 3 4 5 6 [torch.FloatTensor of size 2x3] >>> x.narrow(1, 1, 2) 2 3 5 6 8 9 [torch.FloatTensor of size 3x2] ndimension() \u2192 int dim() \u7684\u53e6\u4e00\u79cd\u8868\u793a ne(other) \u2192 Tensor \u8bf7\u67e5\u770b torch.ne() ne_(other) \u2192 Tensor ne() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f neg() \u2192 Tensor \u8bf7\u67e5\u770b torch.neg() neg_() \u2192 Tensor neg() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f nelement() \u2192 int numel() \u7684\u53e6\u4e00\u79cd\u8868\u793a new(*args, **kwargs) \u6784\u9020\u76f8\u540c\u6570\u636e\u7c7b\u578b\u7684\u65b0 tensor. nonzero() \u2192 LongTensor \u8bf7\u67e5\u770b torch.nonzero() norm(p=2, dim=None, keepdim=False) \u2192 float \u8bf7\u67e5\u770b torch.norm() normal_(mean=0, std=1, *, generator=None) \u5c06 tensor \u7528\u5747\u503c\u4e3a mean \u548c\u6807\u51c6\u5dee\u4e3a :attr:[ ](#id1)std \u7684\u6b63\u6001\u5206\u5e03\u586b\u5145. numel() \u2192 int \u8bf7\u67e5\u770b torch.numel() numpy() \u2192 ndarray \u5c06\u8be5 tensor \u4ee5 NumPy ndarray \u7684\u5f62\u5f0f\u8fd4\u56de. \u4e24\u8005\u5171\u4eab\u76f8\u540c\u7684\u5e95\u5c42\u5b58\u50a8. \u539f tensor \u7684\u6539\u53d8\u4f1a\u5f71\u54cd\u5230 ndarray , \u53cd\u4e4b\u4e5f\u4e00\u6837. orgqr(input2) \u2192 Tensor \u8bf7\u67e5\u770b torch.orgqr() ormqr(input2, input3, left=True, transpose=False) \u2192 Tensor \u8bf7\u67e5\u770b torch.ormqr() permute(*dims) \u6392\u5217\u8be5 tensor \u7684\u5c3a\u5bf8. Parameters: *dims ( int... ) \u2013 \u6309\u6240\u671f\u671b\u7684\u7ef4\u6570\u6392\u5e8f Example >>> x = torch.randn(2, 3, 5) >>> x.size() torch.Size([2, 3, 5]) >>> x.permute(2, 0, 1).size() torch.Size([5, 2, 3]) pin_memory() \u5982\u679c tensor \u5c1a\u672a\u56fa\u5b9a, \u5219\u5c06 tensor \u590d\u5236\u5230\u56fa\u5b9a\u5185\u5b58. potrf(upper=True) \u2192 Tensor \u8bf7\u67e5\u770b torch.potrf() potri(upper=True) \u2192 Tensor \u8bf7\u67e5\u770b torch.potri() potrs(input2, upper=True) \u2192 Tensor \u8bf7\u67e5\u770b torch.potrs() pow(exponent) \u8bf7\u67e5\u770b torch.pow() pow_(exponent) pow() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f prod(dim=None, keepdim=False) \u2192 float \u8bf7\u67e5\u770b torch.prod() pstrf(upper=True, tol=-1) -> (Tensor, IntTensor) \u8bf7\u67e5\u770b torch.pstrf() put_(indices, tensor, accumulate=False) \u2192 Tensor \u590d\u5236 tensor \u5185\u7684\u5143\u7d20\u5230 indices \u6307\u5b9a\u7684\u4f4d\u7f6e. \u4e3a\u4e86\u8fbe\u5230\u7d22\u5f15\u7684\u76ee\u7684, self tensor \u88ab\u5f53\u505a\u4e00\u7ef4 (1D) \u7684 tensor. \u5982\u679c accumulate \u662f True , tensor \u5185\u7684\u5143\u7d20\u7d2f\u52a0\u5230 self \u4e2d. \u5982\u679c accumulate \u662f False , \u5728\u7d22\u5f15\u5305\u542b\u91cd\u590d\u7684\u503c\u65f6, \u884c\u4e3a\u672a\u5b9a\u4e49. | Parameters: | indices ( LongTensor ) \u2013 self \u7684\u7d22\u5f15 tensor ( Tensor ) \u2013 \u5305\u542b\u9700\u8981\u590d\u5236\u503c\u7684 tensor accumulate ( bool ) \u2013 \u5982\u679c\u662f True, \u5143\u7d20\u7d2f\u52a0\u5230 self | | --- | --- | Example: >>> src = torch.Tensor([[4, 3, 5], ... [6, 7, 8]]) >>> src.put_(torch.LongTensor([1, 3]), torch.Tensor([9, 10])) 4 9 5 10 7 8 [torch.FloatTensor of size 2x3] qr() -> (Tensor, Tensor) \u8bf7\u67e5\u770b torch.qr() random_(from=0, to=None, *, generator=None) \u5c06 tensor \u7528\u5728 [from, to - 1] \u4e0a\u7684\u79bb\u6563\u5747\u5300\u5206\u5e03\u8fdb\u884c\u586b\u5145. \u5982\u679c\u6ca1\u6709\u7279\u522b\u8bf4\u660e, \u586b\u5165\u7684\u503c\u7531\u672c tensor \u7684\u6570\u636e\u7c7b\u578b\u9650\u5b9a\u8303\u56f4. \u4f46\u662f, \u5bf9\u4e8e\u6d6e\u70b9\u7c7b\u578b (floating point types), \u5982\u679c\u6ca1\u6709\u7279\u522b\u8bf4\u660e, \u53d6\u503c\u8303\u56f4\u662f 0, 2^mantissa , \u4ee5\u786e\u4fdd\u6bcf\u4e2a\u6570\u90fd\u662f\u53ef\u8868\u793a\u7684. \u4f8b\u5982, <cite>torch.DoubleTensor(1).random_()</cite> \u5c06\u5747\u5300\u5206\u5e03\u5728[0, 2^53]. reciprocal() \u2192 Tensor \u8bf7\u67e5\u770b torch.reciprocal() reciprocal_() \u2192 Tensor reciprocal() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f remainder(divisor) \u2192 Tensor \u8bf7\u67e5\u770b torch.remainder() remainder_(divisor) \u2192 Tensor remainder() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f renorm(p, dim, maxnorm) \u2192 Tensor \u8bf7\u67e5\u770b torch.renorm() renorm_(p, dim, maxnorm) \u2192 Tensor renorm() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f repeat(*sizes) \u6cbf\u7740\u6307\u5b9a\u7684\u5c3a\u5bf8\u91cd\u590d tensor. \u548c expand() \u4e0d\u540c, \u8fd9\u4e2a\u51fd\u6570\u590d\u5236 tensor \u7684\u6570\u636e. Parameters: *sizes ( torch.Size or int... ) \u2013 \u6cbf\u6bcf\u4e2a\u7ef4\u5ea6\u91cd\u590d tensor \u7684\u6b21\u6570 Example >>> x = torch.Tensor([1, 2, 3]) >>> x.repeat(4, 2) 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 [torch.FloatTensor of size 4x6] >>> x.repeat(4, 2, 1).size() torch.Size([4, 2, 3]) resize_(*sizes) \u5c06 tensor \u7684\u5927\u5c0f\u8c03\u6574\u4e3a\u6307\u5b9a\u7684\u5927\u5c0f. \u5982\u679c\u5143\u7d20\u4e2a\u6570\u6bd4\u5f53\u524d\u7684\u5185\u5b58\u5927\u5c0f\u5927, \u5c31\u5c06\u5e95\u5c42\u5b58\u50a8\u5927\u5c0f\u8c03\u6574\u4e3a\u4e0e\u65b0\u5143\u7d20\u6570\u76ee\u4e00\u81f4\u7684\u5927\u5c0f. \u5982\u679c\u5143\u7d20\u4e2a\u6570\u6bd4\u5f53\u524d\u5185\u5b58\u5c0f, \u5219\u5e95\u5c42\u5b58\u50a8\u4e0d\u4f1a\u88ab\u6539\u53d8. \u539f\u6765tensor\u4e2d\u88ab\u4fdd\u5b58\u4e0b\u6765\u7684\u5143\u7d20\u5c06\u4fdd\u6301\u4e0d\u53d8, \u4f46\u65b0\u5185\u5b58\u5c06\u4e0d\u4f1a\u88ab\u521d\u59cb\u5316. Parameters: sizes ( torch.Size or int... ) \u2013 \u671f\u671b\u7684\u5927\u5c0f Example >>> x = torch.Tensor([[1, 2], [3, 4], [5, 6]]) >>> x.resize_(2, 2) >>> x 1 2 3 4 [torch.FloatTensor of size 2x2] resize_as_(tensor) \u5c06\u672c tensor \u7684\u5927\u5c0f\u8c03\u6574\u4e3a\u53c2\u6570 tensor \u7684\u5927\u5c0f. \u7b49\u6548\u4e8e: self.resize_(tensor.size()) round() \u2192 Tensor \u8bf7\u67e5\u770b torch.round() round_() \u2192 Tensor round() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f rsqrt() \u2192 Tensor \u8bf7\u67e5\u770b torch.rsqrt() rsqrt_() \u2192 Tensor rsqrt() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f scatter_(dim, index, src) \u2192 Tensor \u5c06 src \u4e2d\u7684\u6240\u6709\u503c\u6309\u7167 index \u786e\u5b9a\u7684\u7d22\u5f15\u987a\u5e8f\u5199\u5165\u672c tensor \u4e2d. \u7ed9\u5b9a\u7684 dim \u58f0\u660e\u7d22\u5f15\u7684\u7ef4\u5ea6, dim \u6309\u7167 gather() \u4e2d\u7684\u63cf\u8ff0\u7684\u89c4\u5219\u6765\u786e\u5b9a. \u6ce8\u610f, \u5173\u4e8e gather, index \u7684\u503c\u5fc5\u987b\u662f <cite>0</cite> \u5230 <cite>(self.size(dim) -1)</cite> \u533a\u95f4, \u800c\u4e14, \u5c5e\u4e8e\u540c\u4e00\u7ef4\u5ea6\u7684\u4e00\u884c\u7684\u503c\u5fc5\u987b\u662f\u552f\u4e00\u7684. | Parameters: | dim ( int ) \u2013 \u7d22\u5f15\u7684\u8f74\u5411 index ( LongTensor ) \u2013 \u6563\u5c04\u5143\u7d20\u7684\u7d22\u5f15\u6307\u6570 src ( Tensor or float ) \u2013 \u6563\u5c04\u7684\u6e90\u5143\u7d20 | | --- | --- | Example: >>> x = torch.rand(2, 5) >>> x 0.4319 0.6500 0.4080 0.8760 0.2355 0.2609 0.4711 0.8486 0.8573 0.1029 [torch.FloatTensor of size 2x5] >>> torch.zeros(3, 5).scatter_(0, torch.LongTensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x) 0.4319 0.4711 0.8486 0.8760 0.2355 0.0000 0.6500 0.0000 0.8573 0.0000 0.2609 0.0000 0.4080 0.0000 0.1029 [torch.FloatTensor of size 3x5] >>> z = torch.zeros(2, 4).scatter_(1, torch.LongTensor([[2], [3]]), 1.23) >>> z 0.0000 0.0000 1.2300 0.0000 0.0000 0.0000 0.0000 1.2300 [torch.FloatTensor of size 2x4] select(dim, index) \u2192 Tensor or number \u6cbf\u7740 dim \u7ed9\u5b9a\u7684\u7ef4\u5ea6, \u6309\u7167 index \u5207\u7247. \u5982\u679c\u8fd9\u4e2a tensor \u662f\u4e00\u7ef4\u7684, \u8fd4\u56de\u4e00\u4e2a\u6570\u5b57. \u5426\u5219, \u8fd4\u56de\u4e00\u4e2a\u7ed9\u5b9a\u7ef4\u5ea6\u5df2\u7ecf\u88ab\u79fb\u9664\u7684 tensor. | Parameters: | dim ( int ) \u2013 \u5207\u7247\u7684\u7ef4\u5ea6 index ( int ) \u2013 \u7528\u6765\u9009\u53d6\u7684\u7d22\u5f15 | | --- | --- | Note select \u7b49\u6548\u4e8e\u5207\u7247. \u4f8b\u5982, `tensor.select(0, index) () \u7b49\u6548\u4e8e tensor[index] \u548c tensor.select(2, index) \u7b49\u6548\u4e8e tensor[:,:,index]`. set_(source=None, storage_offset=0, size=None, stride=None) \u8bbe\u7f6e\u5e95\u5c42\u5b58\u50a8, \u5927\u5c0f, \u548c\u6b65\u957f. \u5982\u679c source \u662f\u4e00\u4e2a tensor \u5bf9\u8c61, \u672c tensor \u548c\u8be5 tensor \u5171\u4eab\u5e95\u5c42\u5b58\u50a8, \u5e76\u4e14\u5927\u5c0f\u548c\u6b65\u957f\u4e00\u6837. \u5728\u5176\u4e2d\u4e00\u4e2a tensor \u4e2d\u6539\u53d8\u5143\u7d20, \u4f1a\u97f3\u54cd\u5230\u53e6\u4e00\u4e2a tensor. \u5982\u679c source \u662f\u4e00\u4e2a Storage , \u5219\u5c06\u8bbe\u7f6e\u5e95\u5c42\u5185\u5b58, \u504f\u79fb\u91cf, \u5927\u5c0f\u548c\u6b65\u957f. | Parameters: | source ( Tensor or Storage ) \u2013 \u7528\u5230\u7684 tensor \u6216 storage storage_offset ( int ) \u2013 storage \u7684\u504f\u79fb\u91cf size ( torch.Size ) \u2013 \u671f\u671b\u7684\u5927\u5c0f. \u9ed8\u8ba4\u4e3a\u6e90 tensor \u7684\u5927\u5c0f. stride ( tuple ) \u2013 \u671f\u671b\u7684\u6b65\u957f. \u9ed8\u8ba4\u4e3a C \u76f8\u90bb\u5185\u5b58\u7684\u6b65\u957f. | | --- | --- | share_memory_() \u5c06\u5e95\u5c42\u5b58\u50a8\u79fb\u5230\u5171\u4eab\u5185\u5b58. \u5982\u679c\u5e95\u5c42\u5b58\u50a8\u5df2\u7ecf\u5728\u5171\u4eab\u5185\u5b58\u548cCUDA tensor \u4e2d, \u5219\u8fd9\u662f\u65e0\u64cd\u4f5c. \u5171\u4eab\u5185\u5b58\u4e2d\u7684 tensor \u4e0d\u80fd\u8c03\u6574\u5927\u5c0f. short() \u5c06\u8fd9\u4e2a tensor \u8f6c\u6362\u4e3a short \u7c7b\u578b sigmoid() \u2192 Tensor \u8bf7\u67e5\u770b torch.sigmoid() sigmoid_() \u2192 Tensor sigmoid() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f sign() \u2192 Tensor \u8bf7\u67e5\u770b torch.sign() sign_() \u2192 Tensor sign() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f sin() \u2192 Tensor \u8bf7\u67e5\u770b torch.sin() sin_() \u2192 Tensor sin() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f sinh() \u2192 Tensor \u8bf7\u67e5\u770b torch.sinh() sinh_() \u2192 Tensor sinh() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f size() \u2192 torch.Size \u8fd4\u56de tensor \u7684\u5927\u5c0f. \u8fd4\u56de\u7684\u503c\u662f tuple \u7684\u5b50\u7c7b. Example >>> torch.Tensor(3, 4, 5).size() torch.Size([3, 4, 5]) sort(dim=None, descending=False) -> (Tensor, LongTensor) \u8bf7\u67e5\u770b torch.sort() split(split_size, dim=0) \u5c06 tensor \u5206\u89e3\u6210 tensor \u5143\u7ec4. See torch.split() . sqrt() \u2192 Tensor \u8bf7\u67e5\u770b torch.sqrt() sqrt_() \u2192 Tensor sqrt() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f squeeze(dim=None) \u8bf7\u67e5\u770b torch.squeeze() squeeze_(dim=None) squeeze() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f std(dim=None, unbiased=True, keepdim=False) \u2192 float \u8bf7\u67e5\u770b torch.std() storage() \u2192 torch.Storage \u8fd4\u56de\u5e95\u5c42\u5b58\u50a8 storage_offset() \u2192 int \u6309\u7167\u50a8\u5b58\u5143\u7d20\u4e2a\u6570\u7684\u504f\u79fb\u8fd4\u56de tensor \u5728\u5e95\u5c42\u5b58\u50a8\u4e2d\u7684\u504f\u79fb\u91cf(\u4e0d\u662f\u6309\u7167\u5b57\u8282\u8ba1\u7b97). Example >>> x = torch.Tensor([1, 2, 3, 4, 5]) >>> x.storage_offset() 0 >>> x[3:].storage_offset() 3 classmethod storage_type () stride(dim) \u2192 tuple or int \u8fd4\u56de tesnor \u7684\u6b65\u957f. \u6b65\u957f\u662f\u6307\u6309\u7167 dim \u6307\u5b9a\u7684\u7ef4\u5ea6, \u4ece\u4e00\u4e2a\u5143\u7d20\u5230\u4e0b\u4e00\u4e2a\u5143\u7d20\u9700\u8981\u8df3\u8dc3\u7684\u8ddd\u79bb. \u5f53\u6ca1\u6709\u6307\u5b9a\u7ef4\u5ea6, \u4f1a\u8ba1\u7b97\u6240\u6709\u7ef4\u5ea6\u7684\u6b65\u957f, \u5e76\u8fd4\u56de\u4e00\u4e2a tuple. \u5f53\u7ed9\u5b9a\u7ef4\u5ea6\u65f6, \u8fd4\u56de\u8fd9\u4e2a\u7ef4\u5ea6\u7684\u6b65\u957f. Parameters: dim ( int ) \u2013 \u671f\u671b\u7684\u9700\u8981\u8ba1\u7b97\u6b65\u957f\u7684\u7ef4\u5ea6. Example >>> x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) >>> x.stride() (5, 1) >>>x.stride(0) 5 >>> x.stride(-1) 1 sub(value, other) \u2192 Tensor \u4ece tensor \u4e2d\u62bd\u53d6\u4e00\u4e2a\u6807\u91cf\u6216\u5f20\u91cf. \u5982\u679c value \u548c other \u90fd\u662f\u7ed9\u5b9a\u7684, \u5219\u5728\u4f7f\u7528\u4e4b\u524d other \u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u4f1a\u88ab :attr: value \u7f29\u653e. \u5982\u679c other \u662f\u4e00\u4e2atensor, other \u7684\u5f62\u72b6\u5fc5\u987b\u4e8e\u57fa\u7840 tensor \u7684\u5f62\u72b6\u662f\u53ef\u5e7f\u64ad\u7684 ( broadcastable ). sub_(x) \u2192 Tensor sub() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f sum(dim=None, keepdim=False) \u2192 float \u8bf7\u67e5\u770b torch.sum() svd(some=True) -> (Tensor, Tensor, Tensor) \u8bf7\u67e5\u770b torch.svd() symeig(eigenvectors=False, upper=True) -> (Tensor, Tensor) \u8bf7\u67e5\u770b torch.symeig() t() \u2192 Tensor \u8bf7\u67e5\u770b torch.t() t_() \u2192 Tensor t() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f take(indices) \u2192 Tensor \u8bf7\u67e5\u770b torch.take() tan() tan_() \u2192 Tensor tan() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f tanh() \u2192 Tensor \u8bf7\u67e5\u770b torch.tanh() tanh_() \u2192 Tensor tanh() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f tolist() \u8fd4\u56de\u6b64 tensor \u7684\u5d4c\u5957\u5217\u8868\u8868\u793a. topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor) \u8bf7\u67e5\u770b torch.topk() trace() \u2192 float \u8bf7\u67e5\u770b torch.trace() transpose(dim0, dim1) \u2192 Tensor \u8bf7\u67e5\u770b torch.transpose() transpose_(dim0, dim1) \u2192 Tensor transpose() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f tril(k=0) \u2192 Tensor \u8bf7\u67e5\u770b torch.tril() tril_(k=0) \u2192 Tensor tril() triu(k=0) \u2192 Tensor \u8bf7\u67e5\u770b torch.triu() triu_(k=0) \u2192 Tensor triu() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f trtrs(A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor) \u8bf7\u67e5\u770b torch.trtrs() trunc() \u2192 Tensor \u8bf7\u67e5\u770b torch.trunc() trunc_() \u2192 Tensor trunc() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f type(new_type=None, async=False) \u5982\u679c\u6ca1\u6709\u6307\u5b9a <cite>new_type</cite> \u5219\u8fd4\u56de\u8be5\u7c7b\u578b , \u5426\u5219\u5c06\u6b64\u5bf9\u8c61\u8f6c\u6362\u4e3a\u6307\u5b9a\u7c7b\u578b . \u5982\u679c\u5df2\u7ecf\u662f\u6b63\u786e\u7684\u7c7b\u578b , \u5219\u4e0d\u6267\u884c\u590d\u5236\u5e76\u76f4\u63a5\u8fd4\u56de\u539f\u5bf9\u8c61 . | Parameters: | new_type ( type or string ) \u2013 \u671f\u671b\u7684\u7c7b\u578b async ( bool ) \u2013 \u5982\u679c\u4e3a True , \u5e76\u4e14\u6e90\u5728\u9501\u5b9a\u5185\u5b58\u4e2d\u800c\u76ee\u6807\u5728GPU\u4e2d , \u5219\u526f\u672c\u5c06\u4e0e\u4e3b\u673a\u5f02\u6b65\u6267\u884c , \u53cd\u4e4b\u4ea6\u7136 . \u5426\u5219\u6b64\u53c2\u6570\u4e0d\u8d77\u6548\u679c . | | --- | --- | type_as(tensor) \u5c06\u6b64\u5f20\u91cf\u8f6c\u6362\u4e3a\u7ed9\u5b9a tensor \u7684\u7c7b\u578b. \u5982\u679c tensor \u5df2\u7ecf\u662f\u6b63\u786e\u7684\u7c7b\u578b, \u8fd9\u662f\u4e00\u4e2a\u65e0\u7528\u7684\u64cd\u4f5c. \u8fd9\u76f8\u5f53\u4e8e: self.type(tensor.type()) Params: tensor (Tensor): tensor \u5177\u6709\u6240\u9700\u7684\u7c7b\u578b unfold(dim, size, step) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u5728 dim \u7ef4\u5ea6\u4e0a\u5305\u542b\u6240\u6709 size \u5927\u5c0f\u5207\u7247\u7684 tensor. step \u8bf4\u660e\u4e24\u4e2a\u5207\u7247\u4e4b\u95f4\u7684\u6b65\u957f. \u5982\u679c <cite>sizedim</cite> \u662f\u539ftensor\u5728 dim \u7ef4\u5ea6\u539f\u6765\u7684\u5927\u5c0f, \u5219\u8fd4\u56de\u7684 tensor \u5728 <cite>dim</cite> \u7ef4\u5ea6\u7684\u5927\u5c0f\u662f <cite>(sizedim - size) / step + 1</cite> \u4e00\u4e2a\u989d\u5916\u7684\u5207\u7247\u5927\u5c0f\u7684\u7ef4\u5ea6\u5df2\u7ecf\u6dfb\u52a0\u5728\u8fd4\u56de\u7684 tensor \u4e2d. | Parameters: | dim ( int ) \u2013 \u9700\u8981\u5c55\u5f00\u7684\u7ef4\u5ea6 size ( int ) \u2013 \u6bcf\u4e00\u4e2a\u5206\u7247\u9700\u8981\u5c55\u5f00\u7684\u5927\u5c0f step ( int ) \u2013 \u76f8\u90bb\u5206\u7247\u4e4b\u95f4\u7684\u6b65\u957f | | --- | --- | Example: >>> x = torch.arange(1, 8) >>> x 1 2 3 4 5 6 7 [torch.FloatTensor of size 7] >>> x.unfold(0, 2, 1) 1 2 2 3 3 4 4 5 5 6 6 7 [torch.FloatTensor of size 6x2] >>> x.unfold(0, 2, 2) 1 2 3 4 5 6 [torch.FloatTensor of size 3x2] uniform_(from=0, to=1) \u2192 Tensor \u5c06 tensor \u7528\u4ece\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u6837\u5f97\u5230\u7684\u503c\u586b\u5145: unsqueeze(dim) \u8bf7\u67e5\u770b torch.unsqueeze() unsqueeze_(dim) unsqueeze() \u7684 in-place \u8fd0\u7b97\u5f62\u5f0f var(dim=None, unbiased=True, keepdim=False) \u2192 float \u8bf7\u67e5\u770b torch.var() view(*args) \u2192 Tensor \u8fd4\u56de\u4e00\u4e2a\u6709\u76f8\u540c\u6570\u636e\u4f46\u5927\u5c0f\u4e0d\u540c\u7684\u65b0\u7684 tensor. \u8fd4\u56de\u7684 tensor \u4e0e\u539f tensor \u5171\u4eab\u76f8\u540c\u7684\u6570\u636e, \u4e00\u5b9a\u6709\u76f8\u540c\u6570\u76ee\u7684\u5143\u7d20, \u4f46\u5927\u5c0f\u4e0d\u540c. \u4e00\u4e2a tensor \u5fc5\u987b\u662f\u8fde\u7eed\u7684 ( contiguous() ) \u624d\u80fd\u88ab\u67e5\u770b. Parameters: args ( torch.Size or int... ) \u2013 \u671f\u671b\u7684\u5927\u5c0f Example >>> x = torch.randn(4, 4) >>> x.size() torch.Size([4, 4]) >>> y = x.view(16) >>> y.size() torch.Size([16]) >>> z = x.view(-1, 8) # the size -1 is inferred from other dimensions >>> z.size() torch.Size([2, 8]) view_as(tensor) \u5c06\u8be5 tensor \u4f5c\u4e3a\u6307\u5b9a\u7684 tensor \u8fd4\u56de\u67e5\u770b. \u8fd9\u76f8\u5f53\u4e8e: self.view(tensor.size()) zero_() \u75280\u586b\u5145\u8be5 tensor. class torch.ByteTensor \u4e0b\u9762\u8fd9\u4e9b\u51fd\u6570\u65b9\u6cd5\u53ea\u5b58\u5728\u4e8e torch.ByteTensor . all() \u2192 bool \u5982\u679c tensor \u91cc\u7684\u6240\u6709\u5143\u7d20\u90fd\u662f\u975e\u96f6\u7684, \u5219\u8fd4\u56de True, \u5426\u5728\u8fd4\u56de False. any() \u2192 bool \u5982\u679c tensor \u91cc\u7684\u5b58\u5728\u5143\u7d20\u662f\u975e\u96f6\u7684, \u5219\u8fd4\u56de True, \u5426\u5728\u8fd4\u56de False.","title":"torch.Tensor"},{"location":"55/","text":"torch.sparse Warning \u6b64 API \u76ee\u524d\u662f\u5b9e\u9a8c\u6027\u7684 , \u53ef\u80fd\u4f1a\u5728\u4e0d\u4e45\u7684\u5c06\u6765\u53d1\u751f\u53d8\u5316 . Torch \u652f\u6301 COO(rdinate) \u683c\u5f0f\u7684\u7a00\u758f\u5f20\u91cf , \u8fd8\u80fd\u9ad8\u6548\u5730\u5b58\u50a8\u548c\u5904\u7406\u5927\u591a\u6570\u5143\u7d20\u4e3a\u96f6\u7684 \u5f20\u91cf . \u4e00\u4e2a\u7a00\u758f\u5f20\u91cf\u53ef\u4ee5\u8868\u793a\u4e3a\u4e00\u5bf9\u7a20\u5bc6\u5f20\u91cf : \u4e00\u4e2a\u5f20\u91cf\u7684\u503c\u548c\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u7684\u6307\u6570 . \u901a\u8fc7\u63d0\u4f9b\u8fd9\u4e24\u4e2a\u5f20\u91cf\u4ee5\u53ca\u7a00\u758f\u5f20\u91cf\u7684\u5927\u5c0f (\u4e0d\u80fd\u4ece\u8fd9\u4e9b\u5f20\u91cf\u63a8\u65ad!) , \u53ef\u4ee5\u6784\u9020\u4e00\u4e2a\u7a00\u758f\u5f20\u91cf . \u5047\u8bbe\u6211\u4eec\u8981\u5728\u4f4d\u7f6e (0,2) \u5904\u5b9a\u4e49\u6761\u76ee3 , \u4f4d\u7f6e (1,0) \u7684\u6761\u76ee4 , \u4f4d\u7f6e (1,2) \u7684\u6761\u76ee5\u7684 \u7a00\u758f\u5f20\u91cf , \u6211\u4eec\u53ef\u4ee5\u8fd9\u6837\u5199 : >>> i = torch.LongTensor([[0, 1, 1], [2, 0, 2]]) >>> v = torch.FloatTensor([3, 4, 5]) >>> torch.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense() 0 0 3 4 0 5 [torch.FloatTensor of size 2x3] \u8bf7\u6ce8\u610f , LongTensor \u7684\u4f20\u5165\u53c2\u6570\u4e0d\u662f\u7d22\u5f15\u5143\u7ec4\u7684\u5217\u8868 . \u5982\u679c\u4f60\u60f3\u7528\u8fd9\u79cd\u65b9\u5f0f\u7f16\u5199\u7d22\u5f15 , \u4f60\u5e94\u8be5\u5728 \u5c06\u5b83\u4eec\u4f20\u9012\u7ed9\u7a00\u758f\u6784\u9020\u51fd\u6570\u4e4b\u524d\u8fdb\u884c\u8f6c\u6362 : >>> i = torch.LongTensor([[0, 2], [1, 0], [1, 2]]) >>> v = torch.FloatTensor([3, 4, 5 ]) >>> torch.sparse.FloatTensor(i.t(), v, torch.Size([2,3])).to_dense() 0 0 3 4 0 5 [torch.FloatTensor of size 2x3] \u4f60\u8fd8\u53ef\u4ee5\u6784\u9020\u6df7\u5408\u7a00\u758f\u5f20\u91cf , \u5176\u4e2d\u53ea\u6709\u7b2c\u4e00\u4e2an\u7ef4\u662f\u7a00\u758f\u7684 , \u800c\u5176\u4f59\u7ef4\u5ea6\u662f\u5bc6\u96c6\u7684 . >>> i = torch.LongTensor([[2, 4]]) >>> v = torch.FloatTensor([[1, 3], [5, 7]]) >>> torch.sparse.FloatTensor(i, v).to_dense() 0 0 0 0 1 3 0 0 5 7 [torch.FloatTensor of size 5x2] \u4e00\u4e2a\u7a7a\u7684\u7a00\u758f\u5f20\u91cf\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u5b83\u7684\u5927\u5c0f\u6765\u6784\u9020 : >>> torch.sparse.FloatTensor(2, 3) SparseFloatTensor of size 2x3 with indices: [torch.LongTensor with no dimension] and values: [torch.FloatTensor with no dimension] Note \u6211\u4eec\u7684\u7a00\u758f\u5f20\u91cf\u683c\u5f0f\u5141\u8bb8\u975e\u805a\u5408\u7a00\u758f\u5f20\u91cf , \u7d22\u5f15\u53ef\u80fd\u5bf9\u5e94\u6709\u91cd\u590d\u7684\u5750\u6807 ; \u5728\u8fd9 \u79cd\u60c5\u51b5\u4e0b , \u8be5\u7d22\u5f15\u5904\u7684\u503c\u4ee3\u8868\u6240\u6709\u91cd\u590d\u6761\u76ee\u503c\u7684\u603b\u548c . \u975e\u805a\u5408\u5f20\u91cf\u5141\u8bb8\u6211\u4eec\u66f4 \u6709\u6548\u5730\u5b9e\u73b0\u786e\u5b9a\u7684\u64cd\u4f5c\u7b26 . \u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b , \u4f60\u4e0d\u5fc5\u5173\u5fc3\u7a00\u758f\u5f20\u91cf\u662f\u5426\u805a\u5408 , \u56e0\u4e3a\u5927\u591a\u6570\u64cd\u4f5c\u5728\u805a\u5408\u6216 \u4e0d\u805a\u5408\u7a00\u758f\u5f20\u91cf\u7684\u60c5\u51b5\u4e0b\u90fd\u4f1a\u4ee5\u76f8\u540c\u7684\u65b9\u5f0f\u5de5\u4f5c . \u4f46\u662f , \u4f60\u53ef\u80fd\u9700\u8981\u5173\u5fc3\u4e24\u79cd\u60c5\u51b5 . \u9996\u5148 , \u5982\u679c\u4f60\u53cd\u590d\u6267\u884c\u53ef\u4ee5\u4ea7\u751f\u91cd\u590d\u6761\u76ee\u7684\u64cd\u4f5c (\u4f8b\u5982 , torch.sparse.FloatTensor.add() ) , \u5219\u5e94\u9002\u5f53\u805a\u5408\u7a00\u758f\u5f20\u91cf\u4ee5\u9632\u6b62\u5b83\u4eec\u53d8\u5f97\u592a\u5927. \u5176\u6b21 , \u4e00\u4e9b\u64cd\u4f5c\u7b26\u5c06\u6839\u636e\u662f\u5426\u805a\u5408 (\u4f8b\u5982 , torch.sparse.FloatTensor._values() \u548c torch.sparse.FloatTensor._indices() , \u8fd8\u6709 torch.Tensor._sparse_mask() ) \u6765\u751f\u6210\u4e0d\u540c\u7684\u503c . \u8fd9\u4e9b\u8fd0\u7b97\u7b26\u524d\u9762\u52a0\u4e0b\u5212\u7ebf\u8868\u793a\u5b83\u4eec\u63ed\u793a \u5185\u90e8\u5b9e\u73b0\u7ec6\u8282 , \u56e0\u6b64\u5e94\u8c28\u614e\u4f7f , \u56e0\u4e3a\u4e0e\u805a\u5408\u7684\u7a00\u758f\u5f20\u91cf\u4e00\u8d77\u5de5\u4f5c\u7684\u4ee3\u7801\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u672a\u805a\u5408\u7684\u7a00\u758f\u5f20\u91cf ; \u4e00\u822c\u6765\u8bf4 , \u5728\u8fd0\u7528\u8fd9\u4e9b\u8fd0\u7b97\u7b26\u4e4b\u524d , \u6700\u5b89\u5168\u7684\u5c31\u662f\u786e\u4fdd\u662f\u805a\u5408\u7684 . \u4f8b\u5982 , \u5047\u8bbe\u6211\u4eec\u60f3\u76f4\u63a5\u901a\u8fc7 torch.sparse.FloatTensor._values() \u6765\u5b9e\u73b0\u4e00\u4e2a\u64cd\u4f5c . \u968f\u7740\u4e58\u6cd5\u5206\u5e03\u7684\u589e\u52a0 , \u6807\u91cf\u7684\u4e58\u6cd5\u53ef\u4ee5\u8f7b\u6613\u5b9e\u73b0 ; \u7136\u800c , \u5e73\u65b9\u6839\u4e0d\u80fd\u76f4\u63a5\u5b9e\u73b0 , sqrt(a + b) != sqrt(a) +sqrt(b) (\u5982\u679c\u7ed9\u5b9a\u4e00\u4e2a\u975e\u805a\u5408\u5f20\u91cf , \u8fd9\u5c06\u88ab\u8ba1\u7b97\u51fa\u6765 . ) class torch.sparse.FloatTensor add() add_() clone() dim() div() div_() get_device() hspmm() mm() mul() mul_() resizeAs_() size() spadd() spmm() sspaddmm() sspmm() sub() sub_() t_() toDense() transpose() transpose_() zero_() coalesce() is_coalesced() _indices() _values() _nnz()","title":"torch.sparse"},{"location":"55/#torchsparse","text":"Warning \u6b64 API \u76ee\u524d\u662f\u5b9e\u9a8c\u6027\u7684 , \u53ef\u80fd\u4f1a\u5728\u4e0d\u4e45\u7684\u5c06\u6765\u53d1\u751f\u53d8\u5316 . Torch \u652f\u6301 COO(rdinate) \u683c\u5f0f\u7684\u7a00\u758f\u5f20\u91cf , \u8fd8\u80fd\u9ad8\u6548\u5730\u5b58\u50a8\u548c\u5904\u7406\u5927\u591a\u6570\u5143\u7d20\u4e3a\u96f6\u7684 \u5f20\u91cf . \u4e00\u4e2a\u7a00\u758f\u5f20\u91cf\u53ef\u4ee5\u8868\u793a\u4e3a\u4e00\u5bf9\u7a20\u5bc6\u5f20\u91cf : \u4e00\u4e2a\u5f20\u91cf\u7684\u503c\u548c\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u7684\u6307\u6570 . \u901a\u8fc7\u63d0\u4f9b\u8fd9\u4e24\u4e2a\u5f20\u91cf\u4ee5\u53ca\u7a00\u758f\u5f20\u91cf\u7684\u5927\u5c0f (\u4e0d\u80fd\u4ece\u8fd9\u4e9b\u5f20\u91cf\u63a8\u65ad!) , \u53ef\u4ee5\u6784\u9020\u4e00\u4e2a\u7a00\u758f\u5f20\u91cf . \u5047\u8bbe\u6211\u4eec\u8981\u5728\u4f4d\u7f6e (0,2) \u5904\u5b9a\u4e49\u6761\u76ee3 , \u4f4d\u7f6e (1,0) \u7684\u6761\u76ee4 , \u4f4d\u7f6e (1,2) \u7684\u6761\u76ee5\u7684 \u7a00\u758f\u5f20\u91cf , \u6211\u4eec\u53ef\u4ee5\u8fd9\u6837\u5199 : >>> i = torch.LongTensor([[0, 1, 1], [2, 0, 2]]) >>> v = torch.FloatTensor([3, 4, 5]) >>> torch.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense() 0 0 3 4 0 5 [torch.FloatTensor of size 2x3] \u8bf7\u6ce8\u610f , LongTensor \u7684\u4f20\u5165\u53c2\u6570\u4e0d\u662f\u7d22\u5f15\u5143\u7ec4\u7684\u5217\u8868 . \u5982\u679c\u4f60\u60f3\u7528\u8fd9\u79cd\u65b9\u5f0f\u7f16\u5199\u7d22\u5f15 , \u4f60\u5e94\u8be5\u5728 \u5c06\u5b83\u4eec\u4f20\u9012\u7ed9\u7a00\u758f\u6784\u9020\u51fd\u6570\u4e4b\u524d\u8fdb\u884c\u8f6c\u6362 : >>> i = torch.LongTensor([[0, 2], [1, 0], [1, 2]]) >>> v = torch.FloatTensor([3, 4, 5 ]) >>> torch.sparse.FloatTensor(i.t(), v, torch.Size([2,3])).to_dense() 0 0 3 4 0 5 [torch.FloatTensor of size 2x3] \u4f60\u8fd8\u53ef\u4ee5\u6784\u9020\u6df7\u5408\u7a00\u758f\u5f20\u91cf , \u5176\u4e2d\u53ea\u6709\u7b2c\u4e00\u4e2an\u7ef4\u662f\u7a00\u758f\u7684 , \u800c\u5176\u4f59\u7ef4\u5ea6\u662f\u5bc6\u96c6\u7684 . >>> i = torch.LongTensor([[2, 4]]) >>> v = torch.FloatTensor([[1, 3], [5, 7]]) >>> torch.sparse.FloatTensor(i, v).to_dense() 0 0 0 0 1 3 0 0 5 7 [torch.FloatTensor of size 5x2] \u4e00\u4e2a\u7a7a\u7684\u7a00\u758f\u5f20\u91cf\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u5b83\u7684\u5927\u5c0f\u6765\u6784\u9020 : >>> torch.sparse.FloatTensor(2, 3) SparseFloatTensor of size 2x3 with indices: [torch.LongTensor with no dimension] and values: [torch.FloatTensor with no dimension] Note \u6211\u4eec\u7684\u7a00\u758f\u5f20\u91cf\u683c\u5f0f\u5141\u8bb8\u975e\u805a\u5408\u7a00\u758f\u5f20\u91cf , \u7d22\u5f15\u53ef\u80fd\u5bf9\u5e94\u6709\u91cd\u590d\u7684\u5750\u6807 ; \u5728\u8fd9 \u79cd\u60c5\u51b5\u4e0b , \u8be5\u7d22\u5f15\u5904\u7684\u503c\u4ee3\u8868\u6240\u6709\u91cd\u590d\u6761\u76ee\u503c\u7684\u603b\u548c . \u975e\u805a\u5408\u5f20\u91cf\u5141\u8bb8\u6211\u4eec\u66f4 \u6709\u6548\u5730\u5b9e\u73b0\u786e\u5b9a\u7684\u64cd\u4f5c\u7b26 . \u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b , \u4f60\u4e0d\u5fc5\u5173\u5fc3\u7a00\u758f\u5f20\u91cf\u662f\u5426\u805a\u5408 , \u56e0\u4e3a\u5927\u591a\u6570\u64cd\u4f5c\u5728\u805a\u5408\u6216 \u4e0d\u805a\u5408\u7a00\u758f\u5f20\u91cf\u7684\u60c5\u51b5\u4e0b\u90fd\u4f1a\u4ee5\u76f8\u540c\u7684\u65b9\u5f0f\u5de5\u4f5c . \u4f46\u662f , \u4f60\u53ef\u80fd\u9700\u8981\u5173\u5fc3\u4e24\u79cd\u60c5\u51b5 . \u9996\u5148 , \u5982\u679c\u4f60\u53cd\u590d\u6267\u884c\u53ef\u4ee5\u4ea7\u751f\u91cd\u590d\u6761\u76ee\u7684\u64cd\u4f5c (\u4f8b\u5982 , torch.sparse.FloatTensor.add() ) , \u5219\u5e94\u9002\u5f53\u805a\u5408\u7a00\u758f\u5f20\u91cf\u4ee5\u9632\u6b62\u5b83\u4eec\u53d8\u5f97\u592a\u5927. \u5176\u6b21 , \u4e00\u4e9b\u64cd\u4f5c\u7b26\u5c06\u6839\u636e\u662f\u5426\u805a\u5408 (\u4f8b\u5982 , torch.sparse.FloatTensor._values() \u548c torch.sparse.FloatTensor._indices() , \u8fd8\u6709 torch.Tensor._sparse_mask() ) \u6765\u751f\u6210\u4e0d\u540c\u7684\u503c . \u8fd9\u4e9b\u8fd0\u7b97\u7b26\u524d\u9762\u52a0\u4e0b\u5212\u7ebf\u8868\u793a\u5b83\u4eec\u63ed\u793a \u5185\u90e8\u5b9e\u73b0\u7ec6\u8282 , \u56e0\u6b64\u5e94\u8c28\u614e\u4f7f , \u56e0\u4e3a\u4e0e\u805a\u5408\u7684\u7a00\u758f\u5f20\u91cf\u4e00\u8d77\u5de5\u4f5c\u7684\u4ee3\u7801\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u672a\u805a\u5408\u7684\u7a00\u758f\u5f20\u91cf ; \u4e00\u822c\u6765\u8bf4 , \u5728\u8fd0\u7528\u8fd9\u4e9b\u8fd0\u7b97\u7b26\u4e4b\u524d , \u6700\u5b89\u5168\u7684\u5c31\u662f\u786e\u4fdd\u662f\u805a\u5408\u7684 . \u4f8b\u5982 , \u5047\u8bbe\u6211\u4eec\u60f3\u76f4\u63a5\u901a\u8fc7 torch.sparse.FloatTensor._values() \u6765\u5b9e\u73b0\u4e00\u4e2a\u64cd\u4f5c . \u968f\u7740\u4e58\u6cd5\u5206\u5e03\u7684\u589e\u52a0 , \u6807\u91cf\u7684\u4e58\u6cd5\u53ef\u4ee5\u8f7b\u6613\u5b9e\u73b0 ; \u7136\u800c , \u5e73\u65b9\u6839\u4e0d\u80fd\u76f4\u63a5\u5b9e\u73b0 , sqrt(a + b) != sqrt(a) +sqrt(b) (\u5982\u679c\u7ed9\u5b9a\u4e00\u4e2a\u975e\u805a\u5408\u5f20\u91cf , \u8fd9\u5c06\u88ab\u8ba1\u7b97\u51fa\u6765 . ) class torch.sparse.FloatTensor add() add_() clone() dim() div() div_() get_device() hspmm() mm() mul() mul_() resizeAs_() size() spadd() spmm() sspaddmm() sspmm() sub() sub_() t_() toDense() transpose() transpose_() zero_() coalesce() is_coalesced() _indices() _values() _nnz()","title":"torch.sparse"},{"location":"56/","text":"torch.Storage \u4e00\u4e2a torch.Storage \u662f\u4e00\u4e2a\u5355\u4e00\u6570\u636e\u7c7b\u578b\u7684\u8fde\u7eed\u4e00\u7ef4\u6570\u7ec4 . \u6bcf\u4e2a torch.Tensor \u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684\u76f8\u540c\u6570\u636e\u7c7b\u578b\u7684\u5b58\u50a8 . class torch.FloatStorage byte() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a byte \u7c7b\u578b char() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a char \u7c7b\u578b clone() \u8fd4\u56de\u6b64\u5b58\u50a8\u7684\u4e00\u4e2a\u526f\u672c copy_() cpu() \u5982\u679c\u5f53\u524d\u6b64\u5b58\u50a8\u4e0d\u5728CPU\u4e0a , \u5219\u8fd4\u56de\u4e00\u4e2a\u5b83\u7684CPU\u526f\u672c . cuda(device=None, async=False) \u8fd4\u56de\u6b64\u5bf9\u8c61\u5728 CUDA \u5185\u5b58\u4e2d\u7684\u4e00\u4e2a\u526f\u672c . \u5982\u679c\u6b64\u5bf9\u8c61\u5df2\u7ecf\u5728 CUDA \u5185\u5b58\u4e2d\u5e76\u4e14\u5728\u6b63\u786e\u7684\u8bbe\u5907\u4e0a , \u90a3\u4e48\u4e0d\u4f1a\u6267\u884c\u590d\u5236\u64cd\u4f5c , \u76f4\u63a5\u8fd4\u56de\u539f\u5bf9\u8c61 . | Parameters: | device ( int ) \u2013 \u76ee\u6807 GPU \u7684 id . \u9ed8\u8ba4\u503c\u662f\u5f53\u524d\u8bbe\u5907 . async ( bool ) \u2013 \u5982\u679c\u4e3a True \u5e76\u4e14\u6e90\u4f4d\u4e8e\u9501\u5b9a\u5185\u5b58\u4e2d , \u5219\u526f\u672c\u76f8\u5bf9\u4e8e\u4e3b\u673a\u662f\u5f02\u6b65\u7684 . \u5426\u5219\u6b64\u53c2\u6570\u4e0d\u8d77\u6548\u679c . | | --- | --- | data_ptr() double() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a double \u7c7b\u578b element_size() fill_() float() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a float \u7c7b\u578b from_buffer() from_file(filename, shared=False, size=0) \u2192 Storage \u5982\u679c shared \u4e3a True , \u90a3\u4e48\u5185\u5b58\u5c06\u4f1a\u5728\u6240\u6709\u8fdb\u7a0b\u95f4\u5171\u4eab . \u6240\u6709\u7684\u66f4\u6539\u90fd\u4f1a\u88ab\u5199\u5165\u6587\u4ef6 . \u5982\u679c shared \u4e3a False , \u90a3\u4e48\u5bf9\u4e8e\u5185\u5b58\u7684\u4fee\u6539 , \u5219\u4e0d\u4f1a\u5f71\u54cd\u5230\u6587\u4ef6 . size \u662f\u5b58\u50a8\u4e2d\u6240\u5305\u542b\u7684\u5143\u7d20\u4e2a\u6570 . \u5982\u679c shared \u4e3a False \u5219\u6587\u4ef6\u5fc5\u987b\u5305\u542b\u81f3\u5c11 <cite>size * sizeof(Type)</cite> \u5b57\u8282 ( <cite>Type</cite> \u662f\u6240\u5b58\u50a8\u7684\u7c7b\u578b) . \u5982\u679c shared \u4e3a True , \u6587\u4ef6\u4f1a\u5728\u9700\u8981\u7684\u65f6\u5019\u88ab\u521b\u5efa . | Parameters: | filename ( str ) \u2013 \u8981\u6620\u5c04\u5230\u7684\u6587\u4ef6\u540d shared ( bool ) \u2013 \u662f\u5426\u5171\u4eab\u5185\u5b58 size ( int ) \u2013 \u5b58\u50a8\u4e2d\u5305\u542b\u5143\u7d20\u7684\u4e2a\u6570 | | --- | --- | half() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a half \u7c7b\u578b int() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a int \u7c7b\u578b is_cuda = False is_pinned() is_shared() is_sparse = False long() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a long \u7c7b\u578b new() pin_memory() \u5982\u679c\u6b64\u5b58\u50a8\u5f53\u524d\u672a\u88ab\u9501\u5b9a , \u5219\u5c06\u5b83\u590d\u5236\u5230\u9501\u5b9a\u5185\u5b58\u4e2d . resize_() share_memory_() \u5c06\u5b58\u50a8\u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58\u4e2d . \u8fd9\u5bf9\u4e8e\u5df2\u7ecf\u5b58\u5728\u4e8e\u5171\u4eab\u5185\u5b58\u4e2d\u7684\u5b58\u50a8\u6216\u8005 CUDA \u5b58\u50a8\u65e0\u6548 , \u5b83\u4eec\u4e0d\u9700\u8981\u79fb\u52a8\u5c31\u80fd\u5728\u8fdb\u7a0b\u95f4\u5171\u4eab . \u5171\u4eab\u5185\u5b58\u4e2d\u7684\u5b58\u50a8\u4e0d\u80fd\u8c03\u6574\u5927\u5c0f . Returns: self short() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a short \u7c7b\u578b size() tolist() \u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u6b64\u5b58\u50a8\u4e2d\u7684\u5143\u7d20\u7684\u5217\u8868 type(new_type=None, async=False) \u5982\u679c\u6ca1\u6709\u6307\u5b9a <cite>new_type</cite> \u5219\u8fd4\u56de\u8be5\u7c7b\u578b , \u5426\u5219\u5c06\u6b64\u5bf9\u8c61\u8f6c\u6362\u4e3a\u6307\u5b9a\u7c7b\u578b . \u5982\u679c\u5df2\u7ecf\u662f\u6b63\u786e\u7684\u7c7b\u578b , \u5219\u4e0d\u6267\u884c\u590d\u5236\u5e76\u76f4\u63a5\u8fd4\u56de\u539f\u5bf9\u8c61 . | Parameters: | new_type ( type or string ) \u2013 \u671f\u671b\u7684\u7c7b\u578b async ( bool ) \u2013 \u5982\u679c\u4e3a True , \u5e76\u4e14\u6e90\u5728\u9501\u5b9a\u5185\u5b58\u4e2d\u800c\u76ee\u6807\u5728GPU\u4e2d , \u5219\u526f\u672c\u5c06\u4e0e\u4e3b\u673a\u5f02\u6b65\u6267\u884c , \u53cd\u4e4b\u4ea6\u7136 . \u5426\u5219\u6b64\u53c2\u6570\u4e0d\u8d77\u6548\u679c . | | --- | --- |","title":"torch.Storage"},{"location":"56/#torchstorage","text":"\u4e00\u4e2a torch.Storage \u662f\u4e00\u4e2a\u5355\u4e00\u6570\u636e\u7c7b\u578b\u7684\u8fde\u7eed\u4e00\u7ef4\u6570\u7ec4 . \u6bcf\u4e2a torch.Tensor \u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684\u76f8\u540c\u6570\u636e\u7c7b\u578b\u7684\u5b58\u50a8 . class torch.FloatStorage byte() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a byte \u7c7b\u578b char() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a char \u7c7b\u578b clone() \u8fd4\u56de\u6b64\u5b58\u50a8\u7684\u4e00\u4e2a\u526f\u672c copy_() cpu() \u5982\u679c\u5f53\u524d\u6b64\u5b58\u50a8\u4e0d\u5728CPU\u4e0a , \u5219\u8fd4\u56de\u4e00\u4e2a\u5b83\u7684CPU\u526f\u672c . cuda(device=None, async=False) \u8fd4\u56de\u6b64\u5bf9\u8c61\u5728 CUDA \u5185\u5b58\u4e2d\u7684\u4e00\u4e2a\u526f\u672c . \u5982\u679c\u6b64\u5bf9\u8c61\u5df2\u7ecf\u5728 CUDA \u5185\u5b58\u4e2d\u5e76\u4e14\u5728\u6b63\u786e\u7684\u8bbe\u5907\u4e0a , \u90a3\u4e48\u4e0d\u4f1a\u6267\u884c\u590d\u5236\u64cd\u4f5c , \u76f4\u63a5\u8fd4\u56de\u539f\u5bf9\u8c61 . | Parameters: | device ( int ) \u2013 \u76ee\u6807 GPU \u7684 id . \u9ed8\u8ba4\u503c\u662f\u5f53\u524d\u8bbe\u5907 . async ( bool ) \u2013 \u5982\u679c\u4e3a True \u5e76\u4e14\u6e90\u4f4d\u4e8e\u9501\u5b9a\u5185\u5b58\u4e2d , \u5219\u526f\u672c\u76f8\u5bf9\u4e8e\u4e3b\u673a\u662f\u5f02\u6b65\u7684 . \u5426\u5219\u6b64\u53c2\u6570\u4e0d\u8d77\u6548\u679c . | | --- | --- | data_ptr() double() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a double \u7c7b\u578b element_size() fill_() float() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a float \u7c7b\u578b from_buffer() from_file(filename, shared=False, size=0) \u2192 Storage \u5982\u679c shared \u4e3a True , \u90a3\u4e48\u5185\u5b58\u5c06\u4f1a\u5728\u6240\u6709\u8fdb\u7a0b\u95f4\u5171\u4eab . \u6240\u6709\u7684\u66f4\u6539\u90fd\u4f1a\u88ab\u5199\u5165\u6587\u4ef6 . \u5982\u679c shared \u4e3a False , \u90a3\u4e48\u5bf9\u4e8e\u5185\u5b58\u7684\u4fee\u6539 , \u5219\u4e0d\u4f1a\u5f71\u54cd\u5230\u6587\u4ef6 . size \u662f\u5b58\u50a8\u4e2d\u6240\u5305\u542b\u7684\u5143\u7d20\u4e2a\u6570 . \u5982\u679c shared \u4e3a False \u5219\u6587\u4ef6\u5fc5\u987b\u5305\u542b\u81f3\u5c11 <cite>size * sizeof(Type)</cite> \u5b57\u8282 ( <cite>Type</cite> \u662f\u6240\u5b58\u50a8\u7684\u7c7b\u578b) . \u5982\u679c shared \u4e3a True , \u6587\u4ef6\u4f1a\u5728\u9700\u8981\u7684\u65f6\u5019\u88ab\u521b\u5efa . | Parameters: | filename ( str ) \u2013 \u8981\u6620\u5c04\u5230\u7684\u6587\u4ef6\u540d shared ( bool ) \u2013 \u662f\u5426\u5171\u4eab\u5185\u5b58 size ( int ) \u2013 \u5b58\u50a8\u4e2d\u5305\u542b\u5143\u7d20\u7684\u4e2a\u6570 | | --- | --- | half() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a half \u7c7b\u578b int() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a int \u7c7b\u578b is_cuda = False is_pinned() is_shared() is_sparse = False long() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a long \u7c7b\u578b new() pin_memory() \u5982\u679c\u6b64\u5b58\u50a8\u5f53\u524d\u672a\u88ab\u9501\u5b9a , \u5219\u5c06\u5b83\u590d\u5236\u5230\u9501\u5b9a\u5185\u5b58\u4e2d . resize_() share_memory_() \u5c06\u5b58\u50a8\u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58\u4e2d . \u8fd9\u5bf9\u4e8e\u5df2\u7ecf\u5b58\u5728\u4e8e\u5171\u4eab\u5185\u5b58\u4e2d\u7684\u5b58\u50a8\u6216\u8005 CUDA \u5b58\u50a8\u65e0\u6548 , \u5b83\u4eec\u4e0d\u9700\u8981\u79fb\u52a8\u5c31\u80fd\u5728\u8fdb\u7a0b\u95f4\u5171\u4eab . \u5171\u4eab\u5185\u5b58\u4e2d\u7684\u5b58\u50a8\u4e0d\u80fd\u8c03\u6574\u5927\u5c0f . Returns: self short() \u5c06\u6b64\u5b58\u50a8\u8f6c\u6362\u4e3a short \u7c7b\u578b size() tolist() \u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u6b64\u5b58\u50a8\u4e2d\u7684\u5143\u7d20\u7684\u5217\u8868 type(new_type=None, async=False) \u5982\u679c\u6ca1\u6709\u6307\u5b9a <cite>new_type</cite> \u5219\u8fd4\u56de\u8be5\u7c7b\u578b , \u5426\u5219\u5c06\u6b64\u5bf9\u8c61\u8f6c\u6362\u4e3a\u6307\u5b9a\u7c7b\u578b . \u5982\u679c\u5df2\u7ecf\u662f\u6b63\u786e\u7684\u7c7b\u578b , \u5219\u4e0d\u6267\u884c\u590d\u5236\u5e76\u76f4\u63a5\u8fd4\u56de\u539f\u5bf9\u8c61 . | Parameters: | new_type ( type or string ) \u2013 \u671f\u671b\u7684\u7c7b\u578b async ( bool ) \u2013 \u5982\u679c\u4e3a True , \u5e76\u4e14\u6e90\u5728\u9501\u5b9a\u5185\u5b58\u4e2d\u800c\u76ee\u6807\u5728GPU\u4e2d , \u5219\u526f\u672c\u5c06\u4e0e\u4e3b\u673a\u5f02\u6b65\u6267\u884c , \u53cd\u4e4b\u4ea6\u7136 . \u5426\u5219\u6b64\u53c2\u6570\u4e0d\u8d77\u6548\u679c . | | --- | --- |","title":"torch.Storage"},{"location":"57/","text":"torch.nn Parameters (\u53c2\u6570) class torch.nn.Parameter Variable \u7684\u4e00\u79cd, \u5e38\u88ab\u7528\u4e8e module parameter\uff08\u6a21\u5757\u53c2\u6570\uff09. Parameters \u662f Variable \u7684\u5b50\u7c7b, \u5f53\u5b83\u548c Module \u4e00\u8d77\u4f7f\u7528\u7684\u65f6\u5019\u4f1a\u6709\u4e00\u4e9b\u7279\u6b8a\u7684\u5c5e\u6027 - \u5f53\u5b83\u4eec\u88ab\u8d4b\u503c\u7ed9 Module \u5c5e\u6027\u65f6, \u5b83\u4f1a\u81ea\u52a8\u7684\u88ab\u52a0\u5230 Module \u7684\u53c2\u6570\u5217\u8868\u4e2d, \u5e76\u4e14\u4f1a\u51fa\u73b0\u5728 parameters() iterator \u8fed\u4ee3\u5668\u65b9\u6cd5\u4e2d. \u5c06 Varibale \u8d4b\u503c\u7ed9 Module \u5c5e\u6027\u5219\u4e0d\u4f1a\u6709\u8fd9\u6837\u7684\u5f71\u54cd. \u8fd9\u6837\u505a\u7684\u539f\u56e0\u662f: \u6211\u4eec\u6709\u65f6\u5019\u4f1a\u9700\u8981\u7f13\u5b58\u4e00\u4e9b\u4e34\u65f6\u7684 state\uff08\u72b6\u6001\uff09, \u4f8b\u5982: \u6a21\u578b RNN \u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u72b6\u6001. \u5982\u679c\u6ca1\u6709 Parameter \u8fd9\u4e2a\u7c7b\u7684\u8bdd, \u90a3\u4e48\u8fd9\u4e9b\u4e34\u65f6\u8868\u4e5f\u4f1a\u6ce8\u518c\u4e3a\u6a21\u578b\u53d8\u91cf. Variable \u4e0e Parameter \u7684\u53e6\u4e00\u4e2a\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e, Parameter \u4e0d\u80fd\u88ab volatile (\u5373: \u65e0\u6cd5\u8bbe\u7f6e volatile=True) \u800c\u4e14\u9ed8\u8ba4 requires_grad=True. Variable \u9ed8\u8ba4 requires_grad=False. | Parameters: | data ( Tensor ) \u2013 parameter tensor. requires_grad ( bool , optional ) \u2013 \u5982\u679c\u53c2\u6570\u9700\u8981\u68af\u5ea6. \u66f4\u591a\u7ec6\u8282\u8bf7\u53c2\u9605 \u53cd\u5411\u6392\u9664 subgraphs (\u5b50\u56fe) . | | --- | --- | Containers (\u5bb9\u5668) Module class torch.nn.Module \u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u7c7b. \u4f60\u7684\u6a21\u578b\u5e94\u8be5\u4e5f\u662f\u8be5\u7c7b\u7684\u5b50\u7c7b. Modules \u4e5f\u53ef\u4ee5\u5305\u542b\u5176\u5b83 Modules, \u5141\u8bb8\u4f7f\u7528\u6811\u7ed3\u6784\u5d4c\u5165\u5b83\u4eec. \u4f60\u53ef\u4ee5\u5c06\u5b50\u6a21\u5757\u8d4b\u503c\u7ed9\u6a21\u578b\u5c5e\u6027 import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) \u4ee5\u8fd9\u79cd\u65b9\u5f0f\u5206\u914d\u7684\u5b50\u6a21\u5757\u5c06\u88ab\u6ce8\u518c, \u5e76\u4e14\u5728\u8c03\u7528 .cuda() \u7b49\u7b49\u65b9\u6cd5\u65f6\u4e5f\u5c06\u8f6c\u6362\u5b83\u4eec\u7684\u53c2\u6570. add_module(name, module) \u6dfb\u52a0\u4e00\u4e2a child module\uff08\u5b50\u6a21\u5757\uff09\u5230\u5f53\u524d\u7684 module\uff08\u6a21\u5757\uff09\u4e2d. \u88ab\u6dfb\u52a0\u7684 module \u8fd8\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u7684 name \u5c5e\u6027\u6765\u83b7\u53d6\u5b83. | Parameters: | name ( string ) \u2013 \u5b50\u6a21\u5757\u7684\u540d\u79f0. \u53ef\u4ee5\u4f7f\u7528\u6307\u5b9a\u7684 name \u4ece\u8be5\u6a21\u5757\u8bbf\u95ee\u5b50\u6a21\u5757 parameter ( Module ) \u2013 \u88ab\u6dfb\u52a0\u5230\u6a21\u5757\u7684\u5b50\u6a21\u5757. | | --- | --- | apply(fn) \u5c06 fn \u51fd\u6570\u9012\u5f52\u7684\u5e94\u7528\u5230\u6bcf\u4e00\u4e2a\u5b50\u6a21\u5757 (\u7531 .children() \u65b9\u6cd5\u6240\u8fd4\u56de\u7684) \u4ee5\u53ca self. \u5178\u578b\u7684\u7528\u4e8e\u5305\u62ec\u521d\u59cb\u5316\u6a21\u578b\u7684\u53c2\u6570 (\u4e5f\u53ef\u53c2\u9605 torch-nn-init). Parameters: fn ( Module -> None) \u2013 \u8981\u88ab\u5e94\u7528\u5230\u6bcf\u4e00\u4e2a\u5b50\u6a21\u5757\u4e0a\u7684\u51fd\u6570 Returns: self --- --- Return type: Module --- --- Example >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.data.fill_(1.0) >>> print(m.weight) >>> >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear (2 -> 2) Parameter containing: 1 1 1 1 [torch.FloatTensor of size 2x2] Linear (2 -> 2) Parameter containing: 1 1 1 1 [torch.FloatTensor of size 2x2] Sequential ( (0): Linear (2 -> 2) (1): Linear (2 -> 2) ) children() \u8fd4\u56de\u4e00\u4e2a\u6700\u8fd1\u5b50\u6a21\u5757\u7684 iterator\uff08\u8fed\u4ee3\u5668\uff09. Yields: Module \u2013 \u4e00\u4e2a\u5b50\u6a21\u5757 cpu() \u5c06\u6240\u6709\u7684\u6a21\u578b\u53c2\u6570\u548c\u7f13\u51b2\u533a\u79fb\u52a8\u5230 CPU. Returns: self Return type: Module --- --- cuda(device=None) \u5c06\u6240\u6709\u7684\u6a21\u578b\u53c2\u6570\u548c\u7f13\u51b2\u533a\u79fb\u52a8\u5230 GPU. \u8fd9\u5c06\u4f1a\u5173\u8054\u4e00\u4e9b\u53c2\u6570\u5e76\u4e14\u7f13\u5b58\u4e0d\u540c\u7684\u5bf9\u8c61. \u6240\u4ee5\u5728\u6784\u5efa\u4f18\u5316\u5668\u4e4b\u524d\u5e94\u8be5\u8c03\u7528\u5b83, \u5982\u679c\u6a21\u5757\u5728\u4f18\u5316\u7684\u60c5\u51b5\u4e0b\u4f1a\u751f\u5b58\u5728 GPU \u4e0a. Parameters: device ( int , optional ) \u2013 \u5982\u679c\u6307\u5b9a, \u6240\u6709\u53c2\u6570\u5c06\u88ab\u590d\u5236\u5230\u6307\u5b9a\u7684\u8bbe\u5907\u4e0a Returns: self --- --- Return type: Module --- --- double() \u5c06\u6240\u6709\u7684 parameters \u548c buffers \u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u6210 double. Returns: self Return type: Module --- --- eval() \u5c06\u6a21\u5757\u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f. \u8fd9\u79cd\u65b9\u5f0f\u53ea\u5bf9 Dropout \u6216 BatchNorm \u7b49\u6a21\u5757\u6709\u6548. float() \u5c06\u6240\u6709\u7684 parameters \u548c buffers \u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u6210float. Returns: self Return type: Module --- --- forward(*input) \u5b9a\u4e49\u6bcf\u6b21\u8c03\u7528\u65f6\u6267\u884c\u7684\u8ba1\u7b97. \u5e94\u8be5\u88ab\u6240\u6709\u7684\u5b50\u7c7b\u91cd\u5199. Note \u5c3d\u7ba1\u9700\u8981\u5728\u6b64\u51fd\u6570\u4e2d\u5b9a\u4e49\u6b63\u5411\u4f20\u9012\u7684\u65b9\u5f0f, \u4f46\u662f\u5e94\u8be5\u4e8b\u540e\u5c3d\u91cf\u8c03\u7528 Module \u5b9e\u4f8b, \u56e0\u4e3a\u524d\u8005\u8d1f\u8d23\u8fd0\u884c\u5df2\u6ce8\u518c\u7684\u94a9\u5b50, \u800c\u540e\u8005\u9759\u9ed8\u7684\u5ffd\u7565\u5b83\u4eec. half() \u5c06\u6240\u6709\u7684 parameters \u548c buffers \u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u6210 half. Returns: self Return type: Module --- --- load_state_dict(state_dict, strict=True) \u5c06 state_dict \u4e2d\u7684 parameters \u548c buffers \u590d\u5236\u5230\u6b64\u6a21\u5757\u548c\u5b83\u7684\u5b50\u540e\u4ee3\u4e2d. \u5982\u679c strict \u4e3a True , \u5219 state_dict \u7684 key \u5fc5\u987b\u548c\u6a21\u5757\u7684 state_dict() \u51fd\u6570\u8fd4\u56de\u7684 key \u4e00\u81f4. | Parameters: | state_dict ( dict ) \u2013 \u4e00\u4e2a\u5305\u542b parameters \u548c persistent buffers\uff08\u6301\u4e45\u5316\u7f13\u5b58\u7684\uff09\u5b57\u5178. strict ( bool ) \u2013 \u4e25\u683c\u7684\u5f3a\u5236 state_dict \u5c5e\u6027\u4e2d\u7684 key \u4e0e\u8be5\u6a21\u5757\u7684\u51fd\u6570 <cite>:func:`state_dict()</cite> \u8fd4\u56de\u7684 keys \u76f8\u5339\u914d. | | --- | --- | modules() \u8fd4\u56de\u4e00\u4e2a\u8986\u76d6\u795e\u7ecf\u7f51\u7edc\u4e2d\u6240\u6709\u6a21\u5757\u7684 iterator\uff08\u8fed\u4ee3\u5668\uff09. Yields: Module \u2013 a module in the network Note \u91cd\u590d\u7684\u6a21\u5757\u53ea\u8fd4\u56de\u4e00\u6b21. \u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d, 1 \u53ea\u4f1a\u88ab\u8fd4\u56de\u4e00\u6b21. example, l will be returned only once. >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): >>> print(idx, '->', m) 0 -> Sequential ( (0): Linear (2 -> 2) (1): Linear (2 -> 2) ) 1 -> Linear (2 -> 2) named_children() \u8fd4\u56de\u4e00\u4e2a iterator\uff08\u8fed\u4ee3\u5668\uff09, \u800c\u4e0d\u662f\u6700\u63a5\u8fd1\u7684\u5b50\u6a21\u5757, \u4ea7\u751f\u6a21\u5757\u7684 name \u4ee5\u53ca\u6a21\u5757\u672c\u8eab. Yields: (string, Module) \u2013 \u5305\u542b\u540d\u79f0\u548c\u5b50\u6a21\u5757\u7684 Tuple\uff08\u5143\u7ec4\uff09 Example >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) named_modules(memo=None, prefix='') \u8fd4\u56de\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u4e2d\u6240\u6709\u6a21\u5757\u7684 iterator\uff08\u8fed\u4ee3\u5668\uff09, \u4ea7\u751f\u6a21\u5757\u7684 name \u4ee5\u53ca\u6a21\u5757\u672c\u8eab. Yields: (string, Module) \u2013 \u540d\u5b57\u548c\u6a21\u5757\u7684 Tuple\uff08\u5143\u7ec4\uff09 Note \u91cd\u590d\u7684\u6a21\u5757\u53ea\u8fd4\u56de\u4e00\u6b21. \u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d, 1 \u53ea\u4f1a\u88ab\u8fd4\u56de\u4e00\u6b21. >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): >>> print(idx, '->', m) 0 -> ('', Sequential ( (0): Linear (2 -> 2) (1): Linear (2 -> 2) )) 1 -> ('0', Linear (2 -> 2)) named_parameters(memo=None, prefix='') \u8fd4\u56de\u6a21\u5757\u53c2\u6570\u7684\u8fed\u4ee3\u5668, \u4ea7\u751f\u53c2\u6570\u7684\u540d\u79f0\u4ee5\u53ca\u53c2\u6570\u672c\u8eab Yields: (string, Parameter) \u2013 Tuple \u5305\u542b\u540d\u79f0\u5f88\u53c2\u6570\u7684 Tuple\uff08\u5143\u7ec4\uff09 Example >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) parameters() \u8fd4\u56de\u4e00\u4e2a\u6a21\u5757\u53c2\u6570\u7684\u8fed\u4ee3\u5668. \u8fd9\u901a\u5e38\u4f20\u9012\u7ed9\u4f18\u5316\u5668. Yields: Parameter \u2013 \u6a21\u578b\u53c2\u6570 Example >>> for param in model.parameters(): >>> print(type(param.data), param.size()) <class 'torch.FloatTensor'> (20L,) <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L) register_backward_hook(hook) \u5728\u6a21\u5757\u4e0a\u6ce8\u518c\u4e00\u4e2a backward hook\uff08\u53cd\u5411\u94a9\u5b50\uff09. \u6bcf\u6b21\u8ba1\u7b97\u5173\u4e8e\u6a21\u5757\u8f93\u5165\u7684\u68af\u5ea6\u65f6, \u90fd\u4f1a\u8c03\u7528\u8be5\u94a9\u5b50. \u94a9\u5b50\u5e94\u8be5\u6709\u4ee5\u4e0b\u7ed3\u6784: hook(module, grad_input, grad_output) -> Tensor or None \u5982\u679c module \u6709\u591a\u4e2a\u8f93\u5165\u6216\u8f93\u51fa\u7684\u8bdd, \u90a3\u4e48 grad_input \u548c grad_output \u5c06\u4f1a\u662f\u4e2a tuple. hook \u4e0d\u5e94\u8be5\u4fee\u6539\u5b83\u7684\u53c2\u6570, \u4f46\u662f\u5b83\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5173\u4e8e\u8f93\u5165\u7684\u68af\u5ea6, \u8fd9\u4e2a\u8fd4\u56de\u7684\u68af\u5ea6\u5728\u540e\u7eed\u7684\u8ba1\u7b97\u4e2d\u4f1a\u66ff\u4ee3 grad_input . Returns: \u901a\u8fc7\u8c03\u7528 handle.remove() \u65b9\u6cd5\u53ef\u4ee5\u5220\u9664\u6dfb\u52a0\u94a9\u5b50\u7684\u53e5\u67c4 handle.remove() Return type: torch.utils.hooks.RemovableHandle --- --- register_buffer(name, tensor) \u7ed9\u6a21\u5757\u6dfb\u52a0\u4e00\u4e2a\u6301\u4e45\u5316\u7684 buffer. \u6301\u4e45\u5316\u7684 buffer \u901a\u5e38\u88ab\u7528\u5728\u8fd9\u4e48\u4e00\u79cd\u60c5\u51b5: \u6211\u4eec\u9700\u8981\u4fdd\u5b58\u4e00\u4e2a\u72b6\u6001, \u4f46\u662f\u8fd9\u4e2a\u72b6\u6001\u4e0d\u80fd\u770b\u4f5c\u6210\u4e3a\u6a21\u578b\u53c2\u6570. \u4f8b\u5982: BatchNorm \u7684 running_mean \u4e0d\u662f\u4e00\u4e2a parameter, \u4f46\u662f\u5b83\u4e5f\u662f\u9700\u8981\u4fdd\u5b58\u7684\u72b6\u6001\u4e4b\u4e00. Buffers \u53ef\u4ee5\u4f7f\u7528\u6307\u5b9a\u7684 name \u4f5c\u4e3a\u5c5e\u6027\u8bbf\u95ee. | Parameters: | name ( string ) \u2013 buffer \u7684\u540d\u79f0. \u53ef\u4ee5\u4f7f\u7528\u6307\u5b9a\u7684 name \u4ece\u8be5\u6a21\u5757\u8bbf\u95ee buffer tensor ( Tensor ) \u2013 \u88ab\u6ce8\u518c\u7684 buffer. | | --- | --- | Example >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook(hook) \u5728\u6a21\u5757\u4e0a\u6ce8\u518c\u4e00\u4e2a forward hook\uff08\u524d\u5411\u94a9\u5b50\uff09. \u6bcf\u4e00\u6b21 forward() \u51fd\u6570\u8ba1\u7b97\u51fa\u4e00\u4e2a\u8f93\u51fa\u540e, \u8be5\u94a9\u5b50\u5c06\u4f1a\u88ab\u8c03\u7528. \u5b83\u5e94\u8be5\u5177\u6709\u4ee5\u4e0b\u7ed3\u6784 hook(module, input, output) -> None \u8be5\u94a9\u5b50\u5e94\u8be5\u4e0d\u4f1a\u4fee\u6539\u8f93\u5165\u6216\u8f93\u51fa. Returns: \u901a\u8fc7\u8c03\u7528 handle.remove() \u65b9\u6cd5\u53ef\u4ee5\u5220\u9664\u6dfb\u52a0\u94a9\u5b50\u7684\u53e5\u67c4 Return type: torch.utils.hooks.RemovableHandle --- --- register_forward_pre_hook(hook) \u5728\u6a21\u5757\u4e0a\u6ce8\u518c\u4e00\u4e2a\u9884\u524d\u5411\u94a9\u5b50. \u6bcf\u4e00\u6b21\u5728\u8c03\u7528 forward() \u51fd\u6570\u524d\u90fd\u4f1a\u8c03\u7528\u8be5\u94a9\u5b50. \u5b83\u5e94\u8be5\u6709\u4ee5\u4e0b\u7ed3\u6784: hook(module, input) -> None \u8be5\u94a9\u5b50\u4e0d\u5e94\u8be5\u4fee\u6539\u8f93\u5165. Returns: \u901a\u8fc7\u8c03\u7528 handle.remove() \u65b9\u6cd5\u53ef\u4ee5\u5220\u9664\u6dfb\u52a0\u94a9\u5b50\u7684\u53e5\u67c4 handle.remove() Return type: torch.utils.hooks.RemovableHandle --- --- register_parameter(name, param) \u6dfb\u52a0\u4e00\u4e2a\u53c2\u6570\u5230\u6a21\u5757\u4e2d. \u53ef\u4ee5\u4f7f\u7528\u6307\u5b9a\u7684 name \u5c5e\u6027\u6765\u8bbf\u95ee\u53c2\u6570. | Parameters: | name ( string ) \u2013 \u53c2\u6570\u540d. \u53ef\u4ee5\u4f7f\u7528\u6307\u5b9a\u7684 name \u6765\u4ece\u8be5\u6a21\u5757\u4e2d\u8bbf\u95ee\u53c2\u6570 parameter ( Parameter ) \u2013 \u8981\u88ab\u6dfb\u52a0\u5230\u6a21\u5757\u7684\u53c2\u6570. | | --- | --- | state_dict(destination=None, prefix='', keep_vars=False) \u8fd4\u56de\u4e00\u4e2a\u5b57\u5178, \u5b83\u5305\u542b\u6574\u4e2a\u6a21\u5757\u7684\u72b6\u6001. \u5305\u62ec\u53c2\u6570\u548c\u6301\u4e45\u5316\u7684\u7f13\u51b2\u533a (\u4f8b\u5982. \u8fd0\u884c\u4e2d\u7684\u5e73\u5747\u503c). Keys \u662f\u4e0e\u4e4b\u5bf9\u5e94\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a\u7684 name. \u5f53 keep_vars \u4e3a True \u65f6, \u5b83\u4e3a\u6bcf\u4e00\u4e2a\u53c2\u6570\uff08\u800c\u4e0d\u662f\u4e00\u4e2a\u5f20\u91cf\uff09\u8fd4\u56de\u4e00\u4e2a Variable. | Parameters: | destination ( dict , optional ) \u2013 \u5982\u679c\u4e0d\u662f None, \u8be5\u8fd4\u56de\u7684\u5b57\u5178\u5e94\u8be5\u88ab\u5b58\u50a8\u5230 destination \u4e2d. Default: None prefix ( string , optional ) \u2013 \u5411\u7ed3\u679c\u5b57\u5178\u4e2d\u7684\u6bcf\u4e2a\u53c2\u6570\u548c\u7f13\u51b2\u533a\u7684 key\uff08\u540d\u79f0\uff09\u6dfb\u52a0\u4e00\u4e2a\u524d\u7f00. Default: \u2018\u2019 keep_vars ( bool , optional ) \u2013 \u5982\u679c\u4e3a True , \u4e3a\u6bcf\u4e00\u4e2a\u53c2\u6570\u8fd4\u56de\u4e00\u4e2a Variable. \u5982\u679c\u4e3a False , \u4e3a\u6bcf\u4e00\u4e2a\u53c2\u6570\u8fd4\u56de\u4e00\u4e2a Tensor. Default: False | | --- | --- | | Returns: | \u5305\u542b\u6a21\u5757\u6574\u4f53\u72b6\u6001\u7684\u5b57\u5178 | | --- | --- | | Return type: | dict | | --- | --- | Example >>> module.state_dict().keys() ['bias', 'weight'] train(mode=True) \u8bbe\u7f6e\u6a21\u5757\u4e3a\u8bad\u7ec3\u6a21\u5f0f. \u8fd9\u53ea\u5bf9\u8bf8\u5982 Dropout \u6216 BatchNorm \u7b49\u6a21\u5757\u65f6\u624d\u4f1a\u6709\u5f71\u54cd. Returns: self Return type: Module --- --- type(dst_type) \u8f6c\u6362\u6240\u6709\u53c2\u6570\u548c\u7f13\u51b2\u533a\u4e3a dst_type. Parameters: dst_type ( type or string ) \u2013 \u7406\u60f3\u7684\u7c7b\u578b Returns: self --- --- Return type: Module --- --- zero_grad() \u5c06\u6240\u6709\u6a21\u578b\u53c2\u6570\u7684\u68af\u5ea6\u8bbe\u7f6e\u4e3a\u96f6. Sequential class torch.nn.Sequential(*args) \u4e00\u4e2a\u987a\u5e8f\u7684\u5bb9\u5668. \u6a21\u5757\u5c06\u6309\u7167\u5b83\u4eec\u5728\u6784\u9020\u51fd\u6570\u4e2d\u4f20\u9012\u7684\u987a\u5e8f\u6dfb\u52a0\u5230\u5b83. \u6216\u8005, \u4e5f\u53ef\u4ee5\u4f20\u5165\u6a21\u5757\u7684\u6709\u5e8f\u5b57\u5178. \u4e3a\u4e86\u66f4\u5bb9\u6613\u7406\u89e3, \u5217\u4e3e\u5c0f\u4f8b\u6765\u8bf4\u660e # \u4f7f\u7528 Sequential \u7684\u4f8b\u5b50 model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # \u4e0e OrderedDict \u4e00\u8d77\u4f7f\u7528 Sequential \u7684\u4f8b\u5b50 model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ])) ModuleList class torch.nn.ModuleList(modules=None) \u5c06\u5b50\u6a21\u5757\u653e\u5165\u4e00\u4e2a list \u4e2d. ModuleList \u53ef\u4ee5\u50cf\u666e\u901a\u7684 Python list \u4e00\u6837\u88ab\u7d22\u5f15, \u4f46\u662f\u5b83\u5305\u542b\u7684\u6a21\u5757\u5df2\u7ecf\u88ab\u6b63\u786e\u7684\u6ce8\u518c\u4e86, \u5e76\u4e14\u6240\u6709\u7684 Module \u65b9\u6cd5\u90fd\u662f\u53ef\u89c1\u7684. Parameters: modules ( list , optional ) \u2013 \u8981\u6dfb\u52a0\u7684\u6a21\u5757\u5217\u8868 Example: class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)]) def forward(self, x): # ModuleList can act as an iterable, or be indexed using ints for i, l in enumerate(self.linears): x = self.linears[i // 2](x) + l(x) return x append(module) \u6dfb\u52a0\u4e00\u4e2a\u6307\u5b9a\u7684\u6a21\u5757\u5230 list \u5c3e\u90e8. Parameters: module ( nn.Module ) \u2013 \u8981\u88ab\u6dfb\u52a0\u7684\u6a21\u5757 extend(modules) \u5728\u6700\u540e\u6dfb\u52a0 Python list \u4e2d\u7684\u6a21\u5757. Parameters: modules ( list ) \u2013 \u8981\u88ab\u6dfb\u52a0\u7684\u6a21\u5757\u5217\u8868 ParameterList class torch.nn.ParameterList(parameters=None) \u4fdd\u5b58 list \u4e2d\u7684 parameter. ParameterList \u53ef\u4ee5\u50cf\u666e\u901a\u7684 Python list \u90a3\u6837\u88ab\u7d22\u5f15, \u4f46\u662f\u5b83\u6240\u5305\u542b\u7684\u53c2\u6570\u88ab\u6b63\u786e\u7684\u6ce8\u518c\u4e86, \u5e76\u4e14\u6240\u6709\u7684 Module \u65b9\u6cd5\u90fd\u53ef\u89c1\u7684. Parameters: modules ( list , optional ) \u2013 \u8981\u88ab\u6dfb\u52a0\u7684 `Parameter`` \u5217\u8868 Example: class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)]) def forward(self, x): # ModuleList \u53ef\u4ee5\u5145\u5f53 iterable\uff08\u8fed\u4ee3\u5668\uff09, \u6216\u8005\u53ef\u4ee5\u4f7f\u7528\u6574\u6570\u8fdb\u884c\u7d22\u5f15 for i, p in enumerate(self.params): x = self.params[i // 2].mm(x) + p.mm(x) return x append(parameter) \u6dfb\u52a0\u4e00\u4e2a\u6307\u5b9a\u7684\u53c2\u6570\u5230 list \u5c3e\u90e8. Parameters: parameter ( nn.Parameter ) \u2013 parameter to append extend(parameters) \u5728\u6700\u540e\u6dfb\u52a0 Python list \u4e2d\u7684\u53c2\u6570. Parameters: parameters ( list ) \u2013 list of parameters to append Convolution Layers (\u5377\u79ef\u5c42) Conv1d class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) \u4e00\u7ef4\u5377\u79ef\u5c42 \u8f93\u5165\u77e9\u9635\u7684\u7ef4\u5ea6\u4e3a \\((N, C_{in}, L)\\) , \u8f93\u51fa\u77e9\u9635\u7ef4\u5ea6\u4e3a \\((N, C_{out}, L_{out})\\) . \u5176\u4e2dN\u4e3a\u8f93\u5165\u6570\u91cf, C\u4e3a\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u7684\u901a\u9053\u6570\u91cf, L\u4e3a\u6837\u672c\u4e2d\u4e00\u4e2a\u901a\u9053\u4e0b\u7684\u6570\u636e\u7684\u957f\u5ea6. \u7b97\u6cd5\u5982\u4e0b: \\[\\begin{array}{ll} out(N_i, C_{out_j}) = bias(C_{out_j}) + \\sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k) \\star input(N_i, k) \\end{array}\\] \\(\\star\\) \u662f\u4e92\u76f8\u5173\u8fd0\u7b97\u7b26, \u4e0a\u5f0f\u5e26 \\(\\star\\) \u9879\u4e3a\u5377\u79ef\u9879. stride \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u7684\u6b65\u957f, \u53ef\u4ee5\u4e3a tuple . padding \u5904\u7406\u8fb9\u754c\u65f6\u5728\u4e24\u4fa7\u88650\u6570\u91cf dilation \u91c7\u6837\u95f4\u9694\u6570\u91cf. \u5927\u4e8e1\u65f6\u4e3a\u975e\u81f4\u5bc6\u91c7\u6837, \u5982\u5bf9(a,b,c,d,e)\u91c7\u6837\u65f6, \u82e5\u6c60\u5316\u89c4\u6a21\u4e3a2, dilation \u4e3a1\u65f6, \u4f7f\u7528 (a,b);(b,c)\u2026 \u8fdb\u884c\u6c60\u5316, dilation \u4e3a1\u65f6, \u4f7f\u7528 (a,c);(b,d)\u2026 \u8fdb\u884c\u6c60\u5316. | groups \u63a7\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5, group=1, \u8f93\u51fa\u662f\u6240\u6709\u8f93\u5165\u7684\u5377\u79ef\uff1bgroup=2, \u6b64\u65f6\u76f8\u5f53\u4e8e \u6709\u5e76\u6392\u7684\u4e24\u4e2a\u5377\u57fa\u5c42, \u6bcf\u4e2a\u5377\u79ef\u5c42\u53ea\u5728\u5bf9\u5e94\u7684\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u8ba1\u7b97, \u5e76\u4e14\u8f93\u51fa\u65f6\u4f1a\u5c06\u6240\u6709 \u8f93\u51fa\u901a\u9053\u7b80\u5355\u7684\u9996\u5c3e\u76f8\u63a5\u4f5c\u4e3a\u7ed3\u679c\u8f93\u51fa. <cite>in_channels</cite> \u548c [ ](#id1)out_channels \u90fd\u8981\u53ef\u4ee5\u88ab groups \u6574\u9664. Note \u6570\u636e\u7684\u6700\u540e\u4e00\u5217\u53ef\u80fd\u4f1a\u56e0\u4e3a kernal \u5927\u5c0f\u8bbe\u5b9a\u4e0d\u5f53\u800c\u88ab\u4e22\u5f03\uff08\u5927\u90e8\u5206\u53d1\u751f\u5728 kernal \u5927\u5c0f\u4e0d\u80fd\u88ab\u8f93\u5165 \u6574\u9664\u7684\u65f6\u5019, \u9002\u5f53\u7684 padding \u53ef\u4ee5\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff09. | Parameters: | in_channels ( - ) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u6570. out_channels ( - ) \u2013 \u5377\u79ef\u540e\u8f93\u51fa\u7ed3\u679c\u7684\u901a\u9053\u6570. kernel_size ( - ) \u2013 \u5377\u79ef\u6838\u7684\u5f62\u72b6. stride ( - ) \u2013 \u5377\u79ef\u6bcf\u6b21\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u4e3a1. padding ( - ) \u2013 \u5904\u7406\u8fb9\u754c\u65f6\u586b\u51450\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0(\u4e0d\u586b\u5145). dilation ( - ) \u2013 \u91c7\u6837\u95f4\u9694\u6570\u91cf, \u9ed8\u8ba4\u4e3a1, \u65e0\u95f4\u9694\u91c7\u6837. groups ( - ) \u2013 \u8f93\u5165\u4e0e\u8f93\u51fa\u901a\u9053\u7684\u5206\u7ec4\u6570\u91cf. \u5f53\u4e0d\u4e3a1\u65f6, \u9ed8\u8ba4\u4e3a1(\u5168\u8fde\u63a5). bias ( - ) \u2013 \u4e3a True \u65f6, \u6dfb\u52a0\u504f\u7f6e. | | --- | --- | Shape: \u8f93\u5165 Input: \\((N, C_{in}, L_{in})\\) \u8f93\u51fa Output: \\((N, C_{out}, L_{out})\\) \u5176\u4e2d \\(L_{out} = floor((L_{in} + 2 * padding - dilation * (kernel\\_size - 1) - 1) / stride + 1)\\) | Variables: | weight ( Tensor ) \u2013 \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels, in_channels, kernel_size) bias ( Tensor ) \u2013 \u504f\u7f6e, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels) | | --- | --- | Examples: >>> m = nn.Conv1d(16, 33, 3, stride=2) >>> input = autograd.Variable(torch.randn(20, 16, 50)) >>> output = m(input) Conv2d class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) \u4e8c\u7ef4\u5377\u79ef\u5c42 \u8f93\u5165\u77e9\u9635\u7684\u7ef4\u5ea6\u4e3a \\((N, C_{in}, H, W)\\) , \u8f93\u51fa\u77e9\u9635\u7ef4\u5ea6\u4e3a \\((N, C_{out}, H_{out}, W_{out})\\) . \u5176\u4e2dN\u4e3a\u8f93\u5165\u6570\u91cf, C\u4e3a\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u7684\u901a\u9053\u6570\u91cf, H, W \u5206\u522b\u4e3a\u6837\u672c\u4e2d\u4e00\u4e2a\u901a\u9053\u4e0b\u7684\u6570\u636e\u7684\u5f62\u72b6. \u7b97\u6cd5\u5982\u4e0b: \\[\\begin{array}{ll} out(N_i, C_{out_j}) = bias(C_{out_j}) + \\sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k) \\star input(N_i, k) \\end{array}\\] \\(\\star\\) \u662f\u4e92\u76f8\u5173\u8fd0\u7b97\u7b26, \u4e0a\u5f0f\u5e26:math:[ ](#id3)star \u9879\u4e3a\u5377\u79ef\u9879. stride \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u7684\u6b65\u957f, \u53ef\u4ee5\u4e3a tuple . padding \u5904\u7406\u8fb9\u754c\u65f6\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u6570\u91cf. dilation \u91c7\u6837\u95f4\u9694\u6570\u91cf. \u5927\u4e8e1\u65f6\u4e3a\u975e\u81f4\u5bc6\u91c7\u6837. groups \u63a7\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5, group=1, \u8f93\u51fa\u662f\u6240\u6709\u8f93\u5165\u7684\u5377\u79ef\uff1b group=2, \u6b64\u65f6 \u76f8\u5f53\u4e8e\u6709\u5e76\u6392\u7684\u4e24\u4e2a\u5377\u57fa\u5c42, \u6bcf\u4e2a\u5377\u79ef\u5c42\u53ea\u5728\u5bf9\u5e94\u7684\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u8ba1\u7b97, \u5e76\u4e14\u8f93\u51fa\u65f6\u4f1a\u5c06\u6240\u6709 \u8f93\u51fa\u901a\u9053\u7b80\u5355\u7684\u9996\u5c3e\u76f8\u63a5\u4f5c\u4e3a\u7ed3\u679c\u8f93\u51fa. <cite>in_channels</cite> \u548c [ ](#id5)out_channels \u90fd\u8981\u53ef\u4ee5\u88ab groups \u6574\u9664. kernel_size , stride , padding , dilation \u53ef\u4ee5\u4e3a: \u5355\u4e2a int \u503c \u2013 \u5bbd\u548c\u9ad8\u5747\u88ab\u8bbe\u5b9a\u4e3a\u6b64\u503c. \u7531\u4e24\u4e2a int \u7ec4\u6210\u7684 tuple \u2013 \u7b2c\u4e00\u4e2a int \u4e3a\u9ad8, \u7b2c\u4e8c\u4e2a int \u4e3a\u5bbd. Note \u6570\u636e\u7684\u6700\u540e\u4e00\u5217\u53ef\u80fd\u4f1a\u56e0\u4e3a kernal \u5927\u5c0f\u8bbe\u5b9a\u4e0d\u5f53\u800c\u88ab\u4e22\u5f03\uff08\u5927\u90e8\u5206\u53d1\u751f\u5728 kernal \u5927\u5c0f\u4e0d\u80fd\u88ab\u8f93\u5165 \u6574\u9664\u7684\u65f6\u5019, \u9002\u5f53\u7684 padding \u53ef\u4ee5\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff09. | Parameters: | in_channels ( - ) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u6570. out_channels ( - ) \u2013 \u5377\u79ef\u540e\u8f93\u51fa\u7ed3\u679c\u7684\u901a\u9053\u6570. kernel_size ( - ) \u2013 \u5377\u79ef\u6838\u7684\u5f62\u72b6. stride ( - ) \u2013 \u5377\u79ef\u6bcf\u6b21\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u4e3a1. padding ( - ) \u2013 \u5904\u7406\u8fb9\u754c\u65f6\u586b\u51450\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0(\u4e0d\u586b\u5145). dilation ( - ) \u2013 \u91c7\u6837\u95f4\u9694\u6570\u91cf, \u9ed8\u8ba4\u4e3a1, \u65e0\u95f4\u9694\u91c7\u6837. groups ( - ) \u2013 \u8f93\u5165\u4e0e\u8f93\u51fa\u901a\u9053\u7684\u5206\u7ec4\u6570\u91cf. \u5f53\u4e0d\u4e3a1\u65f6, \u9ed8\u8ba4\u4e3a1(\u5168\u8fde\u63a5). bias ( - ) \u2013 \u4e3a True \u65f6, \u6dfb\u52a0\u504f\u7f6e. | | --- | --- | Shape: \u8f93\u5165 Input: \\((N, C_{in}, H_{in}, W_{in})\\) \u8f93\u51fa Output: \\((N, C_{out}, H_{out}, W_{out})\\) \u5176\u4e2d \\(H_{out} = floor((H_{in} + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[1] - dilation[1] * (kernel\\_size[1] - 1) - 1) / stride[1] + 1)\\) | Variables: | weight ( Tensor ) \u2013 \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels, in_channels, kernel_size[0], kernel_size[1]) bias ( Tensor ) \u2013 \u504f\u7f6e, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels) | | --- | --- | Examples: >>> # With square kernels and equal stride >>> m = nn.Conv2d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) >>> # non-square kernels and unequal stride and with padding and dilation >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) >>> input = autograd.Variable(torch.randn(20, 16, 50, 100)) >>> output = m(input) Conv3d class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) \u4e09\u7ef4\u5377\u57fa\u5c42 \u8f93\u5165\u77e9\u9635\u7684\u7ef4\u5ea6\u4e3a \\((N, C_{in}, D, H, W)\\) , \u8f93\u51fa\u77e9\u9635\u7ef4\u5ea6\u4e3a: \\((N, C_{out}, D_{out}, H_{out}, W_{out})\\) . \u5176\u4e2dN\u4e3a\u8f93\u5165\u6570\u91cf, C\u4e3a\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u7684\u901a\u9053\u6570\u91cf, D, H, W \u5206\u522b\u4e3a\u6837\u672c\u4e2d\u4e00\u4e2a\u901a\u9053\u4e0b\u7684\u6570\u636e\u7684\u5f62\u72b6. \u7b97\u6cd5\u5982\u4e0b: \\[\\begin{array}{ll} out(N_i, C_{out_j}) = bias(C_{out_j}) + \\sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k) \\star input(N_i, k) \\end{array}\\] \\(\\star\\) \u662f\u4e92\u76f8\u5173\u8fd0\u7b97\u7b26, \u4e0a\u5f0f\u5e26:math:[ ](#id9)star \u9879\u4e3a\u5377\u79ef\u9879. stride \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u7684\u6b65\u957f, \u53ef\u4ee5\u4e3a tuple . padding \u5904\u7406\u8fb9\u754c\u65f6\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u6570\u91cf. dilation \u91c7\u6837\u95f4\u9694\u6570\u91cf. \u5927\u4e8e1\u65f6\u4e3a\u975e\u81f4\u5bc6\u91c7\u6837. groups \u63a7\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5, group=1, \u8f93\u51fa\u662f\u6240\u6709\u8f93\u5165\u7684\u5377\u79ef\uff1b group=2, \u6b64\u65f6 \u76f8\u5f53\u4e8e\u6709\u5e76\u6392\u7684\u4e24\u4e2a\u5377\u57fa\u5c42, \u6bcf\u4e2a\u5377\u79ef\u5c42\u53ea\u5728\u5bf9\u5e94\u7684\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u8ba1\u7b97, \u5e76\u4e14\u8f93\u51fa\u65f6\u4f1a\u5c06\u6240\u6709 \u8f93\u51fa\u901a\u9053\u7b80\u5355\u7684\u9996\u5c3e\u76f8\u63a5\u4f5c\u4e3a\u7ed3\u679c\u8f93\u51fa. <cite>in_channels</cite> \u548c [ ](#id11)out_channels \u90fd\u8981\u53ef\u4ee5\u88ab groups \u6574\u9664. kernel_size , stride , padding , dilation \u53ef\u4ee5\u4e3a: \u5355\u4e2a int \u503c \u2013 \u5bbd\u548c\u9ad8\u548c\u6df1\u5ea6\u5747\u88ab\u8bbe\u5b9a\u4e3a\u6b64\u503c. \u7531\u4e09\u4e2a int \u7ec4\u6210\u7684 tuple \u2013 \u7b2c\u4e00\u4e2a int \u4e3a\u6df1\u5ea6, \u7b2c\u4e8c\u4e2a int \u4e3a\u9ad8\u5ea6, \u7b2c\u4e09\u4e2a int \u4e3a\u5bbd\u5ea6. Note \u6570\u636e\u7684\u6700\u540e\u4e00\u5217\u53ef\u80fd\u4f1a\u56e0\u4e3a kernal \u5927\u5c0f\u8bbe\u5b9a\u4e0d\u5f53\u800c\u88ab\u4e22\u5f03\uff08\u5927\u90e8\u5206\u53d1\u751f\u5728 kernal \u5927\u5c0f\u4e0d\u80fd\u88ab\u8f93\u5165 \u6574\u9664\u7684\u65f6\u5019, \u9002\u5f53\u7684 padding \u53ef\u4ee5\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff09. | Parameters: | in_channels ( - ) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u6570. out_channels ( - ) \u2013 \u5377\u79ef\u540e\u8f93\u51fa\u7ed3\u679c\u7684\u901a\u9053\u6570. kernel_size ( - ) \u2013 \u5377\u79ef\u6838\u7684\u5f62\u72b6. stride ( - ) \u2013 \u5377\u79ef\u6bcf\u6b21\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u4e3a1. padding ( - ) \u2013 \u5904\u7406\u8fb9\u754c\u65f6\u586b\u51450\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0(\u4e0d\u586b\u5145). dilation ( - ) \u2013 \u91c7\u6837\u95f4\u9694\u6570\u91cf, \u9ed8\u8ba4\u4e3a1, \u65e0\u95f4\u9694\u91c7\u6837. groups ( - ) \u2013 \u8f93\u5165\u4e0e\u8f93\u51fa\u901a\u9053\u7684\u5206\u7ec4\u6570\u91cf. \u5f53\u4e0d\u4e3a1\u65f6, \u9ed8\u8ba4\u4e3a1(\u5168\u8fde\u63a5). bias ( - ) \u2013 \u4e3a True \u65f6, \u6dfb\u52a0\u504f\u7f6e. | | --- | --- | Shape: \u8f93\u5165 Input: \\((N, C_{in}, D_{in}, H_{in}, W_{in})\\) \u8f93\u51fa Output: \\((N, C_{out}, D_{out}, H_{out}, W_{out})\\) \u5176\u4e2d \\(D_{out} = floor((D_{in} + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)\\) \\(H_{out} = floor((H_{in} + 2 * padding[1] - dilation[1] * (kernel\\_size[1] - 1) - 1) / stride[1] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[2] - dilation[2] * (kernel\\_size[2] - 1) - 1) / stride[2] + 1)\\) | Variables: | weight ( Tensor ) \u2013 \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2]) bias ( Tensor ) \u2013 \u504f\u7f6e, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels) | | --- | --- | Examples: >>> # With square kernels and equal stride >>> m = nn.Conv3d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0)) >>> input = autograd.Variable(torch.randn(20, 16, 10, 50, 100)) >>> output = m(input) ConvTranspose1d class torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) \u4e00\u7ef4\u53cd\u5377\u79ef\u5c42 \u53cd\u5377\u79ef\u5c42\u53ef\u4ee5\u7406\u89e3\u4e3a\u8f93\u5165\u7684\u6570\u636e\u548c\u5377\u79ef\u6838\u7684\u4f4d\u7f6e\u53cd\u8f6c\u7684\u5377\u79ef\u64cd\u4f5c. \u53cd\u5377\u79ef\u6709\u65f6\u5019\u4e5f\u4f1a\u88ab\u7ffb\u8bd1\u6210\u89e3\u5377\u79ef. stride \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u7684\u6b65\u957f. padding \u5904\u7406\u8fb9\u754c\u65f6\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u6570\u91cf. output_padding \u8f93\u51fa\u65f6\u5019\u5728\u9996\u5c3e\u88650\u7684\u6570\u91cf. \uff08\u5377\u79ef\u65f6, \u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e \u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u5f62\u72b6\u76f8\u540c\u7684\u7ed3\u679c\uff1b\u53cd\u5377\u79ef\u65f6, \u540c\u4e00\u4e2a\u8f93\u5165\u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u591a \u4e2a\u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u51fa, \u800c\u8f93\u51fa\u7ed3\u679c\u53ea\u80fd\u6709\u4e00\u4e2a, \u56e0\u6b64\u5fc5\u987b\u5bf9\u8f93\u51fa\u5f62\u72b6\u8fdb\u884c\u7ea6\u675f\uff09. | dilation \u91c7\u6837\u95f4\u9694\u6570\u91cf. \u5927\u4e8e1\u65f6\u4e3a\u975e\u81f4\u5bc6\u91c7\u6837. | groups \u63a7\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5, group=1, \u8f93\u51fa\u662f\u6240\u6709\u8f93\u5165\u7684\u5377\u79ef\uff1b group=2, \u6b64\u65f6 \u76f8\u5f53\u4e8e\u6709\u5e76\u6392\u7684\u4e24\u4e2a\u5377\u57fa\u5c42, \u6bcf\u4e2a\u5377\u79ef\u5c42\u53ea\u5728\u5bf9\u5e94\u7684\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u8ba1\u7b97, \u5e76\u4e14\u8f93\u51fa\u65f6\u4f1a\u5c06\u6240\u6709 \u8f93\u51fa\u901a\u9053\u7b80\u5355\u7684\u9996\u5c3e\u76f8\u63a5\u4f5c\u4e3a\u7ed3\u679c\u8f93\u51fa. <cite>in_channels</cite> \u548c [ ](#id15)out_channels \u90fd\u8981\u53ef\u4ee5\u88ab groups \u6574\u9664. Note \u6570\u636e\u7684\u6700\u540e\u4e00\u5217\u53ef\u80fd\u4f1a\u56e0\u4e3a kernal \u5927\u5c0f\u8bbe\u5b9a\u4e0d\u5f53\u800c\u88ab\u4e22\u5f03\uff08\u5927\u90e8\u5206\u53d1\u751f\u5728 kernal \u5927\u5c0f\u4e0d\u80fd\u88ab\u8f93\u5165 \u6574\u9664\u7684\u65f6\u5019, \u9002\u5f53\u7684 padding \u53ef\u4ee5\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff09. | Parameters: | in_channels ( - ) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u6570. out_channels ( - ) \u2013 \u5377\u79ef\u540e\u8f93\u51fa\u7ed3\u679c\u7684\u901a\u9053\u6570. kernel_size ( - ) \u2013 \u5377\u79ef\u6838\u7684\u5f62\u72b6. stride ( - ) \u2013 \u5377\u79ef\u6bcf\u6b21\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u4e3a1. padding ( - ) \u2013 \u5904\u7406\u8fb9\u754c\u65f6\u586b\u51450\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0(\u4e0d\u586b\u5145). output_padding ( - ) \u2013 \u8f93\u51fa\u65f6\u5019\u5728\u9996\u5c3e\u8865\u503c\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0. \uff08\u5377\u79ef\u65f6, \u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e \u540c\u4e00\u4e2a\u8f93\u5165\u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u591a ( \u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u5f62\u72b6\u76f8\u540c\u7684\u7ed3\u679c\uff1b\u53cd\u5377\u79ef\u65f6__, ) \u2013 \u800c\u8f93\u51fa\u7ed3\u679c\u53ea\u80fd\u6709\u4e00\u4e2a, \u56e0\u6b64\u5fc5\u987b\u5bf9\u8f93\u51fa\u5f62\u72b6\u8fdb\u884c\u7ea6\u675f\uff09 ( \u4e2a\u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u51fa__, ) \u2013 groups ( - ) \u2013 \u8f93\u5165\u4e0e\u8f93\u51fa\u901a\u9053\u7684\u5206\u7ec4\u6570\u91cf. \u5f53\u4e0d\u4e3a1\u65f6, \u9ed8\u8ba4\u4e3a1(\u5168\u8fde\u63a5). bias ( - ) \u2013 \u4e3a True \u65f6, \u6dfb\u52a0\u504f\u7f6e. dilation ( - ) \u2013 \u91c7\u6837\u95f4\u9694\u6570\u91cf, \u9ed8\u8ba4\u4e3a1, \u65e0\u95f4\u9694\u91c7\u6837. | | --- | --- | Shape: \u8f93\u5165 Input: \\((N, C_{in}, L_{in})\\) \u8f93\u51fa Output: \\((N, C_{out}, L_{out})\\) \u5176\u4e2d \\(L_{out} = (L_{in} - 1) * stride - 2 * padding + kernel\\_size + output\\_padding\\) | Variables: | weight ( Tensor ) \u2013 \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3aweight (Tensor): \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (in_channels, out_channels, kernel_size[0], kernel_size[1]) bias ( Tensor ) \u2013 \u504f\u7f6e, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels) | | --- | --- | ConvTranspose2d class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) \u4e8c\u7ef4\u53cd\u5377\u79ef\u5c42 \u53cd\u5377\u79ef\u5c42\u53ef\u4ee5\u7406\u89e3\u4e3a\u8f93\u5165\u7684\u6570\u636e\u548c\u5377\u79ef\u6838\u7684\u4f4d\u7f6e\u53cd\u8f6c\u7684\u5377\u79ef\u64cd\u4f5c. \u53cd\u5377\u79ef\u6709\u65f6\u5019\u4e5f\u4f1a\u88ab\u7ffb\u8bd1\u6210\u89e3\u5377\u79ef. stride \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u7684\u6b65\u957f. padding \u5904\u7406\u8fb9\u754c\u65f6\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u6570\u91cf. output_padding \u8f93\u51fa\u65f6\u5019\u5728\u6bcf\u4e00\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u7684\u6570\u91cf. \uff08\u5377\u79ef\u65f6, \u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e \u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u5f62\u72b6\u76f8\u540c\u7684\u7ed3\u679c\uff1b\u53cd\u5377\u79ef\u65f6, \u540c\u4e00\u4e2a\u8f93\u5165\u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u591a \u4e2a\u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u51fa, \u800c\u8f93\u51fa\u7ed3\u679c\u53ea\u80fd\u6709\u4e00\u4e2a, \u56e0\u6b64\u5fc5\u987b\u5bf9\u8f93\u51fa\u5f62\u72b6\u8fdb\u884c\u7ea6\u675f\uff09. | dilation \u91c7\u6837\u95f4\u9694\u6570\u91cf. \u5927\u4e8e1\u65f6\u4e3a\u975e\u81f4\u5bc6\u91c7\u6837. | groups \u63a7\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5, group=1, \u8f93\u51fa\u662f\u6240\u6709\u8f93\u5165\u7684\u5377\u79ef\uff1b group=2, \u6b64\u65f6 \u76f8\u5f53\u4e8e\u6709\u5e76\u6392\u7684\u4e24\u4e2a\u5377\u57fa\u5c42, \u6bcf\u4e2a\u5377\u79ef\u5c42\u53ea\u5728\u5bf9\u5e94\u7684\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u8ba1\u7b97, \u5e76\u4e14\u8f93\u51fa\u65f6\u4f1a\u5c06\u6240\u6709 \u8f93\u51fa\u901a\u9053\u7b80\u5355\u7684\u9996\u5c3e\u76f8\u63a5\u4f5c\u4e3a\u7ed3\u679c\u8f93\u51fa. <cite>in_channels</cite> \u548c [ ](#id17)out_channels \u90fd\u5e94\u5f53\u53ef\u4ee5\u88ab groups \u6574\u9664. kernel_size , stride , padding , output_padding \u53ef\u4ee5\u4e3a: \u5355\u4e2a int \u503c \u2013 \u5bbd\u548c\u9ad8\u5747\u88ab\u8bbe\u5b9a\u4e3a\u6b64\u503c. \u7531\u4e24\u4e2a int \u7ec4\u6210\u7684 tuple \u2013 \u7b2c\u4e00\u4e2a int \u4e3a\u9ad8\u5ea6, \u7b2c\u4e8c\u4e2a int \u4e3a\u5bbd\u5ea6. Note \u6570\u636e\u7684\u6700\u540e\u4e00\u5217\u53ef\u80fd\u4f1a\u56e0\u4e3a kernal \u5927\u5c0f\u8bbe\u5b9a\u4e0d\u5f53\u800c\u88ab\u4e22\u5f03\uff08\u5927\u90e8\u5206\u53d1\u751f\u5728 kernal \u5927\u5c0f\u4e0d\u80fd\u88ab\u8f93\u5165 \u6574\u9664\u7684\u65f6\u5019, \u9002\u5f53\u7684 padding \u53ef\u4ee5\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff09. | Parameters: | in_channels ( - ) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u6570. out_channels ( - ) \u2013 \u5377\u79ef\u540e\u8f93\u51fa\u7ed3\u679c\u7684\u901a\u9053\u6570. kernel_size ( - ) \u2013 \u5377\u79ef\u6838\u7684\u5f62\u72b6. stride ( - ) \u2013 \u5377\u79ef\u6bcf\u6b21\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u4e3a1. padding ( - ) \u2013 \u5904\u7406\u8fb9\u754c\u65f6\u586b\u51450\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0(\u4e0d\u586b\u5145). output_padding ( - ) \u2013 \u8f93\u51fa\u65f6\u5019\u5728\u9996\u5c3e\u8865\u503c\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0. \uff08\u5377\u79ef\u65f6, \u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e \u540c\u4e00\u4e2a\u8f93\u5165\u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u591a ( \u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u5f62\u72b6\u76f8\u540c\u7684\u7ed3\u679c\uff1b\u53cd\u5377\u79ef\u65f6__, ) \u2013 \u800c\u8f93\u51fa\u7ed3\u679c\u53ea\u80fd\u6709\u4e00\u4e2a, \u56e0\u6b64\u5fc5\u987b\u5bf9\u8f93\u51fa\u5f62\u72b6\u8fdb\u884c\u7ea6\u675f\uff09 ( \u4e2a\u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u51fa__, ) \u2013 groups ( - ) \u2013 \u8f93\u5165\u4e0e\u8f93\u51fa\u901a\u9053\u7684\u5206\u7ec4\u6570\u91cf. \u5f53\u4e0d\u4e3a1\u65f6, \u9ed8\u8ba4\u4e3a1(\u5168\u8fde\u63a5). bias ( - ) \u2013 \u4e3a True \u65f6, \u6dfb\u52a0\u504f\u7f6e. dilation ( - ) \u2013 \u91c7\u6837\u95f4\u9694\u6570\u91cf, \u9ed8\u8ba4\u4e3a1, \u65e0\u95f4\u9694\u91c7\u6837. | | --- | --- | Shape: \u8f93\u5165 Input: \\((N, C_{in}, H_{in}, W_{in})\\) \u8f93\u51fa Output: \\((N, C_{out}, H_{out}, W_{out})\\) \u5176\u4e2d \\(H_{out} = (H_{in} - 1) * stride[0] - 2 * padding[0] + kernel\\_size[0] + output\\_padding[0]\\) \\(W_{out} = (W_{in} - 1) * stride[1] - 2 * padding[1] + kernel\\_size[1] + output\\_padding[1]\\) | Variables: | weight ( Tensor ) \u2013 \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3aweight (Tensor): \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (in_channels, out_channels, kernel_size[0], kernel_size[1]) bias ( Tensor ) \u2013 \u504f\u7f6e, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels) | | --- | --- | Examples: >>> # With square kernels and equal stride >>> m = nn.ConvTranspose2d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) >>> input = autograd.Variable(torch.randn(20, 16, 50, 100)) >>> output = m(input) >>> # exact output size can be also specified as an argument >>> input = autograd.Variable(torch.randn(1, 16, 12, 12)) >>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1) >>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1) >>> h = downsample(input) >>> h.size() torch.Size([1, 16, 6, 6]) >>> output = upsample(h, output_size=input.size()) >>> output.size() torch.Size([1, 16, 12, 12]) ConvTranspose3d class torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) \u4e09\u7ef4\u53cd\u5377\u79ef\u5c42 \u53cd\u5377\u79ef\u5c42\u53ef\u4ee5\u7406\u89e3\u4e3a\u8f93\u5165\u7684\u6570\u636e\u548c\u5377\u79ef\u6838\u7684\u4f4d\u7f6e\u53cd\u8f6c\u7684\u5377\u79ef\u64cd\u4f5c. \u53cd\u5377\u79ef\u6709\u65f6\u5019\u4e5f\u4f1a\u88ab\u7ffb\u8bd1\u6210\u89e3\u5377\u79ef. stride \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u7684\u6b65\u957f. padding \u5904\u7406\u8fb9\u754c\u65f6\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u6570\u91cf. output_padding \u8f93\u51fa\u65f6\u5019\u5728\u6bcf\u4e00\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u7684\u6570\u91cf. \uff08\u5377\u79ef\u65f6, \u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e \u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u5f62\u72b6\u76f8\u540c\u7684\u7ed3\u679c\uff1b\u53cd\u5377\u79ef\u65f6, \u540c\u4e00\u4e2a\u8f93\u5165\u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u591a \u4e2a\u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u51fa, \u800c\u8f93\u51fa\u7ed3\u679c\u53ea\u80fd\u6709\u4e00\u4e2a, \u56e0\u6b64\u5fc5\u987b\u5bf9\u8f93\u51fa\u5f62\u72b6\u8fdb\u884c\u7ea6\u675f\uff09 | dilation \u91c7\u6837\u95f4\u9694\u6570\u91cf. \u5927\u4e8e1\u65f6\u4e3a\u975e\u81f4\u5bc6\u91c7\u6837. | groups \u63a7\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5, group=1, \u8f93\u51fa\u662f\u6240\u6709\u8f93\u5165\u7684\u5377\u79ef\uff1b group=2, \u6b64\u65f6 \u76f8\u5f53\u4e8e\u6709\u5e76\u6392\u7684\u4e24\u4e2a\u5377\u57fa\u5c42, \u6bcf\u4e2a\u5377\u79ef\u5c42\u53ea\u5728\u5bf9\u5e94\u7684\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u8ba1\u7b97, \u5e76\u4e14\u8f93\u51fa\u65f6\u4f1a\u5c06\u6240\u6709 \u8f93\u51fa\u901a\u9053\u7b80\u5355\u7684\u9996\u5c3e\u76f8\u63a5\u4f5c\u4e3a\u7ed3\u679c\u8f93\u51fa. <cite>in_channels</cite> \u548c [ ](#id21)out_channels \u90fd\u5e94\u5f53\u53ef\u4ee5\u88ab groups \u6574\u9664. kernel_size , stride , padding , output_padding \u53ef\u4ee5\u4e3a: \u5355\u4e2a int \u503c \u2013 \u6df1\u548c\u5bbd\u548c\u9ad8\u5747\u88ab\u8bbe\u5b9a\u4e3a\u6b64\u503c. \u7531\u4e09\u4e2a int \u7ec4\u6210\u7684 tuple \u2013 \u7b2c\u4e00\u4e2a int \u4e3a\u6df1\u5ea6, \u7b2c\u4e8c\u4e2a int \u4e3a\u9ad8\u5ea6,\u7b2c\u4e09\u4e2a int \u4e3a\u5bbd\u5ea6. Note \u6570\u636e\u7684\u6700\u540e\u4e00\u5217\u53ef\u80fd\u4f1a\u56e0\u4e3a kernal \u5927\u5c0f\u8bbe\u5b9a\u4e0d\u5f53\u800c\u88ab\u4e22\u5f03\uff08\u5927\u90e8\u5206\u53d1\u751f\u5728 kernal \u5927\u5c0f\u4e0d\u80fd\u88ab\u8f93\u5165 \u6574\u9664\u7684\u65f6\u5019, \u9002\u5f53\u7684 padding \u53ef\u4ee5\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff09. | Parameters: | in_channels ( - ) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u6570. out_channels ( - ) \u2013 \u5377\u79ef\u540e\u8f93\u51fa\u7ed3\u679c\u7684\u901a\u9053\u6570. kernel_size ( - ) \u2013 \u5377\u79ef\u6838\u7684\u5f62\u72b6. stride ( - ) \u2013 \u5377\u79ef\u6bcf\u6b21\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u4e3a1. padding ( - ) \u2013 \u5904\u7406\u8fb9\u754c\u65f6\u586b\u51450\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0(\u4e0d\u586b\u5145). output_padding ( - ) \u2013 \u8f93\u51fa\u65f6\u5019\u5728\u9996\u5c3e\u8865\u503c\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0. \uff08\u5377\u79ef\u65f6, \u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e \u540c\u4e00\u4e2a\u8f93\u5165\u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u591a ( \u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u5f62\u72b6\u76f8\u540c\u7684\u7ed3\u679c\uff1b\u53cd\u5377\u79ef\u65f6__, ) \u2013 \u800c\u8f93\u51fa\u7ed3\u679c\u53ea\u80fd\u6709\u4e00\u4e2a, \u56e0\u6b64\u5fc5\u987b\u5bf9\u8f93\u51fa\u5f62\u72b6\u8fdb\u884c\u7ea6\u675f\uff09 ( \u4e2a\u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u51fa__, ) \u2013 groups ( - ) \u2013 \u8f93\u5165\u4e0e\u8f93\u51fa\u901a\u9053\u7684\u5206\u7ec4\u6570\u91cf. \u5f53\u4e0d\u4e3a1\u65f6, \u9ed8\u8ba4\u4e3a1(\u5168\u8fde\u63a5). bias ( - ) \u2013 \u4e3a True \u65f6, \u6dfb\u52a0\u504f\u7f6e. dilation ( - ) \u2013 \u91c7\u6837\u95f4\u9694\u6570\u91cf, \u9ed8\u8ba4\u4e3a1, \u65e0\u95f4\u9694\u91c7\u6837. | | --- | --- | Shape: \u8f93\u5165 Input: \\((N, C_{in}, D_{in}, H_{in}, W_{in})\\) \u8f93\u51fa Output: \\((N, C_{out}, D_{out}, H_{out}, W_{out})\\) \u5176\u4e2d \\(D_{out} = (D_{in} - 1) * stride[0] - 2 * padding[0] + kernel\\_size[0] + output\\_padding[0]\\) \\(H_{out} = (H_{in} - 1) * stride[1] - 2 * padding[1] + kernel\\_size[1] + output\\_padding[1]\\) \\(W_{out} = (W_{in} - 1) * stride[2] - 2 * padding[2] + kernel\\_size[2] + output\\_padding[2]\\) | Variables: | \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3aweight ( \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd__, ) \u2013 \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (in_channels, out_channels, kernel_size[0], kernel_size[1], kernel_size[2]) bias ( Tensor ) \u2013 \u504f\u7f6e, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels) | | --- | --- | Examples: >>> # With square kernels and equal stride >>> m = nn.ConvTranspose3d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2)) >>> input = autograd.Variable(torch.randn(20, 16, 10, 50, 100)) >>> output = m(input) Pooling Layers (\u6c60\u5316\u5c42) MaxPool1d class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e00\u7ef4\u7684\u6700\u5927\u6c60\u5316 max pooling \u64cd\u4f5c \u6700\u7b80\u5355\u7684\u4f8b\u5b50, \u5982\u679c\u8f93\u5165\u5927\u5c0f\u4e3a \\((N, C, L)\\) , \u8f93\u51fa\u5927\u5c0f\u4e3a \\((N, C, L_{out})\\) , \u8be5\u5c42\u8f93\u51fa\u503c\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7cbe\u786e\u8ba1\u7b97: \\[\\begin{array}{ll} out(N_i, C_j, k) = \\max_{{m}=0}^{{kernel\\_size}-1} input(N_i, C_j, stride * k + m) \\end{array}\\] \u5982\u679c padding \u4e0d\u662f0,\u90a3\u4e48\u5728\u8f93\u5165\u6570\u636e\u7684\u6bcf\u6761\u8fb9\u4e0a\u4f1a\u9690\u5f0f\u586b\u8865\u5bf9\u5e94 padding \u6570\u91cf\u76840\u503c\u70b9 dilation \u7528\u4e8e\u63a7\u5236\u5185\u6838\u70b9\u4e4b\u95f4\u7684\u95f4\u9694, link \u5f88\u597d\u5730\u53ef\u89c6\u5316\u5c55\u793a\u4e86 dilation \u7684\u529f\u80fd | Parameters: | kernel_size \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7684\u7a97\u53e3\u5927\u5c0f stride \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u9690\u5f0f\u88650\u7684\u6570\u91cf dilation \u2013 \u7528\u4e8e\u63a7\u5236\u7a97\u53e3\u4e2d\u5143\u7d20\u7684\u6b65\u957f\u7684\u53c2\u6570 return_indices \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8fd4\u56de max pooling \u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15. \u8fd9\u5728\u4e4b\u540e\u7684 Unpooling \u65f6\u5f88\u6709\u7528 ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f | | --- | --- | Shape: Input: \\((N, C, L_{in})\\) Output: \\((N, C, L_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(L_{out} = floor((L_{in} + 2 * padding - dilation * (kernel\\_size - 1) - 1) / stride + 1)\\) Examples: >>> # pool of size=3, stride=2 >>> m = nn.MaxPool1d(3, stride=2) >>> input = autograd.Variable(torch.randn(20, 16, 50)) >>> output = m(input) MaxPool2d class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e8c\u7ef4\u7684\u6700\u5927\u6c60\u5316 max pooling \u64cd\u4f5c \u6700\u7b80\u5355\u7684\u4f8b\u5b50, \u5982\u679c\u8f93\u5165\u5927\u5c0f\u4e3a \\((N, C, H, W)\\) , \u8f93\u51fa\u5927\u5c0f\u4e3a \\((N, C, H_{out}, W_{out})\\) , \u6c60\u5316\u7a97\u53e3\u5927\u5c0f kernel_size \u4e3a \\((kH, kW)\\) \u8be5\u5c42\u8f93\u51fa\u503c\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7cbe\u786e\u8ba1\u7b97: \\[\\begin{array}{ll} out(N_i, C_j, h, w) = \\max_{{m}=0}^{kH-1} \\max_{{n}=0}^{kW-1} input(N_i, C_j, stride[0] * h + m, stride[1] * w + n) \\end{array}\\] \u5982\u679c padding \u4e0d\u662f0, \u90a3\u4e48\u5728\u8f93\u5165\u6570\u636e\u7684\u6bcf\u6761\u8fb9\u4e0a\u4f1a\u9690\u5f0f\u586b\u8865\u5bf9\u5e94 padding \u6570\u91cf\u76840\u503c\u70b9 dilation \u7528\u4e8e\u63a7\u5236\u5185\u6838\u70b9\u4e4b\u95f4\u7684\u95f4\u9694, link \u5f88\u597d\u5730\u53ef\u89c6\u5316\u5c55\u793a\u4e86 dilation \u7684\u529f\u80fd \u53c2\u6570 kernel_size , stride , padding , dilation \u53ef\u4ee5\u662f\u4ee5\u4e0b\u4efb\u610f\u4e00\u79cd\u6570\u636e\u7c7b\u578b: \u5355\u4e2a int \u7c7b\u578b\u6570\u636e \u2013 \u6b64\u65f6\u5728 height \u548c width \u7ef4\u5ea6\u4e0a\u5c06\u4f7f\u7528\u76f8\u540c\u7684\u503c \u5305\u542b\u4e24\u4e2a int \u7c7b\u578b\u6570\u636e\u7684 tuple \u5143\u7ec4 \u2013 \u6b64\u65f6\u7b2c\u4e00\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a height \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c, \u7b2c\u4e8c\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a width \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c | Parameters: | kernel_size \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7684\u7a97\u53e3\u5927\u5c0f stride \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u9690\u5f0f\u88650\u7684\u6570\u91cf dilation \u2013 \u7528\u4e8e\u63a7\u5236\u7a97\u53e3\u4e2d\u5143\u7d20\u7684\u6b65\u957f\u7684\u53c2\u6570 return_indices \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8fd4\u56de max pooling \u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15 \u8fd9\u5728\u4e4b\u540e\u7684 Unpooling \u65f6\u5f88\u6709\u7528 ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(H_{out} = floor((H_{in} + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[1] - dilation[1] * (kernel\\_size[1] - 1) - 1) / stride[1] + 1)\\) Examples: >>> # pool of square window of size=3, stride=2 >>> m = nn.MaxPool2d(3, stride=2) >>> # pool of non-square window >>> m = nn.MaxPool2d((3, 2), stride=(2, 1)) >>> input = autograd.Variable(torch.randn(20, 16, 50, 32)) >>> output = m(input) MaxPool3d class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e09\u7ef4\u7684\u6700\u5927\u6c60\u5316 max pooling \u64cd\u4f5c \u6700\u7b80\u5355\u7684\u4f8b\u5b50, \u5982\u679c\u8f93\u5165\u5927\u5c0f\u4e3a \\((N, C, D, H, W)\\) ,\u8f93\u51fa\u5927\u5c0f\u4e3a \\((N, C, D_{out}, H_{out}, W_{out})\\) \u6c60\u5316\u7a97\u53e3\u5927\u5c0f kernel_size \u4e3a \\((kD, kH, kW)\\) \u8be5\u5c42\u8f93\u51fa\u503c\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7cbe\u786e\u8ba1\u7b97: \\[\\begin{array}{ll} out(N_i, C_j, d, h, w) = \\max_{{k}=0}^{kD-1} \\max_{{m}=0}^{kH-1} \\max_{{n}=0}^{kW-1} input(N_i, C_j, stride[0] * k + d, stride[1] * h + m, stride[2] * w + n) \\end{array}\\] \u5982\u679c padding \u4e0d\u662f0, \u90a3\u4e48\u5728\u8f93\u5165\u6570\u636e\u7684\u6bcf\u6761\u8fb9\u4e0a\u4f1a\u9690\u5f0f\u586b\u8865\u5bf9\u5e94 padding \u6570\u91cf\u76840\u503c\u70b9 dilation \u7528\u4e8e\u63a7\u5236\u5185\u6838\u70b9\u4e4b\u95f4\u7684\u95f4\u9694, link \u5f88\u597d\u5730\u53ef\u89c6\u5316\u5c55\u793a\u4e86 dilation \u7684\u529f\u80fd \u53c2\u6570 kernel_size , stride , padding , dilation \u53ef\u4ee5\u662f\u4ee5\u4e0b\u4efb\u610f\u4e00\u79cd\u6570\u636e\u7c7b\u578b: \u5355\u4e2a int \u7c7b\u578b\u6570\u636e \u2013 \u6b64\u65f6\u5728 depth, height \u548c width \u7ef4\u5ea6\u4e0a\u5c06\u4f7f\u7528\u76f8\u540c\u7684\u503c \u5305\u542b\u4e09\u4e2a int \u7c7b\u578b\u6570\u636e\u7684 tuple \u5143\u7ec4 \u2013 \u6b64\u65f6\u7b2c\u4e00\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a depth \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c, \u7b2c\u4e8c\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a height \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c,\u7b2c\u4e09\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a width \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c | Parameters: | kernel_size \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7684\u7a97\u53e3\u5927\u5c0f stride \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding \u2013 \u8f93\u5165\u6240\u6709\u4e09\u6761\u8fb9\u4e0a\u9690\u5f0f\u88650\u7684\u6570\u91cf dilation \u2013 \u7528\u4e8e\u63a7\u5236\u7a97\u53e3\u4e2d\u5143\u7d20\u7684\u6b65\u957f\u7684\u53c2\u6570 return_indices \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8fd4\u56de max pooling \u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15 \u8fd9\u5728\u4e4b\u540e\u7684 Unpooling \u65f6\u5f88\u6709\u7528 ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f | | --- | --- | Shape: Input: \\((N, C, D_{in}, H_{in}, W_{in})\\) Output: \\((N, C, D_{out}, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(D_{out} = floor((D_{in} + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)\\) \\(H_{out} = floor((H_{in} + 2 * padding[1] - dilation[1] * (kernel\\_size[1] - 1) - 1) / stride[1] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[2] - dilation[2] * (kernel\\_size[2] - 1) - 1) / stride[2] + 1)\\) Examples: >>> # pool of square window of size=3, stride=2 >>> m = nn.MaxPool3d(3, stride=2) >>> # pool of non-square window >>> m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2)) >>> input = autograd.Variable(torch.randn(20, 16, 50,44, 31)) >>> output = m(input) MaxUnpool1d class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0) MaxPool1d \u7684\u9006\u8fc7\u7a0b \u8981\u6ce8\u610f\u7684\u662f MaxPool1d \u5e76\u4e0d\u662f\u5b8c\u5168\u53ef\u9006\u7684, \u56e0\u4e3a\u5728max pooling\u8fc7\u7a0b\u4e2d\u975e\u6700\u5927\u503c\u5df2\u7ecf\u4e22\u5931 MaxUnpool1d \u4ee5 MaxPool1d \u7684\u8f93\u51fa, \u5305\u542b\u6700\u5927\u503c\u7684\u7d22\u5f15\u4f5c\u4e3a\u8f93\u5165 \u8ba1\u7b97max poooling\u7684\u90e8\u5206\u9006\u8fc7\u7a0b(\u5bf9\u4e8e\u90a3\u4e9b\u6700\u5927\u503c\u533a\u57df), \u5bf9\u4e8e\u90a3\u4e9b\u975e\u6700\u5927\u503c\u533a\u57df\u5c06\u8bbe\u7f6e\u4e3a0\u503c Note MaxPool1d \u53ef\u4ee5\u5c06\u591a\u4e2a\u8f93\u5165\u5927\u5c0f\u6620\u5c04\u5230\u76f8\u540c\u7684\u8f93\u51fa\u5927\u5c0f, \u56e0\u6b64\u53cd\u6f14\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u6a21\u68f1\u4e24\u53ef \u4e3a\u9002\u5e94\u8fd9\u4e00\u70b9, \u5728\u8c03\u7528forward\u51fd\u6570\u65f6\u53ef\u4ee5\u5c06\u9700\u8981\u7684\u8f93\u51fa\u5927\u5c0f\u4f5c\u4e3a\u989d\u5916\u7684\u53c2\u6570 output_size \u4f20\u5165. \ufffd \u5177\u4f53\u7528\u6cd5,\u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u8f93\u5165\u548c\u793a\u4f8b | Parameters: | kernel_size ( int or tuple ) \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7684\u7a97\u53e3\u5927\u5c0f stride ( int or tuple ) \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding ( int or tuple ) \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u586b\u51450\u503c\u7684\u4e2a\u6570 | | --- | --- | Inputs: <cite>input</cite>: \u9700\u8981\u8f6c\u5316\u7684\u8f93\u5165\u7684 Tensor <cite>indices</cite>: <cite>MaxPool1d</cite> \u63d0\u4f9b\u7684\u6700\u5927\u503c\u7d22\u5f15 <cite>output_size</cite> (\u53ef\u9009) : <cite>torch.Size</cite> \u7c7b\u578b\u7684\u6570\u636e\u6307\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f Shape: Input: \\((N, C, H_{in})\\) Output: \\((N, C, H_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(H_{out} = (H_{in} - 1) * stride[0] - 2 * padding[0] + kernel\\_size[0]\\) \u6216\u8005\u5728\u8c03\u7528\u65f6\u6307\u5b9a\u8f93\u51fa\u5927\u5c0f output_size Example: >>> pool = nn.MaxPool1d(2, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool1d(2, stride=2) >>> input = Variable(torch.Tensor([[[1, 2, 3, 4, 5, 6, 7, 8]]])) >>> output, indices = pool(input) >>> unpool(output, indices) Variable containing: (0 ,.,.) = 0 2 0 4 0 6 0 8 [torch.FloatTensor of size 1x1x8] >>> # Example showcasing the use of output_size >>> input = Variable(torch.Tensor([[[1, 2, 3, 4, 5, 6, 7, 8, 9]]])) >>> output, indices = pool(input) >>> unpool(output, indices, output_size=input.size()) Variable containing: (0 ,.,.) = 0 2 0 4 0 6 0 8 0 [torch.FloatTensor of size 1x1x9] >>> unpool(output, indices) Variable containing: (0 ,.,.) = 0 2 0 4 0 6 0 8 [torch.FloatTensor of size 1x1x8] MaxUnpool2d class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0) MaxPool2d \u7684\u9006\u8fc7\u7a0b \u8981\u6ce8\u610f\u7684\u662f MaxPool2d \u5e76\u4e0d\u662f\u5b8c\u5168\u53ef\u9006\u7684, \u56e0\u4e3a\u5728max pooling\u8fc7\u7a0b\u4e2d\u975e\u6700\u5927\u503c\u5df2\u7ecf\u4e22\u5931 MaxUnpool2d \u4ee5 MaxPool2d \u7684\u8f93\u51fa, \u5305\u542b\u6700\u5927\u503c\u7684\u7d22\u5f15\u4f5c\u4e3a\u8f93\u5165 \u8ba1\u7b97max poooling\u7684\u90e8\u5206\u9006\u8fc7\u7a0b(\u5bf9\u4e8e\u90a3\u4e9b\u6700\u5927\u503c\u533a\u57df), \u5bf9\u4e8e\u90a3\u4e9b\u975e\u6700\u5927\u503c\u533a\u57df\u5c06\u8bbe\u7f6e\u4e3a0\u503c Note MaxPool2d \u53ef\u4ee5\u5c06\u591a\u4e2a\u8f93\u5165\u5927\u5c0f\u6620\u5c04\u5230\u76f8\u540c\u7684\u8f93\u51fa\u5927\u5c0f, \u56e0\u6b64\u53cd\u6f14\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u6a21\u68f1\u4e24\u53ef. \u4e3a\u9002\u5e94\u8fd9\u4e00\u70b9, \u5728\u8c03\u7528forward\u51fd\u6570\u65f6\u53ef\u4ee5\u5c06\u9700\u8981\u7684\u8f93\u51fa\u5927\u5c0f\u4f5c\u4e3a\u989d\u5916\u7684\u53c2\u6570 output_size \u4f20\u5165. \ufffd \u5177\u4f53\u7528\u6cd5,\u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u8f93\u5165\u548c\u793a\u4f8b | Parameters: | kernel_size ( int or tuple ) \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7684\u7a97\u53e3\u5927\u5c0f stride ( int or tuple ) \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding ( int or tuple ) \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u586b\u51450\u503c\u7684\u4e2a\u6570 | | --- | --- | Inputs: <cite>input</cite>: \u9700\u8981\u8f6c\u5316\u7684\u8f93\u5165\u7684 Tensor <cite>indices</cite>: <cite>MaxPool2d</cite> \u63d0\u4f9b\u7684\u6700\u5927\u503c\u7d22\u5f15 <cite>output_size</cite> (\u53ef\u9009) : <cite>torch.Size</cite> \u7c7b\u578b\u7684\u6570\u636e\u6307\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(H_{out} = (H_{in} - 1) * stride[0] -2 * padding[0] + kernel\\_size[0]\\) \\(W_{out} = (W_{in} - 1) * stride[1] -2 * padding[1] + kernel\\_size[1]\\) \u6216\u8005\u5728\u8c03\u7528\u65f6\u6307\u5b9a\u8f93\u51fa\u5927\u5c0f output_size Example: >>> pool = nn.MaxPool2d(2, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool2d(2, stride=2) >>> input = Variable(torch.Tensor([[[[ 1, 2, 3, 4], ... [ 5, 6, 7, 8], ... [ 9, 10, 11, 12], ... [13, 14, 15, 16]]]])) >>> output, indices = pool(input) >>> unpool(output, indices) Variable containing: (0 ,0 ,.,.) = 0 0 0 0 0 6 0 8 0 0 0 0 0 14 0 16 [torch.FloatTensor of size 1x1x4x4] >>> # specify a different output size than input size >>> unpool(output, indices, output_size=torch.Size([1, 1, 5, 5])) Variable containing: (0 ,0 ,.,.) = 0 0 0 0 0 6 0 8 0 0 0 0 0 14 0 16 0 0 0 0 0 0 0 0 0 [torch.FloatTensor of size 1x1x5x5] MaxUnpool3d class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0) MaxPool3d \u7684\u9006\u8fc7\u7a0b \u8981\u6ce8\u610f\u7684\u662f MaxPool3d \u5e76\u4e0d\u662f\u5b8c\u5168\u53ef\u9006\u7684, \u56e0\u4e3a\u5728max pooling\u8fc7\u7a0b\u4e2d\u975e\u6700\u5927\u503c\u5df2\u7ecf\u4e22\u5931 MaxUnpool3d \u4ee5 MaxPool3d \u7684\u8f93\u51fa, \u5305\u542b\u6700\u5927\u503c\u7684\u7d22\u5f15\u4f5c\u4e3a\u8f93\u5165 \u8ba1\u7b97max poooling\u7684\u90e8\u5206\u9006\u8fc7\u7a0b(\u5bf9\u4e8e\u90a3\u4e9b\u6700\u5927\u503c\u533a\u57df), \u5bf9\u4e8e\u90a3\u4e9b\u975e\u6700\u5927\u503c\u533a\u57df\u5c06\u8bbe\u7f6e\u4e3a0\u503c Note MaxPool3d \u53ef\u4ee5\u5c06\u591a\u4e2a\u8f93\u5165\u5927\u5c0f\u6620\u5c04\u5230\u76f8\u540c\u7684\u8f93\u51fa\u5927\u5c0f, \u56e0\u6b64\u53cd\u6f14\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u6a21\u68f1\u4e24\u53ef. \u4e3a\u9002\u5e94\u8fd9\u4e00\u70b9, \u5728\u8c03\u7528forward\u51fd\u6570\u65f6\u53ef\u4ee5\u5c06\u9700\u8981\u7684\u8f93\u51fa\u5927\u5c0f\u4f5c\u4e3a\u989d\u5916\u7684\u53c2\u6570 output_size \u4f20\u5165. \ufffd \u5177\u4f53\u7528\u6cd5,\u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u8f93\u5165\u548c\u793a\u4f8b | Parameters: | kernel_size ( int or tuple ) \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7684\u7a97\u53e3\u5927\u5c0f stride ( int or tuple ) \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding ( int or tuple ) \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u586b\u51450\u503c\u7684\u4e2a\u6570 | | --- | --- | Inputs: <cite>input</cite>: \u9700\u8981\u8f6c\u5316\u7684\u8f93\u5165\u7684 Tensor <cite>indices</cite>: <cite>MaxPool3d</cite> \u63d0\u4f9b\u7684\u6700\u5927\u503c\u7d22\u5f15 <cite>output_size</cite> (\u53ef\u9009) : <cite>torch.Size</cite> \u7c7b\u578b\u7684\u6570\u636e\u6307\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f Shape: Input: \\((N, C, D_{in}, H_{in}, W_{in})\\) Output: \\((N, C, D_{out}, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(D_{out} = (D_{in} - 1) * stride[0] - 2 * padding[0] + kernel\\_size[0]\\) \\(H_{out} = (H_{in} - 1) * stride[1] - 2 * padding[1] + kernel\\_size[1]\\) \\(W_{out} = (W_{in} - 1) * stride[2] - 2 * padding[2] + kernel\\_size[2]\\) \u6216\u8005\u5728\u8c03\u7528\u65f6\u6307\u5b9a\u8f93\u51fa\u5927\u5c0f output_size Example: >>> # pool of square window of size=3, stride=2 >>> pool = nn.MaxPool3d(3, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool3d(3, stride=2) >>> output, indices = pool(Variable(torch.randn(20, 16, 51, 33, 15))) >>> unpooled_output = unpool(output, indices) >>> unpooled_output.size() torch.Size([20, 16, 51, 33, 15]) AvgPool1d class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e00\u7ef4\u7684\u5e73\u5747\u6c60\u5316 average pooling \u64cd\u4f5c \u6700\u7b80\u5355\u7684\u4f8b\u5b50, \u5982\u679c\u8f93\u5165\u5927\u5c0f\u4e3a \\((N, C, L)\\) , \u8f93\u51fa\u5927\u5c0f\u4e3a \\((N, C, L_{out})\\) , \u6c60\u5316\u7a97\u53e3\u5927\u5c0f kernel_size \u4e3a \\(k\\) \u8be5\u5c42\u8f93\u51fa\u503c\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7cbe\u786e\u8ba1\u7b97: \\[\\begin{array}{ll} out(N_i, C_j, l) = 1 / k * \\sum_{{m}=0}^{k} input(N_i, C_j, stride * l + m) \\end{array}\\] \u5982\u679c padding \u4e0d\u662f0, \u90a3\u4e48\u5728\u8f93\u5165\u6570\u636e\u7684\u6bcf\u6761\u8fb9\u4e0a\u4f1a\u9690\u5f0f\u586b\u8865\u5bf9\u5e94 padding \u6570\u91cf\u76840\u503c\u70b9 \u53c2\u6570 kernel_size , stride , padding \u53ef\u4ee5\u4e3a\u5355\u4e2a int \u7c7b\u578b\u7684\u6570\u636e \u6216\u8005\u662f\u4e00\u4e2a\u5355\u5143\u7d20\u7684tuple\u5143\u7ec4 | Parameters: | kernel_size \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u53d6\u5e73\u5747\u503c\u7684\u7a97\u53e3\u7684\u5927\u5c0f stride \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u9690\u5f0f\u88650\u7684\u6570\u91cf ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f count_include_pad \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u5e73\u5747\u6c60\u5316\u7684\u503c\u65f6,\u5c06\u8003\u8651 padding \u586b\u5145\u76840 | | --- | --- | Shape: Input: \\((N, C, L_{in})\\) Output: \\((N, C, L_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(L_{out} = floor((L_{in} + 2 * padding - kernel\\_size) / stride + 1)\\) Examples: >>> # pool with window of size=3, stride=2 >>> m = nn.AvgPool1d(3, stride=2) >>> m(Variable(torch.Tensor([[[1,2,3,4,5,6,7]]]))) Variable containing: (0 ,.,.) = 2 4 6 [torch.FloatTensor of size 1x1x3] AvgPool2d class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e8c\u7ef4\u7684\u5e73\u5747\u6c60\u5316 average pooling \u64cd\u4f5c \u6700\u7b80\u5355\u7684\u4f8b\u5b50,\u5982\u679c\u8f93\u5165\u5927\u5c0f\u4e3a \\((N, C, H, W)\\) ,\u8f93\u51fa\u5927\u5c0f\u4e3a \\((N, C, H_{out}, W_{out})\\) , \u6c60\u5316\u7a97\u53e3\u5927\u5c0f kernel_size \u4e3a \\((kH, kW)\\) \u8be5\u5c42\u8f93\u51fa\u503c\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7cbe\u786e\u8ba1\u7b97: \\[\\begin{array}{ll} out(N_i, C_j, h, w) = 1 / (kH * kW) * \\sum_{{m}=0}^{kH-1} \\sum_{{n}=0}^{kW-1} input(N_i, C_j, stride[0] * h + m, stride[1] * w + n) \\end{array}\\] \u5982\u679c padding \u4e0d\u662f0, \u90a3\u4e48\u5728\u8f93\u5165\u6570\u636e\u7684\u6bcf\u6761\u8fb9\u4e0a\u4f1a\u9690\u5f0f\u586b\u8865\u5bf9\u5e94 padding \u6570\u91cf\u76840\u503c\u70b9 \u53c2\u6570 kernel_size , stride , padding \u53ef\u4ee5\u662f\u4ee5\u4e0b\u4efb\u610f\u4e00\u79cd\u6570\u636e\u7c7b\u578b: \u5355\u4e2a int \u7c7b\u578b\u6570\u636e \u2013 \u6b64\u65f6\u5728 height \u548c width \u7ef4\u5ea6\u4e0a\u5c06\u4f7f\u7528\u76f8\u540c\u7684\u503c \u5305\u542b\u4e24\u4e2a int \u7c7b\u578b\u6570\u636e\u7684 tuple \u5143\u7ec4 \u2013 \u6b64\u65f6\u7b2c\u4e00\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a height \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c, \u7b2c\u4e8c\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a width \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c | Parameters: | kernel_size \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u53d6\u5e73\u5747\u503c\u7684\u7a97\u53e3\u7684\u5927\u5c0f stride \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u9690\u5f0f\u88650\u7684\u6570\u91cf ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f count_include_pad \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u5e73\u5747\u6c60\u5316\u7684\u503c\u65f6,\u5c06\u8003\u8651 padding \u586b\u5145\u76840 | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(H_{out} = floor((H_{in} + 2 * padding[0] - kernel\\_size[0]) / stride[0] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[1] - kernel\\_size[1]) / stride[1] + 1)\\) Examples: >>> # pool of square window of size=3, stride=2 >>> m = nn.AvgPool2d(3, stride=2) >>> # pool of non-square window >>> m = nn.AvgPool2d((3, 2), stride=(2, 1)) >>> input = autograd.Variable(torch.randn(20, 16, 50, 32)) >>> output = m(input) AvgPool3d class torch.nn.AvgPool3d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e09\u7ef4\u7684\u5e73\u5747\u6c60\u5316 average pooling \u64cd\u4f5c \u6700\u7b80\u5355\u7684\u4f8b\u5b50, \u5982\u679c\u8f93\u5165\u5927\u5c0f\u4e3a \\((N, C, D, H, W)\\) ,\u8f93\u51fa\u5927\u5c0f\u4e3a \\((N, C, D_{out}, H_{out}, W_{out})\\) \u6c60\u5316\u7a97\u53e3\u5927\u5c0f kernel_size \u4e3a \\((kD, kH, kW)\\) \u8be5\u5c42\u8f93\u51fa\u503c\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7cbe\u786e\u8ba1\u7b97: \\[\\begin{array}{ll} out(N_i, C_j, d, h, w) = 1 / (kD * kH * kW) * \\sum_{{k}=0}^{kD-1} \\sum_{{m}=0}^{kH-1} \\sum_{{n}=0}^{kW-1} input(N_i, C_j, stride[0] * d + k, stride[1] * h + m, stride[2] * w + n) \\end{array}\\] \u5982\u679c padding \u4e0d\u662f0, \u90a3\u4e48\u5728\u8f93\u5165\u6570\u636e\u7684\u6bcf\u6761\u8fb9\u4e0a\u4f1a\u9690\u5f0f\u586b\u8865\u5bf9\u5e94 padding \u6570\u91cf\u76840\u503c\u70b9 \u53c2\u6570 kernel_size , stride \u53ef\u4ee5\u662f\u4ee5\u4e0b\u4efb\u610f\u4e00\u79cd\u6570\u636e\u7c7b\u578b: \u5355\u4e2a int \u7c7b\u578b\u6570\u636e \u2013 \u6b64\u65f6\u5728 depth, height \u548c width \u7ef4\u5ea6\u4e0a\u5c06\u4f7f\u7528\u76f8\u540c\u7684\u503c \u5305\u542b\u4e09\u4e2a int \u7c7b\u578b\u6570\u636e\u7684 tuple \u5143\u7ec4 \u2013 \u6b64\u65f6\u7b2c\u4e00\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a depth \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c, \u7b2c\u4e8c\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a height \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c,\u7b2c\u4e09\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a width \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c | Parameters: | kernel_size \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u53d6\u5e73\u5747\u503c\u7684\u7a97\u53e3\u7684\u5927\u5c0f stride \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u9690\u5f0f\u88650\u7684\u6570\u91cf ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f count_include_pad \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u5e73\u5747\u6c60\u5316\u7684\u503c\u65f6,\u5c06\u8003\u8651 padding \u586b\u5145\u76840 | | --- | --- | Shape: Input: \\((N, C, D_{in}, H_{in}, W_{in})\\) Output: \\((N, C, D_{out}, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(D_{out} = floor((D_{in} + 2 * padding[0] - kernel\\_size[0]) / stride[0] + 1)\\) \\(H_{out} = floor((H_{in} + 2 * padding[1] - kernel\\_size[1]) / stride[1] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[2] - kernel\\_size[2]) / stride[2] + 1)\\) Examples: >>> # pool of square window of size=3, stride=2 >>> m = nn.AvgPool3d(3, stride=2) >>> # pool of non-square window >>> m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2)) >>> input = autograd.Variable(torch.randn(20, 16, 50,44, 31)) >>> output = m(input) FractionalMaxPool2d class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e8c\u7ef4\u7684\u5206\u6570\u6700\u5927\u6c60\u5316 fractional max pooling \u64cd\u4f5c \u5206\u6570\u6700\u5927\u6c60\u5316 Fractiona MaxPooling \u7684\u5177\u4f53\u7ec6\u8282\u63cf\u8ff0,\u8be6\u89c1Ben Graham\u8bba\u6587 Fractional MaxPooling \u7531\u76ee\u6807\u8f93\u51fa\u5927\u5c0f\u786e\u5b9a\u968f\u673a\u6b65\u957f,\u5728 kH x kW \u533a\u57df\u5185\u8fdb\u884c\u6700\u5927\u6c60\u5316\u7684\u64cd\u4f5c \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c | Parameters: | kernel_size \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u7684\u5927\u5c0f. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57 k (\u7b49\u4ef7\u4e8e k x k \u7684\u6b63\u65b9\u5f62\u7a97\u53e3) \u6216\u8005\u662f \u4e00\u4e2a\u5143\u7ec4 tuple (kh x kw) output_size \u2013 oH x oW \u5f62\u5f0f\u7684\u8f93\u51fa\u56fe\u50cf\u7684\u5c3a\u5bf8. \u53ef\u4ee5\u7528 \u4e00\u4e2a tuple \u5143\u7ec4 (oH, oW) \u8868\u793a oH x oW \u7684\u8f93\u51fa\u5c3a\u5bf8, \u6216\u8005\u662f\u5355\u4e2a\u7684\u6570\u5b57 oH \u8868\u793a oH x oH \u7684\u8f93\u51fa\u5c3a\u5bf8 output_ratio \u2013 \u5982\u679c\u60f3\u7528\u8f93\u5165\u56fe\u50cf\u7684\u767e\u5206\u6bd4\u6765\u6307\u5b9a\u8f93\u51fa\u56fe\u50cf\u7684\u5927\u5c0f,\u53ef\u9009\u7528\u8be5\u9009\u9879. \u4f7f\u7528\u8303\u56f4\u5728 (0,1) \u4e4b\u95f4\u7684\u4e00\u4e2a\u503c\u6765\u6307\u5b9a. return_indices \u2013 \u5982\u679c\u7b49\u4e8e True ,\u5728\u8fd4\u56de\u8f93\u51fa\u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15,\u8be5\u7d22\u5f15\u5bf9 nn.MaxUnpool2d \u6709\u7528. \u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8be5\u503c\u7b49\u4e8e False | | --- | --- | Examples >>> # pool of square window of size=3, and target output size 13x12 >>> m = nn.FractionalMaxPool2d(3, output_size=(13, 12)) >>> # pool of square window and target output size being half of input image size >>> m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5)) >>> input = autograd.Variable(torch.randn(20, 16, 50, 32)) >>> output = m(input) LPPool2d class torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e8c\u7ef4\u7684\u5e42\u5e73\u5747\u6c60\u5316 power-average pooling \u64cd\u4f5c \u5728\u6bcf\u4e2a\u7a97\u53e3\u5185, \u8f93\u51fa\u7684\u8ba1\u7b97\u65b9\u5f0f: \\(f(X) = pow(sum(pow(X, p)), 1/p)\\) \u5f53 p \u65e0\u7a77\u5927\u65f6,\u7b49\u4ef7\u4e8e\u6700\u5927\u6c60\u5316 Max Pooling \u64cd\u4f5c \u5f53 p=1 \u65f6, \u7b49\u4ef7\u4e8e\u5e73\u5747\u6c60\u5316 Average Pooling \u64cd\u4f5c \u53c2\u6570 kernel_size , stride \u53ef\u4ee5\u662f\u4ee5\u4e0b\u4efb\u610f\u4e00\u79cd\u6570\u636e\u7c7b\u578b: \u5355\u4e2a int \u7c7b\u578b\u6570\u636e \u2013 \u6b64\u65f6\u5728height\u548cwidth\u7ef4\u5ea6\u4e0a\u5c06\u4f7f\u7528\u76f8\u540c\u7684\u503c \u5305\u542b\u4e24\u4e2a int \u7c7b\u578b\u6570\u636e\u7684 tuple \u5143\u7ec4 \u2013 \u6b64\u65f6\u7b2c\u4e00\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a height \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c, \u7b2c\u4e8c\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a width \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c | Parameters: | kernel_size \u2013 \u5e42\u5e73\u5747\u6c60\u5316\u65f6\u7a97\u53e3\u7684\u5927\u5c0f stride \u2013 \u5e42\u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(H_{out} = floor((H_{in} + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[1] - dilation[1] * (kernel\\_size[1] - 1) - 1) / stride[1] + 1)\\) Examples: >>> # power-2 pool of square window of size=3, stride=2 >>> m = nn.LPPool2d(2, 3, stride=2) >>> # pool of non-square window of power 1.2 >>> m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1)) >>> input = autograd.Variable(torch.randn(20, 16, 50, 32)) >>> output = m(input) AdaptiveMaxPool1d class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e00\u7ef4\u7684\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316 adaptive max pooling \u64cd\u4f5c \u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165,\u53ef\u4ee5\u6307\u5b9a\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a H \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c. | Parameters: | output_size \u2013 \u76ee\u6807\u8f93\u51fa\u7684\u5c3a\u5bf8 H return_indices \u2013 \u5982\u679c\u7b49\u4e8e True ,\u5728\u8fd4\u56de\u8f93\u51fa\u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15,\u8be5\u7d22\u5f15\u5bf9 nn.MaxUnpool1d \u6709\u7528. \u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8be5\u503c\u7b49\u4e8e False | | --- | --- | Examples >>> # target output size of 5 >>> m = nn.AdaptiveMaxPool1d(5) >>> input = autograd.Variable(torch.randn(1, 64, 8)) >>> output = m(input) AdaptiveMaxPool2d class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e8c\u7ef4\u7684\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316 adaptive max pooling \u64cd\u4f5c \u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165,\u53ef\u4ee5\u6307\u5b9a\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a H x W \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c. | Parameters: | output_size \u2013 H x W \u5f62\u5f0f\u7684\u8f93\u51fa\u56fe\u50cf\u7684\u5c3a\u5bf8. \u53ef\u4ee5\u7528 \u4e00\u4e2a tuple \u5143\u7ec4 (H, W) \u8868\u793a H x W \u7684\u8f93\u51fa\u5c3a\u5bf8, \u6216\u8005\u662f\u5355\u4e2a\u7684\u6570\u5b57 H \u8868\u793a H x H \u7684\u8f93\u51fa\u5c3a\u5bf8 return_indices \u2013 \u5982\u679c\u7b49\u4e8e True ,\u5728\u8fd4\u56de\u8f93\u51fa\u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15,\u8be5\u7d22\u5f15\u5bf9 nn.MaxUnpool2d \u6709\u7528. \u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8be5\u503c\u7b49\u4e8e False | | --- | --- | Examples >>> # target output size of 5x7 >>> m = nn.AdaptiveMaxPool2d((5,7)) >>> input = autograd.Variable(torch.randn(1, 64, 8, 9)) >>> output = m(input) >>> # target output size of 7x7 (square) >>> m = nn.AdaptiveMaxPool2d(7) >>> input = autograd.Variable(torch.randn(1, 64, 10, 9)) >>> output = m(input) AdaptiveMaxPool3d class torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e09\u7ef4\u7684\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316 adaptive max pooling \u64cd\u4f5c \u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165,\u53ef\u4ee5\u6307\u5b9a\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a D x H x W \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c. | Parameters: | output_size \u2013 D x H x W \u5f62\u5f0f\u7684\u8f93\u51fa\u56fe\u50cf\u7684\u5c3a\u5bf8. \u53ef\u4ee5\u7528 \u4e00\u4e2a tuple \u5143\u7ec4 (D, H, W) \u8868\u793a D x H x W \u7684\u8f93\u51fa\u5c3a\u5bf8, \u6216\u8005\u662f\u5355\u4e2a\u7684\u6570\u5b57 D \u8868\u793a D x D x D \u7684\u8f93\u51fa\u5c3a\u5bf8 return_indices \u2013 \u5982\u679c\u7b49\u4e8e True ,\u5728\u8fd4\u56de\u8f93\u51fa\u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15,\u8be5\u7d22\u5f15\u5bf9 nn.MaxUnpool3d \u6709\u7528. \u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8be5\u503c\u7b49\u4e8e False | | --- | --- | Examples >>> # target output size of 5x7x9 >>> m = nn.AdaptiveMaxPool3d((5,7,9)) >>> input = autograd.Variable(torch.randn(1, 64, 8, 9, 10)) >>> output = m(input) >>> # target output size of 7x7x7 (cube) >>> m = nn.AdaptiveMaxPool3d(7) >>> input = autograd.Variable(torch.randn(1, 64, 10, 9, 8)) >>> output = m(input) AdaptiveAvgPool1d class torch.nn.AdaptiveAvgPool1d(output_size) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e00\u7ef4\u7684\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316 adaptive average pooling \u64cd\u4f5c \u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165,\u53ef\u4ee5\u6307\u5b9a\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a H \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c. Parameters: output_size \u2013 \u76ee\u6807\u8f93\u51fa\u7684\u5c3a\u5bf8 H Examples >>> # target output size of 5 >>> m = nn.AdaptiveAvgPool1d(5) >>> input = autograd.Variable(torch.randn(1, 64, 8)) >>> output = m(input) AdaptiveAvgPool2d class torch.nn.AdaptiveAvgPool2d(output_size) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e8c\u7ef4\u7684\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316 adaptive average pooling \u64cd\u4f5c \u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165,\u53ef\u4ee5\u6307\u5b9a\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a H x W \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c. Parameters: output_size \u2013 H x W \u5f62\u5f0f\u7684\u8f93\u51fa\u56fe\u50cf\u7684\u5c3a\u5bf8. \u53ef\u4ee5\u7528 \u4e00\u4e2a tuple \u5143\u7ec4 (H, W) \u8868\u793a H x W \u7684\u8f93\u51fa\u5c3a\u5bf8, \u6216\u8005\u662f\u5355\u4e2a\u7684\u6570\u5b57 H \u8868\u793a H x H \u7684\u8f93\u51fa\u5c3a\u5bf8 Examples >>> # target output size of 5x7 >>> m = nn.AdaptiveAvgPool2d((5,7)) >>> input = autograd.Variable(torch.randn(1, 64, 8, 9)) >>> output = m(input) >>> # target output size of 7x7 (square) >>> m = nn.AdaptiveAvgPool2d(7) >>> input = autograd.Variable(torch.randn(1, 64, 10, 9)) >>> output = m(input) AdaptiveAvgPool3d class torch.nn.AdaptiveAvgPool3d(output_size) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e09\u7ef4\u7684\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316 adaptive average pooling \u64cd\u4f5c \u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165,\u53ef\u4ee5\u6307\u5b9a\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a D x H x W \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c. Parameters: output_size \u2013 D x H x W \u5f62\u5f0f\u7684\u8f93\u51fa\u56fe\u50cf\u7684\u5c3a\u5bf8. \u53ef\u4ee5\u7528 \u4e00\u4e2a tuple \u5143\u7ec4 (D, H, W) \u8868\u793a D x H x W \u7684\u8f93\u51fa\u5c3a\u5bf8, \u6216\u8005\u662f\u5355\u4e2a\u7684\u6570\u5b57 D \u8868\u793a D x D x D \u7684\u8f93\u51fa\u5c3a\u5bf8 Examples >>> # target output size of 5x7x9 >>> m = nn.AdaptiveAvgPool3d((5,7,9)) >>> input = autograd.Variable(torch.randn(1, 64, 8, 9, 10)) >>> output = m(input) >>> # target output size of 7x7x7 (cube) >>> m = nn.AdaptiveAvgPool3d(7) >>> input = autograd.Variable(torch.randn(1, 64, 10, 9, 8)) >>> output = m(input) Padding Layers (\u586b\u5145\u5c42) ReflectionPad2d class torch.nn.ReflectionPad2d(padding) \u4f7f\u7528\u8f93\u5165\u8fb9\u754c\u7684\u53cd\u5c04\u586b\u5145\u8f93\u5165\u5f20\u91cf. | Parameters: | padding ( int , tuple ) \u2013 \u586b\u5145\u7684\u5927\u5c0f. \u5982\u679c\u662fint, \u5219\u5728\u6240\u6709\u8fb9\u754c\u586b\u5145\u4f7f\u7528\u76f8\u540c\u7684. \u5219\u4f7f\u7528 ( \u5982\u679c\u662f4\u4e2a\u5143\u7ec4__, ) \u2013 | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) where \\(H_{out} = H_{in} + paddingTop + paddingBottom\\) \\(W_{out} = W_{in} + paddingLeft + paddingRight\\) Example: >>> m = nn.ReflectionPad2d(3) >>> input = autograd.Variable(torch.randn(16, 3, 320, 480)) >>> output = m(input) >>> # \u4f7f\u7528\u4e0d\u540c\u7684\u586b\u5145 >>> m = nn.ReflectionPad2d((3, 3, 6, 6)) >>> output = m(input) ReplicationPad2d class torch.nn.ReplicationPad2d(padding) \u4f7f\u7528\u8f93\u5165\u8fb9\u754c\u7684\u590d\u5236\u586b\u5145\u8f93\u5165\u5f20\u91cf. Parameters: padding ( int , tuple ) \u2013 \u586b\u5145\u7684\u5927\u5c0f. \u5982\u679c\u662fint, \u5219\u5728\u6240\u6709\u8fb9\u754c\u4f7f\u7528\u76f8\u540c\u7684\u586b\u5145. \u5982\u679c\u662f4\u4e2a\u5143\u7ec4, \u5219\u4f7f\u7528(paddingLeft, paddingRight, paddingTop, paddingBottom) Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) where \\(H_{out} = H_{in} + paddingTop + paddingBottom\\) \\(W_{out} = W_{in} + paddingLeft + paddingRight\\) Example: >>> m = nn.ReplicationPad2d(3) >>> input = autograd.Variable(torch.randn(16, 3, 320, 480)) >>> output = m(input) >>> # \u4f7f\u7528\u4e0d\u540c\u7684\u586b\u5145 >>> m = nn.ReplicationPad2d((3, 3, 6, 6)) >>> output = m(input) ReplicationPad3d class torch.nn.ReplicationPad3d(padding) \u4f7f\u7528\u8f93\u5165\u8fb9\u754c\u7684\u590d\u5236\u586b\u5145\u8f93\u5165\u5f20\u91cf. | Parameters: | padding ( int , tuple ) \u2013 \u586b\u5145\u7684\u5927\u5c0f. \u5982\u679c\u662fint, \u5219\u5728\u6240\u6709\u8fb9\u754c\u4f7f\u7528\u76f8\u540c\u7684\u586b\u5145. \u5219\u4f7f\u7528 (paddingLeft, paddingRight, ( \u5982\u679c\u662f\u56db\u4e2a\u5143\u7ec4__, ) \u2013 paddingBottom, paddingFront, paddingBack) ( paddingTop__, ) \u2013 | | --- | --- | Shape: Input: \\((N, C, D_{in}, H_{in}, W_{in})\\) Output: \\((N, C, D_{out}, H_{out}, W_{out})\\) where \\(D_{out} = D_{in} + paddingFront + paddingBack\\) \\(H_{out} = H_{in} + paddingTop + paddingBottom\\) \\(W_{out} = W_{in} + paddingLeft + paddingRight\\) Example: >>> m = nn.ReplicationPad3d(3) >>> input = autograd.Variable(torch.randn(16, 3, 8, 320, 480)) >>> output = m(input) >>> # \u4f7f\u7528\u4e0d\u540c\u7684\u586b\u5145 >>> m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1)) >>> output = m(input) ZeroPad2d class torch.nn.ZeroPad2d(padding) \u7528\u96f6\u586b\u5145\u8f93\u5165\u5f20\u91cf\u8fb9\u754c. | Parameters: | padding ( int , tuple ) \u2013 \u586b\u5145\u7684\u5927\u5c0f. \u5982\u679c\u662fint, \u5219\u5728\u6240\u6709\u8fb9\u754c\u4f7f\u7528\u76f8\u540c\u7684\u586b\u5145. \u5982\u679c\u662f\u56db\u4e2a\u5143\u7ec4, \u5219\u4f7f\u7528 () \u2013 | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) where \\(H_{out} = H_{in} + paddingTop + paddingBottom\\) \\(W_{out} = W_{in} + paddingLeft + paddingRight\\) Example: >>> m = nn.ZeroPad2d(3) >>> input = autograd.Variable(torch.randn(16, 3, 320, 480)) >>> output = m(input) >>> # \u4f7f\u7528\u4e0d\u540c\u7684\u586b\u5145 >>> m = nn.ZeroPad2d((3, 3, 6, 6)) >>> output = m(input) ConstantPad2d class torch.nn.ConstantPad2d(padding, value) \u7528\u4e00\u4e2a\u5e38\u6570\u503c\u586b\u5145\u8f93\u5165\u5f20\u91cf\u8fb9\u754c. \u5bf9\u4e8e Nd-padding, \u4f7f\u7528 nn.functional.pad(). | Parameters: | padding ( int , tuple ) \u2013 \u586b\u5145\u7684\u5927\u5c0f. \u5982\u679c\u662fint, \u5219\u5728\u6240\u6709\u8fb9\u754c\u4f7f\u7528\u76f8\u540c\u7684\u586b\u5145. \u4f7f\u7528 ( \u5982\u679c\u662f4\u4e2a\u5143\u7ec4__, ) \u2013 | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) where \\(H_{out} = H_{in} + paddingTop + paddingBottom\\) \\(W_{out} = W_{in} + paddingLeft + paddingRight\\) Example: >>> m = nn.ConstantPad2d(3, 3.5) >>> input = autograd.Variable(torch.randn(16, 3, 320, 480)) >>> output = m(input) >>> # \u4f7f\u7528\u4e0d\u540c\u7684\u586b\u5145 >>> m = nn.ConstantPad2d((3, 3, 6, 6), 3.5) >>> output = m(input) Non-linear Activations (\u975e\u7ebf\u6027\u5c42) ReLU class torch.nn.ReLU(inplace=False) \u5bf9\u8f93\u5165\u8fd0\u7528\u4fee\u6b63\u7ebf\u6027\u5355\u5143\u51fd\u6570 \\({ReLU}(x)= max(0, x)\\) Parameters: inplace \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97 Default: False Shape: Input: \\((N, *)\\) <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u4e0e\u8f93\u5165\u62e5\u6709\u540c\u6837\u7684 shape \u5c5e\u6027 Examples: >>> m = nn.ReLU() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) ReLU6 class torch.nn.ReLU6(inplace=False) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570 \\({ReLU6}(x) = min(max(0,x), 6)\\) Parameters: inplace \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97 \u9ed8\u8ba4\u503c: False Shape: Input: \\((N, *)\\) , <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u4e0e\u8f93\u5165\u62e5\u6709\u540c\u6837\u7684 shape \u5c5e\u6027 Examples: >>> m = nn.ReLU6() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) ELU class torch.nn.ELU(alpha=1.0, inplace=False) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570, \\(f(x) = max(0,x) + min(0, alpha * (exp(x) - 1))\\) | Parameters: | alpha \u2013 ELU \u5b9a\u4e49\u516c\u5f0f\u4e2d\u7684 alpha \u503c. \u9ed8\u8ba4\u503c: 1.0 inplace \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97 \u9ed8\u8ba4\u503c: False | | --- | --- | Shape: Input: \\((N, *)\\) <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u4e0e\u8f93\u5165\u62e5\u6709\u540c\u6837\u7684 shape \u5c5e\u6027 Examples: >>> m = nn.ELU() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) SELU class torch.nn.SELU(inplace=False) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570, \\(f(x) = scale * (\\max(0,x) + \\min(0, alpha * (\\exp(x) - 1)))\\) , alpha=1.6732632423543772848170429916717 , scale=1.0507009873554804934193349852946 . \u66f4\u591a\u5730\u7ec6\u8282\u53ef\u4ee5\u53c2\u9605\u8bba\u6587 Self-Normalizing Neural Networks . Parameters: inplace ( bool , optional ) \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97. \u9ed8\u8ba4\u503c: False Shape: Input: \\((N, *)\\) where <cite>*</cite> means, any number of additional dimensions Output: \\((N, *)\\) , same shape as the input Examples: >>> m = nn.SELU() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) PReLU class torch.nn.PReLU(num_parameters=1, init=0.25) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570 \\(PReLU(x) = max(0,x) + a * min(0,x)\\) \u8fd9\u91cc\u7684 \u201ca\u201d \u662f\u81ea\u5b66\u4e60\u7684\u53c2\u6570. \u5f53\u4e0d\u5e26\u53c2\u6570\u5730\u8c03\u7528\u65f6, nn.PReLU() \u5728\u6240\u6709\u8f93\u5165\u901a\u9053\u4e2d\u4f7f\u7528\u5355\u4e2a\u53c2\u6570 \u201ca\u201d . \u800c\u5982\u679c\u7528 nn.PReLU(nChannels) \u8c03\u7528, \u201ca\u201d \u5c06\u5e94\u7528\u5230\u6bcf\u4e2a\u8f93\u5165. Note \u5f53\u4e3a\u4e86\u8868\u73b0\u66f4\u4f73\u7684\u6a21\u578b\u800c\u5b66\u4e60\u53c2\u6570 \u201ca\u201d \u65f6\u4e0d\u8981\u4f7f\u7528\u6743\u91cd\u8870\u51cf (weight decay) | Parameters: | num_parameters \u2013 \u9700\u8981\u5b66\u4e60\u7684 \u201ca\u201d \u7684\u4e2a\u6570. \u9ed8\u8ba4\u7b49\u4e8e1 init \u2013 \u201ca\u201d \u7684\u521d\u59cb\u503c. \u9ed8\u8ba4\u7b49\u4e8e0.25 | | --- | --- | Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0f shape \u4e00\u81f4 \u4f8b: >>> m = nn.PReLU() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) LeakyReLU class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528, \\(f(x) = max(0, x) + {negative\\_slope} * min(0, x)\\) | Parameters: | negative_slope \u2013 \u63a7\u5236\u8d1f\u659c\u7387\u7684\u89d2\u5ea6, \u9ed8\u8ba4\u503c: 1e-2 inplace \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97 \u9ed8\u8ba4\u503c: False | | --- | --- | Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0fshape\u4e00\u81f4 \u4f8b: >>> m = nn.LeakyReLU(0.1) >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) Threshold class torch.nn.Threshold(threshold, value, inplace=False) \u57fa\u4e8e Tensor \u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u521b\u9020\u9608\u503c\u51fd\u6570 Threshold \u88ab\u5b9a\u4e49\u4e3a y = x if x > threshold value if x <= threshold | Parameters: | threshold \u2013 \u9608\u503c value \u2013 \u8f93\u5165\u503c\u5c0f\u4e8e\u9608\u503c\u5219\u4f1a\u88ab value \u4ee3\u66ff inplace \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97. \u9ed8\u8ba4\u503c: False | | --- | --- | Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0f shape \u4e00\u81f4 \u4f8b: >>> m = nn.Threshold(0.1, 20) >>> input = Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) Hardtanh class torch.nn.Hardtanh(min_val=-1, max_val=1, inplace=False, min_value=None, max_value=None) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528 HardTanh HardTanh \u88ab\u5b9a\u4e49\u4e3a: f(x) = +1, if x > 1 f(x) = -1, if x < -1 f(x) = x, otherwise \u7ebf\u6027\u533a\u57df\u7684\u8303\u56f4 \\([-1, 1]\\) \u53ef\u4ee5\u88ab\u8c03\u6574 | Parameters: | min_val \u2013 \u7ebf\u6027\u533a\u57df\u8303\u56f4\u6700\u5c0f\u503c. \u9ed8\u8ba4\u503c: -1 max_val \u2013 \u7ebf\u6027\u533a\u57df\u8303\u56f4\u6700\u5927\u503c. \u9ed8\u8ba4\u503c: 1 inplace \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97. \u9ed8\u8ba4\u503c: False | | --- | --- | \u5173\u952e\u5b57\u53c2\u6570 min_value \u4ee5\u53ca max_value \u5df2\u88ab\u5f03\u7528. \u66f4\u6539\u4e3a min_val \u548c max_val Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u7ef4\u5ea6\u7ec4\u5408 Output: \\((N, *)\\) , \u4e0e\u8f93\u5165\u6709\u76f8\u540c\u7684 shape \u5c5e\u6027 \u4f8b >>> m = nn.Hardtanh(-2, 2) >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) Sigmoid class torch.nn.Sigmoid \u5bf9\u6bcf\u4e2a\u5143\u7d20\u8fd0\u7528 Sigmoid \u51fd\u6570. Sigmoid \u5b9a\u4e49\u5982\u4e0b \\(f(x) = 1 / ( 1 + exp(-x))\\) Shape: Input: \\((N, *)\\) <cite>*</cite> \u8868\u793a\u4efb\u610f\u7ef4\u5ea6\u7ec4\u5408 Output: \\((N, *)\\) , \u4e0e\u8f93\u5165\u6709\u76f8\u540c\u7684 shape \u5c5e\u6027 Examples: >>> m = nn.Sigmoid() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) Tanh class torch.nn.Tanh \u5bf9\u8f93\u5165\u7684\u6bcf\u4e2a\u5143\u7d20, \\(f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\\) Shape: Input: \\((N, *)\\) <cite>*</cite> \u8868\u793a\u4efb\u610f\u7ef4\u5ea6\u7ec4\u5408 Output: \\((N, *)\\) , \u4e0e\u8f93\u5165\u6709\u76f8\u540c\u7684 shape \u5c5e\u6027 Examples: >>> m = nn.Tanh() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) LogSigmoid class torch.nn.LogSigmoid \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570 \\(LogSigmoid(x) = log( 1 / (1 + exp(-x_i)))\\) Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0fshape\u4e00\u81f4 \u4f8b: >>> m = nn.LogSigmoid() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) Softplus class torch.nn.Softplus(beta=1, threshold=20) \u5bf9\u6bcf\u4e2a\u5143\u7d20\u8fd0\u7528Softplus\u51fd\u6570, Softplus \u5b9a\u4e49\u5982\u4e0b :: \\(f(x) = 1/beta * log(1 + exp(beta * x_i))\\) Softplus \u51fd\u6570\u662fReLU\u51fd\u6570\u7684\u5e73\u6ed1\u903c\u8fd1. Softplus \u51fd\u6570\u53ef\u4ee5\u4f7f\u5f97\u8f93\u51fa\u503c\u9650\u5b9a\u4e3a\u6b63\u6570. \u4e3a\u4e86\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a\u6027. \u7ebf\u6027\u51fd\u6570\u7684\u8f6c\u6362\u53ef\u4ee5\u4f7f\u8f93\u51fa\u5927\u4e8e\u67d0\u4e2a\u503c. | Parameters: | beta \u2013 Softplus \u516c\u5f0f\u4e2d\u7684 beta \u503c. \u9ed8\u8ba4\u503c: 1 threshold \u2013 \u9608\u503c. \u5f53\u8f93\u5165\u5230\u8be5\u503c\u4ee5\u4e0a\u65f6\u6211\u4eec\u7684SoftPlus\u5b9e\u73b0\u5c06\u8fd8\u539f\u4e3a\u7ebf\u6027\u51fd\u6570. \u9ed8\u8ba4\u503c: 20 | | --- | --- | Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 dimensions Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0fshape\u4e00\u81f4 \u4f8b: >>> m = nn.Softplus() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) Softshrink class torch.nn.Softshrink(lambd=0.5) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528 soft shrinkage \u51fd\u6570 SoftShrinkage \u8fd0\u7b97\u7b26\u5b9a\u4e49\u4e3a: f(x) = x-lambda, if x > lambda > f(x) = x+lambda, if x < -lambda f(x) = 0, otherwise Parameters: lambd \u2013 Softshrink \u516c\u5f0f\u4e2d\u7684 lambda \u503c. \u9ed8\u8ba4\u503c: 0.5 Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0f shape \u4e00\u81f4 \u4f8b: >>> m = nn.Softshrink() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) Softsign class torch.nn.Softsign \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570 \\(f(x) = x / (1 + |x|)\\) Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0f shape \u4e00\u81f4 \u4f8b: >>> m = nn.Softsign() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) Tanhshrink class torch.nn.Tanhshrink \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570, \\(Tanhshrink(x) = x - Tanh(x)\\) Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0fshape\u4e00\u81f4 \u4f8b: >>> m = nn.Tanhshrink() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input)) Softmin class torch.nn.Softmin(dim=None) \u5bf9n\u7ef4\u8f93\u5165\u5f20\u91cf\u8fd0\u7528 Softmin \u51fd\u6570, \u5c06\u5f20\u91cf\u7684\u6bcf\u4e2a\u5143\u7d20\u7f29\u653e\u5230 (0,1) \u533a\u95f4\u4e14\u548c\u4e3a 1. \\(f(x) = \\frac{\\exp(-x_i)}{\\sum_j \\exp(-x_j)}\\) Shape: Input: \u4efb\u610fshape Output: \u548c\u8f93\u5165\u76f8\u540c Parameters: dim ( int ) \u2013 \u8fd9\u662f\u5c06\u8ba1\u7b97 Softmax \u7684\u7ef4\u5ea6 (\u6240\u4ee5\u6bcf\u4e2a\u6cbf\u7740 dim \u7684\u5207\u7247\u548c\u4e3a 1). Returns: \u8fd4\u56de\u7ed3\u679c\u662f\u4e00\u4e2a\u4e0e\u8f93\u5165\u7ef4\u5ea6\u76f8\u540c\u7684\u5f20\u91cf, \u6bcf\u4e2a\u5143\u7d20\u7684\u53d6\u503c\u8303\u56f4\u5728 [0, 1] \u533a\u95f4. --- --- \u4f8b: >>> m = nn.Softmin() >>> input = autograd.Variable(torch.randn(2, 3)) >>> print(input) >>> print(m(input)) Softmax class torch.nn.Softmax(dim=None) \u5bf9n\u7ef4\u8f93\u5165\u5f20\u91cf\u8fd0\u7528 Softmax \u51fd\u6570, \u5c06\u5f20\u91cf\u7684\u6bcf\u4e2a\u5143\u7d20\u7f29\u653e\u5230 (0,1) \u533a\u95f4\u4e14\u548c\u4e3a 1. Softmax \u51fd\u6570\u5b9a\u4e49\u5982\u4e0b \\(f_i(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\) Shape: Input: \u4efb\u610fshape Output: \u548c\u8f93\u5165\u76f8\u540c Returns: \u8fd4\u56de\u7ed3\u679c\u662f\u4e00\u4e2a\u4e0e\u8f93\u5165\u7ef4\u5ea6\u76f8\u540c\u7684\u5f20\u91cf, \u6bcf\u4e2a\u5143\u7d20\u7684\u53d6\u503c\u8303\u56f4\u5728 [0, 1] \u533a\u95f4. Parameters: dim ( int ) \u2013 \u8fd9\u662f\u5c06\u8ba1\u7b97 Softmax \u7684\u90a3\u4e2a\u7ef4\u5ea6 (\u6240\u4ee5\u6bcf\u4e2a\u6cbf\u7740 dim \u7684\u5207\u7247\u548c\u4e3a 1). --- --- Note \u5982\u679c\u4f60\u60f3\u5bf9\u539f\u59cb Softmax \u6570\u636e\u8ba1\u7b97 Log \u8fdb\u884c\u6536\u7f29, \u5e76\u4e0d\u80fd\u4f7f\u8be5\u6a21\u5757\u76f4\u63a5\u4f7f\u7528 NLLLoss \u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u51fd\u6570. \u53d6\u800c\u4ee3\u4e4b, \u5e94\u8be5\u4f7f\u7528 Logsoftmax (\u5b83\u6709\u66f4\u5feb\u7684\u8fd0\u7b97\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6570\u503c\u6027\u8d28). \u4f8b: >>> m = nn.Softmax() >>> input = autograd.Variable(torch.randn(2, 3)) >>> print(input) >>> print(m(input)) Softmax2d class torch.nn.Softmax2d \u628a SoftMax \u5e94\u7528\u4e8e\u6bcf\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\u7684\u7279\u5f81. \u7ed9\u5b9a\u56fe\u7247\u7684 \u901a\u9053\u6570 Channels x \u9ad8 Height x \u5bbd Width, \u5b83\u5c06\u5bf9\u56fe\u7247\u7684\u6bcf\u4e00\u4e2a\u4f4d\u7f6e \u4f7f\u7528 Softmax \\((Channels, h_i, w_j)\\) Shape: Input: \\((N, C, H, W)\\) Output: \\((N, C, H, W)\\) (\u683c\u5f0f shape \u4e0e\u8f93\u5165\u76f8\u540c) Returns: \u4e00\u4e2a\u7ef4\u5ea6\u53ca\u683c\u5f0f shape \u90fd\u548c\u8f93\u5165\u76f8\u540c\u7684 Tensor, \u53d6\u503c\u8303\u56f4\u5728[0, 1] \u4f8b: >>> m = nn.Softmax2d() >>> # you softmax over the 2nd dimension >>> input = autograd.Variable(torch.randn(2, 3, 12, 13)) >>> print(input) >>> print(m(input)) LogSoftmax class torch.nn.LogSoftmax(dim=None) \u5bf9\u6bcf\u4e2a\u8f93\u5165\u7684 n \u7ef4 Tensor \u4f7f\u7528 Log(Softmax(x)). LogSoftmax \u516c\u5f0f\u53ef\u7b80\u5316\u4e3a \\(f_i(x) = log(exp(x_i) / sum_j exp(x_j) )\\) Shape: Input: \u4efb\u610f\u683c\u5f0f shape Output: \u548c\u8f93\u5165\u7684\u683c\u5f0f shape \u4e00\u81f4 Parameters: dim ( int ) \u2013 \u8fd9\u662f\u5c06\u8ba1\u7b97 Softmax \u7684\u90a3\u4e2a\u7ef4\u5ea6 (\u6240\u4ee5\u6bcf\u4e2a\u6cbf\u7740 dim \u7684\u5207\u7247\u548c\u4e3a1). Returns: \u4e00\u4e2a\u7ef4\u5ea6\u53ca\u683c\u5f0f shape \u90fd\u548c\u8f93\u5165\u76f8\u540c\u7684 Tensor, \u53d6\u503c\u8303\u56f4\u5728 [-inf, 0) --- --- \u4f8b: >>> m = nn.LogSoftmax() >>> input = autograd.Variable(torch.randn(2, 3)) >>> print(input) >>> print(m(input)) Normalization layers (\u5f52\u4e00\u5316\u5c42) BatchNorm1d class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True) \u5bf9 2d \u6216\u8005 3d \u7684\u5c0f\u6279\u91cf (mini-batch) \u6570\u636e\u8fdb\u884c\u6279\u6807\u51c6\u5316 (Batch Normalization) \u64cd\u4f5c. \\[y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\\] \u6bcf\u4e2a\u5c0f\u6279\u91cf\u6570\u636e\u4e2d,\u8ba1\u7b97\u5404\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee,\u5e76\u4e14 gamma \u548c beta \u662f\u5927\u5c0f\u4e3a C \u7684\u53ef\u5b66\u4e60, \u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570\u5411\u91cf( C \u4e3a\u8f93\u5165\u5927\u5c0f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d,\u8be5\u5c42\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5e76\u8fdb\u884c\u5e73\u5747\u79fb\u52a8,\u9ed8\u8ba4\u7684\u5e73\u5747\u79fb\u52a8\u52a8\u91cf\u503c\u4e3a 0.1. \u5728\u9a8c\u8bc1\u65f6,\u8bad\u7ec3\u5f97\u5230\u7684\u5747\u503c/\u65b9\u5dee,\u7528\u4e8e\u6807\u51c6\u5316\u9a8c\u8bc1\u6570\u636e. BatchNorm \u5728 \u2018C\u2019 \u7ef4\u4e0a\u5904\u7406,\u5373 \u2018(N,L)\u2019 \u90e8\u5206\u8fd0\u884c,\u88ab\u79f0\u4f5c \u2018Temporal BatchNorm\u2019 | Parameters: | num_features \u2013 \u9884\u671f\u8f93\u5165\u7684\u7279\u5f81\u6570,\u5927\u5c0f\u4e3a \u2018batch_size x num_features [x width]\u2019 eps \u2013 \u7ed9\u5206\u6bcd\u52a0\u4e0a\u7684\u503c,\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a(\u5206\u6bcd\u4e0d\u80fd\u8d8b\u8fd10\u6216\u53d60),\u9ed8\u8ba4\u4e3a 1e-5 momentum \u2013 \u52a8\u6001\u5747\u503c\u548c\u52a8\u6001\u65b9\u5dee\u4f7f\u7528\u7684\u79fb\u52a8\u52a8\u91cf\u503c,\u9ed8\u8ba4\u4e3a 0.1 affine \u2013 \u5e03\u5c14\u503c,\u8bbe\u4e3a True \u65f6,\u8868\u793a\u8be5\u5c42\u6dfb\u52a0\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570,\u5373 gamma \u548c beta,\u9ed8\u8ba4\u4e3a True | | --- | --- | Shape: Input: \\((N, C)\\) or \\((N, C, L)\\) Output: \\((N, C)\\) or \\((N, C, L)\\) (same shape as input) Examples >>> # With Learnable Parameters >>> m = nn.BatchNorm1d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm1d(100, affine=False) >>> input = autograd.Variable(torch.randn(20, 100)) >>> output = m(input) BatchNorm2d class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True) \u5bf9\u5c0f\u6279\u91cf (mini-batch) 3d \u6570\u636e\u7ec4\u6210\u7684 4d \u8f93\u5165\u8fdb\u884c\u6807\u51c6\u5316 (Batch Normalization) \u64cd\u4f5c. \\[y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\\] \u6bcf\u4e2a\u5c0f\u6279\u91cf\u6570\u636e\u4e2d,\u8ba1\u7b97\u5404\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee, \u5e76\u4e14 gamma \u548c beta \u662f\u5927\u5c0f\u4e3a C \u7684\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570\u5411\u91cf (C \u4e3a\u8f93\u5165\u5927\u5c0f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d,\u8be5\u5c42\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5e76\u8fdb\u884c\u5e73\u5747\u79fb\u52a8.\u9ed8\u8ba4\u7684\u5e73\u5747\u79fb\u52a8\u52a8\u91cf\u503c\u4e3a 0.1. \u5728\u9a8c\u8bc1\u65f6,\u8bad\u7ec3\u5f97\u5230\u7684\u5747\u503c/\u65b9\u5dee,\u7528\u4e8e\u6807\u51c6\u5316\u9a8c\u8bc1\u6570\u636e. BatchNorm \u5728 \u2018C\u2019 \u7ef4\u4e0a\u5904\u7406,\u5373 \u2018(N, H, W)\u2019 \u90e8\u5206\u8fd0\u884c,\u88ab\u79f0\u4f5c \u2018Spatial BatchNorm\u2019. | Parameters: | num_features \u2013 \u9884\u671f\u8f93\u5165\u7684\u7279\u5f81\u6570,\u5927\u5c0f\u4e3a \u2018batch_size x num_features x height x width\u2019 eps \u2013 \u7ed9\u5206\u6bcd\u52a0\u4e0a\u7684\u503c,\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a(\u5206\u6bcd\u4e0d\u80fd\u8d8b\u8fd10\u6216\u53d60),\u9ed8\u8ba4\u4e3a 1e-5 momentum \u2013 \u52a8\u6001\u5747\u503c\u548c\u52a8\u6001\u65b9\u5dee\u4f7f\u7528\u7684\u79fb\u52a8\u52a8\u91cf\u503c,\u9ed8\u8ba4\u4e3a 0.1 affine \u2013 \u5e03\u5c14\u503c,\u8bbe\u4e3a True \u65f6,\u8868\u793a\u8be5\u5c42\u6dfb\u52a0\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570,\u5373 gamma \u548c beta,\u9ed8\u8ba4\u4e3a True | | --- | --- | Shape: Input: \\((N, C, H, W)\\) Output: \\((N, C, H, W)\\) (same shape as input) Examples >>> # With Learnable Parameters >>> m = nn.BatchNorm2d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm2d(100, affine=False) >>> input = autograd.Variable(torch.randn(20, 100, 35, 45)) >>> output = m(input) BatchNorm3d class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True) \u5bf9\u5c0f\u6279\u91cf (mini-batch) 4d \u6570\u636e\u7ec4\u6210\u7684 5d \u8f93\u5165\u8fdb\u884c\u6807\u51c6\u5316 (Batch Normalization) \u64cd\u4f5c. \\[y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\\] \u6bcf\u4e2a\u5c0f\u6279\u91cf\u6570\u636e\u4e2d,\u8ba1\u7b97\u5404\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee, \u5e76\u4e14 gamma \u548c beta \u662f\u5927\u5c0f\u4e3a C \u7684\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570\u5411\u91cf (C \u4e3a\u8f93\u5165\u5927\u5c0f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d,\u8be5\u5c42\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5e76\u8fdb\u884c\u5e73\u5747\u79fb\u52a8.\u9ed8\u8ba4\u7684\u5e73\u5747\u79fb\u52a8\u52a8\u91cf\u503c\u4e3a 0.1. \u5728\u9a8c\u8bc1\u65f6,\u8bad\u7ec3\u5f97\u5230\u7684\u5747\u503c/\u65b9\u5dee,\u7528\u4e8e\u6807\u51c6\u5316\u9a8c\u8bc1\u6570\u636e. BatchNorm \u5728 \u2018C\u2019 \u7ef4\u4e0a\u5904\u7406,\u5373 \u2018(N, D, H, W)\u2019 \u90e8\u5206\u8fd0\u884c,\u88ab\u79f0\u4f5c \u2018Volumetric BatchNorm\u2019 \u6216\u8005 \u2018Spatio-temporal BatchNorm\u2019 | Parameters: | num_features \u2013 \u9884\u671f\u8f93\u5165\u7684\u7279\u5f81\u6570,\u5927\u5c0f\u4e3a \u2018batch_size x num_features x depth x height x width\u2019 eps \u2013 \u7ed9\u5206\u6bcd\u52a0\u4e0a\u7684\u503c,\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a(\u5206\u6bcd\u4e0d\u80fd\u8d8b\u8fd10\u6216\u53d60),\u9ed8\u8ba4\u4e3a 1e-5 momentum \u2013 \u52a8\u6001\u5747\u503c\u548c\u52a8\u6001\u65b9\u5dee\u4f7f\u7528\u7684\u79fb\u52a8\u52a8\u91cf\u503c,\u9ed8\u8ba4\u4e3a 0.1 affine \u2013 \u5e03\u5c14\u503c,\u8bbe\u4e3a True \u65f6,\u8868\u793a\u8be5\u5c42\u6dfb\u52a0\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570,\u5373 gamma \u548c beta,\u9ed8\u8ba4\u4e3a True | | --- | --- | Shape: Input: \\((N, C, D, H, W)\\) Output: \\((N, C, D, H, W)\\) (same shape as input) Examples >>> # With Learnable Parameters >>> m = nn.BatchNorm3d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm3d(100, affine=False) >>> input = autograd.Variable(torch.randn(20, 100, 35, 45, 10)) >>> output = m(input) InstanceNorm1d class torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False) \u5bf9 2d \u6216\u8005 3d \u7684\u5c0f\u6279\u91cf (mini-batch) \u6570\u636e\u8fdb\u884c\u5b9e\u4f8b\u6807\u51c6\u5316 (Instance Normalization) \u64cd\u4f5c. .. math: y = \\frac{x - mean[x]}{ \\sqrt{Var[x]} + \\epsilon} * gamma + beta \u5bf9\u5c0f\u6279\u91cf\u6570\u636e\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5bf9\u8c61,\u8ba1\u7b97\u5176\u5404\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee,\u5e76\u4e14 gamma \u548c beta \u662f\u5927\u5c0f\u4e3a C \u7684\u53ef\u5b66\u4e60, \u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570\u5411\u91cf( C \u4e3a\u8f93\u5165\u5927\u5c0f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d,\u8be5\u5c42\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5e76\u8fdb\u884c\u5e73\u5747\u79fb\u52a8,\u9ed8\u8ba4\u7684\u5e73\u5747\u79fb\u52a8\u52a8\u91cf\u503c\u4e3a 0.1. \u5728\u9a8c\u8bc1\u65f6 (<cite>.eval()</cite>),InstanceNorm \u6a21\u578b\u9ed8\u8ba4\u4fdd\u6301\u4e0d\u53d8,\u5373\u6c42\u5f97\u7684\u5747\u503c/\u65b9\u5dee\u4e0d\u7528\u4e8e\u6807\u51c6\u5316\u9a8c\u8bc1\u6570\u636e, \u4f46\u53ef\u4ee5\u7528 <cite>.train(False)</cite> \u65b9\u6cd5\u5f3a\u5236\u4f7f\u7528\u5b58\u50a8\u7684\u5747\u503c\u548c\u65b9\u5dee. | Parameters: | num_features \u2013 \u9884\u671f\u8f93\u5165\u7684\u7279\u5f81\u6570,\u5927\u5c0f\u4e3a \u2018batch_size x num_features x width\u2019 eps \u2013 \u7ed9\u5206\u6bcd\u52a0\u4e0a\u7684\u503c,\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a(\u5206\u6bcd\u4e0d\u80fd\u8d8b\u8fd10\u6216\u53d60),\u9ed8\u8ba4\u4e3a 1e-5 momentum \u2013 \u52a8\u6001\u5747\u503c\u548c\u52a8\u6001\u65b9\u5dee\u4f7f\u7528\u7684\u79fb\u52a8\u52a8\u91cf\u503c,\u9ed8\u8ba4\u4e3a 0.1 affine \u2013 \u5e03\u5c14\u503c,\u8bbe\u4e3a True \u65f6,\u8868\u793a\u8be5\u5c42\u6dfb\u52a0\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570,\u5373 gamma \u548c beta,\u9ed8\u8ba4\u4e3a False | | --- | --- | Shape: Input: \\((N, C, L)\\) Output: \\((N, C, L)\\) (same shape as input) Examples >>> # Without Learnable Parameters >>> m = nn.InstanceNorm1d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm1d(100, affine=True) >>> input = autograd.Variable(torch.randn(20, 100, 40)) >>> output = m(input) InstanceNorm2d class torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False) \u5bf9\u5c0f\u6279\u91cf (mini-batch) 3d \u6570\u636e\u7ec4\u6210\u7684 4d \u8f93\u5165\u8fdb\u884c\u5b9e\u4f8b\u6807\u51c6\u5316 (Batch Normalization) \u64cd\u4f5c. .. math: y = \\frac{x - mean[x]}{ \\sqrt{Var[x]} + \\epsilon} * gamma + beta \u5bf9\u5c0f\u6279\u91cf\u6570\u636e\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5bf9\u8c61,\u8ba1\u7b97\u5176\u5404\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee,\u5e76\u4e14 gamma \u548c beta \u662f\u5927\u5c0f\u4e3a C \u7684\u53ef\u5b66\u4e60, \u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570\u5411\u91cf( C \u4e3a\u8f93\u5165\u5927\u5c0f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d,\u8be5\u5c42\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5e76\u8fdb\u884c\u5e73\u5747\u79fb\u52a8,\u9ed8\u8ba4\u7684\u5e73\u5747\u79fb\u52a8\u52a8\u91cf\u503c\u4e3a 0.1. \u5728\u9a8c\u8bc1\u65f6 (<cite>.eval()</cite>),InstanceNorm \u6a21\u578b\u9ed8\u8ba4\u4fdd\u6301\u4e0d\u53d8,\u5373\u6c42\u5f97\u7684\u5747\u503c/\u65b9\u5dee\u4e0d\u7528\u4e8e\u6807\u51c6\u5316\u9a8c\u8bc1\u6570\u636e, \u4f46\u53ef\u4ee5\u7528 <cite>.train(False)</cite> \u65b9\u6cd5\u5f3a\u5236\u4f7f\u7528\u5b58\u50a8\u7684\u5747\u503c\u548c\u65b9\u5dee. | Parameters: | num_features \u2013 \u9884\u671f\u8f93\u5165\u7684\u7279\u5f81\u6570,\u5927\u5c0f\u4e3a \u2018batch_size x num_features x height x width\u2019 eps \u2013 \u7ed9\u5206\u6bcd\u52a0\u4e0a\u7684\u503c,\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a(\u5206\u6bcd\u4e0d\u80fd\u8d8b\u8fd10\u6216\u53d60),\u9ed8\u8ba4\u4e3a 1e-5 momentum \u2013 \u52a8\u6001\u5747\u503c\u548c\u52a8\u6001\u65b9\u5dee\u4f7f\u7528\u7684\u79fb\u52a8\u52a8\u91cf\u503c,\u9ed8\u8ba4\u4e3a 0.1 affine \u2013 \u5e03\u5c14\u503c,\u8bbe\u4e3a True \u65f6,\u8868\u793a\u8be5\u5c42\u6dfb\u52a0\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570,\u5373 gamma \u548c beta,\u9ed8\u8ba4\u4e3a False | | --- | --- | Shape: Input: \\((N, C, H, W)\\) Output: \\((N, C, H, W)\\) (same shape as input) Examples >>> # Without Learnable Parameters >>> m = nn.InstanceNorm2d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm2d(100, affine=True) >>> input = autograd.Variable(torch.randn(20, 100, 35, 45)) >>> output = m(input) InstanceNorm3d class torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1, affine=False) \u5bf9\u5c0f\u6279\u91cf (mini-batch) 4d \u6570\u636e\u7ec4\u6210\u7684 5d \u8f93\u5165\u8fdb\u884c\u5b9e\u4f8b\u6807\u51c6\u5316 (Batch Normalization) \u64cd\u4f5c. .. math: y = \\frac{x - mean[x]}{ \\sqrt{Var[x]} + \\epsilon} * gamma + beta \u5bf9\u5c0f\u6279\u91cf\u6570\u636e\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5bf9\u8c61,\u8ba1\u7b97\u5176\u5404\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee,\u5e76\u4e14 gamma \u548c beta \u662f\u5927\u5c0f\u4e3a C \u7684\u53ef\u5b66\u4e60, \u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570\u5411\u91cf( C \u4e3a\u8f93\u5165\u5927\u5c0f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d,\u8be5\u5c42\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5e76\u8fdb\u884c\u5e73\u5747\u79fb\u52a8,\u9ed8\u8ba4\u7684\u5e73\u5747\u79fb\u52a8\u52a8\u91cf\u503c\u4e3a 0.1. \u5728\u9a8c\u8bc1\u65f6 (<cite>.eval()</cite>),InstanceNorm \u6a21\u578b\u9ed8\u8ba4\u4fdd\u6301\u4e0d\u53d8,\u5373\u6c42\u5f97\u7684\u5747\u503c/\u65b9\u5dee\u4e0d\u7528\u4e8e\u6807\u51c6\u5316\u9a8c\u8bc1\u6570\u636e, \u4f46\u53ef\u4ee5\u7528 <cite>.train(False)</cite> \u65b9\u6cd5\u5f3a\u5236\u4f7f\u7528\u5b58\u50a8\u7684\u5747\u503c\u548c\u65b9\u5dee. | Parameters: | num_features \u2013 \u9884\u671f\u8f93\u5165\u7684\u7279\u5f81\u6570,\u5927\u5c0f\u4e3a \u2018batch_size x num_features x depth x height x width\u2019 eps \u2013 \u7ed9\u5206\u6bcd\u52a0\u4e0a\u7684\u503c,\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a(\u5206\u6bcd\u4e0d\u80fd\u8d8b\u8fd10\u6216\u53d60),\u9ed8\u8ba4\u4e3a 1e-5 momentum \u2013 \u52a8\u6001\u5747\u503c\u548c\u52a8\u6001\u65b9\u5dee\u4f7f\u7528\u7684\u79fb\u52a8\u52a8\u91cf\u503c,\u9ed8\u8ba4\u4e3a 0.1 affine \u2013 \u5e03\u5c14\u503c,\u8bbe\u4e3a True \u65f6,\u8868\u793a\u8be5\u5c42\u6dfb\u52a0\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570,\u5373 gamma \u548c beta,\u9ed8\u8ba4\u4e3a False | | --- | --- | Shape: Input: \\((N, C, D, H, W)\\) Output: \\((N, C, D, H, W)\\) (same shape as input) Examples >>> # Without Learnable Parameters >>> m = nn.InstanceNorm3d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm3d(100, affine=True) >>> input = autograd.Variable(torch.randn(20, 100, 35, 45, 10)) >>> output = m(input) Recurrent layers (\u5faa\u73af\u5c42) RNN class torch.nn.RNN(*args, **kwargs) \u5bf9\u4e8e\u8f93\u5165\u5e8f\u5217\u4f7f\u7528\u4e00\u4e2a\u591a\u5c42\u7684 Elman RNN , \u5b83\u7684\u6fc0\u6d3b\u51fd\u6570\u4e3a tanh \u6216\u8005 ReLU . \u5bf9\u8f93\u5165\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u5143\u7d20, \u6bcf\u5c42\u8ba1\u7b97\u516c\u5f0f\u4e3a: \\[h_t = \\tanh(w_{ih} * x_t + b_{ih} + w_{hh} * h_{(t-1)} + b_{hh})\\] \u8fd9\u91cc \\(h_t\\) \u662f\u5f53\u524d\u5728\u65f6\u523b <cite>t</cite> \u7684\u9690\u72b6\u6001, \u5e76\u4e14 \\(x_t\\) \u662f\u4e4b\u524d\u4e00\u5c42\u5728 <cite>t</cite> \u65f6\u523b\u7684\u9690\u72b6\u6001, \u6216\u8005\u662f\u7b2c\u4e00\u5c42\u7684\u8f93\u5165. \u5982\u679c nonlinearity='relu' ,\u90a3\u4e48\u5c06\u4f7f\u7528 relu \u4ee3\u66ff tanh \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570. | Parameters: | input_size \u2013 \u8f93\u5165 x \u7684\u7279\u5f81\u6570\u91cf hidden_size \u2013 \u9690\u72b6\u6001 h \u4e2d\u7684\u7279\u5f81\u6570\u91cf num_layers \u2013 RNN \u7684\u5c42\u6570 nonlinearity \u2013 \u6307\u5b9a\u975e\u7ebf\u6027\u51fd\u6570\u4f7f\u7528 [\u2018tanh\u2019|\u2019relu\u2019]. \u9ed8\u8ba4: \u2018tanh\u2019 bias \u2013 \u5982\u679c\u662f False , \u90a3\u4e48 RNN \u5c42\u5c31\u4e0d\u4f1a\u4f7f\u7528\u504f\u7f6e\u6743\u91cd b_ih \u548c b_hh, \u9ed8\u8ba4: True batch_first \u2013 \u5982\u679c True , \u90a3\u4e48\u8f93\u5165 Tensor \u7684 shape \u5e94\u8be5\u662f (batch, seq, feature),\u5e76\u4e14\u8f93\u51fa\u4e5f\u662f\u4e00\u6837 dropout \u2013 \u5982\u679c\u503c\u975e\u96f6, \u90a3\u4e48\u9664\u4e86\u6700\u540e\u4e00\u5c42\u5916, \u5176\u5b83\u5c42\u7684\u8f93\u51fa\u90fd\u4f1a\u5957\u4e0a\u4e00\u4e2a dropout \u5c42 bidirectional \u2013 \u5982\u679c True , \u5c06\u4f1a\u53d8\u6210\u4e00\u4e2a\u53cc\u5411 RNN, \u9ed8\u8ba4\u4e3a False | | --- | --- | Inputs: input, h_0 input (seq_len, batch, input_size): \u5305\u542b\u8f93\u5165\u5e8f\u5217\u7279\u5f81\u7684 tensor , input \u53ef\u4ee5\u662f\u88ab\u586b\u5145\u7684\u53d8\u957f\u5e8f\u5217.\u7ec6\u8282\u8bf7\u770b torch.nn.utils.rnn.pack_padded_sequence() . h_0 (num_layers * num_directions, batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u4fdd\u5b58\u7740\u521d\u59cb\u9690\u72b6\u6001\u7684 tensor Outputs: output, h_n output (seq_len, batch, hidden_size * num_directions): \u5305\u542b RNN \u6700\u540e\u4e00\u5c42\u8f93\u51fa\u7279\u5f81 (h_k) \u7684 tensor \u5bf9\u4e8e\u6bcf\u4e2a k ,\u5982\u679c\u8f93\u5165\u662f\u4e00\u4e2a torch.nn.utils.rnn.PackedSequence , \u90a3\u4e48\u8f93\u51fa\u4e5f\u662f\u4e00\u4e2a\u53ef\u4ee5\u662f\u88ab\u586b\u5145\u7684\u53d8\u957f\u5e8f\u5217. h_n (num_layers * num_directions, batch, hidden_size): \u5305\u542b k= seq_len \u9690\u72b6\u6001\u7684 tensor . | Variables: | weight_ih_l[k] \u2013 \u7b2c k \u5c42\u7684 input-hidden \u6743\u91cd,\u53ef\u5b66\u4e60, shape \u662f <cite>(input_size x hidden_size)</cite> weight_hh_l[k] \u2013 \u7b2c k \u5c42\u7684 hidden-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, shape \u662f <cite>(hidden_size x hidden_size)</cite> bias_ih_l[k] \u2013 \u7b2c k \u5c42\u7684 input-hidden \u504f\u7f6e, \u53ef\u5b66\u4e60, shape \u662f <cite>(hidden_size)</cite> bias_hh_l[k] \u2013 \u7b2c k \u5c42\u7684 hidden-hidden \u504f\u7f6e, \u53ef\u5b66\u4e60, shape \u662f <cite>(hidden_size)</cite> | | --- | --- | Examples: >>> rnn = nn.RNN(10, 20, 2) >>> input = Variable(torch.randn(5, 3, 10)) >>> h0 = Variable(torch.randn(2, 3, 20)) >>> output, hn = rnn(input, h0) LSTM class torch.nn.LSTM(*args, **kwargs) \u5bf9\u4e8e\u8f93\u5165\u5e8f\u5217\u4f7f\u7528\u4e00\u4e2a\u591a\u5c42\u7684 LSTM ( long short-term memory ). \u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u6bcf\u4e2a\u5143\u7d20, LSTM \u7684\u6bcf\u5c42\u90fd\u4f1a\u6267\u884c\u4ee5\u4e0b\u8ba1\u7b97: \\[\\begin{split}\\begin{array}{ll} i_t = \\mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\ f_t = \\mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\ g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\ o_t = \\mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\ c_t = f_t * c_{(t-1)} + i_t * g_t \\\\ h_t = o_t * \\tanh(c_t) \\end{array}\\end{split}\\] \u8fd9\u91cc \\(h_t\\) \u662f\u5728\u65f6\u523b <cite>t</cite> \u7684\u9690\u72b6\u6001, \\(c_t\\) \u662f\u5728\u65f6\u523b <cite>t</cite> \u7684\u7ec6\u80de\u72b6\u6001 (cell state), \\(x_t\\) \u662f\u4e0a\u4e00\u5c42\u7684\u5728\u65f6\u523b <cite>t</cite> \u7684\u9690\u72b6\u6001\u6216\u8005\u662f\u7b2c\u4e00\u5c42\u7684 \\(input_t\\) , \u800c \\(i_t\\) , \\(f_t\\) , \\(g_t\\) , \\(o_t\\) \u5206\u522b\u4ee3\u8868 \u8f93\u5165\u95e8,\u9057\u5fd8\u95e8,\u7ec6\u80de\u548c\u8f93\u51fa\u95e8. | Parameters: | input_size \u2013 \u8f93\u5165\u7684\u7279\u5f81\u7ef4\u5ea6 hidden_size \u2013 \u9690\u72b6\u6001\u7684\u7279\u5f81\u7ef4\u5ea6 num_layers \u2013 \u5c42\u6570(\u548c\u65f6\u5e8f\u5c55\u5f00\u8981\u533a\u5206\u5f00) bias \u2013 \u5982\u679c\u4e3a False ,\u90a3\u4e48 LSTM \u5c06\u4e0d\u4f1a\u4f7f\u7528 b_ih \u548c b_hh ,\u9ed8\u8ba4: True batch_first \u2013 \u5982\u679c\u4e3a True , \u90a3\u4e48\u8f93\u5165\u548c\u8f93\u51fa Tensor \u7684\u5f62\u72b6\u4e3a (batch, seq, feature) dropout \u2013 \u5982\u679c\u975e\u96f6\u7684\u8bdd, \u5c06\u4f1a\u5728 RNN \u7684\u8f93\u51fa\u4e0a\u52a0\u4e2a dropout , \u6700\u540e\u4e00\u5c42\u9664\u5916 bidirectional \u2013 \u5982\u679c\u4e3a True ,\u5c06\u4f1a\u53d8\u6210\u4e00\u4e2a\u53cc\u5411 RNN ,\u9ed8\u8ba4\u4e3a False | | --- | --- | Inputs: input, (h_0, c_0) input (seq_len, batch, input_size): \u5305\u542b\u8f93\u5165\u5e8f\u5217\u7279\u5f81\u7684 tensor . \u4e5f\u53ef\u4ee5\u662f packed variable length sequence , \u8be6\u89c1 torch.nn.utils.rnn.pack_padded_sequence() . h_0 (num_layers * num_directions, batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u9690\u72b6\u6001\u7684 tensor . c_0 (num_layers * num_directions, batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u7ec6\u80de\u72b6\u6001\u7684 tensor . Outputs: output, (h_n, c_n) output (seq_len, batch, hidden_size * num_directions): \u5305\u542b RNN \u6700\u540e\u4e00\u5c42\u7684\u8f93\u51fa\u7279\u5f81 <cite>(h_t)</cite> \u7684 tensor , \u5bf9\u4e8e\u6bcf\u4e2a t . \u5982\u679c\u8f93\u5165\u662f torch.nn.utils.rnn.PackedSequence \u90a3\u4e48\u8f93\u51fa\u4e5f\u662f\u4e00\u4e2a\u53ef\u4ee5\u662f\u88ab\u586b\u5145\u7684\u53d8\u957f\u5e8f\u5217. h_n (num_layers * num_directions, batch, hidden_size): \u5305\u542b t=seq_len \u9690\u72b6\u6001\u7684 tensor . c_n (num_layers * num_directions, batch, hidden_size): \u5305\u542b t=seq_len \u7ec6\u80de\u72b6\u6001\u7684 tensor . | Variables: | weight_ih_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 input-hidden \u6743\u91cd <cite>(W_ii|W_if|W_ig|W_io)</cite>, shape \u662f <cite>(4*hidden_size x input_size)</cite> weight_hh_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 hidden-hidden \u6743\u91cd <cite>(W_hi|W_hf|W_hg|W_ho)</cite>, shape \u662f <cite>(4*hidden_size x hidden_size)</cite> bias_ih_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 input-hidden \u504f\u7f6e <cite>(b_ii|b_if|b_ig|b_io)</cite>, shape \u662f <cite>(4*hidden_size)</cite> bias_hh_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 hidden-hidden \u504f\u7f6e <cite>(b_hi|b_hf|b_hg|b_ho)</cite>, shape \u662f <cite>(4*hidden_size)</cite> | | --- | --- | Examples: >>> rnn = nn.LSTM(10, 20, 2) >>> input = Variable(torch.randn(5, 3, 10)) >>> h0 = Variable(torch.randn(2, 3, 20)) >>> c0 = Variable(torch.randn(2, 3, 20)) >>> output, hn = rnn(input, (h0, c0)) GRU class torch.nn.GRU(*args, **kwargs) \u5bf9\u4e8e\u8f93\u5165\u5e8f\u5217\u4f7f\u7528\u4e00\u4e2a\u591a\u5c42\u7684 GRU (gated recurrent unit). \u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u6bcf\u4e2a\u5143\u7d20, \u6bcf\u5c42\u90fd\u4f1a\u6267\u884c\u4ee5\u4e0b\u8ba1\u7b97: \\[\\begin{split}\\begin{array}{ll} r_t = \\mathrm{sigmoid}(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\ z_t = \\mathrm{sigmoid}(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\ n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \\\\ \\end{array}\\end{split}\\] \u8fd9\u91cc \\(h_t\\) \u662f\u5728\u65f6\u523b <cite>t</cite> \u7684\u9690\u72b6\u6001, \\(x_t\\) \u662f\u524d\u4e00\u5c42\u5728\u65f6\u523b <cite>t</cite> \u7684\u9690\u72b6\u6001\u6216\u8005\u662f\u7b2c\u4e00\u5c42\u7684 \\(input_t\\) , \u800c \\(r_t\\) , \\(z_t\\) , \\(n_t\\) \u5206\u522b\u662f\u91cd\u7f6e\u95e8,\u8f93\u5165\u95e8\u548c\u65b0\u95e8. | Parameters: | input_size \u2013 \u8f93\u5165\u7684\u7279\u5f81\u7ef4\u5ea6 hidden_size \u2013 \u9690\u72b6\u6001\u7684\u7279\u5f81\u7ef4\u5ea6 num_layers \u2013 RNN \u7684\u5c42\u6570 bias \u2013 \u5982\u679c\u4e3a False , \u90a3\u4e48 RNN \u5c42\u5c06\u4e0d\u4f1a\u4f7f\u7528\u504f\u7f6e\u6743\u91cd b_ih \u548c b_hh \u9ed8\u8ba4: True batch_first \u2013 \u5982\u679c\u4e3a True , \u90a3\u4e48\u8f93\u5165\u548c\u8f93\u51fa\u7684 tensor \u7684\u5f62\u72b6\u662f (batch, seq, feature) dropout \u2013 \u5982\u679c\u975e\u96f6\u7684\u8bdd,\u5c06\u4f1a\u5728 RNN \u7684\u8f93\u51fa\u4e0a\u52a0\u4e2a dropout ,\u6700\u540e\u4e00\u5c42\u9664\u5916 bidirectional \u2013 \u5982\u679c\u4e3a True , \u5c06\u4f1a\u53d8\u6210\u4e00\u4e2a\u53cc\u5411 RNN . \u9ed8\u8ba4: False | | --- | --- | Inputs: input, h_0 input (seq_len, batch, input_size): \u5305\u542b\u8f93\u5165\u5e8f\u5217\u7279\u5f81\u7684 tensor . \u4e5f\u53ef\u4ee5\u662f packed variable length sequence , \u8be6\u89c1 torch.nn.utils.rnn.pack_padded_sequence() . h_0 (num_layers * num_directions, batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u9690\u72b6\u6001\u7684 tensor Outputs: output, h_n output (seq_len, batch, hidden_size * num_directions): \u5305\u542b RNN \u6700\u540e\u4e00\u5c42\u7684\u8f93\u51fa\u7279\u5f81 <cite>(h_t)</cite> \u7684 tensor , \u5bf9\u4e8e\u6bcf\u4e2a t . \u5982\u679c\u8f93\u5165\u662f torch.nn.utils.rnn.PackedSequence \u90a3\u4e48\u8f93\u51fa\u4e5f\u662f\u4e00\u4e2a\u53ef\u4ee5\u662f\u88ab\u586b\u5145\u7684\u53d8\u957f\u5e8f\u5217. h_n (num_layers * num_directions, batch, hidden_size): \u5305\u542b t=seq_len \u9690\u72b6\u6001\u7684 tensor . | Variables: | weight_ih_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 input-hidden \u6743\u91cd (W_ir|W_iz|W_in), shape \u4e3a <cite>(3*hidden_size x input_size)</cite> weight_hh_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 hidden-hidden \u6743\u91cd (W_hr|W_hz|W_hn), shape \u4e3a <cite>(3*hidden_size x hidden_size)</cite> bias_ih_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 input-hidden \u504f\u7f6e (b_ir|b_iz|b_in), shape \u4e3a <cite>(3*hidden_size)</cite> bias_hh_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 hidden-hidden \u504f\u7f6e (b_hr|b_hz|b_hn), shape \u4e3a <cite>(3*hidden_size)</cite> | | --- | --- | Examples: >>> rnn = nn.GRU(10, 20, 2) >>> input = Variable(torch.randn(5, 3, 10)) >>> h0 = Variable(torch.randn(2, 3, 20)) >>> output, hn = rnn(input, h0) RNNCell class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh') \u4e00\u4e2a Elan RNN cell , \u6fc0\u6d3b\u51fd\u6570\u662f tanh \u6216 ReLU , \u7528\u4e8e\u8f93\u5165\u5e8f\u5217. \\[h' = \\tanh(w_{ih} * x + b_{ih} + w_{hh} * h + b_{hh})\\] \u5982\u679c nonlinearity=\u2019relu\u2019, \u90a3\u4e48\u5c06\u4f1a\u4f7f\u7528 ReLU \u6765\u4ee3\u66ff tanh . | Parameters: | input_size \u2013 \u8f93\u5165\u7684\u7279\u5f81\u7ef4\u5ea6 hidden_size \u2013 \u9690\u72b6\u6001\u7684\u7279\u5f81\u7ef4\u5ea6 bias \u2013 \u5982\u679c\u4e3a False , \u90a3\u4e48RNN\u5c42\u5c06\u4e0d\u4f1a\u4f7f\u7528\u504f\u7f6e\u6743\u91cd b_ih \u548c b_hh. \u9ed8\u8ba4: True nonlinearity \u2013 \u7528\u4e8e\u9009\u62e9\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570 [\u2018tanh\u2019|\u2019relu\u2019]. \u9ed8\u8ba4: \u2018tanh\u2019 | | --- | --- | Inputs: input, hidden input (batch, input_size): \u5305\u542b\u8f93\u5165\u7279\u5f81\u7684 tensor . hidden (batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u9690\u72b6\u6001\u7684 tensor . Outputs: h\u2019 h\u2019 (batch, hidden_size): \u4fdd\u5b58\u7740 batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u4e0b\u4e00\u5c42\u9690\u72b6\u6001\u7684 tensor . | Variables: | weight_ih \u2013 input-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, shape \u4e3a <cite>(input_size x hidden_size)</cite> weight_hh \u2013 hidden-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, shape \u4e3a <cite>(hidden_size x hidden_size)</cite> bias_ih \u2013 input-hidden \u504f\u7f6e,\u53ef\u5b66\u4e60, shape \u4e3a <cite>(hidden_size)</cite> bias_hh \u2013 hidden-hidden \u504f\u7f6e,\u53ef\u5b66\u4e60, shape \u4e3a <cite>(hidden_size)</cite> | | --- | --- | Examples: >>> rnn = nn.RNNCell(10, 20) >>> input = Variable(torch.randn(6, 3, 10)) >>> hx = Variable(torch.randn(3, 20)) >>> output = [] >>> for i in range(6): ... hx = rnn(input[i], hx) ... output.append(hx) LSTMCell class torch.nn.LSTMCell(input_size, hidden_size, bias=True) LSTM \u7ec6\u80de. \\[\\begin{split}\\begin{array}{ll} i = \\mathrm{sigmoid}(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\ f = \\mathrm{sigmoid}(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\ g = \\tanh(W_{ig} x + b_{ig} + W_{hc} h + b_{hg}) \\\\ o = \\mathrm{sigmoid}(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\ c' = f * c + i * g \\\\ h' = o * \\tanh(c') \\\\ \\end{array}\\end{split}\\] | Parameters: | input_size \u2013 \u8f93\u5165\u7684\u7279\u5f81\u7ef4\u5ea6 hidden_size \u2013 \u9690\u72b6\u6001\u7684\u7ef4\u5ea6 bias \u2013 \u5982\u679c\u4e3a <cite>False</cite>, \u90a3\u4e48RNN\u5c42\u5c06\u4e0d\u4f1a\u4f7f\u7528\u504f\u7f6e\u6743\u91cd b_ih \u548c b_hh \u9ed8\u8ba4: True | | --- | --- | Inputs: input, (h_0, c_0) input (batch, input_size): \u5305\u542b\u8f93\u5165\u7279\u5f81\u7684 tensor . h_0 (batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u9690\u72b6\u6001\u7684 tensor . c_0 (batch. hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u7ec6\u80de\u72b6\u6001\u7684 tensor Outputs: h_1, c_1 h_1 (batch, hidden_size): \u4fdd\u5b58\u7740 batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u4e0b\u4e00\u5c42\u9690\u72b6\u6001\u7684 tensor c_1 (batch, hidden_size): \u4fdd\u5b58\u7740 batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u4e0b\u4e00\u7ec6\u80de\u72b6\u6001\u7684 tensor | Variables: | weight_ih \u2013 input-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, \u5f62\u72b6\u4e3a <cite>(4*hidden_size x input_size)</cite> weight_hh \u2013 hidden-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, \u5f62\u72b6\u4e3a <cite>(4*hidden_size x hidden_size)</cite> bias_ih \u2013 input-hidden \u504f\u7f6e, \u53ef\u5b66\u4e60, \u5f62\u72b6\u4e3a <cite>(4*hidden_size)</cite> bias_hh \u2013 hidden-hidden \u504f\u7f6e, \u53ef\u5b66\u4e60, \u5f62\u72b6\u4e3a <cite>(4*hidden_size)</cite> | | --- | --- | Examples: >>> rnn = nn.LSTMCell(10, 20) >>> input = Variable(torch.randn(6, 3, 10)) >>> hx = Variable(torch.randn(3, 20)) >>> cx = Variable(torch.randn(3, 20)) >>> output = [] >>> for i in range(6): ... hx, cx = rnn(input[i], (hx, cx)) ... output.append(hx) GRUCell class torch.nn.GRUCell(input_size, hidden_size, bias=True) GRU \u7ec6\u80de \\[\\begin{split}\\begin{array}{ll} r = \\mathrm{sigmoid}(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\ z = \\mathrm{sigmoid}(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\ n = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\ h' = (1 - z) * n + z * h \\end{array}\\end{split}\\] | Parameters: | input_size \u2013 \u8f93\u5165\u7684\u7279\u5f81\u7ef4\u5ea6 hidden_size \u2013 \u9690\u72b6\u6001\u7684\u7ef4\u5ea6 bias \u2013 \u5982\u679c\u4e3a <cite>False</cite>, \u90a3\u4e48RNN\u5c42\u5c06\u4e0d\u4f1a\u4f7f\u7528\u504f\u7f6e\u6743\u91cd b_ih \u548c b_hh \u9ed8\u8ba4: True | | --- | --- | Inputs: input, hidden input (batch, input_size): \u5305\u542b\u8f93\u5165\u7279\u5f81\u7684 tensor . hidden (batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u9690\u72b6\u6001\u7684 tensor . Outputs: h\u2019 h\u2019 : (batch, hidden_size): \u4fdd\u5b58\u7740 batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u4e0b\u4e00\u5c42\u9690\u72b6\u6001\u7684 tensor | Variables: | weight_ih \u2013 input-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, shape \u4e3a, <cite>(3*hidden_size x input_size)</cite> weight_hh \u2013 hidden-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, shape \u4e3a <cite>(3*hidden_size x hidden_size)</cite> bias_ih \u2013 input-hidden \u504f\u7f6e, \u53ef\u5b66\u4e60, shape \u4e3a <cite>(3*hidden_size)</cite> bias_hh \u2013 hidden-hidden \u504f\u7f6e, \u53ef\u5b66\u4e60, shape \u4e3a <cite>(3*hidden_size)</cite> | | --- | --- | Examples: >>> rnn = nn.GRUCell(10, 20) >>> input = Variable(torch.randn(6, 3, 10)) >>> hx = Variable(torch.randn(3, 20)) >>> output = [] >>> for i in range(6): ... hx = rnn(input[i], hx) ... output.append(hx) Linear layers (\u7ebf\u6027\u5c42) Linear class torch.nn.Linear(in_features, out_features, bias=True) \u5bf9\u8f93\u5165\u6570\u636e\u8fdb\u884c\u7ebf\u6027\u53d8\u6362: \\(y = Ax + b\\) | Parameters: | in_features \u2013 \u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u7684\u5927\u5c0f out_features \u2013 \u6bcf\u4e2a\u8f93\u51fa\u6837\u672c\u7684\u5927\u5c0f bias \u2013 \u82e5\u8bbe\u7f6e\u4e3a False, \u8fd9\u5c42\u4e0d\u4f1a\u5b66\u4e60\u504f\u7f6e. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: Input: \\((N, *, in\\_features)\\) \u8fd9\u91cc <cite>*</cite> \u610f\u5473\u7740\u53ef\u4ee5\u6dfb\u52a0\u4efb\u610f\u6570\u91cf\u7684\u5176\u4ed6\u7ef4\u5ea6 Output: \\((N, *, out\\_features)\\) \u9664\u4e86\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u5916, \u5176\u4f59\u7684\u90fd\u4e0e\u8f93\u5165\u76f8\u540c | Variables: | weight \u2013 \u5f62\u72b6\u4e3a (out_features x in_features) \u7684\u6a21\u5757\u4e2d\u53ef\u5b66\u4e60\u7684\u6743\u503c bias \u2013 \u5f62\u72b6\u4e3a (out_features) \u7684\u6a21\u5757\u4e2d\u53ef\u5b66\u4e60\u7684\u504f\u7f6e | | --- | --- | Examples: >>> m = nn.Linear(20, 30) >>> input = autograd.Variable(torch.randn(128, 20)) >>> output = m(input) >>> print(output.size()) Bilinear class torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True) \u5bf9\u8f93\u5165\u6570\u636e\u8fdb\u884c\u53cc\u7ebf\u6027\u53d8\u6362: \\(y = x_1 * A * x_2 + b\\) | Parameters: | in1_features \u2013 \u8f93\u5165\u4e00\u7684\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u7684\u5927\u5c0f in2_features \u2013 \u8f93\u5165\u4e8c\u7684\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u7684\u5927\u5c0f out_features \u2013 \u6bcf\u4e2a\u8f93\u51fa\u6837\u672c\u7684\u5927\u5c0f bias \u2013 \u82e5\u8bbe\u7f6e\u4e3aFalse, \u8fd9\u5c42\u4e0d\u4f1a\u5b66\u4e60\u504f\u7f6e. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: Input: \\((N, in1\\_features)\\) , \\((N, in2\\_features)\\) Output: \\((N, out\\_features)\\) | Variables: | weight \u2013 \u5f62\u72b6\u4e3a (out_features x in1_features x in2_features) \u7684\u6a21\u5757\u4e2d\u53ef\u5b66\u4e60\u7684\u6743\u503c bias \u2013 \u5f62\u72b6\u4e3a (out_features) \u7684\u6a21\u5757\u4e2d\u53ef\u5b66\u4e60\u7684\u504f\u7f6e | | --- | --- | Examples: >>> m = nn.Bilinear(20, 30, 40) >>> input1 = autograd.Variable(torch.randn(128, 20)) >>> input2 = autograd.Variable(torch.randn(128, 30)) >>> output = m(input1, input2) >>> print(output.size()) Dropout layers Dropout class torch.nn.Dropout(p=0.5, inplace=False) Dropout \u5728\u8bad\u7ec3\u671f\u95f4, \u6309\u7167\u4f2f\u52aa\u5229\u6982\u7387\u5206\u5e03, \u4ee5\u6982\u7387 p \u968f\u673a\u5730\u5c06\u8f93\u5165\u5f20\u91cf\u4e2d\u7684\u90e8\u5206\u5143\u7d20 \u7f6e\u4e3a 0, \u5728\u6bcf\u6b21\u8c03\u7528\u65f6, \u88ab\u7f6e\u4e3a 0 \u7684\u5143\u7d20\u662f\u968f\u673a\u7684. Dropout \u5df2\u88ab\u8bc1\u660e\u662f\u6b63\u5219\u5316\u7684\u4e00\u4e2a\u884c\u4e4b\u6709\u6548\u7684\u6280\u672f, \u5e76\u4e14\u5728\u9632\u6b62\u795e\u7ecf\u5143\u4e4b\u95f4\u4e92\u9002\u5e94\u95ee\u9898\u4e0a \u4e5f\u5353\u6709\u6210\u6548.\uff08\u795e\u7ecf\u5143\u4e92\u9002\u5e94\u95ee\u9898\u8be6\u89c1\u8bba\u6587 Improving neural networks by preventing co-adaptation of feature detectors \uff09 \u5e76\u4e14, Dropout \u7684\u8f93\u51fa\u5747\u4e0e 1/(1-p) \u7684\u6bd4\u4f8b\u7cfb\u6570\u8fdb\u884c\u4e86\u76f8\u4e58, \u4fdd\u8bc1\u4e86\u6c42\u503c\u65f6\u51fd\u6570\u662f\u5f52\u4e00\u5316\u7684. Args: p: \u5143\u7d20\u88ab\u7f6e\u4e3a0\u7684\u6982\u7387, \u9ed8\u8ba4\u503c: 0.5 inplace: \u5982\u679c\u4e3a True, \u7f6e0\u64cd\u4f5c\u5c06\u76f4\u63a5\u53d1\u751f\u5728\u4f20\u5165\u7684\u5143\u7d20\u4e0a.\u9ed8\u8ba4\u503c: false Shape: Input: any.\u8f93\u5165\u6570\u636e\u53ef\u4ee5\u662f\u4efb\u4f55\u5927\u5c0f Output: Same.\u8f93\u51fa\u6570\u636e\u5927\u5c0f\u4e0e\u8f93\u5165\u76f8\u540c Examples: >>> m = nn.Dropout(p=0.2) >>> input = autograd.Variable(torch.randn(20, 16)) >>> output = m(input) Dropout2d class torch.nn.Dropout2d(p=0.5, inplace=False) Dropout2d \u5c06\u8f93\u5165\u5f20\u91cf\u7684\u6240\u6709\u901a\u9053\u968f\u673a\u5730\u7f6e\u4e3a 0.\u88ab\u7f6e\u4e3a 0 \u7684\u901a\u9053\u5728\u6bcf\u6b21\u8c03\u7528\u65f6\u662f\u968f\u673a\u7684. \u901a\u5e38\u8f93\u5165\u6570\u636e\u6765\u81ea Conv2d \u6a21\u5757. \u5728\u8bba\u6587 Efficient Object Localization Using Convolutional Networks \u4e2d\u6709\u5982\u4e0b \u63cf\u8ff0: \u5982\u679c\u7279\u5f81\u6620\u5c04\u4e2d\u7684\u90bb\u63a5\u50cf\u7d20\u662f\u5f3a\u76f8\u5173\u7684\uff08\u5728\u65e9\u671f\u7684\u5377\u79ef\u5c42\u4e2d\u5f88\u5e38\u89c1\uff09, \u90a3\u4e48\u72ec\u7acb\u540c\u5206\u5e03 \u7684 dropout \u5c06\u4e0d\u4f1a\u6b63\u5219\u5316\u6fc0\u6d3b\u51fd\u6570, \u76f8\u53cd\u5176\u4f1a\u5bfc\u81f4\u6709\u6548\u7684\u5b66\u4e60\u7387\u7684\u4e0b\u964d. \u5728\u8fd9\u6837\u7684\u60c5\u51b5\u4e0b, \u5e94\u8be5\u4f7f\u7528\u51fd\u6570\u51fd\u6570 nn.Dropout2d , \u5b83\u80fd\u591f\u63d0\u5347\u7279\u5f81\u6620\u5c04\u4e4b\u95f4\u7684\u72ec\u7acb\u6027. Args: p (float,optional): \u5143\u7d20\u88ab\u7f6e0\u7684\u6982\u7387 inplace\uff08bool, optional\uff09: \u5982\u679c\u88ab\u8bbe\u4e3a\u2019True\u2019, \u7f6e0\u64cd\u4f5c\u5c06\u76f4\u63a5\u4f5c\u7528\u5728\u8f93\u5165\u5143\u7d20\u4e0a Shape: Input: math:(N, C, H, W) Output: math:(N, C, H, W) \uff08\u4e0e\u8f93\u5165\u76f8\u540c\uff09 Examples: ```py >>> m = nn.Dropout2d(p=0.2) ``` ```py >>> input = autograd.Variable(torch.randn(20, 16, 32, 32)) >>> output = m(input) ``` Dropout3d class torch.nn.Dropout3d(p=0.5, inplace=False) Dropout3d \u5c06\u8f93\u5165\u5f20\u91cf\u7684\u6240\u6709\u901a\u9053\u968f\u673a\u5730\u7f6e\u4e3a 0.\u88ab\u7f6e\u4e3a 0 \u7684\u901a\u9053\u5728\u6bcf\u6b21\u8c03\u7528\u65f6\u662f\u968f\u673a\u7684. \u901a\u5e38\u8f93\u5165\u6570\u636e\u6765\u81ea Conv3d \u6a21\u5757. \u5728\u8bba\u6587 Efficient Object Localization Using Convolutional Networks \u4e2d\u6709\u5982\u4e0b \u63cf\u8ff0: \u5982\u679c\u7279\u5f81\u6620\u5c04\u4e2d\u7684\u90bb\u63a5\u50cf\u7d20\u662f\u5f3a\u76f8\u5173\u7684\uff08\u5728\u65e9\u671f\u7684\u5377\u79ef\u5c42\u4e2d\u5f88\u5e38\u89c1\uff09, \u90a3\u4e48\u72ec\u7acb\u540c\u5206\u5e03 \u7684 dropout \u5c06\u4e0d\u4f1a\u6b63\u5219\u5316\u6fc0\u6d3b\u51fd\u6570, \u76f8\u53cd\u5176\u4f1a\u5bfc\u81f4\u6709\u6548\u7684\u5b66\u4e60\u7387\u7684\u4e0b\u964d. \u5728\u8fd9\u6837\u7684\u60c5\u51b5\u4e0b, \u5e94\u8be5\u4f7f\u7528\u51fd\u6570\u51fd\u6570 nn.Dropout3d , \u5b83\u80fd\u591f\u4fc3\u8fdb\u7279\u5f81\u6620\u5c04\u4e4b\u95f4\u7684\u72ec\u7acb\u6027. Args: p (float,optional): \u5143\u7d20\u88ab\u7f6e0\u7684\u6982\u7387 inplace\uff08bool, optional\uff09: \u5982\u679c\u88ab\u8bbe\u4e3a True , \u7f6e0\u64cd\u4f5c\u5c06\u76f4\u63a5\u4f5c\u7528\u5728\u8f93\u5165\u5143\u7d20\u4e0a Shape: Input: math:(N, C, H, W) Output: math:(N, C, H, W) \uff08\u4e0e\u8f93\u5165\u76f8\u540c\uff09 Examples: ```py >>> m = nn.Dropout3d(p=0.2) ``` ```py >>> input = autograd.Variable(torch.randn(20, 16, 4, 32, 32)) >>> output = m(input) ``` AlphaDropout class torch.nn.AlphaDropout(p=0.5) \u5728\u8f93\u5165\u4e0a\u5e94\u7528 Alpha Dropout. Alpha Dropout \u662f\u4e00\u79cd\u7ef4\u6301\u81ea\u6b63\u4ea4\u6027\u8d28\u7684 Dropout . \u5bf9\u4e8e\u4e00\u4e2a\u5747\u503c\u4e3a 0 \u548c\u6807\u51c6\u5dee\u4e3a 1 \u7684\u8f93\u5165 \u6765\u8bf4, Alpha Dropout \u80fd\u4fdd\u6301\u539f\u59cb\u6570\u636e\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee.Alpha Dropout \u548c SELU \u6fc0\u6d3b\u51fd\u6570 \u643a\u624b\u540c\u884c, \u540e\u8005\u4e5f\u4fdd\u8bc1\u4e86\u8f93\u51fa\u62e5\u6709\u4e0e\u8f93\u5165\u76f8\u540c\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee. Alpha Dropout \u5728\u8bad\u7ec3\u671f\u95f4, \u6309\u7167\u4f2f\u52aa\u5229\u6982\u7387\u5206\u5e03, \u4ee5\u6982\u7387 p \u968f\u673a\u5730\u5c06\u8f93\u5165\u5f20\u91cf\u4e2d\u7684\u90e8\u5206\u5143\u7d20 \u7f6e\u8fdb\u884c\u63a9\u76d6, \u5728\u6bcf\u6b21\u8c03\u7528\u4e2d, \u88ab\u63a9\u76d6\u7684\u5143\u7d20\u662f\u968f\u673a\u7684, \u5e76\u4e14\u5bf9\u8f93\u51fa\u4f1a\u8fdb\u884c\u7f29\u653e\u3001\u53d8\u6362\u7b49\u64cd\u4f5c \u4ee5\u4fdd\u6301\u5747\u503c\u4e3a 0\u3001\u6807\u51c6\u5dee\u4e3a 1. \u5728\u6c42\u503c\u671f\u95f4, \u6a21\u5757\u7b80\u5355\u7684\u8ba1\u7b97\u4e00\u4e2a\u5f52\u4e00\u5316\u7684\u51fd\u6570. \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u8003\u8bba\u6587: Self-Normalizing Neural Networks Args: p\uff08float\uff09: \u5143\u7d20\u88ab\u63a9\u76d6\u7684\u6982\u7387, \u9ed8\u8ba4\u503c: 0.5 Shape: Input: any.\u8f93\u5165\u6570\u636e\u53ef\u4ee5\u662f\u4efb\u4f55\u5927\u5c0f Output: Same.\u8f93\u51fa\u6570\u636e\u5927\u5c0f\u4e0e\u8f93\u5165\u76f8\u540c Examples: ```py >>> m = nn.AlphaDropout(p=0.2) ``` ```py >>> input = autograd.Variable(torch.randn(20, 16)) >>> output = m(input) ``` Sparse layers (\u7a00\u758f\u5c42) Embedding class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False) \u4e00\u4e2a\u7b80\u5355\u7684\u67e5\u627e\u8868, \u5b58\u50a8\u4e86\u56fa\u5b9a\u5b57\u5178\u548c\u5927\u5c0f\u7684 embedding. \u8fd9\u4e2a\u6a21\u5757\u7ecf\u5e38\u7528\u6765\u5b58\u50a8 word embeddings, \u5e76\u901a\u8fc7\u7d22\u5f15\u6765\u68c0\u7d22, \u6a21\u5757\u7684\u8f93\u5165\u662f\u7d22\u5f15\u6784\u6210\u7684\u5217\u8868, \u8f93\u51fa\u662f\u5bf9\u5e94\u7684 word embeddings. | Parameters: | num_embeddings ( int ) \u2013 embeddings \u5b57\u5178\u7684\u5927\u5c0f embedding_dim ( int ) \u2013 \u6bcf\u4e2a embedding \u5411\u91cf\u7684\u5927\u5c0f padding_idx ( int , optional ) \u2013 \u5982\u679c\u7ed9\u51fa, \u5728\u7d22\u5f15\u5904, \u8f93\u51fa\u8865\u96f6 max_norm ( float , optional ) \u2013 \u5982\u679c\u7ed9\u51fa, \u91cd\u65b0\u5f52\u4e00\u5316 embeddings, \u4f7f\u5176\u8303\u6570\u5c0f\u4e8e\u8be5\u503c norm_type ( float , optional ) \u2013 \u4e3a max_norm \u9009\u9879\u8ba1\u7b97 p \u8303\u6570\u65f6 P scale_grad_by_freq ( boolean__, optional ) \u2013 \u5982\u679c\u7ed9\u51fa, \u4f1a\u6839\u636e words \u5728 mini-batch \u4e2d\u7684\u9891\u7387\u7f29\u653e\u68af\u5ea6 sparse ( boolean__, optional ) \u2013 \u5982\u679c\u4e3a True , \u5173\u4e8e\u6743\u91cd\u77e9\u9635\u7684\u68af\u5ea6\u662f\u4e00\u4e2a\u7a00\u758f\u5f20\u91cf, \u8be6\u60c5\u8bf7\u53c2\u8003\u7a00\u758f\u68af\u5ea6 | | --- | --- | | Variables: | weight ( Tensor ) \u2013 shape \u4e3a (num_embeddings, embedding_dim) \u7684\u6a21\u5757\u7684\u53ef\u5b66\u4e60\u6743\u91cd | | --- | --- | Shape: Input: LongTensor <cite>(N, W)</cite>, N = mini-batch, W = \u6bcf\u4e2a mini-batch \u4e2d\u7528\u6765\u63d0\u53d6\u7684\u7d22\u5f15\u6570 Output: <cite>(N, W, embedding_dim)</cite> Notes \u8bf7\u6ce8\u610f, \u53ea\u652f\u6301\u6709\u9650\u6570\u91cf\u7684\u4f18\u5316\u5668. \u7a00\u758f\u68af\u5ea6: \u5f53\u524d\u662f (<cite>cuda</cite> \u548c <cite>cpu</cite>) \u7248\u672c\u7684 <cite>optim.SGD</cite>, \u548c (<cite>cpu</cite>) \u7248\u672c\u7684 <cite>optim.Adagrad</cite>. Examples: >>> # an Embedding module containing 10 tensors of size 3 >>> embedding = nn.Embedding(10, 3) >>> # a batch of 2 samples of 4 indices each >>> input = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]])) >>> embedding(input) Variable containing: (0 ,.,.) = -1.0822 1.2522 0.2434 0.8393 -0.6062 -0.3348 0.6597 0.0350 0.0837 0.5521 0.9447 0.0498 (1 ,.,.) = 0.6597 0.0350 0.0837 -0.1527 0.0877 0.4260 0.8393 -0.6062 -0.3348 -0.8738 -0.9054 0.4281 [torch.FloatTensor of size 2x4x3] >>> # example with padding_idx >>> embedding = nn.Embedding(10, 3, padding_idx=0) >>> input = Variable(torch.LongTensor([[0,2,0,5]])) >>> embedding(input) Variable containing: (0 ,.,.) = 0.0000 0.0000 0.0000 0.3452 0.4937 -0.9361 0.0000 0.0000 0.0000 0.0706 -2.1962 -0.6276 [torch.FloatTensor of size 1x4x3] EmbeddingBag class torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean') \u8ba1\u7b97\u4e00 \u4e2a\u2019bags\u2019 \u91cc\u7684 embedding s\u7684\u5747\u503c\u6216\u548c, \u4e0d\u7528\u5b9e\u4f8b\u5316\u4e2d\u95f4\u7684 embeddings \u5bf9\u4e8e\u56fa\u5b9a\u957f\u5ea6\u7684 bags nn.EmbeddingBag \u548c <cite>mode=sum</cite> \u76f8\u5f53\u4e8e nn.Embedding \u4e0e\u4e4b\u540e\u7684 <cite>torch.sum(dim=1)</cite> \u5176\u4e0e <cite>mode=mean</cite> \u76f8\u5f53\u4e8e nn.Embedding \u4e0e\u4e4b\u540e\u7684 <cite>torch.mean(dim=1)</cite> \u7136\u800c, \u6bd4\u8d77\u4e00\u8fde\u4e32\u8fd9\u6837\u7684\u64cd\u4f5c, nn.EmbeddingBag \u5728\u65f6\u95f4\u548c\u5185\u5b58\u4e0a\u66f4\u52a0\u9ad8\u6548. | Parameters: | num_embeddings ( int ) \u2013 embeddings \u5b57\u5178\u7684\u5927\u5c0f embedding_dim ( int ) \u2013 \u6bcf\u4e2a embedding \u5411\u91cf\u7684\u5927\u5c0f max_norm ( float , optional ) \u2013 \u5982\u679c\u7ed9\u51fa, \u91cd\u65b0\u5f52\u4e00\u5316 embeddings, \u4f7f\u5176\u8303\u6570\u5c0f\u4e8e\u8be5\u503c norm_type ( float , optional ) \u2013 \u4e3a max_norm \u9009\u9879\u8ba1\u7b97 p \u8303\u6570\u65f6\u7684 P scale_grad_by_freq ( boolean__, optional ) \u2013 \u5982\u679c\u7ed9\u51fa, \u4f1a\u6839\u636e words \u5728 mini-batch \u4e2d\u7684\u9891\u7387\u7f29\u653e\u68af\u5ea6 mode ( string , optional ) \u2013 \u2018sum\u2019 | \u2018mean\u2019. \u6307\u5b9a\u51cf\u5c11 bag \u7684\u65b9\u5f0f. \u9ed8\u8ba4: \u2018mean\u2019 | | --- | --- | | Variables: | weight ( Tensor ) \u2013 shape \u4e3a (num_embeddings, embedding_dim) \u7684\u6a21\u5757\u7684\u53ef\u5b66\u4e60\u6743\u91cd | | --- | --- | Inputs: input, offsets input (N or BxN): LongTensor, \u5305\u62ec\u8981\u63d0\u53d6\u7684 embeddings \u7684\u7d22\u5f15, \u5f53 <cite>input</cite> \u662f\u5f62\u72b6\u4e3a <cite>N</cite> \u7684 1D \u5f20\u91cf\u65f6, \u4e00\u4e2a\u7ed9\u51fa\u7684 <cite>offsets</cite> \u5f20\u91cf\u4e2d\u5305\u62ec: mini-batch \u4e2d\u6bcf\u4e2a\u65b0\u5e8f\u5217\u7684\u8d77\u59cb\u4f4d\u7f6e offsets (B or None): LongTensor, \u5305\u62ec\u4e00\u4e2a mini-batch \u7684\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u65b0\u6837\u672c\u7684\u8d77\u59cb\u4f4d\u7f6e \u5982\u679c <cite>input</cite> \u662f 2D (BxN) \u7684, offset \u5c31\u4e0d\u7528\u518d\u7ed9\u51fa; \u5982\u679c <cite>input</cite> \u662f\u4e00\u4e2a mini-batch \u7684\u56fa\u5b9a\u957f\u5ea6\u7684\u5e8f\u5217, \u6bcf\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6\u4e3a <cite>N</cite> Shape: Input: LongTensor <cite>N</cite>, N = \u8981\u63d0\u53d6\u7684 embeddings \u7684\u6570\u91cf, \u6216\u8005\u662f LongTensor <cite>BxN</cite>, B = mini-batch \u4e2d\u5e8f\u5217\u7684\u6570\u91cf, N = \u6bcf\u4e2a\u5e8f\u5217\u4e2d embeddings \u7684\u6570\u91cf Offsets: LongTensor <cite>B</cite>, B = bags \u7684\u6570\u91cf, \u503c\u4e3a\u6bcf\u4e2a bag \u4e2d <cite>input</cite> \u7684 offset, i.e. \u662f\u957f\u5ea6\u7684\u7d2f\u52a0. Offsets \u4e0d\u4f1a\u7ed9\u51fa, \u5982\u679c Input\u662f 2D \u7684 BxN \u5f20\u91cf, \u8f93\u5165\u88ab\u8ba4\u4e3a\u662f\u56fa\u5b9a\u957f\u5ea6\u7684\u5e8f\u5217 Output: <cite>(B, embedding_dim)</cite> Examples: >>> # an Embedding module containing 10 tensors of size 3 >>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum') >>> # a batch of 2 samples of 4 indices each >>> input = Variable(torch.LongTensor([1,2,4,5,4,3,2,9])) >>> offsets = Variable(torch.LongTensor([0,4])) >>> embedding_sum(input, offsets) Variable containing: -0.7296 -4.6926 0.3295 -0.5186 -0.5631 -0.2792 [torch.FloatTensor of size 2x3] Distance functions (\u8ddd\u79bb\u51fd\u6570) CosineSimilarity class torch.nn.CosineSimilarity(dim=1, eps=1e-08) \u8fd4\u56de\u6cbf\u7740 dim \u65b9\u5411\u8ba1\u7b97\u7684 x1 \u4e0e x2 \u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6. \\[\\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}\\] | Parameters: | dim ( int , optional ) \u2013 \u8ba1\u7b97\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u7ef4\u5ea6. Default: 1 eps ( float , optional ) \u2013 \u5c0f\u7684\u503c\u4ee5\u907f\u514d\u88ab\u96f6\u9664. Default: 1e-8 | | --- | --- | Shape: Input1: \\((\\ast_1, D, \\ast_2)\\) , \u5176\u4e2d\u7684 D \u8868\u793a <cite>dim</cite> \u7684\u4f4d\u7f6e Input2: \\((\\ast_1, D, \\ast_2)\\) , \u4e0e Input1 \u4e00\u6837\u7684 shape Output: \\((\\ast_1, \\ast_2)\\) Examples: >>> input1 = autograd.Variable(torch.randn(100, 128)) >>> input2 = autograd.Variable(torch.randn(100, 128)) >>> cos = nn.CosineSimilarity(dim=1, eps=1e-6) >>> output = cos(input1, input2) >>> print(output) PairwiseDistance class torch.nn.PairwiseDistance(p=2, eps=1e-06) \u8ba1\u7b97\u5411\u91cf v1, v2 \u4e4b\u95f4\u7684 batchwise pairwise distance(\u5206\u6279\u6210\u5bf9\u8ddd\u79bb): \\[\\Vert x \\Vert _p := \\left( \\sum_{i=1}^n \\vert x_i \\vert ^ p \\right) ^ {1/p}\\] | Parameters: | p ( real ) \u2013 norm degree(\u89c4\u8303\u7a0b\u5ea6). Default: 2 eps ( float , optional ) \u2013 \u5c0f\u7684\u503c\u4ee5\u907f\u514d\u88ab\u96f6\u9664. Default: 1e-6 | | --- | --- | Shape: Input1: \\((N, D)\\) , \u5176\u4e2d\u7684 <cite>D = vector dimension(\u5411\u91cf\u7ef4\u5ea6)</cite> Input2: \\((N, D)\\) , \u4e0e Input1 \u7684 shape \u4e00\u6837 Output: \\((N, 1)\\) Examples: >>> pdist = nn.PairwiseDistance(p=2) >>> input1 = autograd.Variable(torch.randn(100, 128)) >>> input2 = autograd.Variable(torch.randn(100, 128)) >>> output = pdist(input1, input2) Loss functions (\u635f\u5931\u51fd\u6570) L1Loss class torch.nn.L1Loss(size_average=True, reduce=True) \u521b\u5efa\u4e00\u4e2a\u8861\u91cf\u8f93\u5165 <cite>x</cite> \u4e0e\u76ee\u6807 <cite>y</cite> \u4e4b\u95f4\u5dee\u7684\u7edd\u5bf9\u503c\u7684\u5e73\u5747\u503c\u7684\u6807\u51c6, \u8be5 \u51fd\u6570\u4f1a\u9010\u5143\u7d20\u5730\u6c42\u51fa <cite>x</cite> \u548c <cite>y</cite> \u4e4b\u95f4\u5dee\u7684\u7edd\u5bf9\u503c, \u6700\u540e\u8fd4\u56de\u7edd\u5bf9\u503c\u7684\u5e73\u5747\u503c. \\({loss}(x, y) = 1/n \\sum |x_i - y_i|\\) <cite>x</cite> \u548c <cite>y</cite> \u53ef\u4ee5\u662f\u4efb\u610f\u7ef4\u5ea6\u7684\u6570\u7ec4, \u4f46\u9700\u8981\u6709\u76f8\u540c\u6570\u91cf\u7684n\u4e2a\u5143\u7d20. \u6c42\u548c\u64cd\u4f5c\u4f1a\u5bf9n\u4e2a\u5143\u7d20\u6c42\u548c, \u6700\u540e\u9664\u4ee5 <cite>n</cite> . \u5728\u6784\u9020\u51fd\u6570\u7684\u53c2\u6570\u4e2d\u4f20\u5165 <cite>size_average=False</cite>, \u6700\u540e\u6c42\u51fa\u6765\u7684\u7edd\u5bf9\u503c\u5c06\u4e0d\u4f1a\u9664\u4ee5 <cite>n</cite>. | Parameters: | size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u5c06\u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u7d2f\u52a0, \u800c\u4e0d\u4f1a\u53d6\u5e73\u5747\u503c. \u5f53 reduce \u7684\u503c\u4e3a False \u65f6\u8be5\u5b57\u6bb5\u4f1a\u88ab\u5ffd\u7565. \u9ed8\u8ba4\u503c: True reduce ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u5e73\u5747\u503c\u6216\u8005 \u6c42\u548c. \u5f53 reduce \u662f False \u65f6, \u635f\u5931\u51fd\u6570\u4f1a\u5bf9\u6bcf\u4e2a batch \u5143\u7d20\u90fd\u8fd4\u56de\u4e00\u4e2a loss \u5e76\u5ffd \u7565 size_average \u5b57\u6bb5. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, *)\\) , <cite>*</cite> \u8868\u793a\u4efb\u610f\u6570\u91cf\u7684\u989d\u5916\u7ef4\u5ea6 \u76ee\u6807: \\((N, *)\\) , \u548c\u8f93\u5165\u7684shape\u76f8\u540c \u8f93\u51fa: \u6807\u91cf. \u5982\u679c reduce \u662f False , \u5219\u8f93\u51fa\u4e3a \\((N, *)\\) , shape\u4e0e\u8f93\u51fa\u76f8\u540c Examples: >>> loss = nn.L1Loss() >>> input = autograd.Variable(torch.randn(3, 5), requires_grad=True) >>> target = autograd.Variable(torch.randn(3, 5)) >>> output = loss(input, target) >>> output.backward() MSELoss class torch.nn.MSELoss(size_average=True, reduce=True) \u8f93\u5165 <cite>x</cite> \u548c \u76ee\u6807 <cite>y</cite> \u4e4b\u95f4\u7684\u5747\u65b9\u5dee \\({loss}(x, y) = 1/n \\sum |x_i - y_i|^2\\) <cite>x</cite> \u548c <cite>y</cite> \u53ef\u4ee5\u662f\u4efb\u610f\u7ef4\u5ea6\u7684\u6570\u7ec4, \u4f46\u9700\u8981\u6709\u76f8\u540c\u6570\u91cf\u7684n\u4e2a\u5143\u7d20. \u6c42\u548c\u64cd\u4f5c\u4f1a\u5bf9n\u4e2a\u5143\u7d20\u6c42\u548c, \u6700\u540e\u9664\u4ee5 <cite>n</cite>. \u5728\u6784\u9020\u51fd\u6570\u7684\u53c2\u6570\u4e2d\u4f20\u5165 <cite>size_average=False</cite> , \u6700\u540e\u6c42\u51fa\u6765\u7684\u7edd\u5bf9\u503c\u5c06\u4e0d\u4f1a\u9664\u4ee5 <cite>n</cite>. \u8981\u5f97\u5230\u6bcf\u4e2a batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684 loss, \u8bbe\u7f6e <cite>reduce</cite> \u4e3a False . \u8fd4\u56de\u7684 loss \u5c06\u4e0d\u4f1a \u53d6\u5e73\u5747\u503c, \u4e5f\u4e0d\u4f1a\u88ab <cite>size_average</cite> \u5f71\u54cd. | Parameters: | size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u4f1a\u5728\u6bcf \u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u548c. \u53ea\u6709\u5f53 reduce \u7684\u503c\u4e3a True \u624d\u4f1a\u751f\u6548. \u9ed8\u8ba4\u503c: True reduce ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u6839\u636e size_average \u7684\u503c\u5728\u6bcf \u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u5e73\u5747\u503c\u6216\u8005\u6c42\u548c. \u5f53 reduce \u662f False \u65f6, \u635f\u5931\u51fd\u6570\u4f1a\u5bf9\u6bcf \u4e2a batch \u5143\u7d20\u90fd\u8fd4\u56de\u4e00\u4e2a loss \u5e76\u5ffd\u7565 size_average\u5b57\u6bb5. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, *)\\) , \u5176\u4e2d <cite>*</cite> \u8868\u793a\u4efb\u610f\u6570\u91cf\u7684\u989d\u5916\u7ef4\u5ea6. \u76ee\u6807: \\((N, *)\\) , shape \u8ddf\u8f93\u5165\u76f8\u540c Examples: >>> loss = nn.MSELoss() >>> input = autograd.Variable(torch.randn(3, 5), requires_grad=True) >>> target = autograd.Variable(torch.randn(3, 5)) >>> output = loss(input, target) >>> output.backward() CrossEntropyLoss class torch.nn.CrossEntropyLoss(weight=None, size_average=True, ignore_index=-100, reduce=True) \u8be5\u7c7b\u628a <cite>LogSoftMax</cite> \u548c <cite>NLLLoss</cite> \u7ed3\u5408\u5230\u4e86\u4e00\u4e2a\u7c7b\u4e2d \u5f53\u8bad\u7ec3\u6709 <cite>C</cite> \u4e2a\u7c7b\u522b\u7684\u5206\u7c7b\u95ee\u9898\u65f6\u5f88\u6709\u6548. \u53ef\u9009\u53c2\u6570 <cite>weight</cite> \u5fc5\u987b\u662f\u4e00\u4e2a1\u7ef4 Tensor, \u6743\u91cd\u5c06\u88ab\u5206\u914d\u7ed9\u5404\u4e2a\u7c7b\u522b. \u5bf9\u4e8e\u4e0d\u5e73\u8861\u7684\u8bad\u7ec3\u96c6\u975e\u5e38\u6709\u6548. <cite>input</cite> \u542b\u6709\u6bcf\u4e2a\u7c7b\u522b\u7684\u5206\u6570 <cite>input</cite> \u5fc5\u987b\u662f\u4e00\u4e2a2\u7ef4\u7684\u5f62\u5982 <cite>(minibatch, C)</cite> \u7684 <cite>Tensor</cite>. <cite>target</cite> \u662f\u4e00\u4e2a\u7c7b\u522b\u7d22\u5f15 (0 to C-1), \u5bf9\u5e94\u4e8e <cite>minibatch</cite> \u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20 loss \u53ef\u4ee5\u63cf\u8ff0\u4e3a: loss(x, class) = -log(exp(x[class]) / (\\sum_j exp(x[j]))) = -x[class] + log(\\sum_j exp(x[j])) \u5f53 <cite>weight</cite> \u53c2\u6570\u5b58\u5728\u65f6: loss(x, class) = weight[class] * (-x[class] + log(\\sum_j exp(x[j]))) loss \u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u53d6\u5e73\u5747\u503c. | Parameters: | weight ( Tensor , optional ) \u2013 \u81ea\u5b9a\u4e49\u7684\u6bcf\u4e2a\u7c7b\u522b\u7684\u6743\u91cd. \u5fc5\u987b\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>C</cite> \u7684 Tensor size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u5c06\u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u7d2f\u52a0, \u800c\u4e0d\u4f1a\u53d6\u5e73\u5747\u503c. \u5f53 reduce \u7684\u503c\u4e3a False \u65f6\u8be5\u5b57\u6bb5\u4f1a\u88ab\u5ffd\u7565. ignore_index ( int , optional ) \u2013 \u8bbe\u7f6e\u4e00\u4e2a\u76ee\u6807\u503c, \u8be5\u76ee\u6807\u503c\u4f1a\u88ab\u5ffd\u7565, \u4ece\u800c\u4e0d\u4f1a\u5f71\u54cd\u5230 \u8f93\u5165\u7684\u68af\u5ea6. \u5f53 size_average \u5b57\u6bb5\u4e3a True \u65f6, loss \u5c06\u4f1a\u5728\u6ca1\u6709\u88ab\u5ffd\u7565\u7684\u5143\u7d20\u4e0a \u53d6\u5e73\u5747. reduce ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u6839\u636e size_average \u7684\u503c\u5728\u6bcf \u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u5e73\u5747\u503c\u6216\u8005\u6c42\u548c. \u5f53 reduce \u662f False \u65f6, \u635f\u5931\u51fd\u6570\u4f1a\u5bf9 \u6bcf\u4e2a batch \u5143\u7d20\u90fd\u8fd4\u56de\u4e00\u4e2a loss \u5e76\u5ffd\u7565 size_average \u5b57\u6bb5. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, C)\\) , \u5176\u4e2d <cite>C</cite> \u662f\u7c7b\u522b\u7684\u6570\u91cf \u76ee\u6807: \\((N)\\) , \u5176\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u6ee1\u8db3 <cite>0 <= targets[i] <= C-1</cite> \u8f93\u51fa: \u6807\u91cf. \u5982\u679c reduce \u662f False , \u5219\u8f93\u51fa\u4e3a \\((N)\\) . Examples: >>> loss = nn.CrossEntropyLoss() >>> input = autograd.Variable(torch.randn(3, 5), requires_grad=True) >>> target = autograd.Variable(torch.LongTensor(3).random_(5)) >>> output = loss(input, target) >>> output.backward() NLLLoss class torch.nn.NLLLoss(weight=None, size_average=True, ignore_index=-100, reduce=True) \u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931. \u7528\u4e8e\u8bad\u7ec3 <cite>C</cite> \u4e2a\u7c7b\u522b\u7684\u5206\u7c7b\u95ee\u9898. \u53ef\u9009\u53c2\u6570 <cite>weight</cite> \u662f \u4e00\u4e2a1\u7ef4\u7684 Tensor, \u7528\u6765\u8bbe\u7f6e\u6bcf\u4e2a\u7c7b\u522b\u7684\u6743\u91cd. \u5f53\u8bad\u7ec3\u96c6\u4e0d\u5e73\u8861\u65f6\u8be5\u53c2\u6570\u5341\u5206\u6709\u7528. \u7531\u524d\u5411\u4f20\u64ad\u5f97\u5230\u7684\u8f93\u5165\u5e94\u8be5\u542b\u6709\u6bcf\u4e2a\u7c7b\u522b\u7684\u5bf9\u6570\u6982\u7387: \u8f93\u5165\u5fc5\u987b\u662f\u5f62\u5982 <cite>(minibatch, C)</cite> \u7684 2\u7ef4 Tensor. \u5728\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u7684\u6700\u540e\u4e00\u5c42\u6dfb\u52a0 <cite>LogSoftmax</cite> \u5c42\u53ef\u4ee5\u5f97\u5230\u5bf9\u6570\u6982\u7387. \u5982\u679c\u4f60\u4e0d\u5e0c\u671b\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d \u52a0\u5165\u989d\u5916\u7684\u4e00\u5c42, \u4e5f\u53ef\u4ee5\u4f7f\u7528 <cite>CrossEntropyLoss</cite> \u51fd\u6570. \u8be5\u635f\u5931\u51fd\u6570\u9700\u8981\u7684\u76ee\u6807\u503c\u662f\u4e00\u4e2a\u7c7b\u522b\u7d22\u5f15 <cite>(0 \u5230 C-1, \u5176\u4e2d C \u662f\u7c7b\u522b\u6570\u91cf)</cite> \u8be5 loss \u53ef\u4ee5\u63cf\u8ff0\u4e3a: loss(x, class) = -x[class] \u6216\u8005\u5f53 weight \u53c2\u6570\u5b58\u5728\u65f6\u53ef\u4ee5\u63cf\u8ff0\u4e3a: loss(x, class) = -weight[class] * x[class] \u53c8\u6216\u8005\u5f53 ignore_index \u53c2\u6570\u5b58\u5728\u65f6\u53ef\u4ee5\u63cf\u8ff0\u4e3a: loss(x, class) = class != ignoreIndex ? -weight[class] * x[class] : 0 | Parameters: | weight ( Tensor , optional ) \u2013 \u81ea\u5b9a\u4e49\u7684\u6bcf\u4e2a\u7c7b\u522b\u7684\u6743\u91cd. \u5fc5\u987b\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>C</cite> \u7684 Tensor size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u5c06\u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u7d2f\u52a0, \u800c\u4e0d\u4f1a\u53d6\u5e73\u5747\u503c. \u5f53 reduce \u7684\u503c\u4e3a False \u65f6\u8be5\u5b57\u6bb5\u4f1a\u88ab\u5ffd\u7565. \u9ed8\u8ba4\u503c: True` ignore_index ( int , optional ) \u2013 \u8bbe\u7f6e\u4e00\u4e2a\u76ee\u6807\u503c, \u8be5\u76ee\u6807\u503c\u4f1a\u88ab\u5ffd\u7565, \u4ece\u800c\u4e0d\u4f1a\u5f71\u54cd\u5230 \u8f93\u5165\u7684\u68af\u5ea6. \u5f53 size_average \u4e3a True \u65f6, loss \u5c06\u4f1a\u5728\u6ca1\u6709\u88ab\u5ffd\u7565\u7684\u5143\u7d20\u4e0a \u53d6\u5e73\u5747\u503c. reduce ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u5e73\u5747\u503c\u6216\u8005 \u6c42\u548c. \u5f53 reduce \u662f False \u65f6, \u635f\u5931\u51fd\u6570\u4f1a\u5bf9\u6bcf\u4e2a batch \u5143\u7d20\u90fd\u8fd4\u56de\u4e00\u4e2a loss \u5e76\u5ffd \u7565 size_average \u5b57\u6bb5. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, C)\\) , \u5176\u4e2d <cite>C</cite> \u662f\u7c7b\u522b\u7684\u6570\u91cf \u76ee\u6807: \\((N)\\) , \u5176\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u6ee1\u8db3 <cite>0 <= targets[i] <= C-1</cite> \u8f93\u51fa: \u6807\u91cf. \u5982\u679c reduce \u662f False , \u5219\u8f93\u51fa\u4e3a \\((N)\\) . Examples: >>> m = nn.LogSoftmax() >>> loss = nn.NLLLoss() >>> # input is of size N x C = 3 x 5 >>> input = autograd.Variable(torch.randn(3, 5), requires_grad=True) >>> # each element in target has to have 0 <= value < C >>> target = autograd.Variable(torch.LongTensor([1, 0, 4])) >>> output = loss(m(input), target) >>> output.backward() PoissonNLLLoss class torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=True, eps=1e-08) \u76ee\u6807\u503c\u4e3a\u6cca\u677e\u5206\u5e03\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931. \u8be5\u635f\u5931\u53ef\u4ee5\u63cf\u8ff0\u4e3a: target ~ Pois(input) loss(input, target) = input - target * log(input) + log(target!) \u6700\u540e\u4e00\u9879\u53ef\u4ee5\u88ab\u7701\u7565\u6216\u8005\u7528 Stirling \u516c\u5f0f\u6765\u8fd1\u4f3c. \u8be5\u8fd1\u4f3c\u7528\u4e8e\u5927\u4e8e1\u7684\u76ee\u6807\u503c. \u5f53\u76ee\u6807\u503c \u5c0f\u4e8e\u6216\u7b49\u4e8e1\u65f6, \u5219\u5c060\u52a0\u5230 loss \u4e2d. | Parameters: | log_input ( bool , optional ) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a True , loss \u5c06\u4f1a\u6309\u7167\u516c \u5f0f <cite>exp(input) - target * input</cite> \u6765\u8ba1\u7b97, \u5982\u679c\u8bbe\u7f6e\u4e3a False , loss \u5c06\u4f1a\u6309\u7167 <cite>input - target * log(input+eps)</cite> \u8ba1\u7b97. full ( bool , optional ) \u2013 \u662f\u5426\u8ba1\u7b97\u5168\u90e8\u7684 loss, i. e. \u52a0\u4e0a Stirling \u8fd1\u4f3c\u9879 <cite>target * log(target) - target + 0.5 * log(2 * pi * target)</cite>. size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u5c06\u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u7d2f\u52a0, \u800c\u4e0d\u4f1a\u53d6\u5e73\u5747\u503c. eps ( float , optional ) \u2013 \u5f53 log_input== False \u65f6, \u53d6\u4e00\u4e2a\u5f88\u5c0f\u7684\u503c\u7528\u6765\u907f\u514d\u8ba1\u7b97 log(0). \u9ed8\u8ba4\u503c: 1e-8 | | --- | --- | Examples: >>> loss = nn.PoissonNLLLoss() >>> log_input = autograd.Variable(torch.randn(5, 2), requires_grad=True) >>> target = autograd.Variable(torch.randn(5, 2)) >>> output = loss(log_input, target) >>> output.backward() NLLLoss2d class torch.nn.NLLLoss2d(weight=None, size_average=True, ignore_index=-100, reduce=True) \u5bf9\u4e8e\u56fe\u7247\u8f93\u5165\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931. \u5b83\u8ba1\u7b97\u6bcf\u4e2a\u50cf\u7d20\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931. | Parameters: | weight ( Tensor , optional ) \u2013 \u81ea\u5b9a\u4e49\u7684\u6bcf\u4e2a\u7c7b\u522b\u7684\u6743\u91cd. \u5fc5\u987b\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>C</cite> \u7684 Tensor size_average \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u5c06\u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u7d2f\u52a0, \u800c\u4e0d\u4f1a\u53d6\u5e73\u5747\u503c. \u5f53 reduce \u7684\u503c\u4e3a False \u65f6\u8be5\u5b57\u6bb5\u4f1a\u88ab\u5ffd\u7565. \u9ed8\u8ba4\u503c: True reduce ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u5e73\u5747\u503c\u6216\u8005 \u6c42\u548c. \u5f53 reduce \u662f False \u65f6, \u635f\u5931\u51fd\u6570\u4f1a\u5bf9\u6bcf\u4e2a batch \u5143\u7d20\u90fd\u8fd4\u56de\u4e00\u4e2a loss \u5e76\u5ffd \u7565 size_average \u5b57\u6bb5. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: Input: \\((N, C, H, W)\\) where <cite>C = number of classes</cite> Target: \\((N, H, W)\\) where each value is <cite>0 <= targets[i] <= C-1</cite> Output: scalar. If reduce is False , then \\((N, H, W)\\) instead. Examples: >>> m = nn.Conv2d(16, 32, (3, 3)).float() >>> loss = nn.NLLLoss2d() >>> # input is of size N x C x height x width >>> input = autograd.Variable(torch.randn(3, 16, 10, 10)) >>> # each element in target has to have 0 <= value < C >>> target = autograd.Variable(torch.LongTensor(3, 8, 8).random_(0, 4)) >>> output = loss(m(input), target) >>> output.backward() KLDivLoss class torch.nn.KLDivLoss(size_average=True, reduce=True) Kullback-Leibler divergence \u635f\u5931 KL \u6563\u5ea6\u53ef\u7528\u4e8e\u8861\u91cf\u4e0d\u540c\u7684\u8fde\u7eed\u5206\u5e03\u4e4b\u95f4\u7684\u8ddd\u79bb, \u5728\u8fde\u7eed\u7684\u8f93\u51fa\u5206\u5e03\u7684\u7a7a\u95f4\u4e0a(\u79bb\u6563\u91c7\u6837)\u4e0a\u8fdb\u884c\u76f4\u63a5\u56de\u5f52\u65f6 \u5f88\u6709\u6548. \u8ddf <cite>NLLLoss</cite> \u4e00\u6837, <cite>input</cite> \u9700\u8981\u542b\u6709 \u5bf9\u6570\u6982\u7387 , \u4e0d\u540c\u4e8e <cite>ClassNLLLoss</cite>, <cite>input</cite> \u53ef \u4ee5\u4e0d\u662f2\u7ef4\u7684 Tensor, \u56e0\u4e3a\u8be5\u51fd\u6570\u4f1a\u9010\u5143\u7d20\u5730\u6c42\u503c. \u8be5\u65b9\u6cd5\u9700\u8981\u4e00\u4e2ashape\u8ddf <cite>input</cite> <cite>Tensor</cite> \u4e00\u6837\u7684 <cite>target</cite> <cite>Tensor</cite>. \u635f\u5931\u53ef\u4ee5\u63cf\u8ff0\u4e3a: \\[loss(x, target) = 1/n \\sum(target_i * (log(target_i) - x_i))\\] \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u548c \u7ef4\u5ea6 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 <cite>size_average</cite> \u8bbe\u7f6e\u4e3a False , \u5219 loss \u4e0d\u4f1a\u53d6\u5e73\u5747\u503c. | Parameters: | size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a \u548c \u7ef4\u5ea6 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u8bbe\u7f6e\u4e3a False , \u5219 loss \u4f1a\u7d2f\u52a0, \u800c\u4e0d\u662f\u53d6\u5e73\u5747\u503c. reduce ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u6839\u636e size_average \u5728\u6bcf \u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u5e73\u5747\u503c\u6216\u8005\u6c42\u548c. \u5f53 reduce \u662f False \u65f6, \u635f\u5931\u51fd\u6570\u4f1a\u5bf9\u6bcf \u4e2a batch \u5143\u7d20\u90fd\u8fd4\u56de\u4e00\u4e2a loss \u5e76\u5ffd\u7565 size_average \u5b57\u6bb5. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, *)\\) , \u5176\u4e2d <cite>*</cite> \u8868\u793a\u4efb\u610f\u6570\u91cf\u7684\u989d\u5916\u7ef4\u5ea6. \u76ee\u6807: \\((N, *)\\) , shape \u8ddf\u8f93\u5165\u76f8\u540c \u8f93\u51fa: \u6807\u91cf. \u5982\u679c <cite>reduce</cite> \u662f True , \u5219\u8f93\u51fa\u4e3a \\((N, *)\\) , shape \u8ddf\u8f93\u5165\u76f8\u540c. BCELoss class torch.nn.BCELoss(weight=None, size_average=True) \u8ba1\u7b97\u76ee\u6807\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u4e8c\u8fdb\u5236\u4ea4\u53c9\u71b5: \\[loss(o, t) = - 1/n \\sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))\\] \u5f53\u5b9a\u4e49\u4e86 weight \u53c2\u6570\u65f6: \\[loss(o, t) = - 1/n \\sum_i weight[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))\\] \u8fd9\u53ef\u7528\u4e8e\u6d4b\u91cf\u91cd\u6784\u7684\u8bef\u5dee, \u4f8b\u5982\u81ea\u52a8\u7f16\u7801\u673a. \u6ce8\u610f\u76ee\u6807\u7684\u503c <cite>t[i]</cite> \u7684\u8303\u56f4\u4e3a0\u52301\u4e4b\u95f4. | Parameters: | weight ( Tensor , optional ) \u2013 \u81ea\u5b9a\u4e49\u7684\u6bcf\u4e2a batch \u5143\u7d20\u7684 loss \u7684\u6743\u91cd. \u5fc5\u987b\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a \u201cnbatch\u201d \u7684 \u7684 Tensor size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u4f1a\u5728\u6bcf \u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u7d2f\u52a0, \u800c\u4e0d\u662f\u53d6\u5e73\u5747\u503c. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, *)\\) , \u5176\u4e2d <cite>*</cite> \u8868\u793a\u4efb\u610f\u6570\u91cf\u7684\u989d\u5916\u7ef4\u5ea6. \u76ee\u6807: \\((N, *)\\) , shape \u8ddf\u8f93\u5165\u76f8\u540c Examples: >>> m = nn.Sigmoid() >>> loss = nn.BCELoss() >>> input = autograd.Variable(torch.randn(3), requires_grad=True) >>> target = autograd.Variable(torch.FloatTensor(3).random_(2)) >>> output = loss(m(input), target) >>> output.backward() BCEWithLogitsLoss class torch.nn.BCEWithLogitsLoss(weight=None, size_average=True) \u8be5\u635f\u5931\u51fd\u6570\u628a <cite>Sigmoid</cite> \u5c42\u96c6\u6210\u5230\u4e86 <cite>BCELoss</cite> \u7c7b\u4e2d. \u8be5\u7248\u6bd4\u7528\u4e00\u4e2a\u7b80\u5355\u7684 <cite>Sigmoid</cite> \u5c42\u548c <cite>BCELoss</cite> \u5728\u6570\u503c\u4e0a\u66f4\u7a33\u5b9a, \u56e0\u4e3a\u628a\u8fd9\u4e24\u4e2a\u64cd\u4f5c\u5408\u5e76\u4e3a\u4e00\u4e2a\u5c42\u4e4b\u540e, \u53ef\u4ee5\u5229\u7528 log-sum-exp \u7684 \u6280\u5de7\u6765\u5b9e\u73b0\u6570\u503c\u7a33\u5b9a. \u76ee\u6807\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u4e8c\u503c\u4ea4\u53c9\u71b5(\u4e0d\u542bsigmoid\u51fd\u6570)\u662f: \\[loss(o, t) = - 1/n \\sum_i (t[i] * log(sigmoid(o[i])) + (1 - t[i]) * log(1 - sigmoid(o[i])))\\] \u5f53\u5b9a\u4e49\u4e86 weight \u53c2\u6570\u4e4b\u540e\u53ef\u63cf\u8ff0\u4e3a: \\[loss(o, t) = - 1/n \\sum_i weight[i] * (t[i] * log(sigmoid(o[i])) + (1 - t[i]) * log(1 - sigmoid(o[i])))\\] \u8fd9\u53ef\u7528\u4e8e\u6d4b\u91cf\u91cd\u6784\u7684\u8bef\u5dee, \u4f8b\u5982\u81ea\u52a8\u7f16\u7801\u673a. \u6ce8\u610f\u76ee\u6807\u7684\u503c <cite>t[i]</cite> \u7684\u8303\u56f4\u4e3a0\u52301\u4e4b\u95f4. | Parameters: | weight ( Tensor , optional ) \u2013 \u81ea\u5b9a\u4e49\u7684\u6bcf\u4e2a batch \u5143\u7d20\u7684 loss \u7684\u6743\u91cd. \u5fc5\u987b\u662f\u4e00\u4e2a\u957f\u5ea6 \u4e3a \u201cnbatch\u201d \u7684 Tensor size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u4f1a\u5728\u6bcf \u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u7d2f\u52a0, \u800c\u4e0d\u662f\u53d6\u5e73\u5747\u503c. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, *)\\) , \u5176\u4e2d <cite>*</cite> \u8868\u793a\u4efb\u610f\u6570\u91cf\u7684\u989d\u5916\u7ef4\u5ea6. \u76ee\u6807: \\((N, *)\\) , shape \u8ddf\u8f93\u5165\u76f8\u540c Examples: >>> loss = nn.BCEWithLogitsLoss() >>> input = autograd.Variable(torch.randn(3), requires_grad=True) >>> target = autograd.Variable(torch.FloatTensor(3).random_(2)) >>> output = loss(input, target) >>> output.backward() MarginRankingLoss class torch.nn.MarginRankingLoss(margin=0, size_average=True) \u521b\u5efa\u4e00\u4e2a\u8861\u91cf mini-batch(\u5c0f\u6279\u91cf) \u4e2d\u76842\u4e2a1\u7ef4 <cite>Tensor</cite> \u7684\u8f93\u5165 <cite>x1</cite> \u548c <cite>x2</cite>, \u548c1\u4e2a1\u7ef4 <cite>Tensor</cite> \u7684\u76ee\u6807 <cite>y ( y</cite> \u7684\u53d6\u503c\u662f <cite>1</cite> \u6216\u8005 <cite>-1</cite>) \u4e4b\u95f4\u635f\u5931\u7684\u6807\u51c6. \u5982\u679c <cite>y == 1</cite> \u5219\u8ba4\u4e3a\u7b2c\u4e00\u4e2a\u8f93\u5165\u503c\u5e94\u8be5\u6392\u5217\u5728\u7b2c\u4e8c\u4e2a\u8f93\u5165\u503c\u4e4b\u4e0a(\u5373\u503c\u66f4\u5927), <cite>y == -1</cite> \u65f6\u5219\u76f8\u53cd. \u5bf9\u4e8e mini-batch(\u5c0f\u6279\u91cf) \u4e2d\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u635f\u5931\u51fd\u6570\u5982\u4e0b: loss(x, y) = max(0, -y * (x1 - x2) + margin) \u5982\u679c\u5185\u90e8\u53d8\u91cf <cite>size_average = True</cite>, \u5219\u635f\u5931\u51fd\u6570\u8ba1\u7b97\u6279\u6b21\u4e2d\u6240\u6709\u5b9e\u4f8b\u7684\u635f\u5931\u503c\u7684\u5e73\u5747\u503c; \u5982\u679c <cite>size_average = False</cite>, \u5219\u635f\u5931\u51fd\u6570\u8ba1\u7b97\u6279\u6b21\u4e2d\u6240\u6709\u5b9e\u4f8b\u7684\u635f\u5931\u81f3\u7684\u5408\u8ba1. <cite>size_average</cite> \u9ed8\u8ba4\u503c\u4e3a True . HingeEmbeddingLoss class torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=True) \u8861\u91cf\u8f93\u5165 Tensor(\u5f20\u91cf) <cite>x</cite> \u548c \u76ee\u6807 Tensor(\u5f20\u91cf) <cite>y</cite> (\u53d6\u503c\u4e3a <cite>1</cite> \u548c <cite>-1</cite>) \u4e4b\u95f4\u7684\u635f\u5931\u503c. \u6b64\u65b9\u6cd5\u901a\u5e38\u7528\u6765\u8861\u91cf\u4e24\u4e2a\u8f93\u5165\u503c\u662f\u5426\u76f8\u4f3c, \u4f8b\u5982\u4f7f\u7528L1\u6210\u5bf9\u8ddd\u79bb\u4f5c\u4e3a <cite>x</cite>, \u5e76\u4e14\u901a\u5e38\u7528\u6765\u8fdb\u884c\u975e\u7ebf\u6027\u5d4c\u5165\u5b66\u4e60\u6216\u8005 \u534a\u76d1\u7763\u5b66\u4e60: { x_i, if y_i == 1 loss(x, y) = 1/n { { max(0, margin - x_i), if y_i == -1 <cite>x</cite> \u548c <cite>y</cite> \u5206\u522b\u53ef\u4ee5\u662f\u5177\u6709 <cite>n</cite> \u4e2a\u5143\u7d20\u7684\u4efb\u610f\u5f62\u72b6. \u5408\u8ba1\u64cd\u4f5c\u5bf9\u6240\u6709\u5143\u7d20\u8fdb\u884c\u8ba1\u7b97. \u5982\u679c <cite>size_average=False</cite>, \u5219\u8ba1\u7b97\u65f6\u4e0d\u4f1a\u9664\u4ee5 <cite>n</cite> \u53d6\u5e73\u5747\u503c. <cite>margin</cite> \u7684\u9ed8\u8ba4\u503c\u662f <cite>1</cite>, \u6216\u8005\u53ef\u4ee5\u901a\u8fc7\u6784\u9020\u51fd\u6570\u6765\u8bbe\u7f6e. MultiLabelMarginLoss class torch.nn.MultiLabelMarginLoss(size_average=True) \u521b\u5efa\u4e00\u4e2a\u6807\u51c6, \u7528\u4ee5\u4f18\u5316\u591a\u5143\u5206\u7c7b\u95ee\u9898\u7684\u5408\u9875\u635f\u5931\u51fd\u6570 (\u57fa\u4e8e\u7a7a\u767d\u7684\u635f\u5931), \u8ba1\u7b97\u635f\u5931\u503c\u65f6 \u9700\u89812\u4e2a\u53c2\u6570\u5206\u522b\u4e3a\u8f93\u5165, <cite>x</cite> (\u4e00\u4e2a2\u7ef4\u5c0f\u6279\u91cf <cite>Tensor</cite>) \u548c\u8f93\u51fa <cite>y</cite> (\u4e00\u4e2a2\u7ef4 <cite>Tensor</cite>, \u5176\u503c\u4e3a <cite>x</cite> \u7684\u7d22\u5f15\u503c). \u5bf9\u4e8emini-batch(\u5c0f\u6279\u91cf) \u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u6309\u5982\u4e0b\u516c\u5f0f\u8ba1\u7b97\u635f\u5931: loss(x, y) = sum_ij(max(0, 1 - (x[y[j]] - x[i]))) / x.size(0) \u5176\u4e2d <cite>i</cite> \u7684\u53d6\u503c\u8303\u56f4\u662f <cite>0</cite> \u5230 <cite>x.size(0)</cite>, <cite>j</cite> \u7684\u53d6\u503c\u8303\u56f4\u662f <cite>0</cite> \u5230 <cite>y.size(0)</cite>, <cite>y[j] >= 0</cite>, \u5e76\u4e14\u5bf9\u4e8e\u6240\u6709 <cite>i</cite> \u548c <cite>j</cite> \u6709 <cite>i != y[j]</cite>. <cite>y</cite> \u548c <cite>x</cite> \u5fc5\u987b\u6709\u76f8\u540c\u7684\u5143\u7d20\u6570\u91cf. \u6b64\u6807\u51c6\u4ec5\u8003\u8651 <cite>y[j]</cite> \u4e2d\u6700\u5148\u51fa\u73b0\u7684\u975e\u96f6\u503c. \u5982\u6b64\u53ef\u4ee5\u5141\u8bb8\u6bcf\u4e2a\u6837\u672c\u53ef\u4ee5\u6709\u6570\u91cf\u4e0d\u540c\u7684\u76ee\u6807\u7c7b\u522b. SmoothL1Loss class torch.nn.SmoothL1Loss(size_average=True, reduce=True) \u521b\u5efa\u4e00\u4e2a\u6807\u51c6, \u5f53\u67d0\u4e2a\u5143\u7d20\u7684\u9519\u8bef\u503c\u7684\u7edd\u5bf9\u503c\u5c0f\u4e8e1\u65f6\u4f7f\u7528\u5e73\u65b9\u9879\u8ba1\u7b97, \u5176\u4ed6\u60c5\u51b5\u5219\u4f7f\u7528L1\u8303\u5f0f\u8ba1\u7b97. \u6b64\u65b9\u6cd5\u521b\u5efa\u7684\u6807\u51c6\u5bf9\u4e8e\u5f02\u5e38\u503c\u4e0d\u5982 [ ](#id31)MSELoss \u654f\u611f, \u4f46\u662f\u540c\u65f6\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u9632\u6b62\u68af\u5ea6\u7206\u70b8 (\u6bd4\u5982 \u53c2\u89c1\u8bba\u6587 \u201cFast R-CNN\u201d \u4f5c\u8005 Ross Girshick). \u4e5f\u88ab\u79f0\u4e3a Huber \u635f\u5931\u51fd\u6570: { 0.5 * (x_i - y_i)^2, if |x_i - y_i| < 1 loss(x, y) = 1/n \\sum { { |x_i - y_i| - 0.5, otherwise <cite>x</cite> \u548c <cite>y</cite> \u53ef\u4ee5\u662f\u4efb\u610f\u5f62\u72b6\u53ea\u8981\u90fd\u5177\u5907\u603b\u8ba1 <cite>n</cite> \u4e2a\u5143\u7d20 \u5408\u8ba1\u4ecd\u7136\u9488\u5bf9\u6240\u6709\u5143\u7d20\u8fdb\u884c\u8ba1\u7b97, \u5e76\u4e14\u6700\u540e\u9664\u4ee5 <cite>n</cite>. \u5982\u679c\u628a\u5185\u90e8\u53d8\u91cf <cite>size_average</cite> \u8bbe\u7f6e\u4e3a False , \u5219\u4e0d\u4f1a\u88ab\u9664\u4ee5 <cite>n</cite>. | Parameters: | size_average ( bool , optional ) \u2013 \u635f\u5931\u503c\u9ed8\u8ba4\u4f1a\u6309\u7167\u6240\u6709\u5143\u7d20\u53d6\u5e73\u5747\u503c. \u4f46\u662f, \u5982\u679c size_average \u88ab \u8bbe\u7f6e\u4e3a False , \u5219\u635f\u5931\u503c\u4e3a\u6240\u6709\u5143\u7d20\u7684\u5408\u8ba1. \u5982\u679c reduce \u53c2\u6570\u8bbe\u4e3a False , \u5219\u5ffd\u7565\u6b64\u53c2\u6570\u7684\u503c. \u9ed8\u8ba4: True reduce ( bool , optional ) \u2013 \u635f\u5931\u503c\u9ed8\u8ba4\u4f1a\u6309\u7167\u6240\u6709\u5143\u7d20\u53d6\u5e73\u5747\u503c\u6216\u8005\u53d6\u5408\u8ba1\u503c. \u5f53 reduce \u8bbe\u7f6e\u4e3a False \u65f6, \u635f\u5931\u51fd\u6570\u5bf9\u4e8e\u6bcf\u4e2a\u5143\u7d20\u90fd\u8fd4\u56de\u635f\u5931\u503c\u5e76\u4e14\u5ffd\u7565 size_average \u53c2\u6570. \u9ed8\u8ba4: True | | --- | --- | Shape: \u8f93\u5165: \\((N, *)\\) <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u4e2a\u5176\u4ed6\u7ef4\u5ea6 \u76ee\u6807: \\((N, *)\\) , \u540c\u8f93\u5165 \u8f93\u51fa: \u6807\u91cf. \u5982\u679c reduce \u8bbe\u4e3a False \u5219\u4e3a \\((N, *)\\) , \u540c\u8f93\u5165 SoftMarginLoss class torch.nn.SoftMarginLoss(size_average=True) \u521b\u5efa\u4e00\u4e2a\u6807\u51c6, \u7528\u4ee5\u4f18\u5316\u4e24\u5206\u7c7b\u7684 logistic loss. \u8f93\u5165\u4e3a <cite>x</cite> (\u4e00\u4e2a2\u7ef4 mini-batch Tensor)\u548c \u76ee\u6807 <cite>y</cite> (\u4e00\u4e2a\u5305\u542b <cite>1</cite> \u6216\u8005 <cite>-1</cite> \u7684 Tensor). loss(x, y) = sum_i (log(1 + exp(-y[i]*x[i]))) / x.nelement() \u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e <cite>self.size_average</cite> \u4e3a False \u6765\u7981\u7528\u6309\u7167\u5143\u7d20\u6570\u91cf\u53d6\u5e73\u5747\u7684\u6b63\u5219\u5316\u64cd\u4f5c. MultiLabelSoftMarginLoss class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=True) \u521b\u5efa\u4e00\u4e2a\u6807\u51c6, \u57fa\u4e8e\u8f93\u5165 <cite>x</cite> \u548c\u76ee\u6807 <cite>y \u7684 max-entropy(\u6700\u5927\u71b5), \u4f18\u5316\u591a\u6807\u7b7e one-versus-all \u635f\u5931. \u8f93\u5165 x</cite> \u4e3a\u4e00\u4e2a2\u7ef4 mini-batch <cite>Tensor</cite>, \u76ee\u6807 <cite>y</cite> \u4e3a2\u8fdb\u52362\u7ef4 <cite>Tensor</cite>. \u5bf9\u6bcf\u4e2a mini-batch \u4e2d\u7684\u6837\u672c, \u5bf9\u5e94\u7684 loss \u4e3a: loss(x, y) = - sum_i (y[i] * log( 1 / (1 + exp(-x[i])) ) + ( (1-y[i]) * log(exp(-x[i]) / (1 + exp(-x[i])) ) ) \u5176\u4e2d <cite>i == 0</cite> \u81f3 <cite>x.nElement()-1</cite>, <cite>y[i] in {0,1}</cite>. <cite>y</cite> \u548c <cite>x</cite> \u5fc5\u987b\u5177\u6709\u76f8\u540c\u7684\u7ef4\u5ea6. CosineEmbeddingLoss class torch.nn.CosineEmbeddingLoss(margin=0, size_average=True) \u65b0\u5efa\u4e00\u4e2a\u6807\u51c6, \u7528\u4ee5\u8861\u91cf\u8f93\u5165 <cite>Tensor</cite> x1, x2 \u548c\u53d6\u503c\u4e3a 1 \u6216\u8005 -1 \u7684\u6807\u7b7e <cite>Tensor</cite> [ ](#id33)y \u4e4b\u95f4\u7684 \u635f\u5931\u503c. \u6b64\u6807\u51c6\u7528 cosine \u8ddd\u79bb\u6765\u8861\u91cf2\u4e2a\u8f93\u5165\u53c2\u6570\u4e4b\u95f4\u662f\u5426\u76f8\u4f3c, \u5e76\u4e14\u4e00\u822c\u7528\u6765\u5b66\u4e60\u975e\u7ebf\u6027 embedding \u6216\u8005\u534a\u76d1\u7763 \u5b66\u4e60. <cite>margin</cite> \u5e94\u8be5\u53d6 <cite>-1</cite> \u5230 <cite>1</cite> \u4e4b\u95f4\u7684\u503c, \u5efa\u8bae\u53d6\u503c\u8303\u56f4\u662f <cite>0</cite> \u5230 <cite>0.5</cite>. \u5982\u679c\u6ca1\u6709\u8bbe\u7f6e <cite>margin</cite> \u53c2\u6570, \u5219\u9ed8\u8ba4\u503c\u53d6 <cite>0</cite>. \u6bcf\u4e2a\u6837\u672c\u7684\u635f\u5931\u51fd\u6570\u5982\u4e0b: { 1 - cos(x1, x2), if y == 1 loss(x, y) = { { max(0, cos(x1, x2) - margin), if y == -1 \u5982\u679c\u5185\u90e8\u53d8\u91cf <cite>size_average</cite> \u8bbe\u7f6e\u4e3a True , \u5219\u635f\u5931\u51fd\u6570\u4ee5 batch \u4e2d\u6240\u6709\u7684\u6837\u672c\u6570\u53d6\u5e73\u5747\u503c; \u5982\u679c <cite>size_average</cite> \u8bbe\u7f6e\u4e3a False , \u5219\u635f\u5931\u51fd\u6570\u5bf9 batch \u4e2d\u6240\u6709\u7684\u6837\u672c\u6c42\u548c. \u9ed8\u8ba4\u60c5\u51b5\u4e0b, <cite>size_average = True</cite>. MultiMarginLoss class torch.nn.MultiMarginLoss(p=1, margin=1, weight=None, size_average=True) \u521b\u5efa\u4e00\u4e2a\u6807\u51c6, \u7528\u4ee5\u4f18\u5316\u591a\u5143\u5206\u7c7b\u95ee\u9898\u7684\u5408\u9875\u635f\u5931\u51fd\u6570 (\u57fa\u4e8e\u7a7a\u767d\u7684\u635f\u5931), \u8ba1\u7b97\u635f\u5931\u503c\u65f6 \u9700\u89812\u4e2a\u53c2\u6570\u5206\u522b\u4e3a\u8f93\u5165, <cite>x</cite> (\u4e00\u4e2a2\u7ef4\u5c0f\u6279\u91cf <cite>Tensor</cite>) \u548c\u8f93\u51fa <cite>y</cite> (\u4e00\u4e2a1\u7ef4 <cite>Tensor</cite>, \u5176\u503c\u4e3a <cite>x</cite> \u7684\u7d22\u5f15\u503c, <cite>0</cite> <= <cite>y</cite> <= <cite>x.size(1)</cite>): \u5bf9\u4e8e\u6bcf\u4e2a mini-batch(\u5c0f\u6279\u91cf) \u6837\u672c: loss(x, y) = sum_i(max(0, (margin - x[y] + x[i]))^p) / x.size(0) \u5176\u4e2d `i == 0` \u81f3 `x.size(0)` \u5e76\u4e14 `i != y`. \u53ef\u9009\u62e9\u7684, \u5982\u679c\u60a8\u4e0d\u60f3\u6240\u6709\u7684\u7c7b\u62e5\u6709\u540c\u6837\u7684\u6743\u91cd\u7684\u8bdd, \u60a8\u53ef\u4ee5\u901a\u8fc7\u5728\u6784\u9020\u51fd\u6570\u4e2d\u4f20\u5165 <cite>weight</cite> \u53c2\u6570\u6765 \u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898, <cite>weight</cite> \u662f\u4e00\u4e2a1\u7ef4 Tensor. \u4f20\u5165 <cite>weight</cite> \u540e, \u635f\u5931\u51fd\u6570\u53d8\u4e3a: loss(x, y) = sum_i(max(0, w[y] * (margin - x[y] - x[i]))^p) / x.size(0) \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6c42\u51fa\u7684\u635f\u5931\u503c\u4f1a\u5bf9\u6bcf\u4e2a minibatch \u6837\u672c\u7684\u7ed3\u679c\u53d6\u5e73\u5747. \u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e <cite>size_average</cite> \u4e3a False \u6765\u7528\u5408\u8ba1\u64cd\u4f5c\u53d6\u4ee3\u53d6\u5e73\u5747\u64cd\u4f5c. TripletMarginLoss class torch.nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-06, swap=False) \u521b\u5efa\u4e00\u4e2a\u6807\u51c6, \u7528\u4ee5\u8861\u91cf\u4e09\u5143\u7ec4\u5408\u7684\u635f\u5931\u503c, \u8ba1\u7b97\u635f\u5931\u503c\u65f6\u9700\u89813\u4e2a\u8f93\u5165\u5f20\u91cf <cite>x1</cite>, <cite>x2</cite>, <cite>x3</cite> \u548c \u4e00\u4e2a\u5927\u4e8e\u96f6\u7684 <cite>margin</cite> \u503c. \u6b64\u6807\u51c6\u53ef\u4ee5\u7528\u6765\u8861\u91cf\u8f93\u5165\u6837\u672c\u95f4\u7684\u76f8\u5bf9\u76f8\u4f3c\u6027. \u4e00\u4e2a\u4e09\u5143\u8f93\u5165\u7ec4\u5408\u7531 <cite>a</cite>, <cite>p</cite> \u548c <cite>n</cite>: anchor, positive \u6837\u672c \u548c negative \u6837\u672c\u7ec4\u6210. \u6240\u6709\u8f93\u5165\u53d8\u91cf\u7684\u5f62\u5f0f\u5fc5\u987b\u4e3a \\((N, D)\\) . \u8ddd\u79bb\u4ea4\u6362\u7684\u8be6\u7ec6\u8bf4\u660e\u8bf7\u53c2\u8003\u8bba\u6587 Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. \\[L(a, p, n) = \\frac{1}{N} \\left( \\sum_{i=1}^N \\max \\{d(a_i, p_i) - d(a_i, n_i) + {\\rm margin}, 0\\} \\right)\\] \u5176\u4e2d \\(d(x_i, y_i) = \\left\\lVert {\\bf x}_i - {\\bf y}_i \\right\\rVert_p\\) . | Parameters: | anchor \u2013 anchor \u8f93\u5165 tensor positive \u2013 positive \u8f93\u5165 tensor negative \u2013 negative \u8f93\u5165 tensor p \u2013 \u6b63\u5219\u5316\u7387. Default: 2 | | --- | --- | Shape: Input: \\((N, D)\\) \u5176\u4e2d <cite>D = vector dimension</cite> Output: \\((N, 1)\\) >>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2) >>> input1 = autograd.Variable(torch.randn(100, 128)) >>> input2 = autograd.Variable(torch.randn(100, 128)) >>> input3 = autograd.Variable(torch.randn(100, 128)) >>> output = triplet_loss(input1, input2, input3) >>> output.backward() Vision layers (\u89c6\u89c9\u5c42) PixelShuffle class torch.nn.PixelShuffle(upscale_factor) \u5bf9\u5f20\u91cf\u4e2d\u5f62\u5982 \\((*, C * r^2, H, W]\\) \u7684\u5143\u7d20, \u91cd\u65b0\u6392\u5217\u6210 \\((C, H * r, W * r)\\) . \u5f53\u4f7f\u7528 stride = \\(1/r\\) \u7684\u9ad8\u6548\u5b50\u50cf\u7d20\u5377\u79ef\u5f88\u6709\u7528. \u53c2\u8003\u5982\u4e0b\u8bba\u6587\u83b7\u5f97\u66f4\u591a\u4fe1\u606f: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network Shi et. al (2016) . Parameters: upscale_factor ( int ) \u2013 \u589e\u52a0\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u56e0\u5b50 Shape: \u8f93\u5165: \\((N, C * {upscale\\_factor}^2, H, W)\\) \u8f93\u51fa: \\((N, C, H * {upscale\\_factor}, W * {upscale\\_factor})\\) Examples: >>> ps = nn.PixelShuffle(3) >>> input = autograd.Variable(torch.Tensor(1, 9, 4, 4)) >>> output = ps(input) >>> print(output.size()) torch.Size([1, 1, 12, 12]) Upsample class torch.nn.Upsample(size=None, scale_factor=None, mode='nearest') \u5bf9\u7ed9\u5b9a\u7684\u591a\u901a\u9053\u4e00\u7ef4\u65f6\u5e8f\u6570\u636e, \u4e8c\u7ef4\u7a7a\u95f4\u6570\u636e, \u6216\u4e09\u7ef4\u5bb9\u79ef\u6570\u636e\u8fdb\u884c\u4e0a\u91c7\u6837. \u8f93\u5165\u6570\u636e\u7684\u683c\u5f0f\u4e3a <cite>minibatch x channels x [depth] x [height] x width</cite>. \u56e0\u6b64, \u5bf9\u4e8e2-D\u7a7a\u95f4\u6570\u636e\u7684\u8f93\u5165, \u671f\u671b\u5f97\u5230\u4e00\u4e2a4-D\u5f20\u91cf\uff1b\u5bf9\u4e8e3-D\u7acb\u4f53\u6570\u636e\u8f93\u5165, \u671f\u671b\u5f97\u5230\u4e00\u4e2a5-D\u5f20\u91cf. \u5bf93D, 4D, 5D\u7684\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u6700\u8fd1\u90bb\u3001\u7ebf\u6027\u3001\u53cc\u7ebf\u6027\u548c\u4e09\u7ebf\u6027\u91c7\u6837, \u53ef\u7528\u4e8e\u8be5\u4e0a\u91c7\u6837\u65b9\u6cd5. \u53ef\u4ee5\u63d0\u4f9b scale_factor \u6216\u76ee\u6807\u8f93\u51fa\u7684 size \u6765\u8ba1\u7b97\u8f93\u51fa\u7684\u5927\u5c0f. \uff08\u4e0d\u80fd\u540c\u65f6\u90fd\u7ed9, \u56e0\u4e3a\u8fd9\u6837\u505a\u662f\u542b\u7cca\u4e0d\u6e05\u7684. \uff09 | Parameters: | size ( tuple , optional ) \u2013 \u6574\u578b\u6570\u7684\u5143\u7ec4 ([D_out], [H_out], W_out) \u8f93\u51fa\u5927\u5c0f scale_factor ( int / tuple of python:ints__, optional ) \u2013 \u56fe\u50cf\u9ad8\u5ea6/\u5bbd\u5ea6/\u6df1\u5ea6\u7684\u4e58\u6570 mode ( string , optional ) \u2013 \u4e0a\u91c7\u6837\u7b97\u6cd5: nearest | linear | bilinear | trilinear. \u9ed8\u8ba4\u4e3a: nearest | | --- | --- | Shape: \u8f93\u5165: \\((N, C, W_{in})\\) , \\((N, C, H_{in}, W_{in})\\) \u6216 \\((N, C, D_{in}, H_{in}, W_{in})\\) \u8f93\u51fa: \\((N, C, W_{out})\\) , \\((N, C, H_{out}, W_{out})\\) \u6216 \\((N, C, D_{out}, H_{out}, W_{out})\\) \u5176\u4e2d: \\(D_{out} = floor(D_{in} * scale\\_factor)\\) \u6216 <cite>size[-3]</cite> \\(H_{out} = floor(H_{in} * scale\\_factor)\\) \u6216 <cite>size[-2]</cite> \\(W_{out} = floor(W_{in} * scale\\_factor)\\) \u6216 <cite>size[-1]</cite> \u793a\u4f8b: >>> inp Variable containing: (0 ,0 ,.,.) = 1 2 3 4 [torch.FloatTensor of size 1x1x2x2] >>> m = nn.Upsample(scale_factor=2, mode='bilinear') >>> m(inp) Variable containing: (0 ,0 ,.,.) = 1.0000 1.3333 1.6667 2.0000 1.6667 2.0000 2.3333 2.6667 2.3333 2.6667 3.0000 3.3333 3.0000 3.3333 3.6667 4.0000 [torch.FloatTensor of size 1x1x4x4] >>> inp Variable containing: (0 ,0 ,.,.) = 1 2 3 4 [torch.FloatTensor of size 1x1x2x2] >>> m = nn.Upsample(scale_factor=2, mode='nearest') >>> m(inp) Variable containing: (0 ,0 ,.,.) = 1 1 2 2 1 1 2 2 3 3 4 4 3 3 4 4 [torch.FloatTensor of size 1x1x4x4] UpsamplingNearest2d class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None) \u5bf9\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c2\u7ef4\u6700\u8fd1\u90bb\u4e0a\u91c7\u6837. \u4e3a\u4e86\u6307\u5b9a\u91c7\u6837\u8303\u56f4, \u63d0\u4f9b\u4e86 size \u6216 scale_factor \u4f5c\u4e3a\u6784\u9020\u53c2\u6570. \u5f53\u7ed9\u5b9a <cite>size</cite>, \u8f93\u51fa\u56fe\u50cf\u7684\u5927\u5c0f\u4e3a (h, w). | Parameters: | size ( tuple , optional ) \u2013 \u8f93\u51fa\u56fe\u7247\u5927\u5c0f\u7684\u6574\u578b\u5143\u7ec4(H_out, W_out) scale_factor ( int , optional ) \u2013 \u56fe\u50cf\u7684 \u957f\u548c\u5bbd\u7684\u4e58\u5b50. | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) \u5176\u4e2d \\(H_{out} = floor(H_{in} * scale\\_factor)\\) \\(W_{out} = floor(W_{in} * scale\\_factor)\\) \u793a\u4f8b: >>> inp Variable containing: (0 ,0 ,.,.) = 1 2 3 4 [torch.FloatTensor of size 1x1x2x2] >>> m = nn.UpsamplingNearest2d(scale_factor=2) >>> m(inp) Variable containing: (0 ,0 ,.,.) = 1 1 2 2 1 1 2 2 3 3 4 4 3 3 4 4 [torch.FloatTensor of size 1x1x4x4] UpsamplingBilinear2d class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None) \u5bf9\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c2\u7ef4\u53cc\u7ebf\u6027\u4e0a\u91c7\u6837. \u4e3a\u4e86\u6307\u5b9a\u91c7\u6837\u8303\u56f4, \u63d0\u4f9b\u4e86 size \u6216 scale_factor \u4f5c\u4e3a\u6784\u9020\u53c2\u6570. \u5f53\u7ed9\u5b9a <cite>size</cite>, \u8f93\u51fa\u56fe\u50cf\u7684\u5927\u5c0f\u4e3a (h, w). | Parameters: | size ( tuple , optional ) \u2013 \u8f93\u51fa\u56fe\u7247\u5927\u5c0f\u7684\u6574\u578b\u5143\u7ec4(H_out, W_out) scale_factor ( int , optional ) \u2013 \u56fe\u50cf\u7684 \u957f\u548c\u5bbd\u7684\u4e58\u5b50. | | --- | --- | shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) \u5176\u4e2d \\(H_{out} = floor(H_{in} * scale\\_factor)\\) \\(W_{out} = floor(W_{in} * scale\\_factor)\\) \u793a\u4f8b::: >>> inp Variable containing: (0 ,0 ,.,.) = 1 2 3 4 [torch.FloatTensor of size 1x1x2x2] >>> m = nn.UpsamplingBilinear2d(scale_factor=2) >>> m(inp) Variable containing: (0 ,0 ,.,.) = 1.0000 1.3333 1.6667 2.0000 1.6667 2.0000 2.3333 2.6667 2.3333 2.6667 3.0000 3.3333 3.0000 3.3333 3.6667 4.0000 [torch.FloatTensor of size 1x1x4x4] DataParallel layers (multi-GPU, distributed) (\u6570\u636e\u5e76\u884c\u5c42, \u591a GPU \u7684, \u5206\u5e03\u5f0f\u7684) DataParallel class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0) \u5728\u6a21\u5757\u7ea7\u522b\u5b9e\u73b0\u6570\u636e\u5e76\u884c\u6027. \u6b64\u5bb9\u5668\u901a\u8fc7\u5728\u6279\u6b21\u7ef4\u5ea6\u4e2d\u5206\u5757, \u5c06\u8f93\u5165\u5206\u5272\u5230\u6307\u5b9a\u8bbe\u5907\u4e0a, \u4ece\u800c\u5e76\u884c\u5316\u7ed9\u5b9a\u6a21\u5757\u7684\u5e94\u7528\u7a0b \u5e8f.\u5728\u6b63\u5411\u4f20\u9012\u4e2d, \u6a21\u5757\u88ab\u590d\u5236\u5230\u6bcf\u4e2a\u8bbe\u5907\u4e0a, \u6bcf\u4e2a\u526f\u672c\u5904\u7406\u4e00\u90e8\u5206\u8f93\u5165.\u5728\u5411\u540e\u4f20\u9012\u671f\u95f4, \u6765\u81ea\u6bcf\u4e2a\u526f\u672c\u7684\u68af\u5ea6\u53d8\u5316\u88ab\u6c47\u603b\u5230\u539f\u59cb\u6a21\u5757\u4e2d. batch size \u5e94\u8be5\u5927\u4e8e GPUs \u7684\u6570\u91cf.\u540c\u65f6\u4e5f\u5e94\u8be5\u662f GPU \u6570\u91cf\u7684\u6574\u6570\u500d, \u4ee5 \u4fbf\u6bcf\u4e2a\u5757\u5927\u5c0f\u76f8\u540c\uff08\u4ee5\u4fbf\u6bcf\u4e2a GPU \u5904\u7406\u76f8\u540c\u6570\u91cf\u7684\u6837\u672c\uff09. \u5f15\u7528 : \u4f7f\u7528 nn.DataParallel \u66ff\u4ee3 multiprocessing \u5141\u8bb8\u5c06\u4efb\u610f\u4f4d\u7f6e\u548c\u5173\u952e\u5b57\u8f93\u5165\u4f20\u5165 DataParallel EXCEPT Tensors. \u6240\u6709\u7684\u53d8\u91cf\u5c06\u88ab\u5206 \u6563\u5728\u6307\u5b9a\u7684\u7ef4\u5ea6\uff08\u9ed8\u8ba4\u4e3a0\uff09.\u539f\u59cb\u7c7b\u578b\u5c06\u88ab\u5e7f\u64ad, \u4f46\u6240\u6709\u5176\u4ed6\u7c7b\u578b\u5c06\u662f\u4e00\u4e2a\u6d45\u5c42\u526f\u672c, \u5982 \u679c\u5199\u5165\u6a21\u578b\u7684\u6b63\u5411\u4f20\u9012, \u53ef\u80fd\u4f1a\u88ab\u635f\u574f. Args : module: \u5e76\u884c\u7684\u6a21\u578b device_ids: CUDA devices\uff08CUDA \u9a71\u52a8\uff09 (default: all devices) output_device: \u8f93\u51fa\u8bbe\u5907\u4f4d\u7f6e (default: device_ids[0]) \u793a\u4f8b :: >>> net = torch.nn.DataParallel(model, device_ids=[0, 1, 2]) >>> output = net(input_var) DistributedDataParallel class torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0) \u5728\u6a21\u5757\u7ea7\u522b\u5b9e\u73b0\u5206\u5e03\u5f0f\u6570\u636e\u5e76\u884c. \u6b64\u5bb9\u5668\u901a\u8fc7\u5728\u6279\u6b21\u7ef4\u5ea6\u4e2d\u5206\u5757, \u5c06\u8f93\u5165\u5206\u5272\u5230\u6307\u5b9a\u8bbe\u5907\u4e0a, \u4ece\u800c\u5e76\u884c\u5316\u7ed9\u5b9a\u6a21\u5757\u7684\u5e94\u7528\u7a0b\u5e8f. \u8be5\u6a21\u5757\u88ab\u590d\u5236\u5230\u6bcf\u53f0\u673a\u5668\u548c\u6bcf\u4e2a\u8bbe\u5907\u4e0a, \u6bcf\u4e2a\u8fd9\u6837\u7684\u526f\u672c\u5904\u7406\u4e00\u90e8\u5206\u8f93\u5165.\u5728\u5411\u540e\u4f20\u9012\u671f\u95f4, \u6765\u81ea\u6bcf\u4e2a\u8282\u70b9\u7684\u68af\u5ea6\u88ab\u5e73\u5747. batch size \u5e94\u8be5\u5927\u4e8e GPUs \u7684\u6570\u91cf.\u540c\u65f6\u4e5f\u5e94\u8be5\u662f GPU \u6570\u91cf\u7684\u6574\u6570\u500d, \u4ee5\u4fbf\u6bcf\u4e2a\u5757\u5927\u5c0f \u76f8\u540c\uff08\u4ee5\u4fbf\u6bcf\u4e2a GPU \u5904\u7406\u76f8\u540c\u6570\u91cf\u7684\u6837\u672c\uff09. \u5f15\u7528 : Basics \u548c \u4f7f\u7528 nn.DataParallel \u66ff\u4ee3 multiprocessing . \u5bf9\u8f93\u5165\u7684\u7ea6\u675f\u548c torch.nn.DataParallel \u4e2d\u4e00\u6837. \u521b\u5efa\u8fd9\u4e2a\u7c7b\u9700\u8981\u5206\u5e03\u5f0f\u5305\u5df2\u7ecf\u5728 process group \u6a21\u5f0f\u4e0b\u88ab\u521d\u59cb\u5316 (\u5f15\u7528 torch.distributed.init_process_group() ). Warning \u8fd9\u4e2a\u6a21\u5757\u53ea\u80fd\u548c gloo \u540e\u7aef\u4e00\u8d77\u5de5\u4f5c. Warning \u6784\u9020\u5668, \u8f6c\u53d1\u65b9\u6cd5\u548c\u8f93\u51fa\uff08\u6216\u8005\u8fd9\u4e2a\u6a21\u5757\u7684\u8f93\u51fa\u529f\u80fd\uff09\u7684\u533a\u5206\u662f\u5206\u5e03\u5f0f\u540c\u6b65\u70b9.\u8003\u8651\u5230\u4e0d\u540c\u7684 \u8fdb\u7a0b\u53ef\u80fd\u4f1a\u6267\u884c\u4e0d\u540c\u7684\u4ee3\u7801. Warning \u8be5\u6a21\u5757\u5047\u8bbe\u6240\u6709\u53c2\u6570\u5728\u521b\u5efa\u65f6\u90fd\u5728\u6a21\u578b\u4e2d\u6ce8\u518c.\u4e4b\u540e\u4e0d\u5e94\u8be5\u6dfb\u52a0\u6216\u5220\u9664\u53c2\u6570.\u540c\u6837\u9002\u7528\u4e8e\u7f13\u51b2\u533a. Warning \u8fd9\u4e2a\u6a21\u5757\u5047\u5b9a\u6240\u6709\u7684\u7f13\u51b2\u533a\u548c\u68af\u5ea6\u90fd\u662f\u5bc6\u96c6\u7684. Warning \u8fd9\u4e2a\u6a21\u5757\u4e0d\u80fd\u7528\u4e8e : func: <cite>torch.autograd.grad</cite> \uff08\u5373\u53ea\u6709\u5728\u53c2\u6570\u7684 .grad \u5c5e\u6027\u4e2d \u7d2f\u79ef\u68af\u5ea6\u624d\u80fd\u4f7f\u7528\uff09. Note \u53c2\u6570\u6c38\u8fdc\u4e0d\u4f1a\u5728\u8fdb\u7a0b\u4e4b\u95f4\u5e7f\u64ad.\u6a21\u5757\u5728\u68af\u5ea6\u4e0a\u6267\u884c\u5168\u90e8\u4f18\u5316\u6b65\u9aa4, \u5e76\u5047\u5b9a\u5b83\u4eec\u5c06\u4ee5\u76f8\u540c\u7684\u65b9\u5f0f\u5728 \u6240\u6709\u8fdb\u7a0b\u4e2d\u8fdb\u884c\u4f18\u5316.\u7f13\u51b2\u533a\uff08e.g. BatchNorm stats\uff09\u5728\u7b49\u7ea70\u7684\u8fc7\u7a0b\u4e2d\u4ece\u6a21\u5757\u5e7f\u64ad\u5230\u7cfb\u7edf \u4e2d\u7684\u6bcf\u4e2a\u8fed\u4ee3\u4e2d\u7684\u6240\u6709\u5176\u4ed6\u526f\u672c. Args : module: \u9700\u8981\u5e76\u884c\u7684\u6a21\u578b device_ids: CUDA devices (default: all devices) output_device: device location of output (default: device_ids[0]) \u793a\u4f8b :: >>> torch.distributed.init_process_group(world_size=4, init_method='...') >>> net = torch.nn.DistributedDataParallel(model) Utilities (\u5de5\u5177\u5305) clip_grad_norm torch.nn.utils.clip_grad_norm(parameters, max_norm, norm_type=2) \u63a5\u6536\u4e00\u4e2a\u5305\u542b Variable \u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61, \u5bf9 Variable \u7684\u68af\u5ea6\u6309\u8303\u6570\u8fdb\u884c\u88c1\u526a. \u8303\u6570\u662f\u5bf9\u6240\u6709\u68af\u5ea6\u8fdb\u884c\u8ba1\u7b97\u7684, \u7b49\u4ef7\u4e8e\u628a\u6240\u6709\u8f93\u5165\u53d8\u91cf\u7684\u68af\u5ea6\u8fde\u63a5\u6210\u4e00\u4e2a\u5411\u91cf, \u7136\u540e\u5bf9\u8fd9\u4e2a\u5411\u91cf\u6309\u8303\u6570\u8fdb\u884c\u88c1\u526a. \u68af\u5ea6\u5c06\u4f1a\u88ab\u539f\u5730\u4fee\u6539. | Parameters: | parameters ( Iterable__[ Variable ] ) \u2013 \u4e00\u4e2a\u53ef\u8fed\u4ee3\u5bf9\u8c61, \u5176\u5305\u542b\u5c06\u8981\u8fdb\u884c\u68af\u5ea6\u6b63\u89c4\u5316\u7684 Variable max_norm ( float or int ) \u2013 \u68af\u5ea6\u7684\u6700\u5927\u8303\u6570 norm_type ( float or int ) \u2013 p \u8303\u6570(\u6307\u5b9a p ). \u7528 'inf' \u8868\u793a\u65e0\u7a77\u8303\u6570 | | --- | --- | | Returns: | \u68af\u5ea6\u7684\u8303\u6570 (\u89c6\u4e3a\u5355\u4e2a\u5411\u91cf\u7684). | | --- | --- | weight_norm torch.nn.utils.weight_norm(module, name='weight', dim=0) \u5c06\u6743\u91cd\u5f52\u4e00\u5316\u5e94\u7528\u4e8e\u7ed9\u5b9a\u6a21\u5757\u4e2d\u7684\u6307\u5b9a\u53c2\u6570. . \\[\\mathbf{w} = g \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\] \u6743\u91cd\u5f52\u4e00\u5316\u662f\u5c06\u6743\u91cd\u5f20\u91cf\u7684\u5927\u5c0f\u548c\u65b9\u5411\u5206\u79bb\u7684\u518d\u53c2\u6570\u5316. \u8be5\u51fd\u6570\u4f1a\u7528\u4e24\u4e2a\u53c2\u6570\u4ee3\u66ff <cite>name</cite> (e.g. \u201cweight\u201d)\u6240\u6307\u5b9a\u7684\u53c2\u6570. \u5728\u65b0\u7684\u53c2\u6570\u4e2d, \u4e00\u4e2a\u6307\u5b9a\u53c2\u6570\u7684\u5927\u5c0f (e.g. \u201cweight_g\u201d), \u4e00\u4e2a\u6307\u5b9a\u53c2\u6570\u7684\u65b9\u5411. \u6743\u91cd\u5f52\u4e00\u5316\u662f\u901a\u8fc7\u4e00\u4e2a\u94a9\u5b50\u5b9e\u73b0\u7684, \u8be5\u94a9\u5b50\u4f1a\u5728 <cite>~Module.forward</cite> \u7684\u6bcf\u6b21\u8c03\u7528\u4e4b\u524d\u6839\u636e\u5927\u5c0f\u548c\u65b9\u5411(\u4e24\u4e2a\u65b0\u53c2\u6570)\u91cd\u65b0\u8ba1\u7b97\u6743\u91cd\u5f20\u91cf. \u9ed8\u8ba4\u60c5\u51b5\u4e0b, <cite>dim=0</cite>, \u8303\u6570\u4f1a\u5728\u6bcf\u4e00\u4e2a\u8f93\u51fa\u7684 channel/plane \u4e0a\u5206\u522b\u8ba1\u7b97. \u82e5\u8981\u5bf9\u6574\u4e2a\u6743\u91cd\u5f20\u91cf\u8ba1\u7b97\u8303\u6570, \u4f7f\u7528 <cite>dim=None</cite>. \u53c2\u89c1 https://arxiv.org/abs/1602.07868 | Parameters: | module ( nn.Module ) \u2013 \u7ed9\u5b9a\u7684 module name ( str , optional ) \u2013 \u6743\u91cd\u53c2\u6570\u7684 name dim ( int , optional ) \u2013 \u8fdb\u884c\u8303\u6570\u8ba1\u7b97\u7684\u7ef4\u5ea6 | | --- | --- | | Returns: | \u6dfb\u52a0\u4e86\u6743\u91cd\u5f52\u4e00\u5316\u94a9\u5b50\u7684\u539f module | | --- | --- | Example: >>> m = weight_norm(nn.Linear(20, 40), name='weight') Linear (20 -> 40) >>> m.weight_g.size() torch.Size([40, 1]) >>> m.weight_v.size() torch.Size([40, 20]) remove_weight_norm torch.nn.utils.remove_weight_norm(module, name='weight') \u4ece\u6a21\u5757\u4e2d\u79fb\u9664\u6743\u91cd\u5f52\u4e00\u5316/\u518d\u53c2\u6570\u5316. | Parameters: | module ( nn.Module ) \u2013 \u7ed9\u5b9a\u7684 module name ( str , optional ) \u2013 \u6743\u91cd\u53c2\u6570\u7684 name | | --- | --- | Example >>> m = weight_norm(nn.Linear(20, 40)) >>> remove_weight_norm(m) PackedSequence torch.nn.utils.rnn.PackedSequence(_cls, data, batch_sizes) \u4fdd\u5b58\u4e00\u4e2a\u6253\u5305\u5e8f\u5217\u7684 data \u548c batch_sizes. \u6240\u6709\u7684 RNN \u6a21\u5757\u90fd\u63a5\u6536\u8fd9\u79cd\u88ab\u5305\u88f9\u540e\u7684\u5e8f\u5217\u4f5c\u4e3a\u5b83\u4eec\u7684\u8f93\u5165. Note \u6c38\u8fdc\u4e0d\u8981\u624b\u52a8\u521b\u5efa\u8fd9\u4e2a\u7c7b\u7684\u5b9e\u4f8b. \u5b83\u4eec\u5e94\u5f53\u88ab pack_padded_sequence() \u8fd9\u6837\u7684\u51fd\u6570\u5b9e\u4f8b\u5316. | Variables: | data ( Variable ) \u2013 \u5305\u542b\u6253\u5305\u540e\u5e8f\u5217\u7684 Variable batch_sizes ( list [ int ] ) \u2013 \u5305\u542b\u6bcf\u4e2a\u5e8f\u5217\u6b65\u7684 batch size \u7684\u5217\u8868 | | --- | --- | pack_padded_sequence torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False) \u5c06\u586b\u5145\u8fc7\u7684\u53d8\u957f\u5e8f\u5217\u6253\u5305(\u538b\u7d27). \u8f93\u5165\u7684\u5f62\u72b6\u53ef\u4ee5\u662f TxBx* . T\u662f\u6700\u957f\u5e8f\u5217\u957f\u5ea6(\u7b49\u4e8e lengths[0] ), B \u662f batch size, * \u4ee3\u8868\u4efb\u610f\u7ef4\u5ea6(\u53ef\u4ee5\u662f0). \u5982\u679c batch_first=True , \u90a3\u4e48\u76f8\u5e94\u7684 input size \u5c31\u662f BxTx* . Variable \u4e2d\u4fdd\u5b58\u7684\u5e8f\u5217, \u5e94\u8be5\u6309\u5e8f\u5217\u957f\u5ea6\u7684\u957f\u77ed\u6392\u5e8f, \u957f\u7684\u5728\u524d, \u77ed\u7684\u5728\u540e. \u5373 input[:,0] \u4ee3\u8868\u7684\u662f\u6700\u957f\u7684\u5e8f\u5217, input[:, B-1] \u4fdd\u5b58\u7684\u662f\u6700\u77ed\u7684\u5e8f\u5217. Note \u53ea\u8981\u662f\u7ef4\u5ea6\u5927\u4e8e\u7b49\u4e8e2\u7684 input \u90fd\u53ef\u4ee5\u4f5c\u4e3a\u8fd9\u4e2a\u51fd\u6570\u7684\u53c2\u6570. \u4f60\u53ef\u4ee5\u7528\u5b83\u6765\u6253\u5305 labels, \u7136\u540e\u7528 RNN \u7684\u8f93\u51fa\u548c\u6253\u5305\u540e\u7684 labels \u6765\u8ba1\u7b97 loss. \u901a\u8fc7 PackedSequence \u5bf9\u8c61\u7684 .data \u5c5e\u6027\u53ef\u4ee5\u83b7\u53d6 Variable. | Parameters: | input ( Variable ) \u2013 \u53d8\u957f\u5e8f\u5217\u88ab\u586b\u5145\u540e\u7684 batch lengths ( list [ int ] ) \u2013 Variable \u4e2d\u6bcf\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6. batch_first ( bool , optional ) \u2013 \u5982\u679c\u662f True , input \u7684\u5f62\u72b6\u5e94\u8be5\u662f BxTx*. | | --- | --- | | Returns: | \u4e00\u4e2a PackedSequence \u5bf9\u8c61. | | --- | --- | pad_packed_sequence torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0) \u586b\u5145\u6253\u5305\u8fc7\u7684\u53d8\u957f\u5e8f\u5217. \u8fd9\u662f pack_padded_sequence() \u7684\u9006\u64cd\u4f5c. \u8fd4\u56de\u7684 Varaible \u7684\u503c\u7684 size \u662f TxBx , T \u662f\u6700\u957f\u5e8f\u5217\u7684\u957f\u5ea6, B \u662f batch_size, \u5982\u679c batch_first=True , \u90a3\u4e48\u8fd4\u56de\u503c\u662f BxTx . Batch\u4e2d\u7684\u5143\u7d20\u5c06\u4f1a\u4ee5\u5b83\u4eec\u957f\u5ea6\u7684\u9006\u5e8f\u6392\u5217. | Parameters: | sequence ( PackedSequence ) \u2013 \u5c06\u8981\u88ab\u586b\u5145\u7684 batch batch_first ( bool , optional ) \u2013 \u5982\u679c\u4e3a <cite>True`</cite> , \u8fd4\u56de\u7684\u6570\u636e\u7684\u683c\u5f0f\u4e3a BxTx*. padding_value ( float , optional ) \u2013 \u7528\u6765\u586b\u5145\u5143\u7d20\u7684\u503c | | --- | --- | | Returns: | \u4e00\u4e2a tuple, \u5305\u542b\u88ab\u586b\u5145\u540e\u7684\u5e8f\u5217, \u548c batch \u4e2d\u5e8f\u5217\u7684\u957f\u5ea6\u5217\u8868. | | --- | --- |","title":"torch.nn"},{"location":"57/#torchnn","text":"","title":"torch.nn"},{"location":"57/#parameters","text":"class torch.nn.Parameter Variable \u7684\u4e00\u79cd, \u5e38\u88ab\u7528\u4e8e module parameter\uff08\u6a21\u5757\u53c2\u6570\uff09. Parameters \u662f Variable \u7684\u5b50\u7c7b, \u5f53\u5b83\u548c Module \u4e00\u8d77\u4f7f\u7528\u7684\u65f6\u5019\u4f1a\u6709\u4e00\u4e9b\u7279\u6b8a\u7684\u5c5e\u6027 - \u5f53\u5b83\u4eec\u88ab\u8d4b\u503c\u7ed9 Module \u5c5e\u6027\u65f6, \u5b83\u4f1a\u81ea\u52a8\u7684\u88ab\u52a0\u5230 Module \u7684\u53c2\u6570\u5217\u8868\u4e2d, \u5e76\u4e14\u4f1a\u51fa\u73b0\u5728 parameters() iterator \u8fed\u4ee3\u5668\u65b9\u6cd5\u4e2d. \u5c06 Varibale \u8d4b\u503c\u7ed9 Module \u5c5e\u6027\u5219\u4e0d\u4f1a\u6709\u8fd9\u6837\u7684\u5f71\u54cd. \u8fd9\u6837\u505a\u7684\u539f\u56e0\u662f: \u6211\u4eec\u6709\u65f6\u5019\u4f1a\u9700\u8981\u7f13\u5b58\u4e00\u4e9b\u4e34\u65f6\u7684 state\uff08\u72b6\u6001\uff09, \u4f8b\u5982: \u6a21\u578b RNN \u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u72b6\u6001. \u5982\u679c\u6ca1\u6709 Parameter \u8fd9\u4e2a\u7c7b\u7684\u8bdd, \u90a3\u4e48\u8fd9\u4e9b\u4e34\u65f6\u8868\u4e5f\u4f1a\u6ce8\u518c\u4e3a\u6a21\u578b\u53d8\u91cf. Variable \u4e0e Parameter \u7684\u53e6\u4e00\u4e2a\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e, Parameter \u4e0d\u80fd\u88ab volatile (\u5373: \u65e0\u6cd5\u8bbe\u7f6e volatile=True) \u800c\u4e14\u9ed8\u8ba4 requires_grad=True. Variable \u9ed8\u8ba4 requires_grad=False. | Parameters: | data ( Tensor ) \u2013 parameter tensor. requires_grad ( bool , optional ) \u2013 \u5982\u679c\u53c2\u6570\u9700\u8981\u68af\u5ea6. \u66f4\u591a\u7ec6\u8282\u8bf7\u53c2\u9605 \u53cd\u5411\u6392\u9664 subgraphs (\u5b50\u56fe) . | | --- | --- |","title":"Parameters (\u53c2\u6570)"},{"location":"57/#containers","text":"","title":"Containers (\u5bb9\u5668)"},{"location":"57/#module","text":"class torch.nn.Module \u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u7c7b. \u4f60\u7684\u6a21\u578b\u5e94\u8be5\u4e5f\u662f\u8be5\u7c7b\u7684\u5b50\u7c7b. Modules \u4e5f\u53ef\u4ee5\u5305\u542b\u5176\u5b83 Modules, \u5141\u8bb8\u4f7f\u7528\u6811\u7ed3\u6784\u5d4c\u5165\u5b83\u4eec. \u4f60\u53ef\u4ee5\u5c06\u5b50\u6a21\u5757\u8d4b\u503c\u7ed9\u6a21\u578b\u5c5e\u6027 import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) \u4ee5\u8fd9\u79cd\u65b9\u5f0f\u5206\u914d\u7684\u5b50\u6a21\u5757\u5c06\u88ab\u6ce8\u518c, \u5e76\u4e14\u5728\u8c03\u7528 .cuda() \u7b49\u7b49\u65b9\u6cd5\u65f6\u4e5f\u5c06\u8f6c\u6362\u5b83\u4eec\u7684\u53c2\u6570. add_module(name, module) \u6dfb\u52a0\u4e00\u4e2a child module\uff08\u5b50\u6a21\u5757\uff09\u5230\u5f53\u524d\u7684 module\uff08\u6a21\u5757\uff09\u4e2d. \u88ab\u6dfb\u52a0\u7684 module \u8fd8\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u7684 name \u5c5e\u6027\u6765\u83b7\u53d6\u5b83. | Parameters: | name ( string ) \u2013 \u5b50\u6a21\u5757\u7684\u540d\u79f0. \u53ef\u4ee5\u4f7f\u7528\u6307\u5b9a\u7684 name \u4ece\u8be5\u6a21\u5757\u8bbf\u95ee\u5b50\u6a21\u5757 parameter ( Module ) \u2013 \u88ab\u6dfb\u52a0\u5230\u6a21\u5757\u7684\u5b50\u6a21\u5757. | | --- | --- | apply(fn) \u5c06 fn \u51fd\u6570\u9012\u5f52\u7684\u5e94\u7528\u5230\u6bcf\u4e00\u4e2a\u5b50\u6a21\u5757 (\u7531 .children() \u65b9\u6cd5\u6240\u8fd4\u56de\u7684) \u4ee5\u53ca self. \u5178\u578b\u7684\u7528\u4e8e\u5305\u62ec\u521d\u59cb\u5316\u6a21\u578b\u7684\u53c2\u6570 (\u4e5f\u53ef\u53c2\u9605 torch-nn-init). Parameters: fn ( Module -> None) \u2013 \u8981\u88ab\u5e94\u7528\u5230\u6bcf\u4e00\u4e2a\u5b50\u6a21\u5757\u4e0a\u7684\u51fd\u6570 Returns: self --- --- Return type: Module --- --- Example >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.data.fill_(1.0) >>> print(m.weight) >>> >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear (2 -> 2) Parameter containing: 1 1 1 1 [torch.FloatTensor of size 2x2] Linear (2 -> 2) Parameter containing: 1 1 1 1 [torch.FloatTensor of size 2x2] Sequential ( (0): Linear (2 -> 2) (1): Linear (2 -> 2) ) children() \u8fd4\u56de\u4e00\u4e2a\u6700\u8fd1\u5b50\u6a21\u5757\u7684 iterator\uff08\u8fed\u4ee3\u5668\uff09. Yields: Module \u2013 \u4e00\u4e2a\u5b50\u6a21\u5757 cpu() \u5c06\u6240\u6709\u7684\u6a21\u578b\u53c2\u6570\u548c\u7f13\u51b2\u533a\u79fb\u52a8\u5230 CPU. Returns: self Return type: Module --- --- cuda(device=None) \u5c06\u6240\u6709\u7684\u6a21\u578b\u53c2\u6570\u548c\u7f13\u51b2\u533a\u79fb\u52a8\u5230 GPU. \u8fd9\u5c06\u4f1a\u5173\u8054\u4e00\u4e9b\u53c2\u6570\u5e76\u4e14\u7f13\u5b58\u4e0d\u540c\u7684\u5bf9\u8c61. \u6240\u4ee5\u5728\u6784\u5efa\u4f18\u5316\u5668\u4e4b\u524d\u5e94\u8be5\u8c03\u7528\u5b83, \u5982\u679c\u6a21\u5757\u5728\u4f18\u5316\u7684\u60c5\u51b5\u4e0b\u4f1a\u751f\u5b58\u5728 GPU \u4e0a. Parameters: device ( int , optional ) \u2013 \u5982\u679c\u6307\u5b9a, \u6240\u6709\u53c2\u6570\u5c06\u88ab\u590d\u5236\u5230\u6307\u5b9a\u7684\u8bbe\u5907\u4e0a Returns: self --- --- Return type: Module --- --- double() \u5c06\u6240\u6709\u7684 parameters \u548c buffers \u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u6210 double. Returns: self Return type: Module --- --- eval() \u5c06\u6a21\u5757\u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f. \u8fd9\u79cd\u65b9\u5f0f\u53ea\u5bf9 Dropout \u6216 BatchNorm \u7b49\u6a21\u5757\u6709\u6548. float() \u5c06\u6240\u6709\u7684 parameters \u548c buffers \u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u6210float. Returns: self Return type: Module --- --- forward(*input) \u5b9a\u4e49\u6bcf\u6b21\u8c03\u7528\u65f6\u6267\u884c\u7684\u8ba1\u7b97. \u5e94\u8be5\u88ab\u6240\u6709\u7684\u5b50\u7c7b\u91cd\u5199. Note \u5c3d\u7ba1\u9700\u8981\u5728\u6b64\u51fd\u6570\u4e2d\u5b9a\u4e49\u6b63\u5411\u4f20\u9012\u7684\u65b9\u5f0f, \u4f46\u662f\u5e94\u8be5\u4e8b\u540e\u5c3d\u91cf\u8c03\u7528 Module \u5b9e\u4f8b, \u56e0\u4e3a\u524d\u8005\u8d1f\u8d23\u8fd0\u884c\u5df2\u6ce8\u518c\u7684\u94a9\u5b50, \u800c\u540e\u8005\u9759\u9ed8\u7684\u5ffd\u7565\u5b83\u4eec. half() \u5c06\u6240\u6709\u7684 parameters \u548c buffers \u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u6210 half. Returns: self Return type: Module --- --- load_state_dict(state_dict, strict=True) \u5c06 state_dict \u4e2d\u7684 parameters \u548c buffers \u590d\u5236\u5230\u6b64\u6a21\u5757\u548c\u5b83\u7684\u5b50\u540e\u4ee3\u4e2d. \u5982\u679c strict \u4e3a True , \u5219 state_dict \u7684 key \u5fc5\u987b\u548c\u6a21\u5757\u7684 state_dict() \u51fd\u6570\u8fd4\u56de\u7684 key \u4e00\u81f4. | Parameters: | state_dict ( dict ) \u2013 \u4e00\u4e2a\u5305\u542b parameters \u548c persistent buffers\uff08\u6301\u4e45\u5316\u7f13\u5b58\u7684\uff09\u5b57\u5178. strict ( bool ) \u2013 \u4e25\u683c\u7684\u5f3a\u5236 state_dict \u5c5e\u6027\u4e2d\u7684 key \u4e0e\u8be5\u6a21\u5757\u7684\u51fd\u6570 <cite>:func:`state_dict()</cite> \u8fd4\u56de\u7684 keys \u76f8\u5339\u914d. | | --- | --- | modules() \u8fd4\u56de\u4e00\u4e2a\u8986\u76d6\u795e\u7ecf\u7f51\u7edc\u4e2d\u6240\u6709\u6a21\u5757\u7684 iterator\uff08\u8fed\u4ee3\u5668\uff09. Yields: Module \u2013 a module in the network Note \u91cd\u590d\u7684\u6a21\u5757\u53ea\u8fd4\u56de\u4e00\u6b21. \u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d, 1 \u53ea\u4f1a\u88ab\u8fd4\u56de\u4e00\u6b21. example, l will be returned only once. >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): >>> print(idx, '->', m) 0 -> Sequential ( (0): Linear (2 -> 2) (1): Linear (2 -> 2) ) 1 -> Linear (2 -> 2) named_children() \u8fd4\u56de\u4e00\u4e2a iterator\uff08\u8fed\u4ee3\u5668\uff09, \u800c\u4e0d\u662f\u6700\u63a5\u8fd1\u7684\u5b50\u6a21\u5757, \u4ea7\u751f\u6a21\u5757\u7684 name \u4ee5\u53ca\u6a21\u5757\u672c\u8eab. Yields: (string, Module) \u2013 \u5305\u542b\u540d\u79f0\u548c\u5b50\u6a21\u5757\u7684 Tuple\uff08\u5143\u7ec4\uff09 Example >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) named_modules(memo=None, prefix='') \u8fd4\u56de\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u4e2d\u6240\u6709\u6a21\u5757\u7684 iterator\uff08\u8fed\u4ee3\u5668\uff09, \u4ea7\u751f\u6a21\u5757\u7684 name \u4ee5\u53ca\u6a21\u5757\u672c\u8eab. Yields: (string, Module) \u2013 \u540d\u5b57\u548c\u6a21\u5757\u7684 Tuple\uff08\u5143\u7ec4\uff09 Note \u91cd\u590d\u7684\u6a21\u5757\u53ea\u8fd4\u56de\u4e00\u6b21. \u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d, 1 \u53ea\u4f1a\u88ab\u8fd4\u56de\u4e00\u6b21. >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): >>> print(idx, '->', m) 0 -> ('', Sequential ( (0): Linear (2 -> 2) (1): Linear (2 -> 2) )) 1 -> ('0', Linear (2 -> 2)) named_parameters(memo=None, prefix='') \u8fd4\u56de\u6a21\u5757\u53c2\u6570\u7684\u8fed\u4ee3\u5668, \u4ea7\u751f\u53c2\u6570\u7684\u540d\u79f0\u4ee5\u53ca\u53c2\u6570\u672c\u8eab Yields: (string, Parameter) \u2013 Tuple \u5305\u542b\u540d\u79f0\u5f88\u53c2\u6570\u7684 Tuple\uff08\u5143\u7ec4\uff09 Example >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) parameters() \u8fd4\u56de\u4e00\u4e2a\u6a21\u5757\u53c2\u6570\u7684\u8fed\u4ee3\u5668. \u8fd9\u901a\u5e38\u4f20\u9012\u7ed9\u4f18\u5316\u5668. Yields: Parameter \u2013 \u6a21\u578b\u53c2\u6570 Example >>> for param in model.parameters(): >>> print(type(param.data), param.size()) <class 'torch.FloatTensor'> (20L,) <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L) register_backward_hook(hook) \u5728\u6a21\u5757\u4e0a\u6ce8\u518c\u4e00\u4e2a backward hook\uff08\u53cd\u5411\u94a9\u5b50\uff09. \u6bcf\u6b21\u8ba1\u7b97\u5173\u4e8e\u6a21\u5757\u8f93\u5165\u7684\u68af\u5ea6\u65f6, \u90fd\u4f1a\u8c03\u7528\u8be5\u94a9\u5b50. \u94a9\u5b50\u5e94\u8be5\u6709\u4ee5\u4e0b\u7ed3\u6784: hook(module, grad_input, grad_output) -> Tensor or None \u5982\u679c module \u6709\u591a\u4e2a\u8f93\u5165\u6216\u8f93\u51fa\u7684\u8bdd, \u90a3\u4e48 grad_input \u548c grad_output \u5c06\u4f1a\u662f\u4e2a tuple. hook \u4e0d\u5e94\u8be5\u4fee\u6539\u5b83\u7684\u53c2\u6570, \u4f46\u662f\u5b83\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5173\u4e8e\u8f93\u5165\u7684\u68af\u5ea6, \u8fd9\u4e2a\u8fd4\u56de\u7684\u68af\u5ea6\u5728\u540e\u7eed\u7684\u8ba1\u7b97\u4e2d\u4f1a\u66ff\u4ee3 grad_input . Returns: \u901a\u8fc7\u8c03\u7528 handle.remove() \u65b9\u6cd5\u53ef\u4ee5\u5220\u9664\u6dfb\u52a0\u94a9\u5b50\u7684\u53e5\u67c4 handle.remove() Return type: torch.utils.hooks.RemovableHandle --- --- register_buffer(name, tensor) \u7ed9\u6a21\u5757\u6dfb\u52a0\u4e00\u4e2a\u6301\u4e45\u5316\u7684 buffer. \u6301\u4e45\u5316\u7684 buffer \u901a\u5e38\u88ab\u7528\u5728\u8fd9\u4e48\u4e00\u79cd\u60c5\u51b5: \u6211\u4eec\u9700\u8981\u4fdd\u5b58\u4e00\u4e2a\u72b6\u6001, \u4f46\u662f\u8fd9\u4e2a\u72b6\u6001\u4e0d\u80fd\u770b\u4f5c\u6210\u4e3a\u6a21\u578b\u53c2\u6570. \u4f8b\u5982: BatchNorm \u7684 running_mean \u4e0d\u662f\u4e00\u4e2a parameter, \u4f46\u662f\u5b83\u4e5f\u662f\u9700\u8981\u4fdd\u5b58\u7684\u72b6\u6001\u4e4b\u4e00. Buffers \u53ef\u4ee5\u4f7f\u7528\u6307\u5b9a\u7684 name \u4f5c\u4e3a\u5c5e\u6027\u8bbf\u95ee. | Parameters: | name ( string ) \u2013 buffer \u7684\u540d\u79f0. \u53ef\u4ee5\u4f7f\u7528\u6307\u5b9a\u7684 name \u4ece\u8be5\u6a21\u5757\u8bbf\u95ee buffer tensor ( Tensor ) \u2013 \u88ab\u6ce8\u518c\u7684 buffer. | | --- | --- | Example >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook(hook) \u5728\u6a21\u5757\u4e0a\u6ce8\u518c\u4e00\u4e2a forward hook\uff08\u524d\u5411\u94a9\u5b50\uff09. \u6bcf\u4e00\u6b21 forward() \u51fd\u6570\u8ba1\u7b97\u51fa\u4e00\u4e2a\u8f93\u51fa\u540e, \u8be5\u94a9\u5b50\u5c06\u4f1a\u88ab\u8c03\u7528. \u5b83\u5e94\u8be5\u5177\u6709\u4ee5\u4e0b\u7ed3\u6784 hook(module, input, output) -> None \u8be5\u94a9\u5b50\u5e94\u8be5\u4e0d\u4f1a\u4fee\u6539\u8f93\u5165\u6216\u8f93\u51fa. Returns: \u901a\u8fc7\u8c03\u7528 handle.remove() \u65b9\u6cd5\u53ef\u4ee5\u5220\u9664\u6dfb\u52a0\u94a9\u5b50\u7684\u53e5\u67c4 Return type: torch.utils.hooks.RemovableHandle --- --- register_forward_pre_hook(hook) \u5728\u6a21\u5757\u4e0a\u6ce8\u518c\u4e00\u4e2a\u9884\u524d\u5411\u94a9\u5b50. \u6bcf\u4e00\u6b21\u5728\u8c03\u7528 forward() \u51fd\u6570\u524d\u90fd\u4f1a\u8c03\u7528\u8be5\u94a9\u5b50. \u5b83\u5e94\u8be5\u6709\u4ee5\u4e0b\u7ed3\u6784: hook(module, input) -> None \u8be5\u94a9\u5b50\u4e0d\u5e94\u8be5\u4fee\u6539\u8f93\u5165. Returns: \u901a\u8fc7\u8c03\u7528 handle.remove() \u65b9\u6cd5\u53ef\u4ee5\u5220\u9664\u6dfb\u52a0\u94a9\u5b50\u7684\u53e5\u67c4 handle.remove() Return type: torch.utils.hooks.RemovableHandle --- --- register_parameter(name, param) \u6dfb\u52a0\u4e00\u4e2a\u53c2\u6570\u5230\u6a21\u5757\u4e2d. \u53ef\u4ee5\u4f7f\u7528\u6307\u5b9a\u7684 name \u5c5e\u6027\u6765\u8bbf\u95ee\u53c2\u6570. | Parameters: | name ( string ) \u2013 \u53c2\u6570\u540d. \u53ef\u4ee5\u4f7f\u7528\u6307\u5b9a\u7684 name \u6765\u4ece\u8be5\u6a21\u5757\u4e2d\u8bbf\u95ee\u53c2\u6570 parameter ( Parameter ) \u2013 \u8981\u88ab\u6dfb\u52a0\u5230\u6a21\u5757\u7684\u53c2\u6570. | | --- | --- | state_dict(destination=None, prefix='', keep_vars=False) \u8fd4\u56de\u4e00\u4e2a\u5b57\u5178, \u5b83\u5305\u542b\u6574\u4e2a\u6a21\u5757\u7684\u72b6\u6001. \u5305\u62ec\u53c2\u6570\u548c\u6301\u4e45\u5316\u7684\u7f13\u51b2\u533a (\u4f8b\u5982. \u8fd0\u884c\u4e2d\u7684\u5e73\u5747\u503c). Keys \u662f\u4e0e\u4e4b\u5bf9\u5e94\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a\u7684 name. \u5f53 keep_vars \u4e3a True \u65f6, \u5b83\u4e3a\u6bcf\u4e00\u4e2a\u53c2\u6570\uff08\u800c\u4e0d\u662f\u4e00\u4e2a\u5f20\u91cf\uff09\u8fd4\u56de\u4e00\u4e2a Variable. | Parameters: | destination ( dict , optional ) \u2013 \u5982\u679c\u4e0d\u662f None, \u8be5\u8fd4\u56de\u7684\u5b57\u5178\u5e94\u8be5\u88ab\u5b58\u50a8\u5230 destination \u4e2d. Default: None prefix ( string , optional ) \u2013 \u5411\u7ed3\u679c\u5b57\u5178\u4e2d\u7684\u6bcf\u4e2a\u53c2\u6570\u548c\u7f13\u51b2\u533a\u7684 key\uff08\u540d\u79f0\uff09\u6dfb\u52a0\u4e00\u4e2a\u524d\u7f00. Default: \u2018\u2019 keep_vars ( bool , optional ) \u2013 \u5982\u679c\u4e3a True , \u4e3a\u6bcf\u4e00\u4e2a\u53c2\u6570\u8fd4\u56de\u4e00\u4e2a Variable. \u5982\u679c\u4e3a False , \u4e3a\u6bcf\u4e00\u4e2a\u53c2\u6570\u8fd4\u56de\u4e00\u4e2a Tensor. Default: False | | --- | --- | | Returns: | \u5305\u542b\u6a21\u5757\u6574\u4f53\u72b6\u6001\u7684\u5b57\u5178 | | --- | --- | | Return type: | dict | | --- | --- | Example >>> module.state_dict().keys() ['bias', 'weight'] train(mode=True) \u8bbe\u7f6e\u6a21\u5757\u4e3a\u8bad\u7ec3\u6a21\u5f0f. \u8fd9\u53ea\u5bf9\u8bf8\u5982 Dropout \u6216 BatchNorm \u7b49\u6a21\u5757\u65f6\u624d\u4f1a\u6709\u5f71\u54cd. Returns: self Return type: Module --- --- type(dst_type) \u8f6c\u6362\u6240\u6709\u53c2\u6570\u548c\u7f13\u51b2\u533a\u4e3a dst_type. Parameters: dst_type ( type or string ) \u2013 \u7406\u60f3\u7684\u7c7b\u578b Returns: self --- --- Return type: Module --- --- zero_grad() \u5c06\u6240\u6709\u6a21\u578b\u53c2\u6570\u7684\u68af\u5ea6\u8bbe\u7f6e\u4e3a\u96f6.","title":"Module"},{"location":"57/#sequential","text":"class torch.nn.Sequential(*args) \u4e00\u4e2a\u987a\u5e8f\u7684\u5bb9\u5668. \u6a21\u5757\u5c06\u6309\u7167\u5b83\u4eec\u5728\u6784\u9020\u51fd\u6570\u4e2d\u4f20\u9012\u7684\u987a\u5e8f\u6dfb\u52a0\u5230\u5b83. \u6216\u8005, \u4e5f\u53ef\u4ee5\u4f20\u5165\u6a21\u5757\u7684\u6709\u5e8f\u5b57\u5178. \u4e3a\u4e86\u66f4\u5bb9\u6613\u7406\u89e3, \u5217\u4e3e\u5c0f\u4f8b\u6765\u8bf4\u660e # \u4f7f\u7528 Sequential \u7684\u4f8b\u5b50 model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # \u4e0e OrderedDict \u4e00\u8d77\u4f7f\u7528 Sequential \u7684\u4f8b\u5b50 model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))","title":"Sequential"},{"location":"57/#modulelist","text":"class torch.nn.ModuleList(modules=None) \u5c06\u5b50\u6a21\u5757\u653e\u5165\u4e00\u4e2a list \u4e2d. ModuleList \u53ef\u4ee5\u50cf\u666e\u901a\u7684 Python list \u4e00\u6837\u88ab\u7d22\u5f15, \u4f46\u662f\u5b83\u5305\u542b\u7684\u6a21\u5757\u5df2\u7ecf\u88ab\u6b63\u786e\u7684\u6ce8\u518c\u4e86, \u5e76\u4e14\u6240\u6709\u7684 Module \u65b9\u6cd5\u90fd\u662f\u53ef\u89c1\u7684. Parameters: modules ( list , optional ) \u2013 \u8981\u6dfb\u52a0\u7684\u6a21\u5757\u5217\u8868 Example: class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)]) def forward(self, x): # ModuleList can act as an iterable, or be indexed using ints for i, l in enumerate(self.linears): x = self.linears[i // 2](x) + l(x) return x append(module) \u6dfb\u52a0\u4e00\u4e2a\u6307\u5b9a\u7684\u6a21\u5757\u5230 list \u5c3e\u90e8. Parameters: module ( nn.Module ) \u2013 \u8981\u88ab\u6dfb\u52a0\u7684\u6a21\u5757 extend(modules) \u5728\u6700\u540e\u6dfb\u52a0 Python list \u4e2d\u7684\u6a21\u5757. Parameters: modules ( list ) \u2013 \u8981\u88ab\u6dfb\u52a0\u7684\u6a21\u5757\u5217\u8868","title":"ModuleList"},{"location":"57/#parameterlist","text":"class torch.nn.ParameterList(parameters=None) \u4fdd\u5b58 list \u4e2d\u7684 parameter. ParameterList \u53ef\u4ee5\u50cf\u666e\u901a\u7684 Python list \u90a3\u6837\u88ab\u7d22\u5f15, \u4f46\u662f\u5b83\u6240\u5305\u542b\u7684\u53c2\u6570\u88ab\u6b63\u786e\u7684\u6ce8\u518c\u4e86, \u5e76\u4e14\u6240\u6709\u7684 Module \u65b9\u6cd5\u90fd\u53ef\u89c1\u7684. Parameters: modules ( list , optional ) \u2013 \u8981\u88ab\u6dfb\u52a0\u7684 `Parameter`` \u5217\u8868 Example: class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)]) def forward(self, x): # ModuleList \u53ef\u4ee5\u5145\u5f53 iterable\uff08\u8fed\u4ee3\u5668\uff09, \u6216\u8005\u53ef\u4ee5\u4f7f\u7528\u6574\u6570\u8fdb\u884c\u7d22\u5f15 for i, p in enumerate(self.params): x = self.params[i // 2].mm(x) + p.mm(x) return x append(parameter) \u6dfb\u52a0\u4e00\u4e2a\u6307\u5b9a\u7684\u53c2\u6570\u5230 list \u5c3e\u90e8. Parameters: parameter ( nn.Parameter ) \u2013 parameter to append extend(parameters) \u5728\u6700\u540e\u6dfb\u52a0 Python list \u4e2d\u7684\u53c2\u6570. Parameters: parameters ( list ) \u2013 list of parameters to append","title":"ParameterList"},{"location":"57/#convolution-layers","text":"","title":"Convolution Layers (\u5377\u79ef\u5c42)"},{"location":"57/#conv1d","text":"class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) \u4e00\u7ef4\u5377\u79ef\u5c42 \u8f93\u5165\u77e9\u9635\u7684\u7ef4\u5ea6\u4e3a \\((N, C_{in}, L)\\) , \u8f93\u51fa\u77e9\u9635\u7ef4\u5ea6\u4e3a \\((N, C_{out}, L_{out})\\) . \u5176\u4e2dN\u4e3a\u8f93\u5165\u6570\u91cf, C\u4e3a\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u7684\u901a\u9053\u6570\u91cf, L\u4e3a\u6837\u672c\u4e2d\u4e00\u4e2a\u901a\u9053\u4e0b\u7684\u6570\u636e\u7684\u957f\u5ea6. \u7b97\u6cd5\u5982\u4e0b: \\[\\begin{array}{ll} out(N_i, C_{out_j}) = bias(C_{out_j}) + \\sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k) \\star input(N_i, k) \\end{array}\\] \\(\\star\\) \u662f\u4e92\u76f8\u5173\u8fd0\u7b97\u7b26, \u4e0a\u5f0f\u5e26 \\(\\star\\) \u9879\u4e3a\u5377\u79ef\u9879. stride \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u7684\u6b65\u957f, \u53ef\u4ee5\u4e3a tuple . padding \u5904\u7406\u8fb9\u754c\u65f6\u5728\u4e24\u4fa7\u88650\u6570\u91cf dilation \u91c7\u6837\u95f4\u9694\u6570\u91cf. \u5927\u4e8e1\u65f6\u4e3a\u975e\u81f4\u5bc6\u91c7\u6837, \u5982\u5bf9(a,b,c,d,e)\u91c7\u6837\u65f6, \u82e5\u6c60\u5316\u89c4\u6a21\u4e3a2, dilation \u4e3a1\u65f6, \u4f7f\u7528 (a,b);(b,c)\u2026 \u8fdb\u884c\u6c60\u5316, dilation \u4e3a1\u65f6, \u4f7f\u7528 (a,c);(b,d)\u2026 \u8fdb\u884c\u6c60\u5316. | groups \u63a7\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5, group=1, \u8f93\u51fa\u662f\u6240\u6709\u8f93\u5165\u7684\u5377\u79ef\uff1bgroup=2, \u6b64\u65f6\u76f8\u5f53\u4e8e \u6709\u5e76\u6392\u7684\u4e24\u4e2a\u5377\u57fa\u5c42, \u6bcf\u4e2a\u5377\u79ef\u5c42\u53ea\u5728\u5bf9\u5e94\u7684\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u8ba1\u7b97, \u5e76\u4e14\u8f93\u51fa\u65f6\u4f1a\u5c06\u6240\u6709 \u8f93\u51fa\u901a\u9053\u7b80\u5355\u7684\u9996\u5c3e\u76f8\u63a5\u4f5c\u4e3a\u7ed3\u679c\u8f93\u51fa. <cite>in_channels</cite> \u548c [ ](#id1)out_channels \u90fd\u8981\u53ef\u4ee5\u88ab groups \u6574\u9664. Note \u6570\u636e\u7684\u6700\u540e\u4e00\u5217\u53ef\u80fd\u4f1a\u56e0\u4e3a kernal \u5927\u5c0f\u8bbe\u5b9a\u4e0d\u5f53\u800c\u88ab\u4e22\u5f03\uff08\u5927\u90e8\u5206\u53d1\u751f\u5728 kernal \u5927\u5c0f\u4e0d\u80fd\u88ab\u8f93\u5165 \u6574\u9664\u7684\u65f6\u5019, \u9002\u5f53\u7684 padding \u53ef\u4ee5\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff09. | Parameters: | in_channels ( - ) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u6570. out_channels ( - ) \u2013 \u5377\u79ef\u540e\u8f93\u51fa\u7ed3\u679c\u7684\u901a\u9053\u6570. kernel_size ( - ) \u2013 \u5377\u79ef\u6838\u7684\u5f62\u72b6. stride ( - ) \u2013 \u5377\u79ef\u6bcf\u6b21\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u4e3a1. padding ( - ) \u2013 \u5904\u7406\u8fb9\u754c\u65f6\u586b\u51450\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0(\u4e0d\u586b\u5145). dilation ( - ) \u2013 \u91c7\u6837\u95f4\u9694\u6570\u91cf, \u9ed8\u8ba4\u4e3a1, \u65e0\u95f4\u9694\u91c7\u6837. groups ( - ) \u2013 \u8f93\u5165\u4e0e\u8f93\u51fa\u901a\u9053\u7684\u5206\u7ec4\u6570\u91cf. \u5f53\u4e0d\u4e3a1\u65f6, \u9ed8\u8ba4\u4e3a1(\u5168\u8fde\u63a5). bias ( - ) \u2013 \u4e3a True \u65f6, \u6dfb\u52a0\u504f\u7f6e. | | --- | --- | Shape: \u8f93\u5165 Input: \\((N, C_{in}, L_{in})\\) \u8f93\u51fa Output: \\((N, C_{out}, L_{out})\\) \u5176\u4e2d \\(L_{out} = floor((L_{in} + 2 * padding - dilation * (kernel\\_size - 1) - 1) / stride + 1)\\) | Variables: | weight ( Tensor ) \u2013 \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels, in_channels, kernel_size) bias ( Tensor ) \u2013 \u504f\u7f6e, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels) | | --- | --- | Examples: >>> m = nn.Conv1d(16, 33, 3, stride=2) >>> input = autograd.Variable(torch.randn(20, 16, 50)) >>> output = m(input)","title":"Conv1d"},{"location":"57/#conv2d","text":"class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) \u4e8c\u7ef4\u5377\u79ef\u5c42 \u8f93\u5165\u77e9\u9635\u7684\u7ef4\u5ea6\u4e3a \\((N, C_{in}, H, W)\\) , \u8f93\u51fa\u77e9\u9635\u7ef4\u5ea6\u4e3a \\((N, C_{out}, H_{out}, W_{out})\\) . \u5176\u4e2dN\u4e3a\u8f93\u5165\u6570\u91cf, C\u4e3a\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u7684\u901a\u9053\u6570\u91cf, H, W \u5206\u522b\u4e3a\u6837\u672c\u4e2d\u4e00\u4e2a\u901a\u9053\u4e0b\u7684\u6570\u636e\u7684\u5f62\u72b6. \u7b97\u6cd5\u5982\u4e0b: \\[\\begin{array}{ll} out(N_i, C_{out_j}) = bias(C_{out_j}) + \\sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k) \\star input(N_i, k) \\end{array}\\] \\(\\star\\) \u662f\u4e92\u76f8\u5173\u8fd0\u7b97\u7b26, \u4e0a\u5f0f\u5e26:math:[ ](#id3)star \u9879\u4e3a\u5377\u79ef\u9879. stride \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u7684\u6b65\u957f, \u53ef\u4ee5\u4e3a tuple . padding \u5904\u7406\u8fb9\u754c\u65f6\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u6570\u91cf. dilation \u91c7\u6837\u95f4\u9694\u6570\u91cf. \u5927\u4e8e1\u65f6\u4e3a\u975e\u81f4\u5bc6\u91c7\u6837. groups \u63a7\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5, group=1, \u8f93\u51fa\u662f\u6240\u6709\u8f93\u5165\u7684\u5377\u79ef\uff1b group=2, \u6b64\u65f6 \u76f8\u5f53\u4e8e\u6709\u5e76\u6392\u7684\u4e24\u4e2a\u5377\u57fa\u5c42, \u6bcf\u4e2a\u5377\u79ef\u5c42\u53ea\u5728\u5bf9\u5e94\u7684\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u8ba1\u7b97, \u5e76\u4e14\u8f93\u51fa\u65f6\u4f1a\u5c06\u6240\u6709 \u8f93\u51fa\u901a\u9053\u7b80\u5355\u7684\u9996\u5c3e\u76f8\u63a5\u4f5c\u4e3a\u7ed3\u679c\u8f93\u51fa. <cite>in_channels</cite> \u548c [ ](#id5)out_channels \u90fd\u8981\u53ef\u4ee5\u88ab groups \u6574\u9664. kernel_size , stride , padding , dilation \u53ef\u4ee5\u4e3a: \u5355\u4e2a int \u503c \u2013 \u5bbd\u548c\u9ad8\u5747\u88ab\u8bbe\u5b9a\u4e3a\u6b64\u503c. \u7531\u4e24\u4e2a int \u7ec4\u6210\u7684 tuple \u2013 \u7b2c\u4e00\u4e2a int \u4e3a\u9ad8, \u7b2c\u4e8c\u4e2a int \u4e3a\u5bbd. Note \u6570\u636e\u7684\u6700\u540e\u4e00\u5217\u53ef\u80fd\u4f1a\u56e0\u4e3a kernal \u5927\u5c0f\u8bbe\u5b9a\u4e0d\u5f53\u800c\u88ab\u4e22\u5f03\uff08\u5927\u90e8\u5206\u53d1\u751f\u5728 kernal \u5927\u5c0f\u4e0d\u80fd\u88ab\u8f93\u5165 \u6574\u9664\u7684\u65f6\u5019, \u9002\u5f53\u7684 padding \u53ef\u4ee5\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff09. | Parameters: | in_channels ( - ) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u6570. out_channels ( - ) \u2013 \u5377\u79ef\u540e\u8f93\u51fa\u7ed3\u679c\u7684\u901a\u9053\u6570. kernel_size ( - ) \u2013 \u5377\u79ef\u6838\u7684\u5f62\u72b6. stride ( - ) \u2013 \u5377\u79ef\u6bcf\u6b21\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u4e3a1. padding ( - ) \u2013 \u5904\u7406\u8fb9\u754c\u65f6\u586b\u51450\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0(\u4e0d\u586b\u5145). dilation ( - ) \u2013 \u91c7\u6837\u95f4\u9694\u6570\u91cf, \u9ed8\u8ba4\u4e3a1, \u65e0\u95f4\u9694\u91c7\u6837. groups ( - ) \u2013 \u8f93\u5165\u4e0e\u8f93\u51fa\u901a\u9053\u7684\u5206\u7ec4\u6570\u91cf. \u5f53\u4e0d\u4e3a1\u65f6, \u9ed8\u8ba4\u4e3a1(\u5168\u8fde\u63a5). bias ( - ) \u2013 \u4e3a True \u65f6, \u6dfb\u52a0\u504f\u7f6e. | | --- | --- | Shape: \u8f93\u5165 Input: \\((N, C_{in}, H_{in}, W_{in})\\) \u8f93\u51fa Output: \\((N, C_{out}, H_{out}, W_{out})\\) \u5176\u4e2d \\(H_{out} = floor((H_{in} + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[1] - dilation[1] * (kernel\\_size[1] - 1) - 1) / stride[1] + 1)\\) | Variables: | weight ( Tensor ) \u2013 \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels, in_channels, kernel_size[0], kernel_size[1]) bias ( Tensor ) \u2013 \u504f\u7f6e, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels) | | --- | --- | Examples: >>> # With square kernels and equal stride >>> m = nn.Conv2d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) >>> # non-square kernels and unequal stride and with padding and dilation >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) >>> input = autograd.Variable(torch.randn(20, 16, 50, 100)) >>> output = m(input)","title":"Conv2d"},{"location":"57/#conv3d","text":"class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) \u4e09\u7ef4\u5377\u57fa\u5c42 \u8f93\u5165\u77e9\u9635\u7684\u7ef4\u5ea6\u4e3a \\((N, C_{in}, D, H, W)\\) , \u8f93\u51fa\u77e9\u9635\u7ef4\u5ea6\u4e3a: \\((N, C_{out}, D_{out}, H_{out}, W_{out})\\) . \u5176\u4e2dN\u4e3a\u8f93\u5165\u6570\u91cf, C\u4e3a\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u7684\u901a\u9053\u6570\u91cf, D, H, W \u5206\u522b\u4e3a\u6837\u672c\u4e2d\u4e00\u4e2a\u901a\u9053\u4e0b\u7684\u6570\u636e\u7684\u5f62\u72b6. \u7b97\u6cd5\u5982\u4e0b: \\[\\begin{array}{ll} out(N_i, C_{out_j}) = bias(C_{out_j}) + \\sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k) \\star input(N_i, k) \\end{array}\\] \\(\\star\\) \u662f\u4e92\u76f8\u5173\u8fd0\u7b97\u7b26, \u4e0a\u5f0f\u5e26:math:[ ](#id9)star \u9879\u4e3a\u5377\u79ef\u9879. stride \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u7684\u6b65\u957f, \u53ef\u4ee5\u4e3a tuple . padding \u5904\u7406\u8fb9\u754c\u65f6\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u6570\u91cf. dilation \u91c7\u6837\u95f4\u9694\u6570\u91cf. \u5927\u4e8e1\u65f6\u4e3a\u975e\u81f4\u5bc6\u91c7\u6837. groups \u63a7\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5, group=1, \u8f93\u51fa\u662f\u6240\u6709\u8f93\u5165\u7684\u5377\u79ef\uff1b group=2, \u6b64\u65f6 \u76f8\u5f53\u4e8e\u6709\u5e76\u6392\u7684\u4e24\u4e2a\u5377\u57fa\u5c42, \u6bcf\u4e2a\u5377\u79ef\u5c42\u53ea\u5728\u5bf9\u5e94\u7684\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u8ba1\u7b97, \u5e76\u4e14\u8f93\u51fa\u65f6\u4f1a\u5c06\u6240\u6709 \u8f93\u51fa\u901a\u9053\u7b80\u5355\u7684\u9996\u5c3e\u76f8\u63a5\u4f5c\u4e3a\u7ed3\u679c\u8f93\u51fa. <cite>in_channels</cite> \u548c [ ](#id11)out_channels \u90fd\u8981\u53ef\u4ee5\u88ab groups \u6574\u9664. kernel_size , stride , padding , dilation \u53ef\u4ee5\u4e3a: \u5355\u4e2a int \u503c \u2013 \u5bbd\u548c\u9ad8\u548c\u6df1\u5ea6\u5747\u88ab\u8bbe\u5b9a\u4e3a\u6b64\u503c. \u7531\u4e09\u4e2a int \u7ec4\u6210\u7684 tuple \u2013 \u7b2c\u4e00\u4e2a int \u4e3a\u6df1\u5ea6, \u7b2c\u4e8c\u4e2a int \u4e3a\u9ad8\u5ea6, \u7b2c\u4e09\u4e2a int \u4e3a\u5bbd\u5ea6. Note \u6570\u636e\u7684\u6700\u540e\u4e00\u5217\u53ef\u80fd\u4f1a\u56e0\u4e3a kernal \u5927\u5c0f\u8bbe\u5b9a\u4e0d\u5f53\u800c\u88ab\u4e22\u5f03\uff08\u5927\u90e8\u5206\u53d1\u751f\u5728 kernal \u5927\u5c0f\u4e0d\u80fd\u88ab\u8f93\u5165 \u6574\u9664\u7684\u65f6\u5019, \u9002\u5f53\u7684 padding \u53ef\u4ee5\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff09. | Parameters: | in_channels ( - ) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u6570. out_channels ( - ) \u2013 \u5377\u79ef\u540e\u8f93\u51fa\u7ed3\u679c\u7684\u901a\u9053\u6570. kernel_size ( - ) \u2013 \u5377\u79ef\u6838\u7684\u5f62\u72b6. stride ( - ) \u2013 \u5377\u79ef\u6bcf\u6b21\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u4e3a1. padding ( - ) \u2013 \u5904\u7406\u8fb9\u754c\u65f6\u586b\u51450\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0(\u4e0d\u586b\u5145). dilation ( - ) \u2013 \u91c7\u6837\u95f4\u9694\u6570\u91cf, \u9ed8\u8ba4\u4e3a1, \u65e0\u95f4\u9694\u91c7\u6837. groups ( - ) \u2013 \u8f93\u5165\u4e0e\u8f93\u51fa\u901a\u9053\u7684\u5206\u7ec4\u6570\u91cf. \u5f53\u4e0d\u4e3a1\u65f6, \u9ed8\u8ba4\u4e3a1(\u5168\u8fde\u63a5). bias ( - ) \u2013 \u4e3a True \u65f6, \u6dfb\u52a0\u504f\u7f6e. | | --- | --- | Shape: \u8f93\u5165 Input: \\((N, C_{in}, D_{in}, H_{in}, W_{in})\\) \u8f93\u51fa Output: \\((N, C_{out}, D_{out}, H_{out}, W_{out})\\) \u5176\u4e2d \\(D_{out} = floor((D_{in} + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)\\) \\(H_{out} = floor((H_{in} + 2 * padding[1] - dilation[1] * (kernel\\_size[1] - 1) - 1) / stride[1] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[2] - dilation[2] * (kernel\\_size[2] - 1) - 1) / stride[2] + 1)\\) | Variables: | weight ( Tensor ) \u2013 \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2]) bias ( Tensor ) \u2013 \u504f\u7f6e, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels) | | --- | --- | Examples: >>> # With square kernels and equal stride >>> m = nn.Conv3d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0)) >>> input = autograd.Variable(torch.randn(20, 16, 10, 50, 100)) >>> output = m(input)","title":"Conv3d"},{"location":"57/#convtranspose1d","text":"class torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) \u4e00\u7ef4\u53cd\u5377\u79ef\u5c42 \u53cd\u5377\u79ef\u5c42\u53ef\u4ee5\u7406\u89e3\u4e3a\u8f93\u5165\u7684\u6570\u636e\u548c\u5377\u79ef\u6838\u7684\u4f4d\u7f6e\u53cd\u8f6c\u7684\u5377\u79ef\u64cd\u4f5c. \u53cd\u5377\u79ef\u6709\u65f6\u5019\u4e5f\u4f1a\u88ab\u7ffb\u8bd1\u6210\u89e3\u5377\u79ef. stride \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u7684\u6b65\u957f. padding \u5904\u7406\u8fb9\u754c\u65f6\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u6570\u91cf. output_padding \u8f93\u51fa\u65f6\u5019\u5728\u9996\u5c3e\u88650\u7684\u6570\u91cf. \uff08\u5377\u79ef\u65f6, \u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e \u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u5f62\u72b6\u76f8\u540c\u7684\u7ed3\u679c\uff1b\u53cd\u5377\u79ef\u65f6, \u540c\u4e00\u4e2a\u8f93\u5165\u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u591a \u4e2a\u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u51fa, \u800c\u8f93\u51fa\u7ed3\u679c\u53ea\u80fd\u6709\u4e00\u4e2a, \u56e0\u6b64\u5fc5\u987b\u5bf9\u8f93\u51fa\u5f62\u72b6\u8fdb\u884c\u7ea6\u675f\uff09. | dilation \u91c7\u6837\u95f4\u9694\u6570\u91cf. \u5927\u4e8e1\u65f6\u4e3a\u975e\u81f4\u5bc6\u91c7\u6837. | groups \u63a7\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5, group=1, \u8f93\u51fa\u662f\u6240\u6709\u8f93\u5165\u7684\u5377\u79ef\uff1b group=2, \u6b64\u65f6 \u76f8\u5f53\u4e8e\u6709\u5e76\u6392\u7684\u4e24\u4e2a\u5377\u57fa\u5c42, \u6bcf\u4e2a\u5377\u79ef\u5c42\u53ea\u5728\u5bf9\u5e94\u7684\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u8ba1\u7b97, \u5e76\u4e14\u8f93\u51fa\u65f6\u4f1a\u5c06\u6240\u6709 \u8f93\u51fa\u901a\u9053\u7b80\u5355\u7684\u9996\u5c3e\u76f8\u63a5\u4f5c\u4e3a\u7ed3\u679c\u8f93\u51fa. <cite>in_channels</cite> \u548c [ ](#id15)out_channels \u90fd\u8981\u53ef\u4ee5\u88ab groups \u6574\u9664. Note \u6570\u636e\u7684\u6700\u540e\u4e00\u5217\u53ef\u80fd\u4f1a\u56e0\u4e3a kernal \u5927\u5c0f\u8bbe\u5b9a\u4e0d\u5f53\u800c\u88ab\u4e22\u5f03\uff08\u5927\u90e8\u5206\u53d1\u751f\u5728 kernal \u5927\u5c0f\u4e0d\u80fd\u88ab\u8f93\u5165 \u6574\u9664\u7684\u65f6\u5019, \u9002\u5f53\u7684 padding \u53ef\u4ee5\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff09. | Parameters: | in_channels ( - ) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u6570. out_channels ( - ) \u2013 \u5377\u79ef\u540e\u8f93\u51fa\u7ed3\u679c\u7684\u901a\u9053\u6570. kernel_size ( - ) \u2013 \u5377\u79ef\u6838\u7684\u5f62\u72b6. stride ( - ) \u2013 \u5377\u79ef\u6bcf\u6b21\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u4e3a1. padding ( - ) \u2013 \u5904\u7406\u8fb9\u754c\u65f6\u586b\u51450\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0(\u4e0d\u586b\u5145). output_padding ( - ) \u2013 \u8f93\u51fa\u65f6\u5019\u5728\u9996\u5c3e\u8865\u503c\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0. \uff08\u5377\u79ef\u65f6, \u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e \u540c\u4e00\u4e2a\u8f93\u5165\u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u591a ( \u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u5f62\u72b6\u76f8\u540c\u7684\u7ed3\u679c\uff1b\u53cd\u5377\u79ef\u65f6__, ) \u2013 \u800c\u8f93\u51fa\u7ed3\u679c\u53ea\u80fd\u6709\u4e00\u4e2a, \u56e0\u6b64\u5fc5\u987b\u5bf9\u8f93\u51fa\u5f62\u72b6\u8fdb\u884c\u7ea6\u675f\uff09 ( \u4e2a\u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u51fa__, ) \u2013 groups ( - ) \u2013 \u8f93\u5165\u4e0e\u8f93\u51fa\u901a\u9053\u7684\u5206\u7ec4\u6570\u91cf. \u5f53\u4e0d\u4e3a1\u65f6, \u9ed8\u8ba4\u4e3a1(\u5168\u8fde\u63a5). bias ( - ) \u2013 \u4e3a True \u65f6, \u6dfb\u52a0\u504f\u7f6e. dilation ( - ) \u2013 \u91c7\u6837\u95f4\u9694\u6570\u91cf, \u9ed8\u8ba4\u4e3a1, \u65e0\u95f4\u9694\u91c7\u6837. | | --- | --- | Shape: \u8f93\u5165 Input: \\((N, C_{in}, L_{in})\\) \u8f93\u51fa Output: \\((N, C_{out}, L_{out})\\) \u5176\u4e2d \\(L_{out} = (L_{in} - 1) * stride - 2 * padding + kernel\\_size + output\\_padding\\) | Variables: | weight ( Tensor ) \u2013 \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3aweight (Tensor): \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (in_channels, out_channels, kernel_size[0], kernel_size[1]) bias ( Tensor ) \u2013 \u504f\u7f6e, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels) | | --- | --- |","title":"ConvTranspose1d"},{"location":"57/#convtranspose2d","text":"class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) \u4e8c\u7ef4\u53cd\u5377\u79ef\u5c42 \u53cd\u5377\u79ef\u5c42\u53ef\u4ee5\u7406\u89e3\u4e3a\u8f93\u5165\u7684\u6570\u636e\u548c\u5377\u79ef\u6838\u7684\u4f4d\u7f6e\u53cd\u8f6c\u7684\u5377\u79ef\u64cd\u4f5c. \u53cd\u5377\u79ef\u6709\u65f6\u5019\u4e5f\u4f1a\u88ab\u7ffb\u8bd1\u6210\u89e3\u5377\u79ef. stride \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u7684\u6b65\u957f. padding \u5904\u7406\u8fb9\u754c\u65f6\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u6570\u91cf. output_padding \u8f93\u51fa\u65f6\u5019\u5728\u6bcf\u4e00\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u7684\u6570\u91cf. \uff08\u5377\u79ef\u65f6, \u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e \u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u5f62\u72b6\u76f8\u540c\u7684\u7ed3\u679c\uff1b\u53cd\u5377\u79ef\u65f6, \u540c\u4e00\u4e2a\u8f93\u5165\u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u591a \u4e2a\u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u51fa, \u800c\u8f93\u51fa\u7ed3\u679c\u53ea\u80fd\u6709\u4e00\u4e2a, \u56e0\u6b64\u5fc5\u987b\u5bf9\u8f93\u51fa\u5f62\u72b6\u8fdb\u884c\u7ea6\u675f\uff09. | dilation \u91c7\u6837\u95f4\u9694\u6570\u91cf. \u5927\u4e8e1\u65f6\u4e3a\u975e\u81f4\u5bc6\u91c7\u6837. | groups \u63a7\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5, group=1, \u8f93\u51fa\u662f\u6240\u6709\u8f93\u5165\u7684\u5377\u79ef\uff1b group=2, \u6b64\u65f6 \u76f8\u5f53\u4e8e\u6709\u5e76\u6392\u7684\u4e24\u4e2a\u5377\u57fa\u5c42, \u6bcf\u4e2a\u5377\u79ef\u5c42\u53ea\u5728\u5bf9\u5e94\u7684\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u8ba1\u7b97, \u5e76\u4e14\u8f93\u51fa\u65f6\u4f1a\u5c06\u6240\u6709 \u8f93\u51fa\u901a\u9053\u7b80\u5355\u7684\u9996\u5c3e\u76f8\u63a5\u4f5c\u4e3a\u7ed3\u679c\u8f93\u51fa. <cite>in_channels</cite> \u548c [ ](#id17)out_channels \u90fd\u5e94\u5f53\u53ef\u4ee5\u88ab groups \u6574\u9664. kernel_size , stride , padding , output_padding \u53ef\u4ee5\u4e3a: \u5355\u4e2a int \u503c \u2013 \u5bbd\u548c\u9ad8\u5747\u88ab\u8bbe\u5b9a\u4e3a\u6b64\u503c. \u7531\u4e24\u4e2a int \u7ec4\u6210\u7684 tuple \u2013 \u7b2c\u4e00\u4e2a int \u4e3a\u9ad8\u5ea6, \u7b2c\u4e8c\u4e2a int \u4e3a\u5bbd\u5ea6. Note \u6570\u636e\u7684\u6700\u540e\u4e00\u5217\u53ef\u80fd\u4f1a\u56e0\u4e3a kernal \u5927\u5c0f\u8bbe\u5b9a\u4e0d\u5f53\u800c\u88ab\u4e22\u5f03\uff08\u5927\u90e8\u5206\u53d1\u751f\u5728 kernal \u5927\u5c0f\u4e0d\u80fd\u88ab\u8f93\u5165 \u6574\u9664\u7684\u65f6\u5019, \u9002\u5f53\u7684 padding \u53ef\u4ee5\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff09. | Parameters: | in_channels ( - ) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u6570. out_channels ( - ) \u2013 \u5377\u79ef\u540e\u8f93\u51fa\u7ed3\u679c\u7684\u901a\u9053\u6570. kernel_size ( - ) \u2013 \u5377\u79ef\u6838\u7684\u5f62\u72b6. stride ( - ) \u2013 \u5377\u79ef\u6bcf\u6b21\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u4e3a1. padding ( - ) \u2013 \u5904\u7406\u8fb9\u754c\u65f6\u586b\u51450\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0(\u4e0d\u586b\u5145). output_padding ( - ) \u2013 \u8f93\u51fa\u65f6\u5019\u5728\u9996\u5c3e\u8865\u503c\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0. \uff08\u5377\u79ef\u65f6, \u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e \u540c\u4e00\u4e2a\u8f93\u5165\u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u591a ( \u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u5f62\u72b6\u76f8\u540c\u7684\u7ed3\u679c\uff1b\u53cd\u5377\u79ef\u65f6__, ) \u2013 \u800c\u8f93\u51fa\u7ed3\u679c\u53ea\u80fd\u6709\u4e00\u4e2a, \u56e0\u6b64\u5fc5\u987b\u5bf9\u8f93\u51fa\u5f62\u72b6\u8fdb\u884c\u7ea6\u675f\uff09 ( \u4e2a\u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u51fa__, ) \u2013 groups ( - ) \u2013 \u8f93\u5165\u4e0e\u8f93\u51fa\u901a\u9053\u7684\u5206\u7ec4\u6570\u91cf. \u5f53\u4e0d\u4e3a1\u65f6, \u9ed8\u8ba4\u4e3a1(\u5168\u8fde\u63a5). bias ( - ) \u2013 \u4e3a True \u65f6, \u6dfb\u52a0\u504f\u7f6e. dilation ( - ) \u2013 \u91c7\u6837\u95f4\u9694\u6570\u91cf, \u9ed8\u8ba4\u4e3a1, \u65e0\u95f4\u9694\u91c7\u6837. | | --- | --- | Shape: \u8f93\u5165 Input: \\((N, C_{in}, H_{in}, W_{in})\\) \u8f93\u51fa Output: \\((N, C_{out}, H_{out}, W_{out})\\) \u5176\u4e2d \\(H_{out} = (H_{in} - 1) * stride[0] - 2 * padding[0] + kernel\\_size[0] + output\\_padding[0]\\) \\(W_{out} = (W_{in} - 1) * stride[1] - 2 * padding[1] + kernel\\_size[1] + output\\_padding[1]\\) | Variables: | weight ( Tensor ) \u2013 \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3aweight (Tensor): \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (in_channels, out_channels, kernel_size[0], kernel_size[1]) bias ( Tensor ) \u2013 \u504f\u7f6e, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels) | | --- | --- | Examples: >>> # With square kernels and equal stride >>> m = nn.ConvTranspose2d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) >>> input = autograd.Variable(torch.randn(20, 16, 50, 100)) >>> output = m(input) >>> # exact output size can be also specified as an argument >>> input = autograd.Variable(torch.randn(1, 16, 12, 12)) >>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1) >>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1) >>> h = downsample(input) >>> h.size() torch.Size([1, 16, 6, 6]) >>> output = upsample(h, output_size=input.size()) >>> output.size() torch.Size([1, 16, 12, 12])","title":"ConvTranspose2d"},{"location":"57/#convtranspose3d","text":"class torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) \u4e09\u7ef4\u53cd\u5377\u79ef\u5c42 \u53cd\u5377\u79ef\u5c42\u53ef\u4ee5\u7406\u89e3\u4e3a\u8f93\u5165\u7684\u6570\u636e\u548c\u5377\u79ef\u6838\u7684\u4f4d\u7f6e\u53cd\u8f6c\u7684\u5377\u79ef\u64cd\u4f5c. \u53cd\u5377\u79ef\u6709\u65f6\u5019\u4e5f\u4f1a\u88ab\u7ffb\u8bd1\u6210\u89e3\u5377\u79ef. stride \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u7684\u6b65\u957f. padding \u5904\u7406\u8fb9\u754c\u65f6\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u6570\u91cf. output_padding \u8f93\u51fa\u65f6\u5019\u5728\u6bcf\u4e00\u4e2a\u7ef4\u5ea6\u9996\u5c3e\u88650\u7684\u6570\u91cf. \uff08\u5377\u79ef\u65f6, \u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e \u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u5f62\u72b6\u76f8\u540c\u7684\u7ed3\u679c\uff1b\u53cd\u5377\u79ef\u65f6, \u540c\u4e00\u4e2a\u8f93\u5165\u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u591a \u4e2a\u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u51fa, \u800c\u8f93\u51fa\u7ed3\u679c\u53ea\u80fd\u6709\u4e00\u4e2a, \u56e0\u6b64\u5fc5\u987b\u5bf9\u8f93\u51fa\u5f62\u72b6\u8fdb\u884c\u7ea6\u675f\uff09 | dilation \u91c7\u6837\u95f4\u9694\u6570\u91cf. \u5927\u4e8e1\u65f6\u4e3a\u975e\u81f4\u5bc6\u91c7\u6837. | groups \u63a7\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5, group=1, \u8f93\u51fa\u662f\u6240\u6709\u8f93\u5165\u7684\u5377\u79ef\uff1b group=2, \u6b64\u65f6 \u76f8\u5f53\u4e8e\u6709\u5e76\u6392\u7684\u4e24\u4e2a\u5377\u57fa\u5c42, \u6bcf\u4e2a\u5377\u79ef\u5c42\u53ea\u5728\u5bf9\u5e94\u7684\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u8ba1\u7b97, \u5e76\u4e14\u8f93\u51fa\u65f6\u4f1a\u5c06\u6240\u6709 \u8f93\u51fa\u901a\u9053\u7b80\u5355\u7684\u9996\u5c3e\u76f8\u63a5\u4f5c\u4e3a\u7ed3\u679c\u8f93\u51fa. <cite>in_channels</cite> \u548c [ ](#id21)out_channels \u90fd\u5e94\u5f53\u53ef\u4ee5\u88ab groups \u6574\u9664. kernel_size , stride , padding , output_padding \u53ef\u4ee5\u4e3a: \u5355\u4e2a int \u503c \u2013 \u6df1\u548c\u5bbd\u548c\u9ad8\u5747\u88ab\u8bbe\u5b9a\u4e3a\u6b64\u503c. \u7531\u4e09\u4e2a int \u7ec4\u6210\u7684 tuple \u2013 \u7b2c\u4e00\u4e2a int \u4e3a\u6df1\u5ea6, \u7b2c\u4e8c\u4e2a int \u4e3a\u9ad8\u5ea6,\u7b2c\u4e09\u4e2a int \u4e3a\u5bbd\u5ea6. Note \u6570\u636e\u7684\u6700\u540e\u4e00\u5217\u53ef\u80fd\u4f1a\u56e0\u4e3a kernal \u5927\u5c0f\u8bbe\u5b9a\u4e0d\u5f53\u800c\u88ab\u4e22\u5f03\uff08\u5927\u90e8\u5206\u53d1\u751f\u5728 kernal \u5927\u5c0f\u4e0d\u80fd\u88ab\u8f93\u5165 \u6574\u9664\u7684\u65f6\u5019, \u9002\u5f53\u7684 padding \u53ef\u4ee5\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff09. | Parameters: | in_channels ( - ) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u6570. out_channels ( - ) \u2013 \u5377\u79ef\u540e\u8f93\u51fa\u7ed3\u679c\u7684\u901a\u9053\u6570. kernel_size ( - ) \u2013 \u5377\u79ef\u6838\u7684\u5f62\u72b6. stride ( - ) \u2013 \u5377\u79ef\u6bcf\u6b21\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u4e3a1. padding ( - ) \u2013 \u5904\u7406\u8fb9\u754c\u65f6\u586b\u51450\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0(\u4e0d\u586b\u5145). output_padding ( - ) \u2013 \u8f93\u51fa\u65f6\u5019\u5728\u9996\u5c3e\u8865\u503c\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a0. \uff08\u5377\u79ef\u65f6, \u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e \u540c\u4e00\u4e2a\u8f93\u5165\u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u591a ( \u5bf9\u76f8\u540c\u7684\u6838\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u5f62\u72b6\u76f8\u540c\u7684\u7ed3\u679c\uff1b\u53cd\u5377\u79ef\u65f6__, ) \u2013 \u800c\u8f93\u51fa\u7ed3\u679c\u53ea\u80fd\u6709\u4e00\u4e2a, \u56e0\u6b64\u5fc5\u987b\u5bf9\u8f93\u51fa\u5f62\u72b6\u8fdb\u884c\u7ea6\u675f\uff09 ( \u4e2a\u5f62\u72b6\u4e0d\u540c\u7684\u8f93\u51fa__, ) \u2013 groups ( - ) \u2013 \u8f93\u5165\u4e0e\u8f93\u51fa\u901a\u9053\u7684\u5206\u7ec4\u6570\u91cf. \u5f53\u4e0d\u4e3a1\u65f6, \u9ed8\u8ba4\u4e3a1(\u5168\u8fde\u63a5). bias ( - ) \u2013 \u4e3a True \u65f6, \u6dfb\u52a0\u504f\u7f6e. dilation ( - ) \u2013 \u91c7\u6837\u95f4\u9694\u6570\u91cf, \u9ed8\u8ba4\u4e3a1, \u65e0\u95f4\u9694\u91c7\u6837. | | --- | --- | Shape: \u8f93\u5165 Input: \\((N, C_{in}, D_{in}, H_{in}, W_{in})\\) \u8f93\u51fa Output: \\((N, C_{out}, D_{out}, H_{out}, W_{out})\\) \u5176\u4e2d \\(D_{out} = (D_{in} - 1) * stride[0] - 2 * padding[0] + kernel\\_size[0] + output\\_padding[0]\\) \\(H_{out} = (H_{in} - 1) * stride[1] - 2 * padding[1] + kernel\\_size[1] + output\\_padding[1]\\) \\(W_{out} = (W_{in} - 1) * stride[2] - 2 * padding[2] + kernel\\_size[2] + output\\_padding[2]\\) | Variables: | \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3aweight ( \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd__, ) \u2013 \u5377\u79ef\u7f51\u7edc\u5c42\u95f4\u8fde\u63a5\u7684\u6743\u91cd, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (in_channels, out_channels, kernel_size[0], kernel_size[1], kernel_size[2]) bias ( Tensor ) \u2013 \u504f\u7f6e, \u662f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u53d8\u91cf, \u5f62\u72b6\u4e3a (out_channels) | | --- | --- | Examples: >>> # With square kernels and equal stride >>> m = nn.ConvTranspose3d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2)) >>> input = autograd.Variable(torch.randn(20, 16, 10, 50, 100)) >>> output = m(input)","title":"ConvTranspose3d"},{"location":"57/#pooling-layers","text":"","title":"Pooling Layers (\u6c60\u5316\u5c42)"},{"location":"57/#maxpool1d","text":"class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e00\u7ef4\u7684\u6700\u5927\u6c60\u5316 max pooling \u64cd\u4f5c \u6700\u7b80\u5355\u7684\u4f8b\u5b50, \u5982\u679c\u8f93\u5165\u5927\u5c0f\u4e3a \\((N, C, L)\\) , \u8f93\u51fa\u5927\u5c0f\u4e3a \\((N, C, L_{out})\\) , \u8be5\u5c42\u8f93\u51fa\u503c\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7cbe\u786e\u8ba1\u7b97: \\[\\begin{array}{ll} out(N_i, C_j, k) = \\max_{{m}=0}^{{kernel\\_size}-1} input(N_i, C_j, stride * k + m) \\end{array}\\] \u5982\u679c padding \u4e0d\u662f0,\u90a3\u4e48\u5728\u8f93\u5165\u6570\u636e\u7684\u6bcf\u6761\u8fb9\u4e0a\u4f1a\u9690\u5f0f\u586b\u8865\u5bf9\u5e94 padding \u6570\u91cf\u76840\u503c\u70b9 dilation \u7528\u4e8e\u63a7\u5236\u5185\u6838\u70b9\u4e4b\u95f4\u7684\u95f4\u9694, link \u5f88\u597d\u5730\u53ef\u89c6\u5316\u5c55\u793a\u4e86 dilation \u7684\u529f\u80fd | Parameters: | kernel_size \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7684\u7a97\u53e3\u5927\u5c0f stride \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u9690\u5f0f\u88650\u7684\u6570\u91cf dilation \u2013 \u7528\u4e8e\u63a7\u5236\u7a97\u53e3\u4e2d\u5143\u7d20\u7684\u6b65\u957f\u7684\u53c2\u6570 return_indices \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8fd4\u56de max pooling \u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15. \u8fd9\u5728\u4e4b\u540e\u7684 Unpooling \u65f6\u5f88\u6709\u7528 ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f | | --- | --- | Shape: Input: \\((N, C, L_{in})\\) Output: \\((N, C, L_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(L_{out} = floor((L_{in} + 2 * padding - dilation * (kernel\\_size - 1) - 1) / stride + 1)\\) Examples: >>> # pool of size=3, stride=2 >>> m = nn.MaxPool1d(3, stride=2) >>> input = autograd.Variable(torch.randn(20, 16, 50)) >>> output = m(input)","title":"MaxPool1d"},{"location":"57/#maxpool2d","text":"class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e8c\u7ef4\u7684\u6700\u5927\u6c60\u5316 max pooling \u64cd\u4f5c \u6700\u7b80\u5355\u7684\u4f8b\u5b50, \u5982\u679c\u8f93\u5165\u5927\u5c0f\u4e3a \\((N, C, H, W)\\) , \u8f93\u51fa\u5927\u5c0f\u4e3a \\((N, C, H_{out}, W_{out})\\) , \u6c60\u5316\u7a97\u53e3\u5927\u5c0f kernel_size \u4e3a \\((kH, kW)\\) \u8be5\u5c42\u8f93\u51fa\u503c\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7cbe\u786e\u8ba1\u7b97: \\[\\begin{array}{ll} out(N_i, C_j, h, w) = \\max_{{m}=0}^{kH-1} \\max_{{n}=0}^{kW-1} input(N_i, C_j, stride[0] * h + m, stride[1] * w + n) \\end{array}\\] \u5982\u679c padding \u4e0d\u662f0, \u90a3\u4e48\u5728\u8f93\u5165\u6570\u636e\u7684\u6bcf\u6761\u8fb9\u4e0a\u4f1a\u9690\u5f0f\u586b\u8865\u5bf9\u5e94 padding \u6570\u91cf\u76840\u503c\u70b9 dilation \u7528\u4e8e\u63a7\u5236\u5185\u6838\u70b9\u4e4b\u95f4\u7684\u95f4\u9694, link \u5f88\u597d\u5730\u53ef\u89c6\u5316\u5c55\u793a\u4e86 dilation \u7684\u529f\u80fd \u53c2\u6570 kernel_size , stride , padding , dilation \u53ef\u4ee5\u662f\u4ee5\u4e0b\u4efb\u610f\u4e00\u79cd\u6570\u636e\u7c7b\u578b: \u5355\u4e2a int \u7c7b\u578b\u6570\u636e \u2013 \u6b64\u65f6\u5728 height \u548c width \u7ef4\u5ea6\u4e0a\u5c06\u4f7f\u7528\u76f8\u540c\u7684\u503c \u5305\u542b\u4e24\u4e2a int \u7c7b\u578b\u6570\u636e\u7684 tuple \u5143\u7ec4 \u2013 \u6b64\u65f6\u7b2c\u4e00\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a height \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c, \u7b2c\u4e8c\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a width \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c | Parameters: | kernel_size \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7684\u7a97\u53e3\u5927\u5c0f stride \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u9690\u5f0f\u88650\u7684\u6570\u91cf dilation \u2013 \u7528\u4e8e\u63a7\u5236\u7a97\u53e3\u4e2d\u5143\u7d20\u7684\u6b65\u957f\u7684\u53c2\u6570 return_indices \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8fd4\u56de max pooling \u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15 \u8fd9\u5728\u4e4b\u540e\u7684 Unpooling \u65f6\u5f88\u6709\u7528 ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(H_{out} = floor((H_{in} + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[1] - dilation[1] * (kernel\\_size[1] - 1) - 1) / stride[1] + 1)\\) Examples: >>> # pool of square window of size=3, stride=2 >>> m = nn.MaxPool2d(3, stride=2) >>> # pool of non-square window >>> m = nn.MaxPool2d((3, 2), stride=(2, 1)) >>> input = autograd.Variable(torch.randn(20, 16, 50, 32)) >>> output = m(input)","title":"MaxPool2d"},{"location":"57/#maxpool3d","text":"class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e09\u7ef4\u7684\u6700\u5927\u6c60\u5316 max pooling \u64cd\u4f5c \u6700\u7b80\u5355\u7684\u4f8b\u5b50, \u5982\u679c\u8f93\u5165\u5927\u5c0f\u4e3a \\((N, C, D, H, W)\\) ,\u8f93\u51fa\u5927\u5c0f\u4e3a \\((N, C, D_{out}, H_{out}, W_{out})\\) \u6c60\u5316\u7a97\u53e3\u5927\u5c0f kernel_size \u4e3a \\((kD, kH, kW)\\) \u8be5\u5c42\u8f93\u51fa\u503c\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7cbe\u786e\u8ba1\u7b97: \\[\\begin{array}{ll} out(N_i, C_j, d, h, w) = \\max_{{k}=0}^{kD-1} \\max_{{m}=0}^{kH-1} \\max_{{n}=0}^{kW-1} input(N_i, C_j, stride[0] * k + d, stride[1] * h + m, stride[2] * w + n) \\end{array}\\] \u5982\u679c padding \u4e0d\u662f0, \u90a3\u4e48\u5728\u8f93\u5165\u6570\u636e\u7684\u6bcf\u6761\u8fb9\u4e0a\u4f1a\u9690\u5f0f\u586b\u8865\u5bf9\u5e94 padding \u6570\u91cf\u76840\u503c\u70b9 dilation \u7528\u4e8e\u63a7\u5236\u5185\u6838\u70b9\u4e4b\u95f4\u7684\u95f4\u9694, link \u5f88\u597d\u5730\u53ef\u89c6\u5316\u5c55\u793a\u4e86 dilation \u7684\u529f\u80fd \u53c2\u6570 kernel_size , stride , padding , dilation \u53ef\u4ee5\u662f\u4ee5\u4e0b\u4efb\u610f\u4e00\u79cd\u6570\u636e\u7c7b\u578b: \u5355\u4e2a int \u7c7b\u578b\u6570\u636e \u2013 \u6b64\u65f6\u5728 depth, height \u548c width \u7ef4\u5ea6\u4e0a\u5c06\u4f7f\u7528\u76f8\u540c\u7684\u503c \u5305\u542b\u4e09\u4e2a int \u7c7b\u578b\u6570\u636e\u7684 tuple \u5143\u7ec4 \u2013 \u6b64\u65f6\u7b2c\u4e00\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a depth \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c, \u7b2c\u4e8c\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a height \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c,\u7b2c\u4e09\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a width \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c | Parameters: | kernel_size \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7684\u7a97\u53e3\u5927\u5c0f stride \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding \u2013 \u8f93\u5165\u6240\u6709\u4e09\u6761\u8fb9\u4e0a\u9690\u5f0f\u88650\u7684\u6570\u91cf dilation \u2013 \u7528\u4e8e\u63a7\u5236\u7a97\u53e3\u4e2d\u5143\u7d20\u7684\u6b65\u957f\u7684\u53c2\u6570 return_indices \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8fd4\u56de max pooling \u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15 \u8fd9\u5728\u4e4b\u540e\u7684 Unpooling \u65f6\u5f88\u6709\u7528 ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f | | --- | --- | Shape: Input: \\((N, C, D_{in}, H_{in}, W_{in})\\) Output: \\((N, C, D_{out}, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(D_{out} = floor((D_{in} + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)\\) \\(H_{out} = floor((H_{in} + 2 * padding[1] - dilation[1] * (kernel\\_size[1] - 1) - 1) / stride[1] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[2] - dilation[2] * (kernel\\_size[2] - 1) - 1) / stride[2] + 1)\\) Examples: >>> # pool of square window of size=3, stride=2 >>> m = nn.MaxPool3d(3, stride=2) >>> # pool of non-square window >>> m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2)) >>> input = autograd.Variable(torch.randn(20, 16, 50,44, 31)) >>> output = m(input)","title":"MaxPool3d"},{"location":"57/#maxunpool1d","text":"class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0) MaxPool1d \u7684\u9006\u8fc7\u7a0b \u8981\u6ce8\u610f\u7684\u662f MaxPool1d \u5e76\u4e0d\u662f\u5b8c\u5168\u53ef\u9006\u7684, \u56e0\u4e3a\u5728max pooling\u8fc7\u7a0b\u4e2d\u975e\u6700\u5927\u503c\u5df2\u7ecf\u4e22\u5931 MaxUnpool1d \u4ee5 MaxPool1d \u7684\u8f93\u51fa, \u5305\u542b\u6700\u5927\u503c\u7684\u7d22\u5f15\u4f5c\u4e3a\u8f93\u5165 \u8ba1\u7b97max poooling\u7684\u90e8\u5206\u9006\u8fc7\u7a0b(\u5bf9\u4e8e\u90a3\u4e9b\u6700\u5927\u503c\u533a\u57df), \u5bf9\u4e8e\u90a3\u4e9b\u975e\u6700\u5927\u503c\u533a\u57df\u5c06\u8bbe\u7f6e\u4e3a0\u503c Note MaxPool1d \u53ef\u4ee5\u5c06\u591a\u4e2a\u8f93\u5165\u5927\u5c0f\u6620\u5c04\u5230\u76f8\u540c\u7684\u8f93\u51fa\u5927\u5c0f, \u56e0\u6b64\u53cd\u6f14\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u6a21\u68f1\u4e24\u53ef \u4e3a\u9002\u5e94\u8fd9\u4e00\u70b9, \u5728\u8c03\u7528forward\u51fd\u6570\u65f6\u53ef\u4ee5\u5c06\u9700\u8981\u7684\u8f93\u51fa\u5927\u5c0f\u4f5c\u4e3a\u989d\u5916\u7684\u53c2\u6570 output_size \u4f20\u5165. \ufffd \u5177\u4f53\u7528\u6cd5,\u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u8f93\u5165\u548c\u793a\u4f8b | Parameters: | kernel_size ( int or tuple ) \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7684\u7a97\u53e3\u5927\u5c0f stride ( int or tuple ) \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding ( int or tuple ) \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u586b\u51450\u503c\u7684\u4e2a\u6570 | | --- | --- | Inputs: <cite>input</cite>: \u9700\u8981\u8f6c\u5316\u7684\u8f93\u5165\u7684 Tensor <cite>indices</cite>: <cite>MaxPool1d</cite> \u63d0\u4f9b\u7684\u6700\u5927\u503c\u7d22\u5f15 <cite>output_size</cite> (\u53ef\u9009) : <cite>torch.Size</cite> \u7c7b\u578b\u7684\u6570\u636e\u6307\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f Shape: Input: \\((N, C, H_{in})\\) Output: \\((N, C, H_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(H_{out} = (H_{in} - 1) * stride[0] - 2 * padding[0] + kernel\\_size[0]\\) \u6216\u8005\u5728\u8c03\u7528\u65f6\u6307\u5b9a\u8f93\u51fa\u5927\u5c0f output_size Example: >>> pool = nn.MaxPool1d(2, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool1d(2, stride=2) >>> input = Variable(torch.Tensor([[[1, 2, 3, 4, 5, 6, 7, 8]]])) >>> output, indices = pool(input) >>> unpool(output, indices) Variable containing: (0 ,.,.) = 0 2 0 4 0 6 0 8 [torch.FloatTensor of size 1x1x8] >>> # Example showcasing the use of output_size >>> input = Variable(torch.Tensor([[[1, 2, 3, 4, 5, 6, 7, 8, 9]]])) >>> output, indices = pool(input) >>> unpool(output, indices, output_size=input.size()) Variable containing: (0 ,.,.) = 0 2 0 4 0 6 0 8 0 [torch.FloatTensor of size 1x1x9] >>> unpool(output, indices) Variable containing: (0 ,.,.) = 0 2 0 4 0 6 0 8 [torch.FloatTensor of size 1x1x8]","title":"MaxUnpool1d"},{"location":"57/#maxunpool2d","text":"class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0) MaxPool2d \u7684\u9006\u8fc7\u7a0b \u8981\u6ce8\u610f\u7684\u662f MaxPool2d \u5e76\u4e0d\u662f\u5b8c\u5168\u53ef\u9006\u7684, \u56e0\u4e3a\u5728max pooling\u8fc7\u7a0b\u4e2d\u975e\u6700\u5927\u503c\u5df2\u7ecf\u4e22\u5931 MaxUnpool2d \u4ee5 MaxPool2d \u7684\u8f93\u51fa, \u5305\u542b\u6700\u5927\u503c\u7684\u7d22\u5f15\u4f5c\u4e3a\u8f93\u5165 \u8ba1\u7b97max poooling\u7684\u90e8\u5206\u9006\u8fc7\u7a0b(\u5bf9\u4e8e\u90a3\u4e9b\u6700\u5927\u503c\u533a\u57df), \u5bf9\u4e8e\u90a3\u4e9b\u975e\u6700\u5927\u503c\u533a\u57df\u5c06\u8bbe\u7f6e\u4e3a0\u503c Note MaxPool2d \u53ef\u4ee5\u5c06\u591a\u4e2a\u8f93\u5165\u5927\u5c0f\u6620\u5c04\u5230\u76f8\u540c\u7684\u8f93\u51fa\u5927\u5c0f, \u56e0\u6b64\u53cd\u6f14\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u6a21\u68f1\u4e24\u53ef. \u4e3a\u9002\u5e94\u8fd9\u4e00\u70b9, \u5728\u8c03\u7528forward\u51fd\u6570\u65f6\u53ef\u4ee5\u5c06\u9700\u8981\u7684\u8f93\u51fa\u5927\u5c0f\u4f5c\u4e3a\u989d\u5916\u7684\u53c2\u6570 output_size \u4f20\u5165. \ufffd \u5177\u4f53\u7528\u6cd5,\u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u8f93\u5165\u548c\u793a\u4f8b | Parameters: | kernel_size ( int or tuple ) \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7684\u7a97\u53e3\u5927\u5c0f stride ( int or tuple ) \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding ( int or tuple ) \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u586b\u51450\u503c\u7684\u4e2a\u6570 | | --- | --- | Inputs: <cite>input</cite>: \u9700\u8981\u8f6c\u5316\u7684\u8f93\u5165\u7684 Tensor <cite>indices</cite>: <cite>MaxPool2d</cite> \u63d0\u4f9b\u7684\u6700\u5927\u503c\u7d22\u5f15 <cite>output_size</cite> (\u53ef\u9009) : <cite>torch.Size</cite> \u7c7b\u578b\u7684\u6570\u636e\u6307\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(H_{out} = (H_{in} - 1) * stride[0] -2 * padding[0] + kernel\\_size[0]\\) \\(W_{out} = (W_{in} - 1) * stride[1] -2 * padding[1] + kernel\\_size[1]\\) \u6216\u8005\u5728\u8c03\u7528\u65f6\u6307\u5b9a\u8f93\u51fa\u5927\u5c0f output_size Example: >>> pool = nn.MaxPool2d(2, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool2d(2, stride=2) >>> input = Variable(torch.Tensor([[[[ 1, 2, 3, 4], ... [ 5, 6, 7, 8], ... [ 9, 10, 11, 12], ... [13, 14, 15, 16]]]])) >>> output, indices = pool(input) >>> unpool(output, indices) Variable containing: (0 ,0 ,.,.) = 0 0 0 0 0 6 0 8 0 0 0 0 0 14 0 16 [torch.FloatTensor of size 1x1x4x4] >>> # specify a different output size than input size >>> unpool(output, indices, output_size=torch.Size([1, 1, 5, 5])) Variable containing: (0 ,0 ,.,.) = 0 0 0 0 0 6 0 8 0 0 0 0 0 14 0 16 0 0 0 0 0 0 0 0 0 [torch.FloatTensor of size 1x1x5x5]","title":"MaxUnpool2d"},{"location":"57/#maxunpool3d","text":"class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0) MaxPool3d \u7684\u9006\u8fc7\u7a0b \u8981\u6ce8\u610f\u7684\u662f MaxPool3d \u5e76\u4e0d\u662f\u5b8c\u5168\u53ef\u9006\u7684, \u56e0\u4e3a\u5728max pooling\u8fc7\u7a0b\u4e2d\u975e\u6700\u5927\u503c\u5df2\u7ecf\u4e22\u5931 MaxUnpool3d \u4ee5 MaxPool3d \u7684\u8f93\u51fa, \u5305\u542b\u6700\u5927\u503c\u7684\u7d22\u5f15\u4f5c\u4e3a\u8f93\u5165 \u8ba1\u7b97max poooling\u7684\u90e8\u5206\u9006\u8fc7\u7a0b(\u5bf9\u4e8e\u90a3\u4e9b\u6700\u5927\u503c\u533a\u57df), \u5bf9\u4e8e\u90a3\u4e9b\u975e\u6700\u5927\u503c\u533a\u57df\u5c06\u8bbe\u7f6e\u4e3a0\u503c Note MaxPool3d \u53ef\u4ee5\u5c06\u591a\u4e2a\u8f93\u5165\u5927\u5c0f\u6620\u5c04\u5230\u76f8\u540c\u7684\u8f93\u51fa\u5927\u5c0f, \u56e0\u6b64\u53cd\u6f14\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u6a21\u68f1\u4e24\u53ef. \u4e3a\u9002\u5e94\u8fd9\u4e00\u70b9, \u5728\u8c03\u7528forward\u51fd\u6570\u65f6\u53ef\u4ee5\u5c06\u9700\u8981\u7684\u8f93\u51fa\u5927\u5c0f\u4f5c\u4e3a\u989d\u5916\u7684\u53c2\u6570 output_size \u4f20\u5165. \ufffd \u5177\u4f53\u7528\u6cd5,\u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u8f93\u5165\u548c\u793a\u4f8b | Parameters: | kernel_size ( int or tuple ) \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7684\u7a97\u53e3\u5927\u5c0f stride ( int or tuple ) \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding ( int or tuple ) \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u586b\u51450\u503c\u7684\u4e2a\u6570 | | --- | --- | Inputs: <cite>input</cite>: \u9700\u8981\u8f6c\u5316\u7684\u8f93\u5165\u7684 Tensor <cite>indices</cite>: <cite>MaxPool3d</cite> \u63d0\u4f9b\u7684\u6700\u5927\u503c\u7d22\u5f15 <cite>output_size</cite> (\u53ef\u9009) : <cite>torch.Size</cite> \u7c7b\u578b\u7684\u6570\u636e\u6307\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f Shape: Input: \\((N, C, D_{in}, H_{in}, W_{in})\\) Output: \\((N, C, D_{out}, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(D_{out} = (D_{in} - 1) * stride[0] - 2 * padding[0] + kernel\\_size[0]\\) \\(H_{out} = (H_{in} - 1) * stride[1] - 2 * padding[1] + kernel\\_size[1]\\) \\(W_{out} = (W_{in} - 1) * stride[2] - 2 * padding[2] + kernel\\_size[2]\\) \u6216\u8005\u5728\u8c03\u7528\u65f6\u6307\u5b9a\u8f93\u51fa\u5927\u5c0f output_size Example: >>> # pool of square window of size=3, stride=2 >>> pool = nn.MaxPool3d(3, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool3d(3, stride=2) >>> output, indices = pool(Variable(torch.randn(20, 16, 51, 33, 15))) >>> unpooled_output = unpool(output, indices) >>> unpooled_output.size() torch.Size([20, 16, 51, 33, 15])","title":"MaxUnpool3d"},{"location":"57/#avgpool1d","text":"class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e00\u7ef4\u7684\u5e73\u5747\u6c60\u5316 average pooling \u64cd\u4f5c \u6700\u7b80\u5355\u7684\u4f8b\u5b50, \u5982\u679c\u8f93\u5165\u5927\u5c0f\u4e3a \\((N, C, L)\\) , \u8f93\u51fa\u5927\u5c0f\u4e3a \\((N, C, L_{out})\\) , \u6c60\u5316\u7a97\u53e3\u5927\u5c0f kernel_size \u4e3a \\(k\\) \u8be5\u5c42\u8f93\u51fa\u503c\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7cbe\u786e\u8ba1\u7b97: \\[\\begin{array}{ll} out(N_i, C_j, l) = 1 / k * \\sum_{{m}=0}^{k} input(N_i, C_j, stride * l + m) \\end{array}\\] \u5982\u679c padding \u4e0d\u662f0, \u90a3\u4e48\u5728\u8f93\u5165\u6570\u636e\u7684\u6bcf\u6761\u8fb9\u4e0a\u4f1a\u9690\u5f0f\u586b\u8865\u5bf9\u5e94 padding \u6570\u91cf\u76840\u503c\u70b9 \u53c2\u6570 kernel_size , stride , padding \u53ef\u4ee5\u4e3a\u5355\u4e2a int \u7c7b\u578b\u7684\u6570\u636e \u6216\u8005\u662f\u4e00\u4e2a\u5355\u5143\u7d20\u7684tuple\u5143\u7ec4 | Parameters: | kernel_size \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u53d6\u5e73\u5747\u503c\u7684\u7a97\u53e3\u7684\u5927\u5c0f stride \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u9690\u5f0f\u88650\u7684\u6570\u91cf ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f count_include_pad \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u5e73\u5747\u6c60\u5316\u7684\u503c\u65f6,\u5c06\u8003\u8651 padding \u586b\u5145\u76840 | | --- | --- | Shape: Input: \\((N, C, L_{in})\\) Output: \\((N, C, L_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(L_{out} = floor((L_{in} + 2 * padding - kernel\\_size) / stride + 1)\\) Examples: >>> # pool with window of size=3, stride=2 >>> m = nn.AvgPool1d(3, stride=2) >>> m(Variable(torch.Tensor([[[1,2,3,4,5,6,7]]]))) Variable containing: (0 ,.,.) = 2 4 6 [torch.FloatTensor of size 1x1x3]","title":"AvgPool1d"},{"location":"57/#avgpool2d","text":"class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e8c\u7ef4\u7684\u5e73\u5747\u6c60\u5316 average pooling \u64cd\u4f5c \u6700\u7b80\u5355\u7684\u4f8b\u5b50,\u5982\u679c\u8f93\u5165\u5927\u5c0f\u4e3a \\((N, C, H, W)\\) ,\u8f93\u51fa\u5927\u5c0f\u4e3a \\((N, C, H_{out}, W_{out})\\) , \u6c60\u5316\u7a97\u53e3\u5927\u5c0f kernel_size \u4e3a \\((kH, kW)\\) \u8be5\u5c42\u8f93\u51fa\u503c\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7cbe\u786e\u8ba1\u7b97: \\[\\begin{array}{ll} out(N_i, C_j, h, w) = 1 / (kH * kW) * \\sum_{{m}=0}^{kH-1} \\sum_{{n}=0}^{kW-1} input(N_i, C_j, stride[0] * h + m, stride[1] * w + n) \\end{array}\\] \u5982\u679c padding \u4e0d\u662f0, \u90a3\u4e48\u5728\u8f93\u5165\u6570\u636e\u7684\u6bcf\u6761\u8fb9\u4e0a\u4f1a\u9690\u5f0f\u586b\u8865\u5bf9\u5e94 padding \u6570\u91cf\u76840\u503c\u70b9 \u53c2\u6570 kernel_size , stride , padding \u53ef\u4ee5\u662f\u4ee5\u4e0b\u4efb\u610f\u4e00\u79cd\u6570\u636e\u7c7b\u578b: \u5355\u4e2a int \u7c7b\u578b\u6570\u636e \u2013 \u6b64\u65f6\u5728 height \u548c width \u7ef4\u5ea6\u4e0a\u5c06\u4f7f\u7528\u76f8\u540c\u7684\u503c \u5305\u542b\u4e24\u4e2a int \u7c7b\u578b\u6570\u636e\u7684 tuple \u5143\u7ec4 \u2013 \u6b64\u65f6\u7b2c\u4e00\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a height \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c, \u7b2c\u4e8c\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a width \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c | Parameters: | kernel_size \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u53d6\u5e73\u5747\u503c\u7684\u7a97\u53e3\u7684\u5927\u5c0f stride \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u9690\u5f0f\u88650\u7684\u6570\u91cf ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f count_include_pad \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u5e73\u5747\u6c60\u5316\u7684\u503c\u65f6,\u5c06\u8003\u8651 padding \u586b\u5145\u76840 | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(H_{out} = floor((H_{in} + 2 * padding[0] - kernel\\_size[0]) / stride[0] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[1] - kernel\\_size[1]) / stride[1] + 1)\\) Examples: >>> # pool of square window of size=3, stride=2 >>> m = nn.AvgPool2d(3, stride=2) >>> # pool of non-square window >>> m = nn.AvgPool2d((3, 2), stride=(2, 1)) >>> input = autograd.Variable(torch.randn(20, 16, 50, 32)) >>> output = m(input)","title":"AvgPool2d"},{"location":"57/#avgpool3d","text":"class torch.nn.AvgPool3d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e09\u7ef4\u7684\u5e73\u5747\u6c60\u5316 average pooling \u64cd\u4f5c \u6700\u7b80\u5355\u7684\u4f8b\u5b50, \u5982\u679c\u8f93\u5165\u5927\u5c0f\u4e3a \\((N, C, D, H, W)\\) ,\u8f93\u51fa\u5927\u5c0f\u4e3a \\((N, C, D_{out}, H_{out}, W_{out})\\) \u6c60\u5316\u7a97\u53e3\u5927\u5c0f kernel_size \u4e3a \\((kD, kH, kW)\\) \u8be5\u5c42\u8f93\u51fa\u503c\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7cbe\u786e\u8ba1\u7b97: \\[\\begin{array}{ll} out(N_i, C_j, d, h, w) = 1 / (kD * kH * kW) * \\sum_{{k}=0}^{kD-1} \\sum_{{m}=0}^{kH-1} \\sum_{{n}=0}^{kW-1} input(N_i, C_j, stride[0] * d + k, stride[1] * h + m, stride[2] * w + n) \\end{array}\\] \u5982\u679c padding \u4e0d\u662f0, \u90a3\u4e48\u5728\u8f93\u5165\u6570\u636e\u7684\u6bcf\u6761\u8fb9\u4e0a\u4f1a\u9690\u5f0f\u586b\u8865\u5bf9\u5e94 padding \u6570\u91cf\u76840\u503c\u70b9 \u53c2\u6570 kernel_size , stride \u53ef\u4ee5\u662f\u4ee5\u4e0b\u4efb\u610f\u4e00\u79cd\u6570\u636e\u7c7b\u578b: \u5355\u4e2a int \u7c7b\u578b\u6570\u636e \u2013 \u6b64\u65f6\u5728 depth, height \u548c width \u7ef4\u5ea6\u4e0a\u5c06\u4f7f\u7528\u76f8\u540c\u7684\u503c \u5305\u542b\u4e09\u4e2a int \u7c7b\u578b\u6570\u636e\u7684 tuple \u5143\u7ec4 \u2013 \u6b64\u65f6\u7b2c\u4e00\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a depth \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c, \u7b2c\u4e8c\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a height \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c,\u7b2c\u4e09\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a width \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c | Parameters: | kernel_size \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u53d6\u5e73\u5747\u503c\u7684\u7a97\u53e3\u7684\u5927\u5c0f stride \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size padding \u2013 \u8f93\u5165\u7684\u6bcf\u6761\u8fb9\u9690\u5f0f\u88650\u7684\u6570\u91cf ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f count_include_pad \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u5e73\u5747\u6c60\u5316\u7684\u503c\u65f6,\u5c06\u8003\u8651 padding \u586b\u5145\u76840 | | --- | --- | Shape: Input: \\((N, C, D_{in}, H_{in}, W_{in})\\) Output: \\((N, C, D_{out}, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(D_{out} = floor((D_{in} + 2 * padding[0] - kernel\\_size[0]) / stride[0] + 1)\\) \\(H_{out} = floor((H_{in} + 2 * padding[1] - kernel\\_size[1]) / stride[1] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[2] - kernel\\_size[2]) / stride[2] + 1)\\) Examples: >>> # pool of square window of size=3, stride=2 >>> m = nn.AvgPool3d(3, stride=2) >>> # pool of non-square window >>> m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2)) >>> input = autograd.Variable(torch.randn(20, 16, 50,44, 31)) >>> output = m(input)","title":"AvgPool3d"},{"location":"57/#fractionalmaxpool2d","text":"class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e8c\u7ef4\u7684\u5206\u6570\u6700\u5927\u6c60\u5316 fractional max pooling \u64cd\u4f5c \u5206\u6570\u6700\u5927\u6c60\u5316 Fractiona MaxPooling \u7684\u5177\u4f53\u7ec6\u8282\u63cf\u8ff0,\u8be6\u89c1Ben Graham\u8bba\u6587 Fractional MaxPooling \u7531\u76ee\u6807\u8f93\u51fa\u5927\u5c0f\u786e\u5b9a\u968f\u673a\u6b65\u957f,\u5728 kH x kW \u533a\u57df\u5185\u8fdb\u884c\u6700\u5927\u6c60\u5316\u7684\u64cd\u4f5c \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c | Parameters: | kernel_size \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u7684\u5927\u5c0f. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57 k (\u7b49\u4ef7\u4e8e k x k \u7684\u6b63\u65b9\u5f62\u7a97\u53e3) \u6216\u8005\u662f \u4e00\u4e2a\u5143\u7ec4 tuple (kh x kw) output_size \u2013 oH x oW \u5f62\u5f0f\u7684\u8f93\u51fa\u56fe\u50cf\u7684\u5c3a\u5bf8. \u53ef\u4ee5\u7528 \u4e00\u4e2a tuple \u5143\u7ec4 (oH, oW) \u8868\u793a oH x oW \u7684\u8f93\u51fa\u5c3a\u5bf8, \u6216\u8005\u662f\u5355\u4e2a\u7684\u6570\u5b57 oH \u8868\u793a oH x oH \u7684\u8f93\u51fa\u5c3a\u5bf8 output_ratio \u2013 \u5982\u679c\u60f3\u7528\u8f93\u5165\u56fe\u50cf\u7684\u767e\u5206\u6bd4\u6765\u6307\u5b9a\u8f93\u51fa\u56fe\u50cf\u7684\u5927\u5c0f,\u53ef\u9009\u7528\u8be5\u9009\u9879. \u4f7f\u7528\u8303\u56f4\u5728 (0,1) \u4e4b\u95f4\u7684\u4e00\u4e2a\u503c\u6765\u6307\u5b9a. return_indices \u2013 \u5982\u679c\u7b49\u4e8e True ,\u5728\u8fd4\u56de\u8f93\u51fa\u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15,\u8be5\u7d22\u5f15\u5bf9 nn.MaxUnpool2d \u6709\u7528. \u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8be5\u503c\u7b49\u4e8e False | | --- | --- | Examples >>> # pool of square window of size=3, and target output size 13x12 >>> m = nn.FractionalMaxPool2d(3, output_size=(13, 12)) >>> # pool of square window and target output size being half of input image size >>> m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5)) >>> input = autograd.Variable(torch.randn(20, 16, 50, 32)) >>> output = m(input)","title":"FractionalMaxPool2d"},{"location":"57/#lppool2d","text":"class torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e8c\u7ef4\u7684\u5e42\u5e73\u5747\u6c60\u5316 power-average pooling \u64cd\u4f5c \u5728\u6bcf\u4e2a\u7a97\u53e3\u5185, \u8f93\u51fa\u7684\u8ba1\u7b97\u65b9\u5f0f: \\(f(X) = pow(sum(pow(X, p)), 1/p)\\) \u5f53 p \u65e0\u7a77\u5927\u65f6,\u7b49\u4ef7\u4e8e\u6700\u5927\u6c60\u5316 Max Pooling \u64cd\u4f5c \u5f53 p=1 \u65f6, \u7b49\u4ef7\u4e8e\u5e73\u5747\u6c60\u5316 Average Pooling \u64cd\u4f5c \u53c2\u6570 kernel_size , stride \u53ef\u4ee5\u662f\u4ee5\u4e0b\u4efb\u610f\u4e00\u79cd\u6570\u636e\u7c7b\u578b: \u5355\u4e2a int \u7c7b\u578b\u6570\u636e \u2013 \u6b64\u65f6\u5728height\u548cwidth\u7ef4\u5ea6\u4e0a\u5c06\u4f7f\u7528\u76f8\u540c\u7684\u503c \u5305\u542b\u4e24\u4e2a int \u7c7b\u578b\u6570\u636e\u7684 tuple \u5143\u7ec4 \u2013 \u6b64\u65f6\u7b2c\u4e00\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a height \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c, \u7b2c\u4e8c\u4e2a <cite>int</cite> \u6570\u636e\u8868\u793a width \u7ef4\u5ea6\u4e0a\u7684\u6570\u503c | Parameters: | kernel_size \u2013 \u5e42\u5e73\u5747\u6c60\u5316\u65f6\u7a97\u53e3\u7684\u5927\u5c0f stride \u2013 \u5e42\u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u65f6\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f, \u9ed8\u8ba4\u503c\u662f kernel_size ceil_mode \u2013 \u5982\u679c\u7b49\u4e8e True , \u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u65f6,\u5c06\u91c7\u7528\u5411\u4e0a\u53d6\u6574\u6765\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u65b9\u5f0f | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) \u9075\u4ece\u5982\u4e0b\u5173\u7cfb \\(H_{out} = floor((H_{in} + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)\\) \\(W_{out} = floor((W_{in} + 2 * padding[1] - dilation[1] * (kernel\\_size[1] - 1) - 1) / stride[1] + 1)\\) Examples: >>> # power-2 pool of square window of size=3, stride=2 >>> m = nn.LPPool2d(2, 3, stride=2) >>> # pool of non-square window of power 1.2 >>> m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1)) >>> input = autograd.Variable(torch.randn(20, 16, 50, 32)) >>> output = m(input)","title":"LPPool2d"},{"location":"57/#adaptivemaxpool1d","text":"class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e00\u7ef4\u7684\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316 adaptive max pooling \u64cd\u4f5c \u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165,\u53ef\u4ee5\u6307\u5b9a\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a H \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c. | Parameters: | output_size \u2013 \u76ee\u6807\u8f93\u51fa\u7684\u5c3a\u5bf8 H return_indices \u2013 \u5982\u679c\u7b49\u4e8e True ,\u5728\u8fd4\u56de\u8f93\u51fa\u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15,\u8be5\u7d22\u5f15\u5bf9 nn.MaxUnpool1d \u6709\u7528. \u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8be5\u503c\u7b49\u4e8e False | | --- | --- | Examples >>> # target output size of 5 >>> m = nn.AdaptiveMaxPool1d(5) >>> input = autograd.Variable(torch.randn(1, 64, 8)) >>> output = m(input)","title":"AdaptiveMaxPool1d"},{"location":"57/#adaptivemaxpool2d","text":"class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e8c\u7ef4\u7684\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316 adaptive max pooling \u64cd\u4f5c \u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165,\u53ef\u4ee5\u6307\u5b9a\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a H x W \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c. | Parameters: | output_size \u2013 H x W \u5f62\u5f0f\u7684\u8f93\u51fa\u56fe\u50cf\u7684\u5c3a\u5bf8. \u53ef\u4ee5\u7528 \u4e00\u4e2a tuple \u5143\u7ec4 (H, W) \u8868\u793a H x W \u7684\u8f93\u51fa\u5c3a\u5bf8, \u6216\u8005\u662f\u5355\u4e2a\u7684\u6570\u5b57 H \u8868\u793a H x H \u7684\u8f93\u51fa\u5c3a\u5bf8 return_indices \u2013 \u5982\u679c\u7b49\u4e8e True ,\u5728\u8fd4\u56de\u8f93\u51fa\u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15,\u8be5\u7d22\u5f15\u5bf9 nn.MaxUnpool2d \u6709\u7528. \u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8be5\u503c\u7b49\u4e8e False | | --- | --- | Examples >>> # target output size of 5x7 >>> m = nn.AdaptiveMaxPool2d((5,7)) >>> input = autograd.Variable(torch.randn(1, 64, 8, 9)) >>> output = m(input) >>> # target output size of 7x7 (square) >>> m = nn.AdaptiveMaxPool2d(7) >>> input = autograd.Variable(torch.randn(1, 64, 10, 9)) >>> output = m(input)","title":"AdaptiveMaxPool2d"},{"location":"57/#adaptivemaxpool3d","text":"class torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e09\u7ef4\u7684\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316 adaptive max pooling \u64cd\u4f5c \u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165,\u53ef\u4ee5\u6307\u5b9a\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a D x H x W \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c. | Parameters: | output_size \u2013 D x H x W \u5f62\u5f0f\u7684\u8f93\u51fa\u56fe\u50cf\u7684\u5c3a\u5bf8. \u53ef\u4ee5\u7528 \u4e00\u4e2a tuple \u5143\u7ec4 (D, H, W) \u8868\u793a D x H x W \u7684\u8f93\u51fa\u5c3a\u5bf8, \u6216\u8005\u662f\u5355\u4e2a\u7684\u6570\u5b57 D \u8868\u793a D x D x D \u7684\u8f93\u51fa\u5c3a\u5bf8 return_indices \u2013 \u5982\u679c\u7b49\u4e8e True ,\u5728\u8fd4\u56de\u8f93\u51fa\u7ed3\u679c\u7684\u540c\u65f6\u8fd4\u56de\u6700\u5927\u503c\u7684\u7d22\u5f15,\u8be5\u7d22\u5f15\u5bf9 nn.MaxUnpool3d \u6709\u7528. \u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8be5\u503c\u7b49\u4e8e False | | --- | --- | Examples >>> # target output size of 5x7x9 >>> m = nn.AdaptiveMaxPool3d((5,7,9)) >>> input = autograd.Variable(torch.randn(1, 64, 8, 9, 10)) >>> output = m(input) >>> # target output size of 7x7x7 (cube) >>> m = nn.AdaptiveMaxPool3d(7) >>> input = autograd.Variable(torch.randn(1, 64, 10, 9, 8)) >>> output = m(input)","title":"AdaptiveMaxPool3d"},{"location":"57/#adaptiveavgpool1d","text":"class torch.nn.AdaptiveAvgPool1d(output_size) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e00\u7ef4\u7684\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316 adaptive average pooling \u64cd\u4f5c \u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165,\u53ef\u4ee5\u6307\u5b9a\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a H \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c. Parameters: output_size \u2013 \u76ee\u6807\u8f93\u51fa\u7684\u5c3a\u5bf8 H Examples >>> # target output size of 5 >>> m = nn.AdaptiveAvgPool1d(5) >>> input = autograd.Variable(torch.randn(1, 64, 8)) >>> output = m(input)","title":"AdaptiveAvgPool1d"},{"location":"57/#adaptiveavgpool2d","text":"class torch.nn.AdaptiveAvgPool2d(output_size) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e8c\u7ef4\u7684\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316 adaptive average pooling \u64cd\u4f5c \u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165,\u53ef\u4ee5\u6307\u5b9a\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a H x W \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c. Parameters: output_size \u2013 H x W \u5f62\u5f0f\u7684\u8f93\u51fa\u56fe\u50cf\u7684\u5c3a\u5bf8. \u53ef\u4ee5\u7528 \u4e00\u4e2a tuple \u5143\u7ec4 (H, W) \u8868\u793a H x W \u7684\u8f93\u51fa\u5c3a\u5bf8, \u6216\u8005\u662f\u5355\u4e2a\u7684\u6570\u5b57 H \u8868\u793a H x H \u7684\u8f93\u51fa\u5c3a\u5bf8 Examples >>> # target output size of 5x7 >>> m = nn.AdaptiveAvgPool2d((5,7)) >>> input = autograd.Variable(torch.randn(1, 64, 8, 9)) >>> output = m(input) >>> # target output size of 7x7 (square) >>> m = nn.AdaptiveAvgPool2d(7) >>> input = autograd.Variable(torch.randn(1, 64, 10, 9)) >>> output = m(input)","title":"AdaptiveAvgPool2d"},{"location":"57/#adaptiveavgpool3d","text":"class torch.nn.AdaptiveAvgPool3d(output_size) \u5bf9\u4e8e\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7,\u5e94\u7528\u4e09\u7ef4\u7684\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316 adaptive average pooling \u64cd\u4f5c \u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165,\u53ef\u4ee5\u6307\u5b9a\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a D x H x W \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u76f8\u540c. Parameters: output_size \u2013 D x H x W \u5f62\u5f0f\u7684\u8f93\u51fa\u56fe\u50cf\u7684\u5c3a\u5bf8. \u53ef\u4ee5\u7528 \u4e00\u4e2a tuple \u5143\u7ec4 (D, H, W) \u8868\u793a D x H x W \u7684\u8f93\u51fa\u5c3a\u5bf8, \u6216\u8005\u662f\u5355\u4e2a\u7684\u6570\u5b57 D \u8868\u793a D x D x D \u7684\u8f93\u51fa\u5c3a\u5bf8 Examples >>> # target output size of 5x7x9 >>> m = nn.AdaptiveAvgPool3d((5,7,9)) >>> input = autograd.Variable(torch.randn(1, 64, 8, 9, 10)) >>> output = m(input) >>> # target output size of 7x7x7 (cube) >>> m = nn.AdaptiveAvgPool3d(7) >>> input = autograd.Variable(torch.randn(1, 64, 10, 9, 8)) >>> output = m(input)","title":"AdaptiveAvgPool3d"},{"location":"57/#padding-layers","text":"","title":"Padding Layers (\u586b\u5145\u5c42)"},{"location":"57/#reflectionpad2d","text":"class torch.nn.ReflectionPad2d(padding) \u4f7f\u7528\u8f93\u5165\u8fb9\u754c\u7684\u53cd\u5c04\u586b\u5145\u8f93\u5165\u5f20\u91cf. | Parameters: | padding ( int , tuple ) \u2013 \u586b\u5145\u7684\u5927\u5c0f. \u5982\u679c\u662fint, \u5219\u5728\u6240\u6709\u8fb9\u754c\u586b\u5145\u4f7f\u7528\u76f8\u540c\u7684. \u5219\u4f7f\u7528 ( \u5982\u679c\u662f4\u4e2a\u5143\u7ec4__, ) \u2013 | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) where \\(H_{out} = H_{in} + paddingTop + paddingBottom\\) \\(W_{out} = W_{in} + paddingLeft + paddingRight\\) Example: >>> m = nn.ReflectionPad2d(3) >>> input = autograd.Variable(torch.randn(16, 3, 320, 480)) >>> output = m(input) >>> # \u4f7f\u7528\u4e0d\u540c\u7684\u586b\u5145 >>> m = nn.ReflectionPad2d((3, 3, 6, 6)) >>> output = m(input)","title":"ReflectionPad2d"},{"location":"57/#replicationpad2d","text":"class torch.nn.ReplicationPad2d(padding) \u4f7f\u7528\u8f93\u5165\u8fb9\u754c\u7684\u590d\u5236\u586b\u5145\u8f93\u5165\u5f20\u91cf. Parameters: padding ( int , tuple ) \u2013 \u586b\u5145\u7684\u5927\u5c0f. \u5982\u679c\u662fint, \u5219\u5728\u6240\u6709\u8fb9\u754c\u4f7f\u7528\u76f8\u540c\u7684\u586b\u5145. \u5982\u679c\u662f4\u4e2a\u5143\u7ec4, \u5219\u4f7f\u7528(paddingLeft, paddingRight, paddingTop, paddingBottom) Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) where \\(H_{out} = H_{in} + paddingTop + paddingBottom\\) \\(W_{out} = W_{in} + paddingLeft + paddingRight\\) Example: >>> m = nn.ReplicationPad2d(3) >>> input = autograd.Variable(torch.randn(16, 3, 320, 480)) >>> output = m(input) >>> # \u4f7f\u7528\u4e0d\u540c\u7684\u586b\u5145 >>> m = nn.ReplicationPad2d((3, 3, 6, 6)) >>> output = m(input)","title":"ReplicationPad2d"},{"location":"57/#replicationpad3d","text":"class torch.nn.ReplicationPad3d(padding) \u4f7f\u7528\u8f93\u5165\u8fb9\u754c\u7684\u590d\u5236\u586b\u5145\u8f93\u5165\u5f20\u91cf. | Parameters: | padding ( int , tuple ) \u2013 \u586b\u5145\u7684\u5927\u5c0f. \u5982\u679c\u662fint, \u5219\u5728\u6240\u6709\u8fb9\u754c\u4f7f\u7528\u76f8\u540c\u7684\u586b\u5145. \u5219\u4f7f\u7528 (paddingLeft, paddingRight, ( \u5982\u679c\u662f\u56db\u4e2a\u5143\u7ec4__, ) \u2013 paddingBottom, paddingFront, paddingBack) ( paddingTop__, ) \u2013 | | --- | --- | Shape: Input: \\((N, C, D_{in}, H_{in}, W_{in})\\) Output: \\((N, C, D_{out}, H_{out}, W_{out})\\) where \\(D_{out} = D_{in} + paddingFront + paddingBack\\) \\(H_{out} = H_{in} + paddingTop + paddingBottom\\) \\(W_{out} = W_{in} + paddingLeft + paddingRight\\) Example: >>> m = nn.ReplicationPad3d(3) >>> input = autograd.Variable(torch.randn(16, 3, 8, 320, 480)) >>> output = m(input) >>> # \u4f7f\u7528\u4e0d\u540c\u7684\u586b\u5145 >>> m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1)) >>> output = m(input)","title":"ReplicationPad3d"},{"location":"57/#zeropad2d","text":"class torch.nn.ZeroPad2d(padding) \u7528\u96f6\u586b\u5145\u8f93\u5165\u5f20\u91cf\u8fb9\u754c. | Parameters: | padding ( int , tuple ) \u2013 \u586b\u5145\u7684\u5927\u5c0f. \u5982\u679c\u662fint, \u5219\u5728\u6240\u6709\u8fb9\u754c\u4f7f\u7528\u76f8\u540c\u7684\u586b\u5145. \u5982\u679c\u662f\u56db\u4e2a\u5143\u7ec4, \u5219\u4f7f\u7528 () \u2013 | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) where \\(H_{out} = H_{in} + paddingTop + paddingBottom\\) \\(W_{out} = W_{in} + paddingLeft + paddingRight\\) Example: >>> m = nn.ZeroPad2d(3) >>> input = autograd.Variable(torch.randn(16, 3, 320, 480)) >>> output = m(input) >>> # \u4f7f\u7528\u4e0d\u540c\u7684\u586b\u5145 >>> m = nn.ZeroPad2d((3, 3, 6, 6)) >>> output = m(input)","title":"ZeroPad2d"},{"location":"57/#constantpad2d","text":"class torch.nn.ConstantPad2d(padding, value) \u7528\u4e00\u4e2a\u5e38\u6570\u503c\u586b\u5145\u8f93\u5165\u5f20\u91cf\u8fb9\u754c. \u5bf9\u4e8e Nd-padding, \u4f7f\u7528 nn.functional.pad(). | Parameters: | padding ( int , tuple ) \u2013 \u586b\u5145\u7684\u5927\u5c0f. \u5982\u679c\u662fint, \u5219\u5728\u6240\u6709\u8fb9\u754c\u4f7f\u7528\u76f8\u540c\u7684\u586b\u5145. \u4f7f\u7528 ( \u5982\u679c\u662f4\u4e2a\u5143\u7ec4__, ) \u2013 | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) where \\(H_{out} = H_{in} + paddingTop + paddingBottom\\) \\(W_{out} = W_{in} + paddingLeft + paddingRight\\) Example: >>> m = nn.ConstantPad2d(3, 3.5) >>> input = autograd.Variable(torch.randn(16, 3, 320, 480)) >>> output = m(input) >>> # \u4f7f\u7528\u4e0d\u540c\u7684\u586b\u5145 >>> m = nn.ConstantPad2d((3, 3, 6, 6), 3.5) >>> output = m(input)","title":"ConstantPad2d"},{"location":"57/#non-linear-activations","text":"","title":"Non-linear Activations (\u975e\u7ebf\u6027\u5c42)"},{"location":"57/#relu","text":"class torch.nn.ReLU(inplace=False) \u5bf9\u8f93\u5165\u8fd0\u7528\u4fee\u6b63\u7ebf\u6027\u5355\u5143\u51fd\u6570 \\({ReLU}(x)= max(0, x)\\) Parameters: inplace \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97 Default: False Shape: Input: \\((N, *)\\) <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u4e0e\u8f93\u5165\u62e5\u6709\u540c\u6837\u7684 shape \u5c5e\u6027 Examples: >>> m = nn.ReLU() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"ReLU"},{"location":"57/#relu6","text":"class torch.nn.ReLU6(inplace=False) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570 \\({ReLU6}(x) = min(max(0,x), 6)\\) Parameters: inplace \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97 \u9ed8\u8ba4\u503c: False Shape: Input: \\((N, *)\\) , <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u4e0e\u8f93\u5165\u62e5\u6709\u540c\u6837\u7684 shape \u5c5e\u6027 Examples: >>> m = nn.ReLU6() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"ReLU6"},{"location":"57/#elu","text":"class torch.nn.ELU(alpha=1.0, inplace=False) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570, \\(f(x) = max(0,x) + min(0, alpha * (exp(x) - 1))\\) | Parameters: | alpha \u2013 ELU \u5b9a\u4e49\u516c\u5f0f\u4e2d\u7684 alpha \u503c. \u9ed8\u8ba4\u503c: 1.0 inplace \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97 \u9ed8\u8ba4\u503c: False | | --- | --- | Shape: Input: \\((N, *)\\) <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u4e0e\u8f93\u5165\u62e5\u6709\u540c\u6837\u7684 shape \u5c5e\u6027 Examples: >>> m = nn.ELU() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"ELU"},{"location":"57/#selu","text":"class torch.nn.SELU(inplace=False) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570, \\(f(x) = scale * (\\max(0,x) + \\min(0, alpha * (\\exp(x) - 1)))\\) , alpha=1.6732632423543772848170429916717 , scale=1.0507009873554804934193349852946 . \u66f4\u591a\u5730\u7ec6\u8282\u53ef\u4ee5\u53c2\u9605\u8bba\u6587 Self-Normalizing Neural Networks . Parameters: inplace ( bool , optional ) \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97. \u9ed8\u8ba4\u503c: False Shape: Input: \\((N, *)\\) where <cite>*</cite> means, any number of additional dimensions Output: \\((N, *)\\) , same shape as the input Examples: >>> m = nn.SELU() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"SELU"},{"location":"57/#prelu","text":"class torch.nn.PReLU(num_parameters=1, init=0.25) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570 \\(PReLU(x) = max(0,x) + a * min(0,x)\\) \u8fd9\u91cc\u7684 \u201ca\u201d \u662f\u81ea\u5b66\u4e60\u7684\u53c2\u6570. \u5f53\u4e0d\u5e26\u53c2\u6570\u5730\u8c03\u7528\u65f6, nn.PReLU() \u5728\u6240\u6709\u8f93\u5165\u901a\u9053\u4e2d\u4f7f\u7528\u5355\u4e2a\u53c2\u6570 \u201ca\u201d . \u800c\u5982\u679c\u7528 nn.PReLU(nChannels) \u8c03\u7528, \u201ca\u201d \u5c06\u5e94\u7528\u5230\u6bcf\u4e2a\u8f93\u5165. Note \u5f53\u4e3a\u4e86\u8868\u73b0\u66f4\u4f73\u7684\u6a21\u578b\u800c\u5b66\u4e60\u53c2\u6570 \u201ca\u201d \u65f6\u4e0d\u8981\u4f7f\u7528\u6743\u91cd\u8870\u51cf (weight decay) | Parameters: | num_parameters \u2013 \u9700\u8981\u5b66\u4e60\u7684 \u201ca\u201d \u7684\u4e2a\u6570. \u9ed8\u8ba4\u7b49\u4e8e1 init \u2013 \u201ca\u201d \u7684\u521d\u59cb\u503c. \u9ed8\u8ba4\u7b49\u4e8e0.25 | | --- | --- | Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0f shape \u4e00\u81f4 \u4f8b: >>> m = nn.PReLU() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"PReLU"},{"location":"57/#leakyrelu","text":"class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528, \\(f(x) = max(0, x) + {negative\\_slope} * min(0, x)\\) | Parameters: | negative_slope \u2013 \u63a7\u5236\u8d1f\u659c\u7387\u7684\u89d2\u5ea6, \u9ed8\u8ba4\u503c: 1e-2 inplace \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97 \u9ed8\u8ba4\u503c: False | | --- | --- | Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0fshape\u4e00\u81f4 \u4f8b: >>> m = nn.LeakyReLU(0.1) >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"LeakyReLU"},{"location":"57/#threshold","text":"class torch.nn.Threshold(threshold, value, inplace=False) \u57fa\u4e8e Tensor \u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u521b\u9020\u9608\u503c\u51fd\u6570 Threshold \u88ab\u5b9a\u4e49\u4e3a y = x if x > threshold value if x <= threshold | Parameters: | threshold \u2013 \u9608\u503c value \u2013 \u8f93\u5165\u503c\u5c0f\u4e8e\u9608\u503c\u5219\u4f1a\u88ab value \u4ee3\u66ff inplace \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97. \u9ed8\u8ba4\u503c: False | | --- | --- | Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0f shape \u4e00\u81f4 \u4f8b: >>> m = nn.Threshold(0.1, 20) >>> input = Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"Threshold"},{"location":"57/#hardtanh","text":"class torch.nn.Hardtanh(min_val=-1, max_val=1, inplace=False, min_value=None, max_value=None) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528 HardTanh HardTanh \u88ab\u5b9a\u4e49\u4e3a: f(x) = +1, if x > 1 f(x) = -1, if x < -1 f(x) = x, otherwise \u7ebf\u6027\u533a\u57df\u7684\u8303\u56f4 \\([-1, 1]\\) \u53ef\u4ee5\u88ab\u8c03\u6574 | Parameters: | min_val \u2013 \u7ebf\u6027\u533a\u57df\u8303\u56f4\u6700\u5c0f\u503c. \u9ed8\u8ba4\u503c: -1 max_val \u2013 \u7ebf\u6027\u533a\u57df\u8303\u56f4\u6700\u5927\u503c. \u9ed8\u8ba4\u503c: 1 inplace \u2013 \u9009\u62e9\u662f\u5426\u8fdb\u884c\u8986\u76d6\u8fd0\u7b97. \u9ed8\u8ba4\u503c: False | | --- | --- | \u5173\u952e\u5b57\u53c2\u6570 min_value \u4ee5\u53ca max_value \u5df2\u88ab\u5f03\u7528. \u66f4\u6539\u4e3a min_val \u548c max_val Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u7ef4\u5ea6\u7ec4\u5408 Output: \\((N, *)\\) , \u4e0e\u8f93\u5165\u6709\u76f8\u540c\u7684 shape \u5c5e\u6027 \u4f8b >>> m = nn.Hardtanh(-2, 2) >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"Hardtanh"},{"location":"57/#sigmoid","text":"class torch.nn.Sigmoid \u5bf9\u6bcf\u4e2a\u5143\u7d20\u8fd0\u7528 Sigmoid \u51fd\u6570. Sigmoid \u5b9a\u4e49\u5982\u4e0b \\(f(x) = 1 / ( 1 + exp(-x))\\) Shape: Input: \\((N, *)\\) <cite>*</cite> \u8868\u793a\u4efb\u610f\u7ef4\u5ea6\u7ec4\u5408 Output: \\((N, *)\\) , \u4e0e\u8f93\u5165\u6709\u76f8\u540c\u7684 shape \u5c5e\u6027 Examples: >>> m = nn.Sigmoid() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"Sigmoid"},{"location":"57/#tanh","text":"class torch.nn.Tanh \u5bf9\u8f93\u5165\u7684\u6bcf\u4e2a\u5143\u7d20, \\(f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\\) Shape: Input: \\((N, *)\\) <cite>*</cite> \u8868\u793a\u4efb\u610f\u7ef4\u5ea6\u7ec4\u5408 Output: \\((N, *)\\) , \u4e0e\u8f93\u5165\u6709\u76f8\u540c\u7684 shape \u5c5e\u6027 Examples: >>> m = nn.Tanh() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"Tanh"},{"location":"57/#logsigmoid","text":"class torch.nn.LogSigmoid \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570 \\(LogSigmoid(x) = log( 1 / (1 + exp(-x_i)))\\) Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0fshape\u4e00\u81f4 \u4f8b: >>> m = nn.LogSigmoid() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"LogSigmoid"},{"location":"57/#softplus","text":"class torch.nn.Softplus(beta=1, threshold=20) \u5bf9\u6bcf\u4e2a\u5143\u7d20\u8fd0\u7528Softplus\u51fd\u6570, Softplus \u5b9a\u4e49\u5982\u4e0b :: \\(f(x) = 1/beta * log(1 + exp(beta * x_i))\\) Softplus \u51fd\u6570\u662fReLU\u51fd\u6570\u7684\u5e73\u6ed1\u903c\u8fd1. Softplus \u51fd\u6570\u53ef\u4ee5\u4f7f\u5f97\u8f93\u51fa\u503c\u9650\u5b9a\u4e3a\u6b63\u6570. \u4e3a\u4e86\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a\u6027. \u7ebf\u6027\u51fd\u6570\u7684\u8f6c\u6362\u53ef\u4ee5\u4f7f\u8f93\u51fa\u5927\u4e8e\u67d0\u4e2a\u503c. | Parameters: | beta \u2013 Softplus \u516c\u5f0f\u4e2d\u7684 beta \u503c. \u9ed8\u8ba4\u503c: 1 threshold \u2013 \u9608\u503c. \u5f53\u8f93\u5165\u5230\u8be5\u503c\u4ee5\u4e0a\u65f6\u6211\u4eec\u7684SoftPlus\u5b9e\u73b0\u5c06\u8fd8\u539f\u4e3a\u7ebf\u6027\u51fd\u6570. \u9ed8\u8ba4\u503c: 20 | | --- | --- | Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 dimensions Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0fshape\u4e00\u81f4 \u4f8b: >>> m = nn.Softplus() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"Softplus"},{"location":"57/#softshrink","text":"class torch.nn.Softshrink(lambd=0.5) \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528 soft shrinkage \u51fd\u6570 SoftShrinkage \u8fd0\u7b97\u7b26\u5b9a\u4e49\u4e3a: f(x) = x-lambda, if x > lambda > f(x) = x+lambda, if x < -lambda f(x) = 0, otherwise Parameters: lambd \u2013 Softshrink \u516c\u5f0f\u4e2d\u7684 lambda \u503c. \u9ed8\u8ba4\u503c: 0.5 Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0f shape \u4e00\u81f4 \u4f8b: >>> m = nn.Softshrink() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"Softshrink"},{"location":"57/#softsign","text":"class torch.nn.Softsign \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570 \\(f(x) = x / (1 + |x|)\\) Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0f shape \u4e00\u81f4 \u4f8b: >>> m = nn.Softsign() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"Softsign"},{"location":"57/#tanhshrink","text":"class torch.nn.Tanhshrink \u5bf9\u8f93\u5165\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u8fd0\u7528\u51fd\u6570, \\(Tanhshrink(x) = x - Tanh(x)\\) Shape: Input: \\((N, *)\\) \u5176\u4e2d <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u6570\u76ee\u7684\u9644\u52a0\u7ef4\u5ea6 Output: \\((N, *)\\) , \u548c\u8f93\u5165\u7684\u683c\u5f0fshape\u4e00\u81f4 \u4f8b: >>> m = nn.Tanhshrink() >>> input = autograd.Variable(torch.randn(2)) >>> print(input) >>> print(m(input))","title":"Tanhshrink"},{"location":"57/#softmin","text":"class torch.nn.Softmin(dim=None) \u5bf9n\u7ef4\u8f93\u5165\u5f20\u91cf\u8fd0\u7528 Softmin \u51fd\u6570, \u5c06\u5f20\u91cf\u7684\u6bcf\u4e2a\u5143\u7d20\u7f29\u653e\u5230 (0,1) \u533a\u95f4\u4e14\u548c\u4e3a 1. \\(f(x) = \\frac{\\exp(-x_i)}{\\sum_j \\exp(-x_j)}\\) Shape: Input: \u4efb\u610fshape Output: \u548c\u8f93\u5165\u76f8\u540c Parameters: dim ( int ) \u2013 \u8fd9\u662f\u5c06\u8ba1\u7b97 Softmax \u7684\u7ef4\u5ea6 (\u6240\u4ee5\u6bcf\u4e2a\u6cbf\u7740 dim \u7684\u5207\u7247\u548c\u4e3a 1). Returns: \u8fd4\u56de\u7ed3\u679c\u662f\u4e00\u4e2a\u4e0e\u8f93\u5165\u7ef4\u5ea6\u76f8\u540c\u7684\u5f20\u91cf, \u6bcf\u4e2a\u5143\u7d20\u7684\u53d6\u503c\u8303\u56f4\u5728 [0, 1] \u533a\u95f4. --- --- \u4f8b: >>> m = nn.Softmin() >>> input = autograd.Variable(torch.randn(2, 3)) >>> print(input) >>> print(m(input))","title":"Softmin"},{"location":"57/#softmax","text":"class torch.nn.Softmax(dim=None) \u5bf9n\u7ef4\u8f93\u5165\u5f20\u91cf\u8fd0\u7528 Softmax \u51fd\u6570, \u5c06\u5f20\u91cf\u7684\u6bcf\u4e2a\u5143\u7d20\u7f29\u653e\u5230 (0,1) \u533a\u95f4\u4e14\u548c\u4e3a 1. Softmax \u51fd\u6570\u5b9a\u4e49\u5982\u4e0b \\(f_i(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\) Shape: Input: \u4efb\u610fshape Output: \u548c\u8f93\u5165\u76f8\u540c Returns: \u8fd4\u56de\u7ed3\u679c\u662f\u4e00\u4e2a\u4e0e\u8f93\u5165\u7ef4\u5ea6\u76f8\u540c\u7684\u5f20\u91cf, \u6bcf\u4e2a\u5143\u7d20\u7684\u53d6\u503c\u8303\u56f4\u5728 [0, 1] \u533a\u95f4. Parameters: dim ( int ) \u2013 \u8fd9\u662f\u5c06\u8ba1\u7b97 Softmax \u7684\u90a3\u4e2a\u7ef4\u5ea6 (\u6240\u4ee5\u6bcf\u4e2a\u6cbf\u7740 dim \u7684\u5207\u7247\u548c\u4e3a 1). --- --- Note \u5982\u679c\u4f60\u60f3\u5bf9\u539f\u59cb Softmax \u6570\u636e\u8ba1\u7b97 Log \u8fdb\u884c\u6536\u7f29, \u5e76\u4e0d\u80fd\u4f7f\u8be5\u6a21\u5757\u76f4\u63a5\u4f7f\u7528 NLLLoss \u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u51fd\u6570. \u53d6\u800c\u4ee3\u4e4b, \u5e94\u8be5\u4f7f\u7528 Logsoftmax (\u5b83\u6709\u66f4\u5feb\u7684\u8fd0\u7b97\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6570\u503c\u6027\u8d28). \u4f8b: >>> m = nn.Softmax() >>> input = autograd.Variable(torch.randn(2, 3)) >>> print(input) >>> print(m(input))","title":"Softmax"},{"location":"57/#softmax2d","text":"class torch.nn.Softmax2d \u628a SoftMax \u5e94\u7528\u4e8e\u6bcf\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\u7684\u7279\u5f81. \u7ed9\u5b9a\u56fe\u7247\u7684 \u901a\u9053\u6570 Channels x \u9ad8 Height x \u5bbd Width, \u5b83\u5c06\u5bf9\u56fe\u7247\u7684\u6bcf\u4e00\u4e2a\u4f4d\u7f6e \u4f7f\u7528 Softmax \\((Channels, h_i, w_j)\\) Shape: Input: \\((N, C, H, W)\\) Output: \\((N, C, H, W)\\) (\u683c\u5f0f shape \u4e0e\u8f93\u5165\u76f8\u540c) Returns: \u4e00\u4e2a\u7ef4\u5ea6\u53ca\u683c\u5f0f shape \u90fd\u548c\u8f93\u5165\u76f8\u540c\u7684 Tensor, \u53d6\u503c\u8303\u56f4\u5728[0, 1] \u4f8b: >>> m = nn.Softmax2d() >>> # you softmax over the 2nd dimension >>> input = autograd.Variable(torch.randn(2, 3, 12, 13)) >>> print(input) >>> print(m(input))","title":"Softmax2d"},{"location":"57/#logsoftmax","text":"class torch.nn.LogSoftmax(dim=None) \u5bf9\u6bcf\u4e2a\u8f93\u5165\u7684 n \u7ef4 Tensor \u4f7f\u7528 Log(Softmax(x)). LogSoftmax \u516c\u5f0f\u53ef\u7b80\u5316\u4e3a \\(f_i(x) = log(exp(x_i) / sum_j exp(x_j) )\\) Shape: Input: \u4efb\u610f\u683c\u5f0f shape Output: \u548c\u8f93\u5165\u7684\u683c\u5f0f shape \u4e00\u81f4 Parameters: dim ( int ) \u2013 \u8fd9\u662f\u5c06\u8ba1\u7b97 Softmax \u7684\u90a3\u4e2a\u7ef4\u5ea6 (\u6240\u4ee5\u6bcf\u4e2a\u6cbf\u7740 dim \u7684\u5207\u7247\u548c\u4e3a1). Returns: \u4e00\u4e2a\u7ef4\u5ea6\u53ca\u683c\u5f0f shape \u90fd\u548c\u8f93\u5165\u76f8\u540c\u7684 Tensor, \u53d6\u503c\u8303\u56f4\u5728 [-inf, 0) --- --- \u4f8b: >>> m = nn.LogSoftmax() >>> input = autograd.Variable(torch.randn(2, 3)) >>> print(input) >>> print(m(input))","title":"LogSoftmax"},{"location":"57/#normalization-layers","text":"","title":"Normalization layers (\u5f52\u4e00\u5316\u5c42)"},{"location":"57/#batchnorm1d","text":"class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True) \u5bf9 2d \u6216\u8005 3d \u7684\u5c0f\u6279\u91cf (mini-batch) \u6570\u636e\u8fdb\u884c\u6279\u6807\u51c6\u5316 (Batch Normalization) \u64cd\u4f5c. \\[y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\\] \u6bcf\u4e2a\u5c0f\u6279\u91cf\u6570\u636e\u4e2d,\u8ba1\u7b97\u5404\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee,\u5e76\u4e14 gamma \u548c beta \u662f\u5927\u5c0f\u4e3a C \u7684\u53ef\u5b66\u4e60, \u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570\u5411\u91cf( C \u4e3a\u8f93\u5165\u5927\u5c0f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d,\u8be5\u5c42\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5e76\u8fdb\u884c\u5e73\u5747\u79fb\u52a8,\u9ed8\u8ba4\u7684\u5e73\u5747\u79fb\u52a8\u52a8\u91cf\u503c\u4e3a 0.1. \u5728\u9a8c\u8bc1\u65f6,\u8bad\u7ec3\u5f97\u5230\u7684\u5747\u503c/\u65b9\u5dee,\u7528\u4e8e\u6807\u51c6\u5316\u9a8c\u8bc1\u6570\u636e. BatchNorm \u5728 \u2018C\u2019 \u7ef4\u4e0a\u5904\u7406,\u5373 \u2018(N,L)\u2019 \u90e8\u5206\u8fd0\u884c,\u88ab\u79f0\u4f5c \u2018Temporal BatchNorm\u2019 | Parameters: | num_features \u2013 \u9884\u671f\u8f93\u5165\u7684\u7279\u5f81\u6570,\u5927\u5c0f\u4e3a \u2018batch_size x num_features [x width]\u2019 eps \u2013 \u7ed9\u5206\u6bcd\u52a0\u4e0a\u7684\u503c,\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a(\u5206\u6bcd\u4e0d\u80fd\u8d8b\u8fd10\u6216\u53d60),\u9ed8\u8ba4\u4e3a 1e-5 momentum \u2013 \u52a8\u6001\u5747\u503c\u548c\u52a8\u6001\u65b9\u5dee\u4f7f\u7528\u7684\u79fb\u52a8\u52a8\u91cf\u503c,\u9ed8\u8ba4\u4e3a 0.1 affine \u2013 \u5e03\u5c14\u503c,\u8bbe\u4e3a True \u65f6,\u8868\u793a\u8be5\u5c42\u6dfb\u52a0\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570,\u5373 gamma \u548c beta,\u9ed8\u8ba4\u4e3a True | | --- | --- | Shape: Input: \\((N, C)\\) or \\((N, C, L)\\) Output: \\((N, C)\\) or \\((N, C, L)\\) (same shape as input) Examples >>> # With Learnable Parameters >>> m = nn.BatchNorm1d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm1d(100, affine=False) >>> input = autograd.Variable(torch.randn(20, 100)) >>> output = m(input)","title":"BatchNorm1d"},{"location":"57/#batchnorm2d","text":"class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True) \u5bf9\u5c0f\u6279\u91cf (mini-batch) 3d \u6570\u636e\u7ec4\u6210\u7684 4d \u8f93\u5165\u8fdb\u884c\u6807\u51c6\u5316 (Batch Normalization) \u64cd\u4f5c. \\[y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\\] \u6bcf\u4e2a\u5c0f\u6279\u91cf\u6570\u636e\u4e2d,\u8ba1\u7b97\u5404\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee, \u5e76\u4e14 gamma \u548c beta \u662f\u5927\u5c0f\u4e3a C \u7684\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570\u5411\u91cf (C \u4e3a\u8f93\u5165\u5927\u5c0f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d,\u8be5\u5c42\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5e76\u8fdb\u884c\u5e73\u5747\u79fb\u52a8.\u9ed8\u8ba4\u7684\u5e73\u5747\u79fb\u52a8\u52a8\u91cf\u503c\u4e3a 0.1. \u5728\u9a8c\u8bc1\u65f6,\u8bad\u7ec3\u5f97\u5230\u7684\u5747\u503c/\u65b9\u5dee,\u7528\u4e8e\u6807\u51c6\u5316\u9a8c\u8bc1\u6570\u636e. BatchNorm \u5728 \u2018C\u2019 \u7ef4\u4e0a\u5904\u7406,\u5373 \u2018(N, H, W)\u2019 \u90e8\u5206\u8fd0\u884c,\u88ab\u79f0\u4f5c \u2018Spatial BatchNorm\u2019. | Parameters: | num_features \u2013 \u9884\u671f\u8f93\u5165\u7684\u7279\u5f81\u6570,\u5927\u5c0f\u4e3a \u2018batch_size x num_features x height x width\u2019 eps \u2013 \u7ed9\u5206\u6bcd\u52a0\u4e0a\u7684\u503c,\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a(\u5206\u6bcd\u4e0d\u80fd\u8d8b\u8fd10\u6216\u53d60),\u9ed8\u8ba4\u4e3a 1e-5 momentum \u2013 \u52a8\u6001\u5747\u503c\u548c\u52a8\u6001\u65b9\u5dee\u4f7f\u7528\u7684\u79fb\u52a8\u52a8\u91cf\u503c,\u9ed8\u8ba4\u4e3a 0.1 affine \u2013 \u5e03\u5c14\u503c,\u8bbe\u4e3a True \u65f6,\u8868\u793a\u8be5\u5c42\u6dfb\u52a0\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570,\u5373 gamma \u548c beta,\u9ed8\u8ba4\u4e3a True | | --- | --- | Shape: Input: \\((N, C, H, W)\\) Output: \\((N, C, H, W)\\) (same shape as input) Examples >>> # With Learnable Parameters >>> m = nn.BatchNorm2d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm2d(100, affine=False) >>> input = autograd.Variable(torch.randn(20, 100, 35, 45)) >>> output = m(input)","title":"BatchNorm2d"},{"location":"57/#batchnorm3d","text":"class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True) \u5bf9\u5c0f\u6279\u91cf (mini-batch) 4d \u6570\u636e\u7ec4\u6210\u7684 5d \u8f93\u5165\u8fdb\u884c\u6807\u51c6\u5316 (Batch Normalization) \u64cd\u4f5c. \\[y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\\] \u6bcf\u4e2a\u5c0f\u6279\u91cf\u6570\u636e\u4e2d,\u8ba1\u7b97\u5404\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee, \u5e76\u4e14 gamma \u548c beta \u662f\u5927\u5c0f\u4e3a C \u7684\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570\u5411\u91cf (C \u4e3a\u8f93\u5165\u5927\u5c0f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d,\u8be5\u5c42\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5e76\u8fdb\u884c\u5e73\u5747\u79fb\u52a8.\u9ed8\u8ba4\u7684\u5e73\u5747\u79fb\u52a8\u52a8\u91cf\u503c\u4e3a 0.1. \u5728\u9a8c\u8bc1\u65f6,\u8bad\u7ec3\u5f97\u5230\u7684\u5747\u503c/\u65b9\u5dee,\u7528\u4e8e\u6807\u51c6\u5316\u9a8c\u8bc1\u6570\u636e. BatchNorm \u5728 \u2018C\u2019 \u7ef4\u4e0a\u5904\u7406,\u5373 \u2018(N, D, H, W)\u2019 \u90e8\u5206\u8fd0\u884c,\u88ab\u79f0\u4f5c \u2018Volumetric BatchNorm\u2019 \u6216\u8005 \u2018Spatio-temporal BatchNorm\u2019 | Parameters: | num_features \u2013 \u9884\u671f\u8f93\u5165\u7684\u7279\u5f81\u6570,\u5927\u5c0f\u4e3a \u2018batch_size x num_features x depth x height x width\u2019 eps \u2013 \u7ed9\u5206\u6bcd\u52a0\u4e0a\u7684\u503c,\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a(\u5206\u6bcd\u4e0d\u80fd\u8d8b\u8fd10\u6216\u53d60),\u9ed8\u8ba4\u4e3a 1e-5 momentum \u2013 \u52a8\u6001\u5747\u503c\u548c\u52a8\u6001\u65b9\u5dee\u4f7f\u7528\u7684\u79fb\u52a8\u52a8\u91cf\u503c,\u9ed8\u8ba4\u4e3a 0.1 affine \u2013 \u5e03\u5c14\u503c,\u8bbe\u4e3a True \u65f6,\u8868\u793a\u8be5\u5c42\u6dfb\u52a0\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570,\u5373 gamma \u548c beta,\u9ed8\u8ba4\u4e3a True | | --- | --- | Shape: Input: \\((N, C, D, H, W)\\) Output: \\((N, C, D, H, W)\\) (same shape as input) Examples >>> # With Learnable Parameters >>> m = nn.BatchNorm3d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm3d(100, affine=False) >>> input = autograd.Variable(torch.randn(20, 100, 35, 45, 10)) >>> output = m(input)","title":"BatchNorm3d"},{"location":"57/#instancenorm1d","text":"class torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False) \u5bf9 2d \u6216\u8005 3d \u7684\u5c0f\u6279\u91cf (mini-batch) \u6570\u636e\u8fdb\u884c\u5b9e\u4f8b\u6807\u51c6\u5316 (Instance Normalization) \u64cd\u4f5c. .. math: y = \\frac{x - mean[x]}{ \\sqrt{Var[x]} + \\epsilon} * gamma + beta \u5bf9\u5c0f\u6279\u91cf\u6570\u636e\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5bf9\u8c61,\u8ba1\u7b97\u5176\u5404\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee,\u5e76\u4e14 gamma \u548c beta \u662f\u5927\u5c0f\u4e3a C \u7684\u53ef\u5b66\u4e60, \u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570\u5411\u91cf( C \u4e3a\u8f93\u5165\u5927\u5c0f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d,\u8be5\u5c42\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5e76\u8fdb\u884c\u5e73\u5747\u79fb\u52a8,\u9ed8\u8ba4\u7684\u5e73\u5747\u79fb\u52a8\u52a8\u91cf\u503c\u4e3a 0.1. \u5728\u9a8c\u8bc1\u65f6 (<cite>.eval()</cite>),InstanceNorm \u6a21\u578b\u9ed8\u8ba4\u4fdd\u6301\u4e0d\u53d8,\u5373\u6c42\u5f97\u7684\u5747\u503c/\u65b9\u5dee\u4e0d\u7528\u4e8e\u6807\u51c6\u5316\u9a8c\u8bc1\u6570\u636e, \u4f46\u53ef\u4ee5\u7528 <cite>.train(False)</cite> \u65b9\u6cd5\u5f3a\u5236\u4f7f\u7528\u5b58\u50a8\u7684\u5747\u503c\u548c\u65b9\u5dee. | Parameters: | num_features \u2013 \u9884\u671f\u8f93\u5165\u7684\u7279\u5f81\u6570,\u5927\u5c0f\u4e3a \u2018batch_size x num_features x width\u2019 eps \u2013 \u7ed9\u5206\u6bcd\u52a0\u4e0a\u7684\u503c,\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a(\u5206\u6bcd\u4e0d\u80fd\u8d8b\u8fd10\u6216\u53d60),\u9ed8\u8ba4\u4e3a 1e-5 momentum \u2013 \u52a8\u6001\u5747\u503c\u548c\u52a8\u6001\u65b9\u5dee\u4f7f\u7528\u7684\u79fb\u52a8\u52a8\u91cf\u503c,\u9ed8\u8ba4\u4e3a 0.1 affine \u2013 \u5e03\u5c14\u503c,\u8bbe\u4e3a True \u65f6,\u8868\u793a\u8be5\u5c42\u6dfb\u52a0\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570,\u5373 gamma \u548c beta,\u9ed8\u8ba4\u4e3a False | | --- | --- | Shape: Input: \\((N, C, L)\\) Output: \\((N, C, L)\\) (same shape as input) Examples >>> # Without Learnable Parameters >>> m = nn.InstanceNorm1d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm1d(100, affine=True) >>> input = autograd.Variable(torch.randn(20, 100, 40)) >>> output = m(input)","title":"InstanceNorm1d"},{"location":"57/#instancenorm2d","text":"class torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False) \u5bf9\u5c0f\u6279\u91cf (mini-batch) 3d \u6570\u636e\u7ec4\u6210\u7684 4d \u8f93\u5165\u8fdb\u884c\u5b9e\u4f8b\u6807\u51c6\u5316 (Batch Normalization) \u64cd\u4f5c. .. math: y = \\frac{x - mean[x]}{ \\sqrt{Var[x]} + \\epsilon} * gamma + beta \u5bf9\u5c0f\u6279\u91cf\u6570\u636e\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5bf9\u8c61,\u8ba1\u7b97\u5176\u5404\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee,\u5e76\u4e14 gamma \u548c beta \u662f\u5927\u5c0f\u4e3a C \u7684\u53ef\u5b66\u4e60, \u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570\u5411\u91cf( C \u4e3a\u8f93\u5165\u5927\u5c0f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d,\u8be5\u5c42\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5e76\u8fdb\u884c\u5e73\u5747\u79fb\u52a8,\u9ed8\u8ba4\u7684\u5e73\u5747\u79fb\u52a8\u52a8\u91cf\u503c\u4e3a 0.1. \u5728\u9a8c\u8bc1\u65f6 (<cite>.eval()</cite>),InstanceNorm \u6a21\u578b\u9ed8\u8ba4\u4fdd\u6301\u4e0d\u53d8,\u5373\u6c42\u5f97\u7684\u5747\u503c/\u65b9\u5dee\u4e0d\u7528\u4e8e\u6807\u51c6\u5316\u9a8c\u8bc1\u6570\u636e, \u4f46\u53ef\u4ee5\u7528 <cite>.train(False)</cite> \u65b9\u6cd5\u5f3a\u5236\u4f7f\u7528\u5b58\u50a8\u7684\u5747\u503c\u548c\u65b9\u5dee. | Parameters: | num_features \u2013 \u9884\u671f\u8f93\u5165\u7684\u7279\u5f81\u6570,\u5927\u5c0f\u4e3a \u2018batch_size x num_features x height x width\u2019 eps \u2013 \u7ed9\u5206\u6bcd\u52a0\u4e0a\u7684\u503c,\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a(\u5206\u6bcd\u4e0d\u80fd\u8d8b\u8fd10\u6216\u53d60),\u9ed8\u8ba4\u4e3a 1e-5 momentum \u2013 \u52a8\u6001\u5747\u503c\u548c\u52a8\u6001\u65b9\u5dee\u4f7f\u7528\u7684\u79fb\u52a8\u52a8\u91cf\u503c,\u9ed8\u8ba4\u4e3a 0.1 affine \u2013 \u5e03\u5c14\u503c,\u8bbe\u4e3a True \u65f6,\u8868\u793a\u8be5\u5c42\u6dfb\u52a0\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570,\u5373 gamma \u548c beta,\u9ed8\u8ba4\u4e3a False | | --- | --- | Shape: Input: \\((N, C, H, W)\\) Output: \\((N, C, H, W)\\) (same shape as input) Examples >>> # Without Learnable Parameters >>> m = nn.InstanceNorm2d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm2d(100, affine=True) >>> input = autograd.Variable(torch.randn(20, 100, 35, 45)) >>> output = m(input)","title":"InstanceNorm2d"},{"location":"57/#instancenorm3d","text":"class torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1, affine=False) \u5bf9\u5c0f\u6279\u91cf (mini-batch) 4d \u6570\u636e\u7ec4\u6210\u7684 5d \u8f93\u5165\u8fdb\u884c\u5b9e\u4f8b\u6807\u51c6\u5316 (Batch Normalization) \u64cd\u4f5c. .. math: y = \\frac{x - mean[x]}{ \\sqrt{Var[x]} + \\epsilon} * gamma + beta \u5bf9\u5c0f\u6279\u91cf\u6570\u636e\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5bf9\u8c61,\u8ba1\u7b97\u5176\u5404\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee,\u5e76\u4e14 gamma \u548c beta \u662f\u5927\u5c0f\u4e3a C \u7684\u53ef\u5b66\u4e60, \u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570\u5411\u91cf( C \u4e3a\u8f93\u5165\u5927\u5c0f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d,\u8be5\u5c42\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5e76\u8fdb\u884c\u5e73\u5747\u79fb\u52a8,\u9ed8\u8ba4\u7684\u5e73\u5747\u79fb\u52a8\u52a8\u91cf\u503c\u4e3a 0.1. \u5728\u9a8c\u8bc1\u65f6 (<cite>.eval()</cite>),InstanceNorm \u6a21\u578b\u9ed8\u8ba4\u4fdd\u6301\u4e0d\u53d8,\u5373\u6c42\u5f97\u7684\u5747\u503c/\u65b9\u5dee\u4e0d\u7528\u4e8e\u6807\u51c6\u5316\u9a8c\u8bc1\u6570\u636e, \u4f46\u53ef\u4ee5\u7528 <cite>.train(False)</cite> \u65b9\u6cd5\u5f3a\u5236\u4f7f\u7528\u5b58\u50a8\u7684\u5747\u503c\u548c\u65b9\u5dee. | Parameters: | num_features \u2013 \u9884\u671f\u8f93\u5165\u7684\u7279\u5f81\u6570,\u5927\u5c0f\u4e3a \u2018batch_size x num_features x depth x height x width\u2019 eps \u2013 \u7ed9\u5206\u6bcd\u52a0\u4e0a\u7684\u503c,\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a(\u5206\u6bcd\u4e0d\u80fd\u8d8b\u8fd10\u6216\u53d60),\u9ed8\u8ba4\u4e3a 1e-5 momentum \u2013 \u52a8\u6001\u5747\u503c\u548c\u52a8\u6001\u65b9\u5dee\u4f7f\u7528\u7684\u79fb\u52a8\u52a8\u91cf\u503c,\u9ed8\u8ba4\u4e3a 0.1 affine \u2013 \u5e03\u5c14\u503c,\u8bbe\u4e3a True \u65f6,\u8868\u793a\u8be5\u5c42\u6dfb\u52a0\u53ef\u5b66\u4e60,\u53ef\u6539\u53d8\u7684\u4eff\u5c04\u53c2\u6570,\u5373 gamma \u548c beta,\u9ed8\u8ba4\u4e3a False | | --- | --- | Shape: Input: \\((N, C, D, H, W)\\) Output: \\((N, C, D, H, W)\\) (same shape as input) Examples >>> # Without Learnable Parameters >>> m = nn.InstanceNorm3d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm3d(100, affine=True) >>> input = autograd.Variable(torch.randn(20, 100, 35, 45, 10)) >>> output = m(input)","title":"InstanceNorm3d"},{"location":"57/#recurrent-layers","text":"","title":"Recurrent layers (\u5faa\u73af\u5c42)"},{"location":"57/#rnn","text":"class torch.nn.RNN(*args, **kwargs) \u5bf9\u4e8e\u8f93\u5165\u5e8f\u5217\u4f7f\u7528\u4e00\u4e2a\u591a\u5c42\u7684 Elman RNN , \u5b83\u7684\u6fc0\u6d3b\u51fd\u6570\u4e3a tanh \u6216\u8005 ReLU . \u5bf9\u8f93\u5165\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u5143\u7d20, \u6bcf\u5c42\u8ba1\u7b97\u516c\u5f0f\u4e3a: \\[h_t = \\tanh(w_{ih} * x_t + b_{ih} + w_{hh} * h_{(t-1)} + b_{hh})\\] \u8fd9\u91cc \\(h_t\\) \u662f\u5f53\u524d\u5728\u65f6\u523b <cite>t</cite> \u7684\u9690\u72b6\u6001, \u5e76\u4e14 \\(x_t\\) \u662f\u4e4b\u524d\u4e00\u5c42\u5728 <cite>t</cite> \u65f6\u523b\u7684\u9690\u72b6\u6001, \u6216\u8005\u662f\u7b2c\u4e00\u5c42\u7684\u8f93\u5165. \u5982\u679c nonlinearity='relu' ,\u90a3\u4e48\u5c06\u4f7f\u7528 relu \u4ee3\u66ff tanh \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570. | Parameters: | input_size \u2013 \u8f93\u5165 x \u7684\u7279\u5f81\u6570\u91cf hidden_size \u2013 \u9690\u72b6\u6001 h \u4e2d\u7684\u7279\u5f81\u6570\u91cf num_layers \u2013 RNN \u7684\u5c42\u6570 nonlinearity \u2013 \u6307\u5b9a\u975e\u7ebf\u6027\u51fd\u6570\u4f7f\u7528 [\u2018tanh\u2019|\u2019relu\u2019]. \u9ed8\u8ba4: \u2018tanh\u2019 bias \u2013 \u5982\u679c\u662f False , \u90a3\u4e48 RNN \u5c42\u5c31\u4e0d\u4f1a\u4f7f\u7528\u504f\u7f6e\u6743\u91cd b_ih \u548c b_hh, \u9ed8\u8ba4: True batch_first \u2013 \u5982\u679c True , \u90a3\u4e48\u8f93\u5165 Tensor \u7684 shape \u5e94\u8be5\u662f (batch, seq, feature),\u5e76\u4e14\u8f93\u51fa\u4e5f\u662f\u4e00\u6837 dropout \u2013 \u5982\u679c\u503c\u975e\u96f6, \u90a3\u4e48\u9664\u4e86\u6700\u540e\u4e00\u5c42\u5916, \u5176\u5b83\u5c42\u7684\u8f93\u51fa\u90fd\u4f1a\u5957\u4e0a\u4e00\u4e2a dropout \u5c42 bidirectional \u2013 \u5982\u679c True , \u5c06\u4f1a\u53d8\u6210\u4e00\u4e2a\u53cc\u5411 RNN, \u9ed8\u8ba4\u4e3a False | | --- | --- | Inputs: input, h_0 input (seq_len, batch, input_size): \u5305\u542b\u8f93\u5165\u5e8f\u5217\u7279\u5f81\u7684 tensor , input \u53ef\u4ee5\u662f\u88ab\u586b\u5145\u7684\u53d8\u957f\u5e8f\u5217.\u7ec6\u8282\u8bf7\u770b torch.nn.utils.rnn.pack_padded_sequence() . h_0 (num_layers * num_directions, batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u4fdd\u5b58\u7740\u521d\u59cb\u9690\u72b6\u6001\u7684 tensor Outputs: output, h_n output (seq_len, batch, hidden_size * num_directions): \u5305\u542b RNN \u6700\u540e\u4e00\u5c42\u8f93\u51fa\u7279\u5f81 (h_k) \u7684 tensor \u5bf9\u4e8e\u6bcf\u4e2a k ,\u5982\u679c\u8f93\u5165\u662f\u4e00\u4e2a torch.nn.utils.rnn.PackedSequence , \u90a3\u4e48\u8f93\u51fa\u4e5f\u662f\u4e00\u4e2a\u53ef\u4ee5\u662f\u88ab\u586b\u5145\u7684\u53d8\u957f\u5e8f\u5217. h_n (num_layers * num_directions, batch, hidden_size): \u5305\u542b k= seq_len \u9690\u72b6\u6001\u7684 tensor . | Variables: | weight_ih_l[k] \u2013 \u7b2c k \u5c42\u7684 input-hidden \u6743\u91cd,\u53ef\u5b66\u4e60, shape \u662f <cite>(input_size x hidden_size)</cite> weight_hh_l[k] \u2013 \u7b2c k \u5c42\u7684 hidden-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, shape \u662f <cite>(hidden_size x hidden_size)</cite> bias_ih_l[k] \u2013 \u7b2c k \u5c42\u7684 input-hidden \u504f\u7f6e, \u53ef\u5b66\u4e60, shape \u662f <cite>(hidden_size)</cite> bias_hh_l[k] \u2013 \u7b2c k \u5c42\u7684 hidden-hidden \u504f\u7f6e, \u53ef\u5b66\u4e60, shape \u662f <cite>(hidden_size)</cite> | | --- | --- | Examples: >>> rnn = nn.RNN(10, 20, 2) >>> input = Variable(torch.randn(5, 3, 10)) >>> h0 = Variable(torch.randn(2, 3, 20)) >>> output, hn = rnn(input, h0)","title":"RNN"},{"location":"57/#lstm","text":"class torch.nn.LSTM(*args, **kwargs) \u5bf9\u4e8e\u8f93\u5165\u5e8f\u5217\u4f7f\u7528\u4e00\u4e2a\u591a\u5c42\u7684 LSTM ( long short-term memory ). \u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u6bcf\u4e2a\u5143\u7d20, LSTM \u7684\u6bcf\u5c42\u90fd\u4f1a\u6267\u884c\u4ee5\u4e0b\u8ba1\u7b97: \\[\\begin{split}\\begin{array}{ll} i_t = \\mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\ f_t = \\mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\ g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\ o_t = \\mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\ c_t = f_t * c_{(t-1)} + i_t * g_t \\\\ h_t = o_t * \\tanh(c_t) \\end{array}\\end{split}\\] \u8fd9\u91cc \\(h_t\\) \u662f\u5728\u65f6\u523b <cite>t</cite> \u7684\u9690\u72b6\u6001, \\(c_t\\) \u662f\u5728\u65f6\u523b <cite>t</cite> \u7684\u7ec6\u80de\u72b6\u6001 (cell state), \\(x_t\\) \u662f\u4e0a\u4e00\u5c42\u7684\u5728\u65f6\u523b <cite>t</cite> \u7684\u9690\u72b6\u6001\u6216\u8005\u662f\u7b2c\u4e00\u5c42\u7684 \\(input_t\\) , \u800c \\(i_t\\) , \\(f_t\\) , \\(g_t\\) , \\(o_t\\) \u5206\u522b\u4ee3\u8868 \u8f93\u5165\u95e8,\u9057\u5fd8\u95e8,\u7ec6\u80de\u548c\u8f93\u51fa\u95e8. | Parameters: | input_size \u2013 \u8f93\u5165\u7684\u7279\u5f81\u7ef4\u5ea6 hidden_size \u2013 \u9690\u72b6\u6001\u7684\u7279\u5f81\u7ef4\u5ea6 num_layers \u2013 \u5c42\u6570(\u548c\u65f6\u5e8f\u5c55\u5f00\u8981\u533a\u5206\u5f00) bias \u2013 \u5982\u679c\u4e3a False ,\u90a3\u4e48 LSTM \u5c06\u4e0d\u4f1a\u4f7f\u7528 b_ih \u548c b_hh ,\u9ed8\u8ba4: True batch_first \u2013 \u5982\u679c\u4e3a True , \u90a3\u4e48\u8f93\u5165\u548c\u8f93\u51fa Tensor \u7684\u5f62\u72b6\u4e3a (batch, seq, feature) dropout \u2013 \u5982\u679c\u975e\u96f6\u7684\u8bdd, \u5c06\u4f1a\u5728 RNN \u7684\u8f93\u51fa\u4e0a\u52a0\u4e2a dropout , \u6700\u540e\u4e00\u5c42\u9664\u5916 bidirectional \u2013 \u5982\u679c\u4e3a True ,\u5c06\u4f1a\u53d8\u6210\u4e00\u4e2a\u53cc\u5411 RNN ,\u9ed8\u8ba4\u4e3a False | | --- | --- | Inputs: input, (h_0, c_0) input (seq_len, batch, input_size): \u5305\u542b\u8f93\u5165\u5e8f\u5217\u7279\u5f81\u7684 tensor . \u4e5f\u53ef\u4ee5\u662f packed variable length sequence , \u8be6\u89c1 torch.nn.utils.rnn.pack_padded_sequence() . h_0 (num_layers * num_directions, batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u9690\u72b6\u6001\u7684 tensor . c_0 (num_layers * num_directions, batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u7ec6\u80de\u72b6\u6001\u7684 tensor . Outputs: output, (h_n, c_n) output (seq_len, batch, hidden_size * num_directions): \u5305\u542b RNN \u6700\u540e\u4e00\u5c42\u7684\u8f93\u51fa\u7279\u5f81 <cite>(h_t)</cite> \u7684 tensor , \u5bf9\u4e8e\u6bcf\u4e2a t . \u5982\u679c\u8f93\u5165\u662f torch.nn.utils.rnn.PackedSequence \u90a3\u4e48\u8f93\u51fa\u4e5f\u662f\u4e00\u4e2a\u53ef\u4ee5\u662f\u88ab\u586b\u5145\u7684\u53d8\u957f\u5e8f\u5217. h_n (num_layers * num_directions, batch, hidden_size): \u5305\u542b t=seq_len \u9690\u72b6\u6001\u7684 tensor . c_n (num_layers * num_directions, batch, hidden_size): \u5305\u542b t=seq_len \u7ec6\u80de\u72b6\u6001\u7684 tensor . | Variables: | weight_ih_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 input-hidden \u6743\u91cd <cite>(W_ii|W_if|W_ig|W_io)</cite>, shape \u662f <cite>(4*hidden_size x input_size)</cite> weight_hh_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 hidden-hidden \u6743\u91cd <cite>(W_hi|W_hf|W_hg|W_ho)</cite>, shape \u662f <cite>(4*hidden_size x hidden_size)</cite> bias_ih_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 input-hidden \u504f\u7f6e <cite>(b_ii|b_if|b_ig|b_io)</cite>, shape \u662f <cite>(4*hidden_size)</cite> bias_hh_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 hidden-hidden \u504f\u7f6e <cite>(b_hi|b_hf|b_hg|b_ho)</cite>, shape \u662f <cite>(4*hidden_size)</cite> | | --- | --- | Examples: >>> rnn = nn.LSTM(10, 20, 2) >>> input = Variable(torch.randn(5, 3, 10)) >>> h0 = Variable(torch.randn(2, 3, 20)) >>> c0 = Variable(torch.randn(2, 3, 20)) >>> output, hn = rnn(input, (h0, c0))","title":"LSTM"},{"location":"57/#gru","text":"class torch.nn.GRU(*args, **kwargs) \u5bf9\u4e8e\u8f93\u5165\u5e8f\u5217\u4f7f\u7528\u4e00\u4e2a\u591a\u5c42\u7684 GRU (gated recurrent unit). \u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u6bcf\u4e2a\u5143\u7d20, \u6bcf\u5c42\u90fd\u4f1a\u6267\u884c\u4ee5\u4e0b\u8ba1\u7b97: \\[\\begin{split}\\begin{array}{ll} r_t = \\mathrm{sigmoid}(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\ z_t = \\mathrm{sigmoid}(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\ n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \\\\ \\end{array}\\end{split}\\] \u8fd9\u91cc \\(h_t\\) \u662f\u5728\u65f6\u523b <cite>t</cite> \u7684\u9690\u72b6\u6001, \\(x_t\\) \u662f\u524d\u4e00\u5c42\u5728\u65f6\u523b <cite>t</cite> \u7684\u9690\u72b6\u6001\u6216\u8005\u662f\u7b2c\u4e00\u5c42\u7684 \\(input_t\\) , \u800c \\(r_t\\) , \\(z_t\\) , \\(n_t\\) \u5206\u522b\u662f\u91cd\u7f6e\u95e8,\u8f93\u5165\u95e8\u548c\u65b0\u95e8. | Parameters: | input_size \u2013 \u8f93\u5165\u7684\u7279\u5f81\u7ef4\u5ea6 hidden_size \u2013 \u9690\u72b6\u6001\u7684\u7279\u5f81\u7ef4\u5ea6 num_layers \u2013 RNN \u7684\u5c42\u6570 bias \u2013 \u5982\u679c\u4e3a False , \u90a3\u4e48 RNN \u5c42\u5c06\u4e0d\u4f1a\u4f7f\u7528\u504f\u7f6e\u6743\u91cd b_ih \u548c b_hh \u9ed8\u8ba4: True batch_first \u2013 \u5982\u679c\u4e3a True , \u90a3\u4e48\u8f93\u5165\u548c\u8f93\u51fa\u7684 tensor \u7684\u5f62\u72b6\u662f (batch, seq, feature) dropout \u2013 \u5982\u679c\u975e\u96f6\u7684\u8bdd,\u5c06\u4f1a\u5728 RNN \u7684\u8f93\u51fa\u4e0a\u52a0\u4e2a dropout ,\u6700\u540e\u4e00\u5c42\u9664\u5916 bidirectional \u2013 \u5982\u679c\u4e3a True , \u5c06\u4f1a\u53d8\u6210\u4e00\u4e2a\u53cc\u5411 RNN . \u9ed8\u8ba4: False | | --- | --- | Inputs: input, h_0 input (seq_len, batch, input_size): \u5305\u542b\u8f93\u5165\u5e8f\u5217\u7279\u5f81\u7684 tensor . \u4e5f\u53ef\u4ee5\u662f packed variable length sequence , \u8be6\u89c1 torch.nn.utils.rnn.pack_padded_sequence() . h_0 (num_layers * num_directions, batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u9690\u72b6\u6001\u7684 tensor Outputs: output, h_n output (seq_len, batch, hidden_size * num_directions): \u5305\u542b RNN \u6700\u540e\u4e00\u5c42\u7684\u8f93\u51fa\u7279\u5f81 <cite>(h_t)</cite> \u7684 tensor , \u5bf9\u4e8e\u6bcf\u4e2a t . \u5982\u679c\u8f93\u5165\u662f torch.nn.utils.rnn.PackedSequence \u90a3\u4e48\u8f93\u51fa\u4e5f\u662f\u4e00\u4e2a\u53ef\u4ee5\u662f\u88ab\u586b\u5145\u7684\u53d8\u957f\u5e8f\u5217. h_n (num_layers * num_directions, batch, hidden_size): \u5305\u542b t=seq_len \u9690\u72b6\u6001\u7684 tensor . | Variables: | weight_ih_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 input-hidden \u6743\u91cd (W_ir|W_iz|W_in), shape \u4e3a <cite>(3*hidden_size x input_size)</cite> weight_hh_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 hidden-hidden \u6743\u91cd (W_hr|W_hz|W_hn), shape \u4e3a <cite>(3*hidden_size x hidden_size)</cite> bias_ih_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 input-hidden \u504f\u7f6e (b_ir|b_iz|b_in), shape \u4e3a <cite>(3*hidden_size)</cite> bias_hh_l[k] \u2013 \u7b2c k \u5c42\u53ef\u5b66\u4e60\u7684 hidden-hidden \u504f\u7f6e (b_hr|b_hz|b_hn), shape \u4e3a <cite>(3*hidden_size)</cite> | | --- | --- | Examples: >>> rnn = nn.GRU(10, 20, 2) >>> input = Variable(torch.randn(5, 3, 10)) >>> h0 = Variable(torch.randn(2, 3, 20)) >>> output, hn = rnn(input, h0)","title":"GRU"},{"location":"57/#rnncell","text":"class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh') \u4e00\u4e2a Elan RNN cell , \u6fc0\u6d3b\u51fd\u6570\u662f tanh \u6216 ReLU , \u7528\u4e8e\u8f93\u5165\u5e8f\u5217. \\[h' = \\tanh(w_{ih} * x + b_{ih} + w_{hh} * h + b_{hh})\\] \u5982\u679c nonlinearity=\u2019relu\u2019, \u90a3\u4e48\u5c06\u4f1a\u4f7f\u7528 ReLU \u6765\u4ee3\u66ff tanh . | Parameters: | input_size \u2013 \u8f93\u5165\u7684\u7279\u5f81\u7ef4\u5ea6 hidden_size \u2013 \u9690\u72b6\u6001\u7684\u7279\u5f81\u7ef4\u5ea6 bias \u2013 \u5982\u679c\u4e3a False , \u90a3\u4e48RNN\u5c42\u5c06\u4e0d\u4f1a\u4f7f\u7528\u504f\u7f6e\u6743\u91cd b_ih \u548c b_hh. \u9ed8\u8ba4: True nonlinearity \u2013 \u7528\u4e8e\u9009\u62e9\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570 [\u2018tanh\u2019|\u2019relu\u2019]. \u9ed8\u8ba4: \u2018tanh\u2019 | | --- | --- | Inputs: input, hidden input (batch, input_size): \u5305\u542b\u8f93\u5165\u7279\u5f81\u7684 tensor . hidden (batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u9690\u72b6\u6001\u7684 tensor . Outputs: h\u2019 h\u2019 (batch, hidden_size): \u4fdd\u5b58\u7740 batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u4e0b\u4e00\u5c42\u9690\u72b6\u6001\u7684 tensor . | Variables: | weight_ih \u2013 input-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, shape \u4e3a <cite>(input_size x hidden_size)</cite> weight_hh \u2013 hidden-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, shape \u4e3a <cite>(hidden_size x hidden_size)</cite> bias_ih \u2013 input-hidden \u504f\u7f6e,\u53ef\u5b66\u4e60, shape \u4e3a <cite>(hidden_size)</cite> bias_hh \u2013 hidden-hidden \u504f\u7f6e,\u53ef\u5b66\u4e60, shape \u4e3a <cite>(hidden_size)</cite> | | --- | --- | Examples: >>> rnn = nn.RNNCell(10, 20) >>> input = Variable(torch.randn(6, 3, 10)) >>> hx = Variable(torch.randn(3, 20)) >>> output = [] >>> for i in range(6): ... hx = rnn(input[i], hx) ... output.append(hx)","title":"RNNCell"},{"location":"57/#lstmcell","text":"class torch.nn.LSTMCell(input_size, hidden_size, bias=True) LSTM \u7ec6\u80de. \\[\\begin{split}\\begin{array}{ll} i = \\mathrm{sigmoid}(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\ f = \\mathrm{sigmoid}(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\ g = \\tanh(W_{ig} x + b_{ig} + W_{hc} h + b_{hg}) \\\\ o = \\mathrm{sigmoid}(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\ c' = f * c + i * g \\\\ h' = o * \\tanh(c') \\\\ \\end{array}\\end{split}\\] | Parameters: | input_size \u2013 \u8f93\u5165\u7684\u7279\u5f81\u7ef4\u5ea6 hidden_size \u2013 \u9690\u72b6\u6001\u7684\u7ef4\u5ea6 bias \u2013 \u5982\u679c\u4e3a <cite>False</cite>, \u90a3\u4e48RNN\u5c42\u5c06\u4e0d\u4f1a\u4f7f\u7528\u504f\u7f6e\u6743\u91cd b_ih \u548c b_hh \u9ed8\u8ba4: True | | --- | --- | Inputs: input, (h_0, c_0) input (batch, input_size): \u5305\u542b\u8f93\u5165\u7279\u5f81\u7684 tensor . h_0 (batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u9690\u72b6\u6001\u7684 tensor . c_0 (batch. hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u7ec6\u80de\u72b6\u6001\u7684 tensor Outputs: h_1, c_1 h_1 (batch, hidden_size): \u4fdd\u5b58\u7740 batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u4e0b\u4e00\u5c42\u9690\u72b6\u6001\u7684 tensor c_1 (batch, hidden_size): \u4fdd\u5b58\u7740 batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u4e0b\u4e00\u7ec6\u80de\u72b6\u6001\u7684 tensor | Variables: | weight_ih \u2013 input-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, \u5f62\u72b6\u4e3a <cite>(4*hidden_size x input_size)</cite> weight_hh \u2013 hidden-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, \u5f62\u72b6\u4e3a <cite>(4*hidden_size x hidden_size)</cite> bias_ih \u2013 input-hidden \u504f\u7f6e, \u53ef\u5b66\u4e60, \u5f62\u72b6\u4e3a <cite>(4*hidden_size)</cite> bias_hh \u2013 hidden-hidden \u504f\u7f6e, \u53ef\u5b66\u4e60, \u5f62\u72b6\u4e3a <cite>(4*hidden_size)</cite> | | --- | --- | Examples: >>> rnn = nn.LSTMCell(10, 20) >>> input = Variable(torch.randn(6, 3, 10)) >>> hx = Variable(torch.randn(3, 20)) >>> cx = Variable(torch.randn(3, 20)) >>> output = [] >>> for i in range(6): ... hx, cx = rnn(input[i], (hx, cx)) ... output.append(hx)","title":"LSTMCell"},{"location":"57/#grucell","text":"class torch.nn.GRUCell(input_size, hidden_size, bias=True) GRU \u7ec6\u80de \\[\\begin{split}\\begin{array}{ll} r = \\mathrm{sigmoid}(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\ z = \\mathrm{sigmoid}(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\ n = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\ h' = (1 - z) * n + z * h \\end{array}\\end{split}\\] | Parameters: | input_size \u2013 \u8f93\u5165\u7684\u7279\u5f81\u7ef4\u5ea6 hidden_size \u2013 \u9690\u72b6\u6001\u7684\u7ef4\u5ea6 bias \u2013 \u5982\u679c\u4e3a <cite>False</cite>, \u90a3\u4e48RNN\u5c42\u5c06\u4e0d\u4f1a\u4f7f\u7528\u504f\u7f6e\u6743\u91cd b_ih \u548c b_hh \u9ed8\u8ba4: True | | --- | --- | Inputs: input, hidden input (batch, input_size): \u5305\u542b\u8f93\u5165\u7279\u5f81\u7684 tensor . hidden (batch, hidden_size): \u5305\u542b batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u521d\u59cb\u5316\u9690\u72b6\u6001\u7684 tensor . Outputs: h\u2019 h\u2019 : (batch, hidden_size): \u4fdd\u5b58\u7740 batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u4e0b\u4e00\u5c42\u9690\u72b6\u6001\u7684 tensor | Variables: | weight_ih \u2013 input-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, shape \u4e3a, <cite>(3*hidden_size x input_size)</cite> weight_hh \u2013 hidden-hidden \u6743\u91cd, \u53ef\u5b66\u4e60, shape \u4e3a <cite>(3*hidden_size x hidden_size)</cite> bias_ih \u2013 input-hidden \u504f\u7f6e, \u53ef\u5b66\u4e60, shape \u4e3a <cite>(3*hidden_size)</cite> bias_hh \u2013 hidden-hidden \u504f\u7f6e, \u53ef\u5b66\u4e60, shape \u4e3a <cite>(3*hidden_size)</cite> | | --- | --- | Examples: >>> rnn = nn.GRUCell(10, 20) >>> input = Variable(torch.randn(6, 3, 10)) >>> hx = Variable(torch.randn(3, 20)) >>> output = [] >>> for i in range(6): ... hx = rnn(input[i], hx) ... output.append(hx)","title":"GRUCell"},{"location":"57/#linear-layers","text":"","title":"Linear layers (\u7ebf\u6027\u5c42)"},{"location":"57/#linear","text":"class torch.nn.Linear(in_features, out_features, bias=True) \u5bf9\u8f93\u5165\u6570\u636e\u8fdb\u884c\u7ebf\u6027\u53d8\u6362: \\(y = Ax + b\\) | Parameters: | in_features \u2013 \u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u7684\u5927\u5c0f out_features \u2013 \u6bcf\u4e2a\u8f93\u51fa\u6837\u672c\u7684\u5927\u5c0f bias \u2013 \u82e5\u8bbe\u7f6e\u4e3a False, \u8fd9\u5c42\u4e0d\u4f1a\u5b66\u4e60\u504f\u7f6e. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: Input: \\((N, *, in\\_features)\\) \u8fd9\u91cc <cite>*</cite> \u610f\u5473\u7740\u53ef\u4ee5\u6dfb\u52a0\u4efb\u610f\u6570\u91cf\u7684\u5176\u4ed6\u7ef4\u5ea6 Output: \\((N, *, out\\_features)\\) \u9664\u4e86\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u5916, \u5176\u4f59\u7684\u90fd\u4e0e\u8f93\u5165\u76f8\u540c | Variables: | weight \u2013 \u5f62\u72b6\u4e3a (out_features x in_features) \u7684\u6a21\u5757\u4e2d\u53ef\u5b66\u4e60\u7684\u6743\u503c bias \u2013 \u5f62\u72b6\u4e3a (out_features) \u7684\u6a21\u5757\u4e2d\u53ef\u5b66\u4e60\u7684\u504f\u7f6e | | --- | --- | Examples: >>> m = nn.Linear(20, 30) >>> input = autograd.Variable(torch.randn(128, 20)) >>> output = m(input) >>> print(output.size())","title":"Linear"},{"location":"57/#bilinear","text":"class torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True) \u5bf9\u8f93\u5165\u6570\u636e\u8fdb\u884c\u53cc\u7ebf\u6027\u53d8\u6362: \\(y = x_1 * A * x_2 + b\\) | Parameters: | in1_features \u2013 \u8f93\u5165\u4e00\u7684\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u7684\u5927\u5c0f in2_features \u2013 \u8f93\u5165\u4e8c\u7684\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u7684\u5927\u5c0f out_features \u2013 \u6bcf\u4e2a\u8f93\u51fa\u6837\u672c\u7684\u5927\u5c0f bias \u2013 \u82e5\u8bbe\u7f6e\u4e3aFalse, \u8fd9\u5c42\u4e0d\u4f1a\u5b66\u4e60\u504f\u7f6e. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: Input: \\((N, in1\\_features)\\) , \\((N, in2\\_features)\\) Output: \\((N, out\\_features)\\) | Variables: | weight \u2013 \u5f62\u72b6\u4e3a (out_features x in1_features x in2_features) \u7684\u6a21\u5757\u4e2d\u53ef\u5b66\u4e60\u7684\u6743\u503c bias \u2013 \u5f62\u72b6\u4e3a (out_features) \u7684\u6a21\u5757\u4e2d\u53ef\u5b66\u4e60\u7684\u504f\u7f6e | | --- | --- | Examples: >>> m = nn.Bilinear(20, 30, 40) >>> input1 = autograd.Variable(torch.randn(128, 20)) >>> input2 = autograd.Variable(torch.randn(128, 30)) >>> output = m(input1, input2) >>> print(output.size())","title":"Bilinear"},{"location":"57/#dropout-layers","text":"","title":"Dropout layers"},{"location":"57/#dropout","text":"class torch.nn.Dropout(p=0.5, inplace=False) Dropout \u5728\u8bad\u7ec3\u671f\u95f4, \u6309\u7167\u4f2f\u52aa\u5229\u6982\u7387\u5206\u5e03, \u4ee5\u6982\u7387 p \u968f\u673a\u5730\u5c06\u8f93\u5165\u5f20\u91cf\u4e2d\u7684\u90e8\u5206\u5143\u7d20 \u7f6e\u4e3a 0, \u5728\u6bcf\u6b21\u8c03\u7528\u65f6, \u88ab\u7f6e\u4e3a 0 \u7684\u5143\u7d20\u662f\u968f\u673a\u7684. Dropout \u5df2\u88ab\u8bc1\u660e\u662f\u6b63\u5219\u5316\u7684\u4e00\u4e2a\u884c\u4e4b\u6709\u6548\u7684\u6280\u672f, \u5e76\u4e14\u5728\u9632\u6b62\u795e\u7ecf\u5143\u4e4b\u95f4\u4e92\u9002\u5e94\u95ee\u9898\u4e0a \u4e5f\u5353\u6709\u6210\u6548.\uff08\u795e\u7ecf\u5143\u4e92\u9002\u5e94\u95ee\u9898\u8be6\u89c1\u8bba\u6587 Improving neural networks by preventing co-adaptation of feature detectors \uff09 \u5e76\u4e14, Dropout \u7684\u8f93\u51fa\u5747\u4e0e 1/(1-p) \u7684\u6bd4\u4f8b\u7cfb\u6570\u8fdb\u884c\u4e86\u76f8\u4e58, \u4fdd\u8bc1\u4e86\u6c42\u503c\u65f6\u51fd\u6570\u662f\u5f52\u4e00\u5316\u7684. Args: p: \u5143\u7d20\u88ab\u7f6e\u4e3a0\u7684\u6982\u7387, \u9ed8\u8ba4\u503c: 0.5 inplace: \u5982\u679c\u4e3a True, \u7f6e0\u64cd\u4f5c\u5c06\u76f4\u63a5\u53d1\u751f\u5728\u4f20\u5165\u7684\u5143\u7d20\u4e0a.\u9ed8\u8ba4\u503c: false Shape: Input: any.\u8f93\u5165\u6570\u636e\u53ef\u4ee5\u662f\u4efb\u4f55\u5927\u5c0f Output: Same.\u8f93\u51fa\u6570\u636e\u5927\u5c0f\u4e0e\u8f93\u5165\u76f8\u540c Examples: >>> m = nn.Dropout(p=0.2) >>> input = autograd.Variable(torch.randn(20, 16)) >>> output = m(input)","title":"Dropout"},{"location":"57/#dropout2d","text":"class torch.nn.Dropout2d(p=0.5, inplace=False) Dropout2d \u5c06\u8f93\u5165\u5f20\u91cf\u7684\u6240\u6709\u901a\u9053\u968f\u673a\u5730\u7f6e\u4e3a 0.\u88ab\u7f6e\u4e3a 0 \u7684\u901a\u9053\u5728\u6bcf\u6b21\u8c03\u7528\u65f6\u662f\u968f\u673a\u7684. \u901a\u5e38\u8f93\u5165\u6570\u636e\u6765\u81ea Conv2d \u6a21\u5757. \u5728\u8bba\u6587 Efficient Object Localization Using Convolutional Networks \u4e2d\u6709\u5982\u4e0b \u63cf\u8ff0: \u5982\u679c\u7279\u5f81\u6620\u5c04\u4e2d\u7684\u90bb\u63a5\u50cf\u7d20\u662f\u5f3a\u76f8\u5173\u7684\uff08\u5728\u65e9\u671f\u7684\u5377\u79ef\u5c42\u4e2d\u5f88\u5e38\u89c1\uff09, \u90a3\u4e48\u72ec\u7acb\u540c\u5206\u5e03 \u7684 dropout \u5c06\u4e0d\u4f1a\u6b63\u5219\u5316\u6fc0\u6d3b\u51fd\u6570, \u76f8\u53cd\u5176\u4f1a\u5bfc\u81f4\u6709\u6548\u7684\u5b66\u4e60\u7387\u7684\u4e0b\u964d. \u5728\u8fd9\u6837\u7684\u60c5\u51b5\u4e0b, \u5e94\u8be5\u4f7f\u7528\u51fd\u6570\u51fd\u6570 nn.Dropout2d , \u5b83\u80fd\u591f\u63d0\u5347\u7279\u5f81\u6620\u5c04\u4e4b\u95f4\u7684\u72ec\u7acb\u6027. Args: p (float,optional): \u5143\u7d20\u88ab\u7f6e0\u7684\u6982\u7387 inplace\uff08bool, optional\uff09: \u5982\u679c\u88ab\u8bbe\u4e3a\u2019True\u2019, \u7f6e0\u64cd\u4f5c\u5c06\u76f4\u63a5\u4f5c\u7528\u5728\u8f93\u5165\u5143\u7d20\u4e0a Shape: Input: math:(N, C, H, W) Output: math:(N, C, H, W) \uff08\u4e0e\u8f93\u5165\u76f8\u540c\uff09 Examples: ```py >>> m = nn.Dropout2d(p=0.2) ``` ```py >>> input = autograd.Variable(torch.randn(20, 16, 32, 32)) >>> output = m(input) ```","title":"Dropout2d"},{"location":"57/#dropout3d","text":"class torch.nn.Dropout3d(p=0.5, inplace=False) Dropout3d \u5c06\u8f93\u5165\u5f20\u91cf\u7684\u6240\u6709\u901a\u9053\u968f\u673a\u5730\u7f6e\u4e3a 0.\u88ab\u7f6e\u4e3a 0 \u7684\u901a\u9053\u5728\u6bcf\u6b21\u8c03\u7528\u65f6\u662f\u968f\u673a\u7684. \u901a\u5e38\u8f93\u5165\u6570\u636e\u6765\u81ea Conv3d \u6a21\u5757. \u5728\u8bba\u6587 Efficient Object Localization Using Convolutional Networks \u4e2d\u6709\u5982\u4e0b \u63cf\u8ff0: \u5982\u679c\u7279\u5f81\u6620\u5c04\u4e2d\u7684\u90bb\u63a5\u50cf\u7d20\u662f\u5f3a\u76f8\u5173\u7684\uff08\u5728\u65e9\u671f\u7684\u5377\u79ef\u5c42\u4e2d\u5f88\u5e38\u89c1\uff09, \u90a3\u4e48\u72ec\u7acb\u540c\u5206\u5e03 \u7684 dropout \u5c06\u4e0d\u4f1a\u6b63\u5219\u5316\u6fc0\u6d3b\u51fd\u6570, \u76f8\u53cd\u5176\u4f1a\u5bfc\u81f4\u6709\u6548\u7684\u5b66\u4e60\u7387\u7684\u4e0b\u964d. \u5728\u8fd9\u6837\u7684\u60c5\u51b5\u4e0b, \u5e94\u8be5\u4f7f\u7528\u51fd\u6570\u51fd\u6570 nn.Dropout3d , \u5b83\u80fd\u591f\u4fc3\u8fdb\u7279\u5f81\u6620\u5c04\u4e4b\u95f4\u7684\u72ec\u7acb\u6027. Args: p (float,optional): \u5143\u7d20\u88ab\u7f6e0\u7684\u6982\u7387 inplace\uff08bool, optional\uff09: \u5982\u679c\u88ab\u8bbe\u4e3a True , \u7f6e0\u64cd\u4f5c\u5c06\u76f4\u63a5\u4f5c\u7528\u5728\u8f93\u5165\u5143\u7d20\u4e0a Shape: Input: math:(N, C, H, W) Output: math:(N, C, H, W) \uff08\u4e0e\u8f93\u5165\u76f8\u540c\uff09 Examples: ```py >>> m = nn.Dropout3d(p=0.2) ``` ```py >>> input = autograd.Variable(torch.randn(20, 16, 4, 32, 32)) >>> output = m(input) ```","title":"Dropout3d"},{"location":"57/#alphadropout","text":"class torch.nn.AlphaDropout(p=0.5) \u5728\u8f93\u5165\u4e0a\u5e94\u7528 Alpha Dropout. Alpha Dropout \u662f\u4e00\u79cd\u7ef4\u6301\u81ea\u6b63\u4ea4\u6027\u8d28\u7684 Dropout . \u5bf9\u4e8e\u4e00\u4e2a\u5747\u503c\u4e3a 0 \u548c\u6807\u51c6\u5dee\u4e3a 1 \u7684\u8f93\u5165 \u6765\u8bf4, Alpha Dropout \u80fd\u4fdd\u6301\u539f\u59cb\u6570\u636e\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee.Alpha Dropout \u548c SELU \u6fc0\u6d3b\u51fd\u6570 \u643a\u624b\u540c\u884c, \u540e\u8005\u4e5f\u4fdd\u8bc1\u4e86\u8f93\u51fa\u62e5\u6709\u4e0e\u8f93\u5165\u76f8\u540c\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee. Alpha Dropout \u5728\u8bad\u7ec3\u671f\u95f4, \u6309\u7167\u4f2f\u52aa\u5229\u6982\u7387\u5206\u5e03, \u4ee5\u6982\u7387 p \u968f\u673a\u5730\u5c06\u8f93\u5165\u5f20\u91cf\u4e2d\u7684\u90e8\u5206\u5143\u7d20 \u7f6e\u8fdb\u884c\u63a9\u76d6, \u5728\u6bcf\u6b21\u8c03\u7528\u4e2d, \u88ab\u63a9\u76d6\u7684\u5143\u7d20\u662f\u968f\u673a\u7684, \u5e76\u4e14\u5bf9\u8f93\u51fa\u4f1a\u8fdb\u884c\u7f29\u653e\u3001\u53d8\u6362\u7b49\u64cd\u4f5c \u4ee5\u4fdd\u6301\u5747\u503c\u4e3a 0\u3001\u6807\u51c6\u5dee\u4e3a 1. \u5728\u6c42\u503c\u671f\u95f4, \u6a21\u5757\u7b80\u5355\u7684\u8ba1\u7b97\u4e00\u4e2a\u5f52\u4e00\u5316\u7684\u51fd\u6570. \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u8003\u8bba\u6587: Self-Normalizing Neural Networks Args: p\uff08float\uff09: \u5143\u7d20\u88ab\u63a9\u76d6\u7684\u6982\u7387, \u9ed8\u8ba4\u503c: 0.5 Shape: Input: any.\u8f93\u5165\u6570\u636e\u53ef\u4ee5\u662f\u4efb\u4f55\u5927\u5c0f Output: Same.\u8f93\u51fa\u6570\u636e\u5927\u5c0f\u4e0e\u8f93\u5165\u76f8\u540c Examples: ```py >>> m = nn.AlphaDropout(p=0.2) ``` ```py >>> input = autograd.Variable(torch.randn(20, 16)) >>> output = m(input) ```","title":"AlphaDropout"},{"location":"57/#sparse-layers","text":"","title":"Sparse layers (\u7a00\u758f\u5c42)"},{"location":"57/#embedding","text":"class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False) \u4e00\u4e2a\u7b80\u5355\u7684\u67e5\u627e\u8868, \u5b58\u50a8\u4e86\u56fa\u5b9a\u5b57\u5178\u548c\u5927\u5c0f\u7684 embedding. \u8fd9\u4e2a\u6a21\u5757\u7ecf\u5e38\u7528\u6765\u5b58\u50a8 word embeddings, \u5e76\u901a\u8fc7\u7d22\u5f15\u6765\u68c0\u7d22, \u6a21\u5757\u7684\u8f93\u5165\u662f\u7d22\u5f15\u6784\u6210\u7684\u5217\u8868, \u8f93\u51fa\u662f\u5bf9\u5e94\u7684 word embeddings. | Parameters: | num_embeddings ( int ) \u2013 embeddings \u5b57\u5178\u7684\u5927\u5c0f embedding_dim ( int ) \u2013 \u6bcf\u4e2a embedding \u5411\u91cf\u7684\u5927\u5c0f padding_idx ( int , optional ) \u2013 \u5982\u679c\u7ed9\u51fa, \u5728\u7d22\u5f15\u5904, \u8f93\u51fa\u8865\u96f6 max_norm ( float , optional ) \u2013 \u5982\u679c\u7ed9\u51fa, \u91cd\u65b0\u5f52\u4e00\u5316 embeddings, \u4f7f\u5176\u8303\u6570\u5c0f\u4e8e\u8be5\u503c norm_type ( float , optional ) \u2013 \u4e3a max_norm \u9009\u9879\u8ba1\u7b97 p \u8303\u6570\u65f6 P scale_grad_by_freq ( boolean__, optional ) \u2013 \u5982\u679c\u7ed9\u51fa, \u4f1a\u6839\u636e words \u5728 mini-batch \u4e2d\u7684\u9891\u7387\u7f29\u653e\u68af\u5ea6 sparse ( boolean__, optional ) \u2013 \u5982\u679c\u4e3a True , \u5173\u4e8e\u6743\u91cd\u77e9\u9635\u7684\u68af\u5ea6\u662f\u4e00\u4e2a\u7a00\u758f\u5f20\u91cf, \u8be6\u60c5\u8bf7\u53c2\u8003\u7a00\u758f\u68af\u5ea6 | | --- | --- | | Variables: | weight ( Tensor ) \u2013 shape \u4e3a (num_embeddings, embedding_dim) \u7684\u6a21\u5757\u7684\u53ef\u5b66\u4e60\u6743\u91cd | | --- | --- | Shape: Input: LongTensor <cite>(N, W)</cite>, N = mini-batch, W = \u6bcf\u4e2a mini-batch \u4e2d\u7528\u6765\u63d0\u53d6\u7684\u7d22\u5f15\u6570 Output: <cite>(N, W, embedding_dim)</cite> Notes \u8bf7\u6ce8\u610f, \u53ea\u652f\u6301\u6709\u9650\u6570\u91cf\u7684\u4f18\u5316\u5668. \u7a00\u758f\u68af\u5ea6: \u5f53\u524d\u662f (<cite>cuda</cite> \u548c <cite>cpu</cite>) \u7248\u672c\u7684 <cite>optim.SGD</cite>, \u548c (<cite>cpu</cite>) \u7248\u672c\u7684 <cite>optim.Adagrad</cite>. Examples: >>> # an Embedding module containing 10 tensors of size 3 >>> embedding = nn.Embedding(10, 3) >>> # a batch of 2 samples of 4 indices each >>> input = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]])) >>> embedding(input) Variable containing: (0 ,.,.) = -1.0822 1.2522 0.2434 0.8393 -0.6062 -0.3348 0.6597 0.0350 0.0837 0.5521 0.9447 0.0498 (1 ,.,.) = 0.6597 0.0350 0.0837 -0.1527 0.0877 0.4260 0.8393 -0.6062 -0.3348 -0.8738 -0.9054 0.4281 [torch.FloatTensor of size 2x4x3] >>> # example with padding_idx >>> embedding = nn.Embedding(10, 3, padding_idx=0) >>> input = Variable(torch.LongTensor([[0,2,0,5]])) >>> embedding(input) Variable containing: (0 ,.,.) = 0.0000 0.0000 0.0000 0.3452 0.4937 -0.9361 0.0000 0.0000 0.0000 0.0706 -2.1962 -0.6276 [torch.FloatTensor of size 1x4x3]","title":"Embedding"},{"location":"57/#embeddingbag","text":"class torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean') \u8ba1\u7b97\u4e00 \u4e2a\u2019bags\u2019 \u91cc\u7684 embedding s\u7684\u5747\u503c\u6216\u548c, \u4e0d\u7528\u5b9e\u4f8b\u5316\u4e2d\u95f4\u7684 embeddings \u5bf9\u4e8e\u56fa\u5b9a\u957f\u5ea6\u7684 bags nn.EmbeddingBag \u548c <cite>mode=sum</cite> \u76f8\u5f53\u4e8e nn.Embedding \u4e0e\u4e4b\u540e\u7684 <cite>torch.sum(dim=1)</cite> \u5176\u4e0e <cite>mode=mean</cite> \u76f8\u5f53\u4e8e nn.Embedding \u4e0e\u4e4b\u540e\u7684 <cite>torch.mean(dim=1)</cite> \u7136\u800c, \u6bd4\u8d77\u4e00\u8fde\u4e32\u8fd9\u6837\u7684\u64cd\u4f5c, nn.EmbeddingBag \u5728\u65f6\u95f4\u548c\u5185\u5b58\u4e0a\u66f4\u52a0\u9ad8\u6548. | Parameters: | num_embeddings ( int ) \u2013 embeddings \u5b57\u5178\u7684\u5927\u5c0f embedding_dim ( int ) \u2013 \u6bcf\u4e2a embedding \u5411\u91cf\u7684\u5927\u5c0f max_norm ( float , optional ) \u2013 \u5982\u679c\u7ed9\u51fa, \u91cd\u65b0\u5f52\u4e00\u5316 embeddings, \u4f7f\u5176\u8303\u6570\u5c0f\u4e8e\u8be5\u503c norm_type ( float , optional ) \u2013 \u4e3a max_norm \u9009\u9879\u8ba1\u7b97 p \u8303\u6570\u65f6\u7684 P scale_grad_by_freq ( boolean__, optional ) \u2013 \u5982\u679c\u7ed9\u51fa, \u4f1a\u6839\u636e words \u5728 mini-batch \u4e2d\u7684\u9891\u7387\u7f29\u653e\u68af\u5ea6 mode ( string , optional ) \u2013 \u2018sum\u2019 | \u2018mean\u2019. \u6307\u5b9a\u51cf\u5c11 bag \u7684\u65b9\u5f0f. \u9ed8\u8ba4: \u2018mean\u2019 | | --- | --- | | Variables: | weight ( Tensor ) \u2013 shape \u4e3a (num_embeddings, embedding_dim) \u7684\u6a21\u5757\u7684\u53ef\u5b66\u4e60\u6743\u91cd | | --- | --- | Inputs: input, offsets input (N or BxN): LongTensor, \u5305\u62ec\u8981\u63d0\u53d6\u7684 embeddings \u7684\u7d22\u5f15, \u5f53 <cite>input</cite> \u662f\u5f62\u72b6\u4e3a <cite>N</cite> \u7684 1D \u5f20\u91cf\u65f6, \u4e00\u4e2a\u7ed9\u51fa\u7684 <cite>offsets</cite> \u5f20\u91cf\u4e2d\u5305\u62ec: mini-batch \u4e2d\u6bcf\u4e2a\u65b0\u5e8f\u5217\u7684\u8d77\u59cb\u4f4d\u7f6e offsets (B or None): LongTensor, \u5305\u62ec\u4e00\u4e2a mini-batch \u7684\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u65b0\u6837\u672c\u7684\u8d77\u59cb\u4f4d\u7f6e \u5982\u679c <cite>input</cite> \u662f 2D (BxN) \u7684, offset \u5c31\u4e0d\u7528\u518d\u7ed9\u51fa; \u5982\u679c <cite>input</cite> \u662f\u4e00\u4e2a mini-batch \u7684\u56fa\u5b9a\u957f\u5ea6\u7684\u5e8f\u5217, \u6bcf\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6\u4e3a <cite>N</cite> Shape: Input: LongTensor <cite>N</cite>, N = \u8981\u63d0\u53d6\u7684 embeddings \u7684\u6570\u91cf, \u6216\u8005\u662f LongTensor <cite>BxN</cite>, B = mini-batch \u4e2d\u5e8f\u5217\u7684\u6570\u91cf, N = \u6bcf\u4e2a\u5e8f\u5217\u4e2d embeddings \u7684\u6570\u91cf Offsets: LongTensor <cite>B</cite>, B = bags \u7684\u6570\u91cf, \u503c\u4e3a\u6bcf\u4e2a bag \u4e2d <cite>input</cite> \u7684 offset, i.e. \u662f\u957f\u5ea6\u7684\u7d2f\u52a0. Offsets \u4e0d\u4f1a\u7ed9\u51fa, \u5982\u679c Input\u662f 2D \u7684 BxN \u5f20\u91cf, \u8f93\u5165\u88ab\u8ba4\u4e3a\u662f\u56fa\u5b9a\u957f\u5ea6\u7684\u5e8f\u5217 Output: <cite>(B, embedding_dim)</cite> Examples: >>> # an Embedding module containing 10 tensors of size 3 >>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum') >>> # a batch of 2 samples of 4 indices each >>> input = Variable(torch.LongTensor([1,2,4,5,4,3,2,9])) >>> offsets = Variable(torch.LongTensor([0,4])) >>> embedding_sum(input, offsets) Variable containing: -0.7296 -4.6926 0.3295 -0.5186 -0.5631 -0.2792 [torch.FloatTensor of size 2x3]","title":"EmbeddingBag"},{"location":"57/#distance-functions","text":"","title":"Distance functions (\u8ddd\u79bb\u51fd\u6570)"},{"location":"57/#cosinesimilarity","text":"class torch.nn.CosineSimilarity(dim=1, eps=1e-08) \u8fd4\u56de\u6cbf\u7740 dim \u65b9\u5411\u8ba1\u7b97\u7684 x1 \u4e0e x2 \u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6. \\[\\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}\\] | Parameters: | dim ( int , optional ) \u2013 \u8ba1\u7b97\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u7ef4\u5ea6. Default: 1 eps ( float , optional ) \u2013 \u5c0f\u7684\u503c\u4ee5\u907f\u514d\u88ab\u96f6\u9664. Default: 1e-8 | | --- | --- | Shape: Input1: \\((\\ast_1, D, \\ast_2)\\) , \u5176\u4e2d\u7684 D \u8868\u793a <cite>dim</cite> \u7684\u4f4d\u7f6e Input2: \\((\\ast_1, D, \\ast_2)\\) , \u4e0e Input1 \u4e00\u6837\u7684 shape Output: \\((\\ast_1, \\ast_2)\\) Examples: >>> input1 = autograd.Variable(torch.randn(100, 128)) >>> input2 = autograd.Variable(torch.randn(100, 128)) >>> cos = nn.CosineSimilarity(dim=1, eps=1e-6) >>> output = cos(input1, input2) >>> print(output)","title":"CosineSimilarity"},{"location":"57/#pairwisedistance","text":"class torch.nn.PairwiseDistance(p=2, eps=1e-06) \u8ba1\u7b97\u5411\u91cf v1, v2 \u4e4b\u95f4\u7684 batchwise pairwise distance(\u5206\u6279\u6210\u5bf9\u8ddd\u79bb): \\[\\Vert x \\Vert _p := \\left( \\sum_{i=1}^n \\vert x_i \\vert ^ p \\right) ^ {1/p}\\] | Parameters: | p ( real ) \u2013 norm degree(\u89c4\u8303\u7a0b\u5ea6). Default: 2 eps ( float , optional ) \u2013 \u5c0f\u7684\u503c\u4ee5\u907f\u514d\u88ab\u96f6\u9664. Default: 1e-6 | | --- | --- | Shape: Input1: \\((N, D)\\) , \u5176\u4e2d\u7684 <cite>D = vector dimension(\u5411\u91cf\u7ef4\u5ea6)</cite> Input2: \\((N, D)\\) , \u4e0e Input1 \u7684 shape \u4e00\u6837 Output: \\((N, 1)\\) Examples: >>> pdist = nn.PairwiseDistance(p=2) >>> input1 = autograd.Variable(torch.randn(100, 128)) >>> input2 = autograd.Variable(torch.randn(100, 128)) >>> output = pdist(input1, input2)","title":"PairwiseDistance"},{"location":"57/#loss-functions","text":"","title":"Loss functions (\u635f\u5931\u51fd\u6570)"},{"location":"57/#l1loss","text":"class torch.nn.L1Loss(size_average=True, reduce=True) \u521b\u5efa\u4e00\u4e2a\u8861\u91cf\u8f93\u5165 <cite>x</cite> \u4e0e\u76ee\u6807 <cite>y</cite> \u4e4b\u95f4\u5dee\u7684\u7edd\u5bf9\u503c\u7684\u5e73\u5747\u503c\u7684\u6807\u51c6, \u8be5 \u51fd\u6570\u4f1a\u9010\u5143\u7d20\u5730\u6c42\u51fa <cite>x</cite> \u548c <cite>y</cite> \u4e4b\u95f4\u5dee\u7684\u7edd\u5bf9\u503c, \u6700\u540e\u8fd4\u56de\u7edd\u5bf9\u503c\u7684\u5e73\u5747\u503c. \\({loss}(x, y) = 1/n \\sum |x_i - y_i|\\) <cite>x</cite> \u548c <cite>y</cite> \u53ef\u4ee5\u662f\u4efb\u610f\u7ef4\u5ea6\u7684\u6570\u7ec4, \u4f46\u9700\u8981\u6709\u76f8\u540c\u6570\u91cf\u7684n\u4e2a\u5143\u7d20. \u6c42\u548c\u64cd\u4f5c\u4f1a\u5bf9n\u4e2a\u5143\u7d20\u6c42\u548c, \u6700\u540e\u9664\u4ee5 <cite>n</cite> . \u5728\u6784\u9020\u51fd\u6570\u7684\u53c2\u6570\u4e2d\u4f20\u5165 <cite>size_average=False</cite>, \u6700\u540e\u6c42\u51fa\u6765\u7684\u7edd\u5bf9\u503c\u5c06\u4e0d\u4f1a\u9664\u4ee5 <cite>n</cite>. | Parameters: | size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u5c06\u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u7d2f\u52a0, \u800c\u4e0d\u4f1a\u53d6\u5e73\u5747\u503c. \u5f53 reduce \u7684\u503c\u4e3a False \u65f6\u8be5\u5b57\u6bb5\u4f1a\u88ab\u5ffd\u7565. \u9ed8\u8ba4\u503c: True reduce ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u5e73\u5747\u503c\u6216\u8005 \u6c42\u548c. \u5f53 reduce \u662f False \u65f6, \u635f\u5931\u51fd\u6570\u4f1a\u5bf9\u6bcf\u4e2a batch \u5143\u7d20\u90fd\u8fd4\u56de\u4e00\u4e2a loss \u5e76\u5ffd \u7565 size_average \u5b57\u6bb5. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, *)\\) , <cite>*</cite> \u8868\u793a\u4efb\u610f\u6570\u91cf\u7684\u989d\u5916\u7ef4\u5ea6 \u76ee\u6807: \\((N, *)\\) , \u548c\u8f93\u5165\u7684shape\u76f8\u540c \u8f93\u51fa: \u6807\u91cf. \u5982\u679c reduce \u662f False , \u5219\u8f93\u51fa\u4e3a \\((N, *)\\) , shape\u4e0e\u8f93\u51fa\u76f8\u540c Examples: >>> loss = nn.L1Loss() >>> input = autograd.Variable(torch.randn(3, 5), requires_grad=True) >>> target = autograd.Variable(torch.randn(3, 5)) >>> output = loss(input, target) >>> output.backward()","title":"L1Loss"},{"location":"57/#mseloss","text":"class torch.nn.MSELoss(size_average=True, reduce=True) \u8f93\u5165 <cite>x</cite> \u548c \u76ee\u6807 <cite>y</cite> \u4e4b\u95f4\u7684\u5747\u65b9\u5dee \\({loss}(x, y) = 1/n \\sum |x_i - y_i|^2\\) <cite>x</cite> \u548c <cite>y</cite> \u53ef\u4ee5\u662f\u4efb\u610f\u7ef4\u5ea6\u7684\u6570\u7ec4, \u4f46\u9700\u8981\u6709\u76f8\u540c\u6570\u91cf\u7684n\u4e2a\u5143\u7d20. \u6c42\u548c\u64cd\u4f5c\u4f1a\u5bf9n\u4e2a\u5143\u7d20\u6c42\u548c, \u6700\u540e\u9664\u4ee5 <cite>n</cite>. \u5728\u6784\u9020\u51fd\u6570\u7684\u53c2\u6570\u4e2d\u4f20\u5165 <cite>size_average=False</cite> , \u6700\u540e\u6c42\u51fa\u6765\u7684\u7edd\u5bf9\u503c\u5c06\u4e0d\u4f1a\u9664\u4ee5 <cite>n</cite>. \u8981\u5f97\u5230\u6bcf\u4e2a batch \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684 loss, \u8bbe\u7f6e <cite>reduce</cite> \u4e3a False . \u8fd4\u56de\u7684 loss \u5c06\u4e0d\u4f1a \u53d6\u5e73\u5747\u503c, \u4e5f\u4e0d\u4f1a\u88ab <cite>size_average</cite> \u5f71\u54cd. | Parameters: | size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u4f1a\u5728\u6bcf \u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u548c. \u53ea\u6709\u5f53 reduce \u7684\u503c\u4e3a True \u624d\u4f1a\u751f\u6548. \u9ed8\u8ba4\u503c: True reduce ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u6839\u636e size_average \u7684\u503c\u5728\u6bcf \u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u5e73\u5747\u503c\u6216\u8005\u6c42\u548c. \u5f53 reduce \u662f False \u65f6, \u635f\u5931\u51fd\u6570\u4f1a\u5bf9\u6bcf \u4e2a batch \u5143\u7d20\u90fd\u8fd4\u56de\u4e00\u4e2a loss \u5e76\u5ffd\u7565 size_average\u5b57\u6bb5. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, *)\\) , \u5176\u4e2d <cite>*</cite> \u8868\u793a\u4efb\u610f\u6570\u91cf\u7684\u989d\u5916\u7ef4\u5ea6. \u76ee\u6807: \\((N, *)\\) , shape \u8ddf\u8f93\u5165\u76f8\u540c Examples: >>> loss = nn.MSELoss() >>> input = autograd.Variable(torch.randn(3, 5), requires_grad=True) >>> target = autograd.Variable(torch.randn(3, 5)) >>> output = loss(input, target) >>> output.backward()","title":"MSELoss"},{"location":"57/#crossentropyloss","text":"class torch.nn.CrossEntropyLoss(weight=None, size_average=True, ignore_index=-100, reduce=True) \u8be5\u7c7b\u628a <cite>LogSoftMax</cite> \u548c <cite>NLLLoss</cite> \u7ed3\u5408\u5230\u4e86\u4e00\u4e2a\u7c7b\u4e2d \u5f53\u8bad\u7ec3\u6709 <cite>C</cite> \u4e2a\u7c7b\u522b\u7684\u5206\u7c7b\u95ee\u9898\u65f6\u5f88\u6709\u6548. \u53ef\u9009\u53c2\u6570 <cite>weight</cite> \u5fc5\u987b\u662f\u4e00\u4e2a1\u7ef4 Tensor, \u6743\u91cd\u5c06\u88ab\u5206\u914d\u7ed9\u5404\u4e2a\u7c7b\u522b. \u5bf9\u4e8e\u4e0d\u5e73\u8861\u7684\u8bad\u7ec3\u96c6\u975e\u5e38\u6709\u6548. <cite>input</cite> \u542b\u6709\u6bcf\u4e2a\u7c7b\u522b\u7684\u5206\u6570 <cite>input</cite> \u5fc5\u987b\u662f\u4e00\u4e2a2\u7ef4\u7684\u5f62\u5982 <cite>(minibatch, C)</cite> \u7684 <cite>Tensor</cite>. <cite>target</cite> \u662f\u4e00\u4e2a\u7c7b\u522b\u7d22\u5f15 (0 to C-1), \u5bf9\u5e94\u4e8e <cite>minibatch</cite> \u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20 loss \u53ef\u4ee5\u63cf\u8ff0\u4e3a: loss(x, class) = -log(exp(x[class]) / (\\sum_j exp(x[j]))) = -x[class] + log(\\sum_j exp(x[j])) \u5f53 <cite>weight</cite> \u53c2\u6570\u5b58\u5728\u65f6: loss(x, class) = weight[class] * (-x[class] + log(\\sum_j exp(x[j]))) loss \u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u53d6\u5e73\u5747\u503c. | Parameters: | weight ( Tensor , optional ) \u2013 \u81ea\u5b9a\u4e49\u7684\u6bcf\u4e2a\u7c7b\u522b\u7684\u6743\u91cd. \u5fc5\u987b\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>C</cite> \u7684 Tensor size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u5c06\u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u7d2f\u52a0, \u800c\u4e0d\u4f1a\u53d6\u5e73\u5747\u503c. \u5f53 reduce \u7684\u503c\u4e3a False \u65f6\u8be5\u5b57\u6bb5\u4f1a\u88ab\u5ffd\u7565. ignore_index ( int , optional ) \u2013 \u8bbe\u7f6e\u4e00\u4e2a\u76ee\u6807\u503c, \u8be5\u76ee\u6807\u503c\u4f1a\u88ab\u5ffd\u7565, \u4ece\u800c\u4e0d\u4f1a\u5f71\u54cd\u5230 \u8f93\u5165\u7684\u68af\u5ea6. \u5f53 size_average \u5b57\u6bb5\u4e3a True \u65f6, loss \u5c06\u4f1a\u5728\u6ca1\u6709\u88ab\u5ffd\u7565\u7684\u5143\u7d20\u4e0a \u53d6\u5e73\u5747. reduce ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u6839\u636e size_average \u7684\u503c\u5728\u6bcf \u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u5e73\u5747\u503c\u6216\u8005\u6c42\u548c. \u5f53 reduce \u662f False \u65f6, \u635f\u5931\u51fd\u6570\u4f1a\u5bf9 \u6bcf\u4e2a batch \u5143\u7d20\u90fd\u8fd4\u56de\u4e00\u4e2a loss \u5e76\u5ffd\u7565 size_average \u5b57\u6bb5. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, C)\\) , \u5176\u4e2d <cite>C</cite> \u662f\u7c7b\u522b\u7684\u6570\u91cf \u76ee\u6807: \\((N)\\) , \u5176\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u6ee1\u8db3 <cite>0 <= targets[i] <= C-1</cite> \u8f93\u51fa: \u6807\u91cf. \u5982\u679c reduce \u662f False , \u5219\u8f93\u51fa\u4e3a \\((N)\\) . Examples: >>> loss = nn.CrossEntropyLoss() >>> input = autograd.Variable(torch.randn(3, 5), requires_grad=True) >>> target = autograd.Variable(torch.LongTensor(3).random_(5)) >>> output = loss(input, target) >>> output.backward()","title":"CrossEntropyLoss"},{"location":"57/#nllloss","text":"class torch.nn.NLLLoss(weight=None, size_average=True, ignore_index=-100, reduce=True) \u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931. \u7528\u4e8e\u8bad\u7ec3 <cite>C</cite> \u4e2a\u7c7b\u522b\u7684\u5206\u7c7b\u95ee\u9898. \u53ef\u9009\u53c2\u6570 <cite>weight</cite> \u662f \u4e00\u4e2a1\u7ef4\u7684 Tensor, \u7528\u6765\u8bbe\u7f6e\u6bcf\u4e2a\u7c7b\u522b\u7684\u6743\u91cd. \u5f53\u8bad\u7ec3\u96c6\u4e0d\u5e73\u8861\u65f6\u8be5\u53c2\u6570\u5341\u5206\u6709\u7528. \u7531\u524d\u5411\u4f20\u64ad\u5f97\u5230\u7684\u8f93\u5165\u5e94\u8be5\u542b\u6709\u6bcf\u4e2a\u7c7b\u522b\u7684\u5bf9\u6570\u6982\u7387: \u8f93\u5165\u5fc5\u987b\u662f\u5f62\u5982 <cite>(minibatch, C)</cite> \u7684 2\u7ef4 Tensor. \u5728\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u7684\u6700\u540e\u4e00\u5c42\u6dfb\u52a0 <cite>LogSoftmax</cite> \u5c42\u53ef\u4ee5\u5f97\u5230\u5bf9\u6570\u6982\u7387. \u5982\u679c\u4f60\u4e0d\u5e0c\u671b\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d \u52a0\u5165\u989d\u5916\u7684\u4e00\u5c42, \u4e5f\u53ef\u4ee5\u4f7f\u7528 <cite>CrossEntropyLoss</cite> \u51fd\u6570. \u8be5\u635f\u5931\u51fd\u6570\u9700\u8981\u7684\u76ee\u6807\u503c\u662f\u4e00\u4e2a\u7c7b\u522b\u7d22\u5f15 <cite>(0 \u5230 C-1, \u5176\u4e2d C \u662f\u7c7b\u522b\u6570\u91cf)</cite> \u8be5 loss \u53ef\u4ee5\u63cf\u8ff0\u4e3a: loss(x, class) = -x[class] \u6216\u8005\u5f53 weight \u53c2\u6570\u5b58\u5728\u65f6\u53ef\u4ee5\u63cf\u8ff0\u4e3a: loss(x, class) = -weight[class] * x[class] \u53c8\u6216\u8005\u5f53 ignore_index \u53c2\u6570\u5b58\u5728\u65f6\u53ef\u4ee5\u63cf\u8ff0\u4e3a: loss(x, class) = class != ignoreIndex ? -weight[class] * x[class] : 0 | Parameters: | weight ( Tensor , optional ) \u2013 \u81ea\u5b9a\u4e49\u7684\u6bcf\u4e2a\u7c7b\u522b\u7684\u6743\u91cd. \u5fc5\u987b\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>C</cite> \u7684 Tensor size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u5c06\u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u7d2f\u52a0, \u800c\u4e0d\u4f1a\u53d6\u5e73\u5747\u503c. \u5f53 reduce \u7684\u503c\u4e3a False \u65f6\u8be5\u5b57\u6bb5\u4f1a\u88ab\u5ffd\u7565. \u9ed8\u8ba4\u503c: True` ignore_index ( int , optional ) \u2013 \u8bbe\u7f6e\u4e00\u4e2a\u76ee\u6807\u503c, \u8be5\u76ee\u6807\u503c\u4f1a\u88ab\u5ffd\u7565, \u4ece\u800c\u4e0d\u4f1a\u5f71\u54cd\u5230 \u8f93\u5165\u7684\u68af\u5ea6. \u5f53 size_average \u4e3a True \u65f6, loss \u5c06\u4f1a\u5728\u6ca1\u6709\u88ab\u5ffd\u7565\u7684\u5143\u7d20\u4e0a \u53d6\u5e73\u5747\u503c. reduce ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u5e73\u5747\u503c\u6216\u8005 \u6c42\u548c. \u5f53 reduce \u662f False \u65f6, \u635f\u5931\u51fd\u6570\u4f1a\u5bf9\u6bcf\u4e2a batch \u5143\u7d20\u90fd\u8fd4\u56de\u4e00\u4e2a loss \u5e76\u5ffd \u7565 size_average \u5b57\u6bb5. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, C)\\) , \u5176\u4e2d <cite>C</cite> \u662f\u7c7b\u522b\u7684\u6570\u91cf \u76ee\u6807: \\((N)\\) , \u5176\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u6ee1\u8db3 <cite>0 <= targets[i] <= C-1</cite> \u8f93\u51fa: \u6807\u91cf. \u5982\u679c reduce \u662f False , \u5219\u8f93\u51fa\u4e3a \\((N)\\) . Examples: >>> m = nn.LogSoftmax() >>> loss = nn.NLLLoss() >>> # input is of size N x C = 3 x 5 >>> input = autograd.Variable(torch.randn(3, 5), requires_grad=True) >>> # each element in target has to have 0 <= value < C >>> target = autograd.Variable(torch.LongTensor([1, 0, 4])) >>> output = loss(m(input), target) >>> output.backward()","title":"NLLLoss"},{"location":"57/#poissonnllloss","text":"class torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=True, eps=1e-08) \u76ee\u6807\u503c\u4e3a\u6cca\u677e\u5206\u5e03\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931. \u8be5\u635f\u5931\u53ef\u4ee5\u63cf\u8ff0\u4e3a: target ~ Pois(input) loss(input, target) = input - target * log(input) + log(target!) \u6700\u540e\u4e00\u9879\u53ef\u4ee5\u88ab\u7701\u7565\u6216\u8005\u7528 Stirling \u516c\u5f0f\u6765\u8fd1\u4f3c. \u8be5\u8fd1\u4f3c\u7528\u4e8e\u5927\u4e8e1\u7684\u76ee\u6807\u503c. \u5f53\u76ee\u6807\u503c \u5c0f\u4e8e\u6216\u7b49\u4e8e1\u65f6, \u5219\u5c060\u52a0\u5230 loss \u4e2d. | Parameters: | log_input ( bool , optional ) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a True , loss \u5c06\u4f1a\u6309\u7167\u516c \u5f0f <cite>exp(input) - target * input</cite> \u6765\u8ba1\u7b97, \u5982\u679c\u8bbe\u7f6e\u4e3a False , loss \u5c06\u4f1a\u6309\u7167 <cite>input - target * log(input+eps)</cite> \u8ba1\u7b97. full ( bool , optional ) \u2013 \u662f\u5426\u8ba1\u7b97\u5168\u90e8\u7684 loss, i. e. \u52a0\u4e0a Stirling \u8fd1\u4f3c\u9879 <cite>target * log(target) - target + 0.5 * log(2 * pi * target)</cite>. size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u5c06\u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u7d2f\u52a0, \u800c\u4e0d\u4f1a\u53d6\u5e73\u5747\u503c. eps ( float , optional ) \u2013 \u5f53 log_input== False \u65f6, \u53d6\u4e00\u4e2a\u5f88\u5c0f\u7684\u503c\u7528\u6765\u907f\u514d\u8ba1\u7b97 log(0). \u9ed8\u8ba4\u503c: 1e-8 | | --- | --- | Examples: >>> loss = nn.PoissonNLLLoss() >>> log_input = autograd.Variable(torch.randn(5, 2), requires_grad=True) >>> target = autograd.Variable(torch.randn(5, 2)) >>> output = loss(log_input, target) >>> output.backward()","title":"PoissonNLLLoss"},{"location":"57/#nllloss2d","text":"class torch.nn.NLLLoss2d(weight=None, size_average=True, ignore_index=-100, reduce=True) \u5bf9\u4e8e\u56fe\u7247\u8f93\u5165\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931. \u5b83\u8ba1\u7b97\u6bcf\u4e2a\u50cf\u7d20\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931. | Parameters: | weight ( Tensor , optional ) \u2013 \u81ea\u5b9a\u4e49\u7684\u6bcf\u4e2a\u7c7b\u522b\u7684\u6743\u91cd. \u5fc5\u987b\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a <cite>C</cite> \u7684 Tensor size_average \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u5c06\u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u7d2f\u52a0, \u800c\u4e0d\u4f1a\u53d6\u5e73\u5747\u503c. \u5f53 reduce \u7684\u503c\u4e3a False \u65f6\u8be5\u5b57\u6bb5\u4f1a\u88ab\u5ffd\u7565. \u9ed8\u8ba4\u503c: True reduce ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u5e73\u5747\u503c\u6216\u8005 \u6c42\u548c. \u5f53 reduce \u662f False \u65f6, \u635f\u5931\u51fd\u6570\u4f1a\u5bf9\u6bcf\u4e2a batch \u5143\u7d20\u90fd\u8fd4\u56de\u4e00\u4e2a loss \u5e76\u5ffd \u7565 size_average \u5b57\u6bb5. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: Input: \\((N, C, H, W)\\) where <cite>C = number of classes</cite> Target: \\((N, H, W)\\) where each value is <cite>0 <= targets[i] <= C-1</cite> Output: scalar. If reduce is False , then \\((N, H, W)\\) instead. Examples: >>> m = nn.Conv2d(16, 32, (3, 3)).float() >>> loss = nn.NLLLoss2d() >>> # input is of size N x C x height x width >>> input = autograd.Variable(torch.randn(3, 16, 10, 10)) >>> # each element in target has to have 0 <= value < C >>> target = autograd.Variable(torch.LongTensor(3, 8, 8).random_(0, 4)) >>> output = loss(m(input), target) >>> output.backward()","title":"NLLLoss2d"},{"location":"57/#kldivloss","text":"class torch.nn.KLDivLoss(size_average=True, reduce=True) Kullback-Leibler divergence \u635f\u5931 KL \u6563\u5ea6\u53ef\u7528\u4e8e\u8861\u91cf\u4e0d\u540c\u7684\u8fde\u7eed\u5206\u5e03\u4e4b\u95f4\u7684\u8ddd\u79bb, \u5728\u8fde\u7eed\u7684\u8f93\u51fa\u5206\u5e03\u7684\u7a7a\u95f4\u4e0a(\u79bb\u6563\u91c7\u6837)\u4e0a\u8fdb\u884c\u76f4\u63a5\u56de\u5f52\u65f6 \u5f88\u6709\u6548. \u8ddf <cite>NLLLoss</cite> \u4e00\u6837, <cite>input</cite> \u9700\u8981\u542b\u6709 \u5bf9\u6570\u6982\u7387 , \u4e0d\u540c\u4e8e <cite>ClassNLLLoss</cite>, <cite>input</cite> \u53ef \u4ee5\u4e0d\u662f2\u7ef4\u7684 Tensor, \u56e0\u4e3a\u8be5\u51fd\u6570\u4f1a\u9010\u5143\u7d20\u5730\u6c42\u503c. \u8be5\u65b9\u6cd5\u9700\u8981\u4e00\u4e2ashape\u8ddf <cite>input</cite> <cite>Tensor</cite> \u4e00\u6837\u7684 <cite>target</cite> <cite>Tensor</cite>. \u635f\u5931\u53ef\u4ee5\u63cf\u8ff0\u4e3a: \\[loss(x, target) = 1/n \\sum(target_i * (log(target_i) - x_i))\\] \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u548c \u7ef4\u5ea6 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 <cite>size_average</cite> \u8bbe\u7f6e\u4e3a False , \u5219 loss \u4e0d\u4f1a\u53d6\u5e73\u5747\u503c. | Parameters: | size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a \u548c \u7ef4\u5ea6 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u8bbe\u7f6e\u4e3a False , \u5219 loss \u4f1a\u7d2f\u52a0, \u800c\u4e0d\u662f\u53d6\u5e73\u5747\u503c. reduce ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u6839\u636e size_average \u5728\u6bcf \u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u6c42\u5e73\u5747\u503c\u6216\u8005\u6c42\u548c. \u5f53 reduce \u662f False \u65f6, \u635f\u5931\u51fd\u6570\u4f1a\u5bf9\u6bcf \u4e2a batch \u5143\u7d20\u90fd\u8fd4\u56de\u4e00\u4e2a loss \u5e76\u5ffd\u7565 size_average \u5b57\u6bb5. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, *)\\) , \u5176\u4e2d <cite>*</cite> \u8868\u793a\u4efb\u610f\u6570\u91cf\u7684\u989d\u5916\u7ef4\u5ea6. \u76ee\u6807: \\((N, *)\\) , shape \u8ddf\u8f93\u5165\u76f8\u540c \u8f93\u51fa: \u6807\u91cf. \u5982\u679c <cite>reduce</cite> \u662f True , \u5219\u8f93\u51fa\u4e3a \\((N, *)\\) , shape \u8ddf\u8f93\u5165\u76f8\u540c.","title":"KLDivLoss"},{"location":"57/#bceloss","text":"class torch.nn.BCELoss(weight=None, size_average=True) \u8ba1\u7b97\u76ee\u6807\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u4e8c\u8fdb\u5236\u4ea4\u53c9\u71b5: \\[loss(o, t) = - 1/n \\sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))\\] \u5f53\u5b9a\u4e49\u4e86 weight \u53c2\u6570\u65f6: \\[loss(o, t) = - 1/n \\sum_i weight[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))\\] \u8fd9\u53ef\u7528\u4e8e\u6d4b\u91cf\u91cd\u6784\u7684\u8bef\u5dee, \u4f8b\u5982\u81ea\u52a8\u7f16\u7801\u673a. \u6ce8\u610f\u76ee\u6807\u7684\u503c <cite>t[i]</cite> \u7684\u8303\u56f4\u4e3a0\u52301\u4e4b\u95f4. | Parameters: | weight ( Tensor , optional ) \u2013 \u81ea\u5b9a\u4e49\u7684\u6bcf\u4e2a batch \u5143\u7d20\u7684 loss \u7684\u6743\u91cd. \u5fc5\u987b\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a \u201cnbatch\u201d \u7684 \u7684 Tensor size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u4f1a\u5728\u6bcf \u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u7d2f\u52a0, \u800c\u4e0d\u662f\u53d6\u5e73\u5747\u503c. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, *)\\) , \u5176\u4e2d <cite>*</cite> \u8868\u793a\u4efb\u610f\u6570\u91cf\u7684\u989d\u5916\u7ef4\u5ea6. \u76ee\u6807: \\((N, *)\\) , shape \u8ddf\u8f93\u5165\u76f8\u540c Examples: >>> m = nn.Sigmoid() >>> loss = nn.BCELoss() >>> input = autograd.Variable(torch.randn(3), requires_grad=True) >>> target = autograd.Variable(torch.FloatTensor(3).random_(2)) >>> output = loss(m(input), target) >>> output.backward()","title":"BCELoss"},{"location":"57/#bcewithlogitsloss","text":"class torch.nn.BCEWithLogitsLoss(weight=None, size_average=True) \u8be5\u635f\u5931\u51fd\u6570\u628a <cite>Sigmoid</cite> \u5c42\u96c6\u6210\u5230\u4e86 <cite>BCELoss</cite> \u7c7b\u4e2d. \u8be5\u7248\u6bd4\u7528\u4e00\u4e2a\u7b80\u5355\u7684 <cite>Sigmoid</cite> \u5c42\u548c <cite>BCELoss</cite> \u5728\u6570\u503c\u4e0a\u66f4\u7a33\u5b9a, \u56e0\u4e3a\u628a\u8fd9\u4e24\u4e2a\u64cd\u4f5c\u5408\u5e76\u4e3a\u4e00\u4e2a\u5c42\u4e4b\u540e, \u53ef\u4ee5\u5229\u7528 log-sum-exp \u7684 \u6280\u5de7\u6765\u5b9e\u73b0\u6570\u503c\u7a33\u5b9a. \u76ee\u6807\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u4e8c\u503c\u4ea4\u53c9\u71b5(\u4e0d\u542bsigmoid\u51fd\u6570)\u662f: \\[loss(o, t) = - 1/n \\sum_i (t[i] * log(sigmoid(o[i])) + (1 - t[i]) * log(1 - sigmoid(o[i])))\\] \u5f53\u5b9a\u4e49\u4e86 weight \u53c2\u6570\u4e4b\u540e\u53ef\u63cf\u8ff0\u4e3a: \\[loss(o, t) = - 1/n \\sum_i weight[i] * (t[i] * log(sigmoid(o[i])) + (1 - t[i]) * log(1 - sigmoid(o[i])))\\] \u8fd9\u53ef\u7528\u4e8e\u6d4b\u91cf\u91cd\u6784\u7684\u8bef\u5dee, \u4f8b\u5982\u81ea\u52a8\u7f16\u7801\u673a. \u6ce8\u610f\u76ee\u6807\u7684\u503c <cite>t[i]</cite> \u7684\u8303\u56f4\u4e3a0\u52301\u4e4b\u95f4. | Parameters: | weight ( Tensor , optional ) \u2013 \u81ea\u5b9a\u4e49\u7684\u6bcf\u4e2a batch \u5143\u7d20\u7684 loss \u7684\u6743\u91cd. \u5fc5\u987b\u662f\u4e00\u4e2a\u957f\u5ea6 \u4e3a \u201cnbatch\u201d \u7684 Tensor size_average ( bool , optional ) \u2013 \u9ed8\u8ba4\u60c5\u51b5\u4e0b, loss \u4f1a\u5728\u6bcf\u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09 \u4e0a\u53d6\u5e73\u5747\u503c. \u5982\u679c\u5b57\u6bb5 size_average \u88ab\u8bbe\u7f6e\u4e3a False , loss \u4f1a\u5728\u6bcf \u4e2a mini-batch\uff08\u5c0f\u6279\u91cf\uff09\u4e0a\u7d2f\u52a0, \u800c\u4e0d\u662f\u53d6\u5e73\u5747\u503c. \u9ed8\u8ba4\u503c: True | | --- | --- | Shape: \u8f93\u5165: \\((N, *)\\) , \u5176\u4e2d <cite>*</cite> \u8868\u793a\u4efb\u610f\u6570\u91cf\u7684\u989d\u5916\u7ef4\u5ea6. \u76ee\u6807: \\((N, *)\\) , shape \u8ddf\u8f93\u5165\u76f8\u540c Examples: >>> loss = nn.BCEWithLogitsLoss() >>> input = autograd.Variable(torch.randn(3), requires_grad=True) >>> target = autograd.Variable(torch.FloatTensor(3).random_(2)) >>> output = loss(input, target) >>> output.backward()","title":"BCEWithLogitsLoss"},{"location":"57/#marginrankingloss","text":"class torch.nn.MarginRankingLoss(margin=0, size_average=True) \u521b\u5efa\u4e00\u4e2a\u8861\u91cf mini-batch(\u5c0f\u6279\u91cf) \u4e2d\u76842\u4e2a1\u7ef4 <cite>Tensor</cite> \u7684\u8f93\u5165 <cite>x1</cite> \u548c <cite>x2</cite>, \u548c1\u4e2a1\u7ef4 <cite>Tensor</cite> \u7684\u76ee\u6807 <cite>y ( y</cite> \u7684\u53d6\u503c\u662f <cite>1</cite> \u6216\u8005 <cite>-1</cite>) \u4e4b\u95f4\u635f\u5931\u7684\u6807\u51c6. \u5982\u679c <cite>y == 1</cite> \u5219\u8ba4\u4e3a\u7b2c\u4e00\u4e2a\u8f93\u5165\u503c\u5e94\u8be5\u6392\u5217\u5728\u7b2c\u4e8c\u4e2a\u8f93\u5165\u503c\u4e4b\u4e0a(\u5373\u503c\u66f4\u5927), <cite>y == -1</cite> \u65f6\u5219\u76f8\u53cd. \u5bf9\u4e8e mini-batch(\u5c0f\u6279\u91cf) \u4e2d\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u635f\u5931\u51fd\u6570\u5982\u4e0b: loss(x, y) = max(0, -y * (x1 - x2) + margin) \u5982\u679c\u5185\u90e8\u53d8\u91cf <cite>size_average = True</cite>, \u5219\u635f\u5931\u51fd\u6570\u8ba1\u7b97\u6279\u6b21\u4e2d\u6240\u6709\u5b9e\u4f8b\u7684\u635f\u5931\u503c\u7684\u5e73\u5747\u503c; \u5982\u679c <cite>size_average = False</cite>, \u5219\u635f\u5931\u51fd\u6570\u8ba1\u7b97\u6279\u6b21\u4e2d\u6240\u6709\u5b9e\u4f8b\u7684\u635f\u5931\u81f3\u7684\u5408\u8ba1. <cite>size_average</cite> \u9ed8\u8ba4\u503c\u4e3a True .","title":"MarginRankingLoss"},{"location":"57/#hingeembeddingloss","text":"class torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=True) \u8861\u91cf\u8f93\u5165 Tensor(\u5f20\u91cf) <cite>x</cite> \u548c \u76ee\u6807 Tensor(\u5f20\u91cf) <cite>y</cite> (\u53d6\u503c\u4e3a <cite>1</cite> \u548c <cite>-1</cite>) \u4e4b\u95f4\u7684\u635f\u5931\u503c. \u6b64\u65b9\u6cd5\u901a\u5e38\u7528\u6765\u8861\u91cf\u4e24\u4e2a\u8f93\u5165\u503c\u662f\u5426\u76f8\u4f3c, \u4f8b\u5982\u4f7f\u7528L1\u6210\u5bf9\u8ddd\u79bb\u4f5c\u4e3a <cite>x</cite>, \u5e76\u4e14\u901a\u5e38\u7528\u6765\u8fdb\u884c\u975e\u7ebf\u6027\u5d4c\u5165\u5b66\u4e60\u6216\u8005 \u534a\u76d1\u7763\u5b66\u4e60: { x_i, if y_i == 1 loss(x, y) = 1/n { { max(0, margin - x_i), if y_i == -1 <cite>x</cite> \u548c <cite>y</cite> \u5206\u522b\u53ef\u4ee5\u662f\u5177\u6709 <cite>n</cite> \u4e2a\u5143\u7d20\u7684\u4efb\u610f\u5f62\u72b6. \u5408\u8ba1\u64cd\u4f5c\u5bf9\u6240\u6709\u5143\u7d20\u8fdb\u884c\u8ba1\u7b97. \u5982\u679c <cite>size_average=False</cite>, \u5219\u8ba1\u7b97\u65f6\u4e0d\u4f1a\u9664\u4ee5 <cite>n</cite> \u53d6\u5e73\u5747\u503c. <cite>margin</cite> \u7684\u9ed8\u8ba4\u503c\u662f <cite>1</cite>, \u6216\u8005\u53ef\u4ee5\u901a\u8fc7\u6784\u9020\u51fd\u6570\u6765\u8bbe\u7f6e.","title":"HingeEmbeddingLoss"},{"location":"57/#multilabelmarginloss","text":"class torch.nn.MultiLabelMarginLoss(size_average=True) \u521b\u5efa\u4e00\u4e2a\u6807\u51c6, \u7528\u4ee5\u4f18\u5316\u591a\u5143\u5206\u7c7b\u95ee\u9898\u7684\u5408\u9875\u635f\u5931\u51fd\u6570 (\u57fa\u4e8e\u7a7a\u767d\u7684\u635f\u5931), \u8ba1\u7b97\u635f\u5931\u503c\u65f6 \u9700\u89812\u4e2a\u53c2\u6570\u5206\u522b\u4e3a\u8f93\u5165, <cite>x</cite> (\u4e00\u4e2a2\u7ef4\u5c0f\u6279\u91cf <cite>Tensor</cite>) \u548c\u8f93\u51fa <cite>y</cite> (\u4e00\u4e2a2\u7ef4 <cite>Tensor</cite>, \u5176\u503c\u4e3a <cite>x</cite> \u7684\u7d22\u5f15\u503c). \u5bf9\u4e8emini-batch(\u5c0f\u6279\u91cf) \u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u6309\u5982\u4e0b\u516c\u5f0f\u8ba1\u7b97\u635f\u5931: loss(x, y) = sum_ij(max(0, 1 - (x[y[j]] - x[i]))) / x.size(0) \u5176\u4e2d <cite>i</cite> \u7684\u53d6\u503c\u8303\u56f4\u662f <cite>0</cite> \u5230 <cite>x.size(0)</cite>, <cite>j</cite> \u7684\u53d6\u503c\u8303\u56f4\u662f <cite>0</cite> \u5230 <cite>y.size(0)</cite>, <cite>y[j] >= 0</cite>, \u5e76\u4e14\u5bf9\u4e8e\u6240\u6709 <cite>i</cite> \u548c <cite>j</cite> \u6709 <cite>i != y[j]</cite>. <cite>y</cite> \u548c <cite>x</cite> \u5fc5\u987b\u6709\u76f8\u540c\u7684\u5143\u7d20\u6570\u91cf. \u6b64\u6807\u51c6\u4ec5\u8003\u8651 <cite>y[j]</cite> \u4e2d\u6700\u5148\u51fa\u73b0\u7684\u975e\u96f6\u503c. \u5982\u6b64\u53ef\u4ee5\u5141\u8bb8\u6bcf\u4e2a\u6837\u672c\u53ef\u4ee5\u6709\u6570\u91cf\u4e0d\u540c\u7684\u76ee\u6807\u7c7b\u522b.","title":"MultiLabelMarginLoss"},{"location":"57/#smoothl1loss","text":"class torch.nn.SmoothL1Loss(size_average=True, reduce=True) \u521b\u5efa\u4e00\u4e2a\u6807\u51c6, \u5f53\u67d0\u4e2a\u5143\u7d20\u7684\u9519\u8bef\u503c\u7684\u7edd\u5bf9\u503c\u5c0f\u4e8e1\u65f6\u4f7f\u7528\u5e73\u65b9\u9879\u8ba1\u7b97, \u5176\u4ed6\u60c5\u51b5\u5219\u4f7f\u7528L1\u8303\u5f0f\u8ba1\u7b97. \u6b64\u65b9\u6cd5\u521b\u5efa\u7684\u6807\u51c6\u5bf9\u4e8e\u5f02\u5e38\u503c\u4e0d\u5982 [ ](#id31)MSELoss \u654f\u611f, \u4f46\u662f\u540c\u65f6\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u9632\u6b62\u68af\u5ea6\u7206\u70b8 (\u6bd4\u5982 \u53c2\u89c1\u8bba\u6587 \u201cFast R-CNN\u201d \u4f5c\u8005 Ross Girshick). \u4e5f\u88ab\u79f0\u4e3a Huber \u635f\u5931\u51fd\u6570: { 0.5 * (x_i - y_i)^2, if |x_i - y_i| < 1 loss(x, y) = 1/n \\sum { { |x_i - y_i| - 0.5, otherwise <cite>x</cite> \u548c <cite>y</cite> \u53ef\u4ee5\u662f\u4efb\u610f\u5f62\u72b6\u53ea\u8981\u90fd\u5177\u5907\u603b\u8ba1 <cite>n</cite> \u4e2a\u5143\u7d20 \u5408\u8ba1\u4ecd\u7136\u9488\u5bf9\u6240\u6709\u5143\u7d20\u8fdb\u884c\u8ba1\u7b97, \u5e76\u4e14\u6700\u540e\u9664\u4ee5 <cite>n</cite>. \u5982\u679c\u628a\u5185\u90e8\u53d8\u91cf <cite>size_average</cite> \u8bbe\u7f6e\u4e3a False , \u5219\u4e0d\u4f1a\u88ab\u9664\u4ee5 <cite>n</cite>. | Parameters: | size_average ( bool , optional ) \u2013 \u635f\u5931\u503c\u9ed8\u8ba4\u4f1a\u6309\u7167\u6240\u6709\u5143\u7d20\u53d6\u5e73\u5747\u503c. \u4f46\u662f, \u5982\u679c size_average \u88ab \u8bbe\u7f6e\u4e3a False , \u5219\u635f\u5931\u503c\u4e3a\u6240\u6709\u5143\u7d20\u7684\u5408\u8ba1. \u5982\u679c reduce \u53c2\u6570\u8bbe\u4e3a False , \u5219\u5ffd\u7565\u6b64\u53c2\u6570\u7684\u503c. \u9ed8\u8ba4: True reduce ( bool , optional ) \u2013 \u635f\u5931\u503c\u9ed8\u8ba4\u4f1a\u6309\u7167\u6240\u6709\u5143\u7d20\u53d6\u5e73\u5747\u503c\u6216\u8005\u53d6\u5408\u8ba1\u503c. \u5f53 reduce \u8bbe\u7f6e\u4e3a False \u65f6, \u635f\u5931\u51fd\u6570\u5bf9\u4e8e\u6bcf\u4e2a\u5143\u7d20\u90fd\u8fd4\u56de\u635f\u5931\u503c\u5e76\u4e14\u5ffd\u7565 size_average \u53c2\u6570. \u9ed8\u8ba4: True | | --- | --- | Shape: \u8f93\u5165: \\((N, *)\\) <cite>*</cite> \u4ee3\u8868\u4efb\u610f\u4e2a\u5176\u4ed6\u7ef4\u5ea6 \u76ee\u6807: \\((N, *)\\) , \u540c\u8f93\u5165 \u8f93\u51fa: \u6807\u91cf. \u5982\u679c reduce \u8bbe\u4e3a False \u5219\u4e3a \\((N, *)\\) , \u540c\u8f93\u5165","title":"SmoothL1Loss"},{"location":"57/#softmarginloss","text":"class torch.nn.SoftMarginLoss(size_average=True) \u521b\u5efa\u4e00\u4e2a\u6807\u51c6, \u7528\u4ee5\u4f18\u5316\u4e24\u5206\u7c7b\u7684 logistic loss. \u8f93\u5165\u4e3a <cite>x</cite> (\u4e00\u4e2a2\u7ef4 mini-batch Tensor)\u548c \u76ee\u6807 <cite>y</cite> (\u4e00\u4e2a\u5305\u542b <cite>1</cite> \u6216\u8005 <cite>-1</cite> \u7684 Tensor). loss(x, y) = sum_i (log(1 + exp(-y[i]*x[i]))) / x.nelement() \u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e <cite>self.size_average</cite> \u4e3a False \u6765\u7981\u7528\u6309\u7167\u5143\u7d20\u6570\u91cf\u53d6\u5e73\u5747\u7684\u6b63\u5219\u5316\u64cd\u4f5c.","title":"SoftMarginLoss"},{"location":"57/#multilabelsoftmarginloss","text":"class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=True) \u521b\u5efa\u4e00\u4e2a\u6807\u51c6, \u57fa\u4e8e\u8f93\u5165 <cite>x</cite> \u548c\u76ee\u6807 <cite>y \u7684 max-entropy(\u6700\u5927\u71b5), \u4f18\u5316\u591a\u6807\u7b7e one-versus-all \u635f\u5931. \u8f93\u5165 x</cite> \u4e3a\u4e00\u4e2a2\u7ef4 mini-batch <cite>Tensor</cite>, \u76ee\u6807 <cite>y</cite> \u4e3a2\u8fdb\u52362\u7ef4 <cite>Tensor</cite>. \u5bf9\u6bcf\u4e2a mini-batch \u4e2d\u7684\u6837\u672c, \u5bf9\u5e94\u7684 loss \u4e3a: loss(x, y) = - sum_i (y[i] * log( 1 / (1 + exp(-x[i])) ) + ( (1-y[i]) * log(exp(-x[i]) / (1 + exp(-x[i])) ) ) \u5176\u4e2d <cite>i == 0</cite> \u81f3 <cite>x.nElement()-1</cite>, <cite>y[i] in {0,1}</cite>. <cite>y</cite> \u548c <cite>x</cite> \u5fc5\u987b\u5177\u6709\u76f8\u540c\u7684\u7ef4\u5ea6.","title":"MultiLabelSoftMarginLoss"},{"location":"57/#cosineembeddingloss","text":"class torch.nn.CosineEmbeddingLoss(margin=0, size_average=True) \u65b0\u5efa\u4e00\u4e2a\u6807\u51c6, \u7528\u4ee5\u8861\u91cf\u8f93\u5165 <cite>Tensor</cite> x1, x2 \u548c\u53d6\u503c\u4e3a 1 \u6216\u8005 -1 \u7684\u6807\u7b7e <cite>Tensor</cite> [ ](#id33)y \u4e4b\u95f4\u7684 \u635f\u5931\u503c. \u6b64\u6807\u51c6\u7528 cosine \u8ddd\u79bb\u6765\u8861\u91cf2\u4e2a\u8f93\u5165\u53c2\u6570\u4e4b\u95f4\u662f\u5426\u76f8\u4f3c, \u5e76\u4e14\u4e00\u822c\u7528\u6765\u5b66\u4e60\u975e\u7ebf\u6027 embedding \u6216\u8005\u534a\u76d1\u7763 \u5b66\u4e60. <cite>margin</cite> \u5e94\u8be5\u53d6 <cite>-1</cite> \u5230 <cite>1</cite> \u4e4b\u95f4\u7684\u503c, \u5efa\u8bae\u53d6\u503c\u8303\u56f4\u662f <cite>0</cite> \u5230 <cite>0.5</cite>. \u5982\u679c\u6ca1\u6709\u8bbe\u7f6e <cite>margin</cite> \u53c2\u6570, \u5219\u9ed8\u8ba4\u503c\u53d6 <cite>0</cite>. \u6bcf\u4e2a\u6837\u672c\u7684\u635f\u5931\u51fd\u6570\u5982\u4e0b: { 1 - cos(x1, x2), if y == 1 loss(x, y) = { { max(0, cos(x1, x2) - margin), if y == -1 \u5982\u679c\u5185\u90e8\u53d8\u91cf <cite>size_average</cite> \u8bbe\u7f6e\u4e3a True , \u5219\u635f\u5931\u51fd\u6570\u4ee5 batch \u4e2d\u6240\u6709\u7684\u6837\u672c\u6570\u53d6\u5e73\u5747\u503c; \u5982\u679c <cite>size_average</cite> \u8bbe\u7f6e\u4e3a False , \u5219\u635f\u5931\u51fd\u6570\u5bf9 batch \u4e2d\u6240\u6709\u7684\u6837\u672c\u6c42\u548c. \u9ed8\u8ba4\u60c5\u51b5\u4e0b, <cite>size_average = True</cite>.","title":"CosineEmbeddingLoss"},{"location":"57/#multimarginloss","text":"class torch.nn.MultiMarginLoss(p=1, margin=1, weight=None, size_average=True) \u521b\u5efa\u4e00\u4e2a\u6807\u51c6, \u7528\u4ee5\u4f18\u5316\u591a\u5143\u5206\u7c7b\u95ee\u9898\u7684\u5408\u9875\u635f\u5931\u51fd\u6570 (\u57fa\u4e8e\u7a7a\u767d\u7684\u635f\u5931), \u8ba1\u7b97\u635f\u5931\u503c\u65f6 \u9700\u89812\u4e2a\u53c2\u6570\u5206\u522b\u4e3a\u8f93\u5165, <cite>x</cite> (\u4e00\u4e2a2\u7ef4\u5c0f\u6279\u91cf <cite>Tensor</cite>) \u548c\u8f93\u51fa <cite>y</cite> (\u4e00\u4e2a1\u7ef4 <cite>Tensor</cite>, \u5176\u503c\u4e3a <cite>x</cite> \u7684\u7d22\u5f15\u503c, <cite>0</cite> <= <cite>y</cite> <= <cite>x.size(1)</cite>): \u5bf9\u4e8e\u6bcf\u4e2a mini-batch(\u5c0f\u6279\u91cf) \u6837\u672c: loss(x, y) = sum_i(max(0, (margin - x[y] + x[i]))^p) / x.size(0) \u5176\u4e2d `i == 0` \u81f3 `x.size(0)` \u5e76\u4e14 `i != y`. \u53ef\u9009\u62e9\u7684, \u5982\u679c\u60a8\u4e0d\u60f3\u6240\u6709\u7684\u7c7b\u62e5\u6709\u540c\u6837\u7684\u6743\u91cd\u7684\u8bdd, \u60a8\u53ef\u4ee5\u901a\u8fc7\u5728\u6784\u9020\u51fd\u6570\u4e2d\u4f20\u5165 <cite>weight</cite> \u53c2\u6570\u6765 \u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898, <cite>weight</cite> \u662f\u4e00\u4e2a1\u7ef4 Tensor. \u4f20\u5165 <cite>weight</cite> \u540e, \u635f\u5931\u51fd\u6570\u53d8\u4e3a: loss(x, y) = sum_i(max(0, w[y] * (margin - x[y] - x[i]))^p) / x.size(0) \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6c42\u51fa\u7684\u635f\u5931\u503c\u4f1a\u5bf9\u6bcf\u4e2a minibatch \u6837\u672c\u7684\u7ed3\u679c\u53d6\u5e73\u5747. \u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e <cite>size_average</cite> \u4e3a False \u6765\u7528\u5408\u8ba1\u64cd\u4f5c\u53d6\u4ee3\u53d6\u5e73\u5747\u64cd\u4f5c.","title":"MultiMarginLoss"},{"location":"57/#tripletmarginloss","text":"class torch.nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-06, swap=False) \u521b\u5efa\u4e00\u4e2a\u6807\u51c6, \u7528\u4ee5\u8861\u91cf\u4e09\u5143\u7ec4\u5408\u7684\u635f\u5931\u503c, \u8ba1\u7b97\u635f\u5931\u503c\u65f6\u9700\u89813\u4e2a\u8f93\u5165\u5f20\u91cf <cite>x1</cite>, <cite>x2</cite>, <cite>x3</cite> \u548c \u4e00\u4e2a\u5927\u4e8e\u96f6\u7684 <cite>margin</cite> \u503c. \u6b64\u6807\u51c6\u53ef\u4ee5\u7528\u6765\u8861\u91cf\u8f93\u5165\u6837\u672c\u95f4\u7684\u76f8\u5bf9\u76f8\u4f3c\u6027. \u4e00\u4e2a\u4e09\u5143\u8f93\u5165\u7ec4\u5408\u7531 <cite>a</cite>, <cite>p</cite> \u548c <cite>n</cite>: anchor, positive \u6837\u672c \u548c negative \u6837\u672c\u7ec4\u6210. \u6240\u6709\u8f93\u5165\u53d8\u91cf\u7684\u5f62\u5f0f\u5fc5\u987b\u4e3a \\((N, D)\\) . \u8ddd\u79bb\u4ea4\u6362\u7684\u8be6\u7ec6\u8bf4\u660e\u8bf7\u53c2\u8003\u8bba\u6587 Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. \\[L(a, p, n) = \\frac{1}{N} \\left( \\sum_{i=1}^N \\max \\{d(a_i, p_i) - d(a_i, n_i) + {\\rm margin}, 0\\} \\right)\\] \u5176\u4e2d \\(d(x_i, y_i) = \\left\\lVert {\\bf x}_i - {\\bf y}_i \\right\\rVert_p\\) . | Parameters: | anchor \u2013 anchor \u8f93\u5165 tensor positive \u2013 positive \u8f93\u5165 tensor negative \u2013 negative \u8f93\u5165 tensor p \u2013 \u6b63\u5219\u5316\u7387. Default: 2 | | --- | --- | Shape: Input: \\((N, D)\\) \u5176\u4e2d <cite>D = vector dimension</cite> Output: \\((N, 1)\\) >>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2) >>> input1 = autograd.Variable(torch.randn(100, 128)) >>> input2 = autograd.Variable(torch.randn(100, 128)) >>> input3 = autograd.Variable(torch.randn(100, 128)) >>> output = triplet_loss(input1, input2, input3) >>> output.backward()","title":"TripletMarginLoss"},{"location":"57/#vision-layers","text":"","title":"Vision layers (\u89c6\u89c9\u5c42)"},{"location":"57/#pixelshuffle","text":"class torch.nn.PixelShuffle(upscale_factor) \u5bf9\u5f20\u91cf\u4e2d\u5f62\u5982 \\((*, C * r^2, H, W]\\) \u7684\u5143\u7d20, \u91cd\u65b0\u6392\u5217\u6210 \\((C, H * r, W * r)\\) . \u5f53\u4f7f\u7528 stride = \\(1/r\\) \u7684\u9ad8\u6548\u5b50\u50cf\u7d20\u5377\u79ef\u5f88\u6709\u7528. \u53c2\u8003\u5982\u4e0b\u8bba\u6587\u83b7\u5f97\u66f4\u591a\u4fe1\u606f: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network Shi et. al (2016) . Parameters: upscale_factor ( int ) \u2013 \u589e\u52a0\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u56e0\u5b50 Shape: \u8f93\u5165: \\((N, C * {upscale\\_factor}^2, H, W)\\) \u8f93\u51fa: \\((N, C, H * {upscale\\_factor}, W * {upscale\\_factor})\\) Examples: >>> ps = nn.PixelShuffle(3) >>> input = autograd.Variable(torch.Tensor(1, 9, 4, 4)) >>> output = ps(input) >>> print(output.size()) torch.Size([1, 1, 12, 12])","title":"PixelShuffle"},{"location":"57/#upsample","text":"class torch.nn.Upsample(size=None, scale_factor=None, mode='nearest') \u5bf9\u7ed9\u5b9a\u7684\u591a\u901a\u9053\u4e00\u7ef4\u65f6\u5e8f\u6570\u636e, \u4e8c\u7ef4\u7a7a\u95f4\u6570\u636e, \u6216\u4e09\u7ef4\u5bb9\u79ef\u6570\u636e\u8fdb\u884c\u4e0a\u91c7\u6837. \u8f93\u5165\u6570\u636e\u7684\u683c\u5f0f\u4e3a <cite>minibatch x channels x [depth] x [height] x width</cite>. \u56e0\u6b64, \u5bf9\u4e8e2-D\u7a7a\u95f4\u6570\u636e\u7684\u8f93\u5165, \u671f\u671b\u5f97\u5230\u4e00\u4e2a4-D\u5f20\u91cf\uff1b\u5bf9\u4e8e3-D\u7acb\u4f53\u6570\u636e\u8f93\u5165, \u671f\u671b\u5f97\u5230\u4e00\u4e2a5-D\u5f20\u91cf. \u5bf93D, 4D, 5D\u7684\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u6700\u8fd1\u90bb\u3001\u7ebf\u6027\u3001\u53cc\u7ebf\u6027\u548c\u4e09\u7ebf\u6027\u91c7\u6837, \u53ef\u7528\u4e8e\u8be5\u4e0a\u91c7\u6837\u65b9\u6cd5. \u53ef\u4ee5\u63d0\u4f9b scale_factor \u6216\u76ee\u6807\u8f93\u51fa\u7684 size \u6765\u8ba1\u7b97\u8f93\u51fa\u7684\u5927\u5c0f. \uff08\u4e0d\u80fd\u540c\u65f6\u90fd\u7ed9, \u56e0\u4e3a\u8fd9\u6837\u505a\u662f\u542b\u7cca\u4e0d\u6e05\u7684. \uff09 | Parameters: | size ( tuple , optional ) \u2013 \u6574\u578b\u6570\u7684\u5143\u7ec4 ([D_out], [H_out], W_out) \u8f93\u51fa\u5927\u5c0f scale_factor ( int / tuple of python:ints__, optional ) \u2013 \u56fe\u50cf\u9ad8\u5ea6/\u5bbd\u5ea6/\u6df1\u5ea6\u7684\u4e58\u6570 mode ( string , optional ) \u2013 \u4e0a\u91c7\u6837\u7b97\u6cd5: nearest | linear | bilinear | trilinear. \u9ed8\u8ba4\u4e3a: nearest | | --- | --- | Shape: \u8f93\u5165: \\((N, C, W_{in})\\) , \\((N, C, H_{in}, W_{in})\\) \u6216 \\((N, C, D_{in}, H_{in}, W_{in})\\) \u8f93\u51fa: \\((N, C, W_{out})\\) , \\((N, C, H_{out}, W_{out})\\) \u6216 \\((N, C, D_{out}, H_{out}, W_{out})\\) \u5176\u4e2d: \\(D_{out} = floor(D_{in} * scale\\_factor)\\) \u6216 <cite>size[-3]</cite> \\(H_{out} = floor(H_{in} * scale\\_factor)\\) \u6216 <cite>size[-2]</cite> \\(W_{out} = floor(W_{in} * scale\\_factor)\\) \u6216 <cite>size[-1]</cite> \u793a\u4f8b: >>> inp Variable containing: (0 ,0 ,.,.) = 1 2 3 4 [torch.FloatTensor of size 1x1x2x2] >>> m = nn.Upsample(scale_factor=2, mode='bilinear') >>> m(inp) Variable containing: (0 ,0 ,.,.) = 1.0000 1.3333 1.6667 2.0000 1.6667 2.0000 2.3333 2.6667 2.3333 2.6667 3.0000 3.3333 3.0000 3.3333 3.6667 4.0000 [torch.FloatTensor of size 1x1x4x4] >>> inp Variable containing: (0 ,0 ,.,.) = 1 2 3 4 [torch.FloatTensor of size 1x1x2x2] >>> m = nn.Upsample(scale_factor=2, mode='nearest') >>> m(inp) Variable containing: (0 ,0 ,.,.) = 1 1 2 2 1 1 2 2 3 3 4 4 3 3 4 4 [torch.FloatTensor of size 1x1x4x4]","title":"Upsample"},{"location":"57/#upsamplingnearest2d","text":"class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None) \u5bf9\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c2\u7ef4\u6700\u8fd1\u90bb\u4e0a\u91c7\u6837. \u4e3a\u4e86\u6307\u5b9a\u91c7\u6837\u8303\u56f4, \u63d0\u4f9b\u4e86 size \u6216 scale_factor \u4f5c\u4e3a\u6784\u9020\u53c2\u6570. \u5f53\u7ed9\u5b9a <cite>size</cite>, \u8f93\u51fa\u56fe\u50cf\u7684\u5927\u5c0f\u4e3a (h, w). | Parameters: | size ( tuple , optional ) \u2013 \u8f93\u51fa\u56fe\u7247\u5927\u5c0f\u7684\u6574\u578b\u5143\u7ec4(H_out, W_out) scale_factor ( int , optional ) \u2013 \u56fe\u50cf\u7684 \u957f\u548c\u5bbd\u7684\u4e58\u5b50. | | --- | --- | Shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) \u5176\u4e2d \\(H_{out} = floor(H_{in} * scale\\_factor)\\) \\(W_{out} = floor(W_{in} * scale\\_factor)\\) \u793a\u4f8b: >>> inp Variable containing: (0 ,0 ,.,.) = 1 2 3 4 [torch.FloatTensor of size 1x1x2x2] >>> m = nn.UpsamplingNearest2d(scale_factor=2) >>> m(inp) Variable containing: (0 ,0 ,.,.) = 1 1 2 2 1 1 2 2 3 3 4 4 3 3 4 4 [torch.FloatTensor of size 1x1x4x4]","title":"UpsamplingNearest2d"},{"location":"57/#upsamplingbilinear2d","text":"class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None) \u5bf9\u591a\u4e2a\u8f93\u5165\u901a\u9053\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c2\u7ef4\u53cc\u7ebf\u6027\u4e0a\u91c7\u6837. \u4e3a\u4e86\u6307\u5b9a\u91c7\u6837\u8303\u56f4, \u63d0\u4f9b\u4e86 size \u6216 scale_factor \u4f5c\u4e3a\u6784\u9020\u53c2\u6570. \u5f53\u7ed9\u5b9a <cite>size</cite>, \u8f93\u51fa\u56fe\u50cf\u7684\u5927\u5c0f\u4e3a (h, w). | Parameters: | size ( tuple , optional ) \u2013 \u8f93\u51fa\u56fe\u7247\u5927\u5c0f\u7684\u6574\u578b\u5143\u7ec4(H_out, W_out) scale_factor ( int , optional ) \u2013 \u56fe\u50cf\u7684 \u957f\u548c\u5bbd\u7684\u4e58\u5b50. | | --- | --- | shape: Input: \\((N, C, H_{in}, W_{in})\\) Output: \\((N, C, H_{out}, W_{out})\\) \u5176\u4e2d \\(H_{out} = floor(H_{in} * scale\\_factor)\\) \\(W_{out} = floor(W_{in} * scale\\_factor)\\) \u793a\u4f8b::: >>> inp Variable containing: (0 ,0 ,.,.) = 1 2 3 4 [torch.FloatTensor of size 1x1x2x2] >>> m = nn.UpsamplingBilinear2d(scale_factor=2) >>> m(inp) Variable containing: (0 ,0 ,.,.) = 1.0000 1.3333 1.6667 2.0000 1.6667 2.0000 2.3333 2.6667 2.3333 2.6667 3.0000 3.3333 3.0000 3.3333 3.6667 4.0000 [torch.FloatTensor of size 1x1x4x4]","title":"UpsamplingBilinear2d"},{"location":"57/#dataparallel-layers-multi-gpu-distributed-gpu","text":"","title":"DataParallel layers (multi-GPU, distributed) (\u6570\u636e\u5e76\u884c\u5c42, \u591a GPU \u7684, \u5206\u5e03\u5f0f\u7684)"},{"location":"57/#dataparallel","text":"class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0) \u5728\u6a21\u5757\u7ea7\u522b\u5b9e\u73b0\u6570\u636e\u5e76\u884c\u6027. \u6b64\u5bb9\u5668\u901a\u8fc7\u5728\u6279\u6b21\u7ef4\u5ea6\u4e2d\u5206\u5757, \u5c06\u8f93\u5165\u5206\u5272\u5230\u6307\u5b9a\u8bbe\u5907\u4e0a, \u4ece\u800c\u5e76\u884c\u5316\u7ed9\u5b9a\u6a21\u5757\u7684\u5e94\u7528\u7a0b \u5e8f.\u5728\u6b63\u5411\u4f20\u9012\u4e2d, \u6a21\u5757\u88ab\u590d\u5236\u5230\u6bcf\u4e2a\u8bbe\u5907\u4e0a, \u6bcf\u4e2a\u526f\u672c\u5904\u7406\u4e00\u90e8\u5206\u8f93\u5165.\u5728\u5411\u540e\u4f20\u9012\u671f\u95f4, \u6765\u81ea\u6bcf\u4e2a\u526f\u672c\u7684\u68af\u5ea6\u53d8\u5316\u88ab\u6c47\u603b\u5230\u539f\u59cb\u6a21\u5757\u4e2d. batch size \u5e94\u8be5\u5927\u4e8e GPUs \u7684\u6570\u91cf.\u540c\u65f6\u4e5f\u5e94\u8be5\u662f GPU \u6570\u91cf\u7684\u6574\u6570\u500d, \u4ee5 \u4fbf\u6bcf\u4e2a\u5757\u5927\u5c0f\u76f8\u540c\uff08\u4ee5\u4fbf\u6bcf\u4e2a GPU \u5904\u7406\u76f8\u540c\u6570\u91cf\u7684\u6837\u672c\uff09. \u5f15\u7528 : \u4f7f\u7528 nn.DataParallel \u66ff\u4ee3 multiprocessing \u5141\u8bb8\u5c06\u4efb\u610f\u4f4d\u7f6e\u548c\u5173\u952e\u5b57\u8f93\u5165\u4f20\u5165 DataParallel EXCEPT Tensors. \u6240\u6709\u7684\u53d8\u91cf\u5c06\u88ab\u5206 \u6563\u5728\u6307\u5b9a\u7684\u7ef4\u5ea6\uff08\u9ed8\u8ba4\u4e3a0\uff09.\u539f\u59cb\u7c7b\u578b\u5c06\u88ab\u5e7f\u64ad, \u4f46\u6240\u6709\u5176\u4ed6\u7c7b\u578b\u5c06\u662f\u4e00\u4e2a\u6d45\u5c42\u526f\u672c, \u5982 \u679c\u5199\u5165\u6a21\u578b\u7684\u6b63\u5411\u4f20\u9012, \u53ef\u80fd\u4f1a\u88ab\u635f\u574f. Args : module: \u5e76\u884c\u7684\u6a21\u578b device_ids: CUDA devices\uff08CUDA \u9a71\u52a8\uff09 (default: all devices) output_device: \u8f93\u51fa\u8bbe\u5907\u4f4d\u7f6e (default: device_ids[0]) \u793a\u4f8b :: >>> net = torch.nn.DataParallel(model, device_ids=[0, 1, 2]) >>> output = net(input_var)","title":"DataParallel"},{"location":"57/#distributeddataparallel","text":"class torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0) \u5728\u6a21\u5757\u7ea7\u522b\u5b9e\u73b0\u5206\u5e03\u5f0f\u6570\u636e\u5e76\u884c. \u6b64\u5bb9\u5668\u901a\u8fc7\u5728\u6279\u6b21\u7ef4\u5ea6\u4e2d\u5206\u5757, \u5c06\u8f93\u5165\u5206\u5272\u5230\u6307\u5b9a\u8bbe\u5907\u4e0a, \u4ece\u800c\u5e76\u884c\u5316\u7ed9\u5b9a\u6a21\u5757\u7684\u5e94\u7528\u7a0b\u5e8f. \u8be5\u6a21\u5757\u88ab\u590d\u5236\u5230\u6bcf\u53f0\u673a\u5668\u548c\u6bcf\u4e2a\u8bbe\u5907\u4e0a, \u6bcf\u4e2a\u8fd9\u6837\u7684\u526f\u672c\u5904\u7406\u4e00\u90e8\u5206\u8f93\u5165.\u5728\u5411\u540e\u4f20\u9012\u671f\u95f4, \u6765\u81ea\u6bcf\u4e2a\u8282\u70b9\u7684\u68af\u5ea6\u88ab\u5e73\u5747. batch size \u5e94\u8be5\u5927\u4e8e GPUs \u7684\u6570\u91cf.\u540c\u65f6\u4e5f\u5e94\u8be5\u662f GPU \u6570\u91cf\u7684\u6574\u6570\u500d, \u4ee5\u4fbf\u6bcf\u4e2a\u5757\u5927\u5c0f \u76f8\u540c\uff08\u4ee5\u4fbf\u6bcf\u4e2a GPU \u5904\u7406\u76f8\u540c\u6570\u91cf\u7684\u6837\u672c\uff09. \u5f15\u7528 : Basics \u548c \u4f7f\u7528 nn.DataParallel \u66ff\u4ee3 multiprocessing . \u5bf9\u8f93\u5165\u7684\u7ea6\u675f\u548c torch.nn.DataParallel \u4e2d\u4e00\u6837. \u521b\u5efa\u8fd9\u4e2a\u7c7b\u9700\u8981\u5206\u5e03\u5f0f\u5305\u5df2\u7ecf\u5728 process group \u6a21\u5f0f\u4e0b\u88ab\u521d\u59cb\u5316 (\u5f15\u7528 torch.distributed.init_process_group() ). Warning \u8fd9\u4e2a\u6a21\u5757\u53ea\u80fd\u548c gloo \u540e\u7aef\u4e00\u8d77\u5de5\u4f5c. Warning \u6784\u9020\u5668, \u8f6c\u53d1\u65b9\u6cd5\u548c\u8f93\u51fa\uff08\u6216\u8005\u8fd9\u4e2a\u6a21\u5757\u7684\u8f93\u51fa\u529f\u80fd\uff09\u7684\u533a\u5206\u662f\u5206\u5e03\u5f0f\u540c\u6b65\u70b9.\u8003\u8651\u5230\u4e0d\u540c\u7684 \u8fdb\u7a0b\u53ef\u80fd\u4f1a\u6267\u884c\u4e0d\u540c\u7684\u4ee3\u7801. Warning \u8be5\u6a21\u5757\u5047\u8bbe\u6240\u6709\u53c2\u6570\u5728\u521b\u5efa\u65f6\u90fd\u5728\u6a21\u578b\u4e2d\u6ce8\u518c.\u4e4b\u540e\u4e0d\u5e94\u8be5\u6dfb\u52a0\u6216\u5220\u9664\u53c2\u6570.\u540c\u6837\u9002\u7528\u4e8e\u7f13\u51b2\u533a. Warning \u8fd9\u4e2a\u6a21\u5757\u5047\u5b9a\u6240\u6709\u7684\u7f13\u51b2\u533a\u548c\u68af\u5ea6\u90fd\u662f\u5bc6\u96c6\u7684. Warning \u8fd9\u4e2a\u6a21\u5757\u4e0d\u80fd\u7528\u4e8e : func: <cite>torch.autograd.grad</cite> \uff08\u5373\u53ea\u6709\u5728\u53c2\u6570\u7684 .grad \u5c5e\u6027\u4e2d \u7d2f\u79ef\u68af\u5ea6\u624d\u80fd\u4f7f\u7528\uff09. Note \u53c2\u6570\u6c38\u8fdc\u4e0d\u4f1a\u5728\u8fdb\u7a0b\u4e4b\u95f4\u5e7f\u64ad.\u6a21\u5757\u5728\u68af\u5ea6\u4e0a\u6267\u884c\u5168\u90e8\u4f18\u5316\u6b65\u9aa4, \u5e76\u5047\u5b9a\u5b83\u4eec\u5c06\u4ee5\u76f8\u540c\u7684\u65b9\u5f0f\u5728 \u6240\u6709\u8fdb\u7a0b\u4e2d\u8fdb\u884c\u4f18\u5316.\u7f13\u51b2\u533a\uff08e.g. BatchNorm stats\uff09\u5728\u7b49\u7ea70\u7684\u8fc7\u7a0b\u4e2d\u4ece\u6a21\u5757\u5e7f\u64ad\u5230\u7cfb\u7edf \u4e2d\u7684\u6bcf\u4e2a\u8fed\u4ee3\u4e2d\u7684\u6240\u6709\u5176\u4ed6\u526f\u672c. Args : module: \u9700\u8981\u5e76\u884c\u7684\u6a21\u578b device_ids: CUDA devices (default: all devices) output_device: device location of output (default: device_ids[0]) \u793a\u4f8b :: >>> torch.distributed.init_process_group(world_size=4, init_method='...') >>> net = torch.nn.DistributedDataParallel(model)","title":"DistributedDataParallel"},{"location":"57/#utilities","text":"","title":"Utilities (\u5de5\u5177\u5305)"},{"location":"57/#clip_grad_norm","text":"torch.nn.utils.clip_grad_norm(parameters, max_norm, norm_type=2) \u63a5\u6536\u4e00\u4e2a\u5305\u542b Variable \u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61, \u5bf9 Variable \u7684\u68af\u5ea6\u6309\u8303\u6570\u8fdb\u884c\u88c1\u526a. \u8303\u6570\u662f\u5bf9\u6240\u6709\u68af\u5ea6\u8fdb\u884c\u8ba1\u7b97\u7684, \u7b49\u4ef7\u4e8e\u628a\u6240\u6709\u8f93\u5165\u53d8\u91cf\u7684\u68af\u5ea6\u8fde\u63a5\u6210\u4e00\u4e2a\u5411\u91cf, \u7136\u540e\u5bf9\u8fd9\u4e2a\u5411\u91cf\u6309\u8303\u6570\u8fdb\u884c\u88c1\u526a. \u68af\u5ea6\u5c06\u4f1a\u88ab\u539f\u5730\u4fee\u6539. | Parameters: | parameters ( Iterable__[ Variable ] ) \u2013 \u4e00\u4e2a\u53ef\u8fed\u4ee3\u5bf9\u8c61, \u5176\u5305\u542b\u5c06\u8981\u8fdb\u884c\u68af\u5ea6\u6b63\u89c4\u5316\u7684 Variable max_norm ( float or int ) \u2013 \u68af\u5ea6\u7684\u6700\u5927\u8303\u6570 norm_type ( float or int ) \u2013 p \u8303\u6570(\u6307\u5b9a p ). \u7528 'inf' \u8868\u793a\u65e0\u7a77\u8303\u6570 | | --- | --- | | Returns: | \u68af\u5ea6\u7684\u8303\u6570 (\u89c6\u4e3a\u5355\u4e2a\u5411\u91cf\u7684). | | --- | --- |","title":"clip_grad_norm"},{"location":"57/#weight_norm","text":"torch.nn.utils.weight_norm(module, name='weight', dim=0) \u5c06\u6743\u91cd\u5f52\u4e00\u5316\u5e94\u7528\u4e8e\u7ed9\u5b9a\u6a21\u5757\u4e2d\u7684\u6307\u5b9a\u53c2\u6570. . \\[\\mathbf{w} = g \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\] \u6743\u91cd\u5f52\u4e00\u5316\u662f\u5c06\u6743\u91cd\u5f20\u91cf\u7684\u5927\u5c0f\u548c\u65b9\u5411\u5206\u79bb\u7684\u518d\u53c2\u6570\u5316. \u8be5\u51fd\u6570\u4f1a\u7528\u4e24\u4e2a\u53c2\u6570\u4ee3\u66ff <cite>name</cite> (e.g. \u201cweight\u201d)\u6240\u6307\u5b9a\u7684\u53c2\u6570. \u5728\u65b0\u7684\u53c2\u6570\u4e2d, \u4e00\u4e2a\u6307\u5b9a\u53c2\u6570\u7684\u5927\u5c0f (e.g. \u201cweight_g\u201d), \u4e00\u4e2a\u6307\u5b9a\u53c2\u6570\u7684\u65b9\u5411. \u6743\u91cd\u5f52\u4e00\u5316\u662f\u901a\u8fc7\u4e00\u4e2a\u94a9\u5b50\u5b9e\u73b0\u7684, \u8be5\u94a9\u5b50\u4f1a\u5728 <cite>~Module.forward</cite> \u7684\u6bcf\u6b21\u8c03\u7528\u4e4b\u524d\u6839\u636e\u5927\u5c0f\u548c\u65b9\u5411(\u4e24\u4e2a\u65b0\u53c2\u6570)\u91cd\u65b0\u8ba1\u7b97\u6743\u91cd\u5f20\u91cf. \u9ed8\u8ba4\u60c5\u51b5\u4e0b, <cite>dim=0</cite>, \u8303\u6570\u4f1a\u5728\u6bcf\u4e00\u4e2a\u8f93\u51fa\u7684 channel/plane \u4e0a\u5206\u522b\u8ba1\u7b97. \u82e5\u8981\u5bf9\u6574\u4e2a\u6743\u91cd\u5f20\u91cf\u8ba1\u7b97\u8303\u6570, \u4f7f\u7528 <cite>dim=None</cite>. \u53c2\u89c1 https://arxiv.org/abs/1602.07868 | Parameters: | module ( nn.Module ) \u2013 \u7ed9\u5b9a\u7684 module name ( str , optional ) \u2013 \u6743\u91cd\u53c2\u6570\u7684 name dim ( int , optional ) \u2013 \u8fdb\u884c\u8303\u6570\u8ba1\u7b97\u7684\u7ef4\u5ea6 | | --- | --- | | Returns: | \u6dfb\u52a0\u4e86\u6743\u91cd\u5f52\u4e00\u5316\u94a9\u5b50\u7684\u539f module | | --- | --- | Example: >>> m = weight_norm(nn.Linear(20, 40), name='weight') Linear (20 -> 40) >>> m.weight_g.size() torch.Size([40, 1]) >>> m.weight_v.size() torch.Size([40, 20])","title":"weight_norm"},{"location":"57/#remove_weight_norm","text":"torch.nn.utils.remove_weight_norm(module, name='weight') \u4ece\u6a21\u5757\u4e2d\u79fb\u9664\u6743\u91cd\u5f52\u4e00\u5316/\u518d\u53c2\u6570\u5316. | Parameters: | module ( nn.Module ) \u2013 \u7ed9\u5b9a\u7684 module name ( str , optional ) \u2013 \u6743\u91cd\u53c2\u6570\u7684 name | | --- | --- | Example >>> m = weight_norm(nn.Linear(20, 40)) >>> remove_weight_norm(m)","title":"remove_weight_norm"},{"location":"57/#packedsequence","text":"torch.nn.utils.rnn.PackedSequence(_cls, data, batch_sizes) \u4fdd\u5b58\u4e00\u4e2a\u6253\u5305\u5e8f\u5217\u7684 data \u548c batch_sizes. \u6240\u6709\u7684 RNN \u6a21\u5757\u90fd\u63a5\u6536\u8fd9\u79cd\u88ab\u5305\u88f9\u540e\u7684\u5e8f\u5217\u4f5c\u4e3a\u5b83\u4eec\u7684\u8f93\u5165. Note \u6c38\u8fdc\u4e0d\u8981\u624b\u52a8\u521b\u5efa\u8fd9\u4e2a\u7c7b\u7684\u5b9e\u4f8b. \u5b83\u4eec\u5e94\u5f53\u88ab pack_padded_sequence() \u8fd9\u6837\u7684\u51fd\u6570\u5b9e\u4f8b\u5316. | Variables: | data ( Variable ) \u2013 \u5305\u542b\u6253\u5305\u540e\u5e8f\u5217\u7684 Variable batch_sizes ( list [ int ] ) \u2013 \u5305\u542b\u6bcf\u4e2a\u5e8f\u5217\u6b65\u7684 batch size \u7684\u5217\u8868 | | --- | --- |","title":"PackedSequence"},{"location":"57/#pack_padded_sequence","text":"torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False) \u5c06\u586b\u5145\u8fc7\u7684\u53d8\u957f\u5e8f\u5217\u6253\u5305(\u538b\u7d27). \u8f93\u5165\u7684\u5f62\u72b6\u53ef\u4ee5\u662f TxBx* . T\u662f\u6700\u957f\u5e8f\u5217\u957f\u5ea6(\u7b49\u4e8e lengths[0] ), B \u662f batch size, * \u4ee3\u8868\u4efb\u610f\u7ef4\u5ea6(\u53ef\u4ee5\u662f0). \u5982\u679c batch_first=True , \u90a3\u4e48\u76f8\u5e94\u7684 input size \u5c31\u662f BxTx* . Variable \u4e2d\u4fdd\u5b58\u7684\u5e8f\u5217, \u5e94\u8be5\u6309\u5e8f\u5217\u957f\u5ea6\u7684\u957f\u77ed\u6392\u5e8f, \u957f\u7684\u5728\u524d, \u77ed\u7684\u5728\u540e. \u5373 input[:,0] \u4ee3\u8868\u7684\u662f\u6700\u957f\u7684\u5e8f\u5217, input[:, B-1] \u4fdd\u5b58\u7684\u662f\u6700\u77ed\u7684\u5e8f\u5217. Note \u53ea\u8981\u662f\u7ef4\u5ea6\u5927\u4e8e\u7b49\u4e8e2\u7684 input \u90fd\u53ef\u4ee5\u4f5c\u4e3a\u8fd9\u4e2a\u51fd\u6570\u7684\u53c2\u6570. \u4f60\u53ef\u4ee5\u7528\u5b83\u6765\u6253\u5305 labels, \u7136\u540e\u7528 RNN \u7684\u8f93\u51fa\u548c\u6253\u5305\u540e\u7684 labels \u6765\u8ba1\u7b97 loss. \u901a\u8fc7 PackedSequence \u5bf9\u8c61\u7684 .data \u5c5e\u6027\u53ef\u4ee5\u83b7\u53d6 Variable. | Parameters: | input ( Variable ) \u2013 \u53d8\u957f\u5e8f\u5217\u88ab\u586b\u5145\u540e\u7684 batch lengths ( list [ int ] ) \u2013 Variable \u4e2d\u6bcf\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6. batch_first ( bool , optional ) \u2013 \u5982\u679c\u662f True , input \u7684\u5f62\u72b6\u5e94\u8be5\u662f BxTx*. | | --- | --- | | Returns: | \u4e00\u4e2a PackedSequence \u5bf9\u8c61. | | --- | --- |","title":"pack_padded_sequence"},{"location":"57/#pad_packed_sequence","text":"torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0) \u586b\u5145\u6253\u5305\u8fc7\u7684\u53d8\u957f\u5e8f\u5217. \u8fd9\u662f pack_padded_sequence() \u7684\u9006\u64cd\u4f5c. \u8fd4\u56de\u7684 Varaible \u7684\u503c\u7684 size \u662f TxBx , T \u662f\u6700\u957f\u5e8f\u5217\u7684\u957f\u5ea6, B \u662f batch_size, \u5982\u679c batch_first=True , \u90a3\u4e48\u8fd4\u56de\u503c\u662f BxTx . Batch\u4e2d\u7684\u5143\u7d20\u5c06\u4f1a\u4ee5\u5b83\u4eec\u957f\u5ea6\u7684\u9006\u5e8f\u6392\u5217. | Parameters: | sequence ( PackedSequence ) \u2013 \u5c06\u8981\u88ab\u586b\u5145\u7684 batch batch_first ( bool , optional ) \u2013 \u5982\u679c\u4e3a <cite>True`</cite> , \u8fd4\u56de\u7684\u6570\u636e\u7684\u683c\u5f0f\u4e3a BxTx*. padding_value ( float , optional ) \u2013 \u7528\u6765\u586b\u5145\u5143\u7d20\u7684\u503c | | --- | --- | | Returns: | \u4e00\u4e2a tuple, \u5305\u542b\u88ab\u586b\u5145\u540e\u7684\u5e8f\u5217, \u548c batch \u4e2d\u5e8f\u5217\u7684\u957f\u5ea6\u5217\u8868. | | --- | --- |","title":"pad_packed_sequence"},{"location":"58/","text":"torch.optim torch.optim is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future. \u5982\u4f55\u4f7f\u7528 optimizer (\u4f18\u5316\u5668) \u4e3a\u4e86\u4f7f\u7528 torch.optim \u4f60\u9700\u8981\u521b\u5efa\u4e00\u4e2a optimizer \u5bf9\u8c61, \u8fd9\u4e2a\u5bf9\u8c61\u80fd\u591f\u4fdd\u6301\u5f53\u524d\u7684\u72b6\u6001\u4ee5\u53ca\u4f9d\u9760\u68af\u5ea6\u8ba1\u7b97 \u6765\u5b8c\u6210\u53c2\u6570\u66f4\u65b0. \u6784\u5efa \u8981\u6784\u5efa\u4e00\u4e2a Optimizer \u4f60\u9700\u8981\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u53c2\u6570 (\u5168\u90e8\u90fd\u5e94\u8be5\u662f Variable ) \u8fdb\u884c\u4f18\u5316. \u7136\u540e, \u4f60\u80fd\u591f\u8bbe\u7f6e\u4f18\u5316\u5668\u7684\u53c2\u6570\u9009\u9879, \u4f8b\u5982\u5b66\u4e60\u7387, \u6743\u91cd\u8870\u51cf\u7b49. Note \u5982\u679c\u4f60\u9700\u8981\u901a\u8fc7 <cite>.cuda()</cite> \u5c06\u6a21\u578b\u79fb\u52a8\u5230 GPU \u4e0a, \u8bf7\u5728\u6784\u5efa\u4f18\u5316\u5668\u4e4b\u524d\u6765\u79fb\u52a8. \u6a21\u578b\u7684\u53c2\u6570\u5728\u8fdb\u884c <cite>.cuda()</cite> \u4e4b\u540e\u5c06\u53d8\u6210\u4e0d\u540c\u7684\u5bf9\u8c61,\u8be5\u5bf9\u8c61\u4e0e\u4e4b\u524d\u8c03\u7528\u7684\u53c2\u6570\u4e0d\u540c. \u901a\u5e38\u6765\u8bf4, \u5728\u5bf9\u4f18\u5316\u5668\u8fdb\u884c\u6784\u5efa\u548c\u8c03\u7528\u7684\u65f6\u5019, \u4f60\u5e94\u8be5\u8981\u786e\u4fdd\u4f18\u5316\u53c2\u6570\u4f4d\u4e8e\u76f8\u540c\u7684 \u5730\u70b9. \u4f8b\u5b50 optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) optimizer = optim.Adam([var1, var2], lr = 0.0001) \u4e3a\u6bcf\u4e2a\u53c2\u6570\u5355\u72ec\u8bbe\u7f6e\u9009\u9879 Optimizer \u4e5f\u652f\u6301\u4e3a\u6bcf\u4e2a\u53c2\u6570\u5355\u72ec\u8bbe\u7f6e\u9009\u9879. \u82e5\u8981\u8fd9\u4e48\u505a, \u4e0d\u8981\u76f4\u63a5\u4f7f\u7528 :class: <cite>~torch.autograd.Variable</cite> \u7684\u8fed\u4ee3, \u800c\u662f\u4f7f\u7528 dict \u7684\u8fed\u4ee3. \u6bcf\u4e00\u4e2a dict \u90fd\u5206\u522b\u5b9a\u4e49\u4e86\u4e00\u7ec4\u53c2\u6570, \u5e76\u4e14\u5e94\u8be5\u8981\u5305\u542b params \u952e,\u8fd9\u4e2a\u952e\u5bf9\u5e94\u5217\u8868\u7684\u53c2\u6570. \u5176\u4ed6\u7684\u952e\u5e94\u8be5\u4e0e optimizer \u6240\u63a5\u53d7\u7684\u5176\u4ed6\u53c2\u6570\u7684\u5173\u952e\u5b57\u76f8\u5339\u914d, \u5e76\u4e14\u4f1a\u88ab\u7528\u4e8e\u5bf9\u8fd9\u7ec4\u53c2\u6570\u7684\u4f18\u5316. Note \u4f60\u4ecd\u7136\u80fd\u591f\u4f20\u9012\u9009\u9879\u4f5c\u4e3a\u5173\u952e\u5b57\u53c2\u6570.\u5728\u672a\u91cd\u5199\u8fd9\u4e9b\u9009\u9879\u7684\u7ec4\u4e2d, \u5b83\u4eec\u4f1a\u88ab\u7528\u4f5c\u9ed8\u8ba4\u503c. \u8fd9\u975e\u5e38\u9002\u7528\u4e8e\u5f53\u4f60\u53ea\u60f3\u6539\u52a8\u4e00\u4e2a\u53c2\u6570\u7ec4\u7684\u9009\u9879, \u4f46\u5176\u4ed6\u53c2\u6570\u7ec4\u7684\u9009\u9879\u4e0d\u53d8\u7684\u60c5\u51b5. \u4f8b\u5982, \u5f53\u6211\u4eec\u60f3\u6307\u5b9a\u6bcf\u4e00\u5c42\u7684\u5b66\u4e60\u7387\u65f6, \u8fd9\u662f\u975e\u5e38\u6709\u7528\u7684: optim.SGD([ {'params': model.base.parameters()}, {'params': model.classifier.parameters(), 'lr': 1e-3} ], lr=1e-2, momentum=0.9) \u8fd9\u610f\u5473\u7740 model.base \u7684\u53c2\u6570\u5c06\u4f1a\u4f7f\u7528 1e-2 \u7684\u5b66\u4e60\u7387, model.classifier \u7684\u53c2\u6570\u5c06\u4f1a\u4f7f\u7528 1e-3 \u7684\u5b66\u4e60\u7387, \u5e76\u4e14 0.9 \u7684 momentum \u5c06\u5e94\u7528\u4e8e\u6240\u6709\u53c2\u6570. \u8fdb\u884c\u5355\u6b65\u4f18\u5316 \u6240\u6709\u7684\u4f18\u5316\u5668\u90fd\u5b9e\u73b0\u4e86 step() \u65b9\u6cd5, \u4e14\u66f4\u65b0\u5230\u6240\u6709\u7684\u53c2\u6570. \u5b83\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u6765\u4f7f\u7528: optimizer.step() \u8fd9\u662f\u5927\u591a\u6570 optimizer \u6240\u652f\u6301\u7684\u7b80\u5316\u7248\u672c. \u4e00\u65e6\u4f7f\u7528 backward() \u4e4b\u7c7b\u7684\u51fd\u6570\u8ba1\u7b97\u51fa\u6765\u68af\u5ea6\u4e4b\u540e\u6211\u4eec\u5c31\u53ef\u4ee5\u8c03\u7528\u8fd9\u4e2a\u51fd\u6570\u4e86. \u4f8b\u5b50 for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() optimizer.step(closure) \u4e00\u4e9b\u4f18\u5316\u7b97\u6cd5\u4f8b\u5982 Conjugate Gradient \u548c LBFGS \u9700\u8981\u91cd\u590d\u591a\u6b21\u8ba1\u7b97\u51fd\u6570, \u56e0\u6b64\u4f60\u9700\u8981\u4f20\u5165\u4e00\u4e2a\u95ed\u5305\u53bb\u5141\u8bb8\u5b83\u4eec\u91cd\u65b0\u8ba1\u7b97\u4f60\u7684\u6a21\u578b. \u8fd9\u4e2a\u95ed\u5305\u5e94\u5f53\u6e05\u7a7a\u68af\u5ea6, \u8ba1\u7b97\u635f\u5931, \u7136\u540e\u8fd4\u56de. \u4f8b\u5b50 for input, target in dataset: def closure(): optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() return loss optimizer.step(closure) \u7b97\u6cd5 class torch.optim.Optimizer(params, defaults) \u4f18\u5316\u5668\u7684\u57fa\u7c7b. Args: * params (iterable): :class: ](#id5)Variable \u6216 :class:[ dict \u7684\u8fed\u4ee3, \u6307\u5b9a\u4e86\u5e94\u8be5\u4f18\u5316\u54ea\u4e9b\u53c2\u6570. * defaults: (dict): \u5305\u542b\u4e86\u4f18\u5316\u9009\u9879\u9ed8\u8ba4\u503c\u7684\u5b57\u5178(\u4e00\u4e2a\u53c2\u6570\u7ec4\u6ca1\u6709\u6307\u5b9a\u7684\u53c2\u6570\u9009\u9879\u5c06\u4f1a\u4f7f\u7528\u9ed8\u8ba4\u503c). add_param_group(param_group) \u589e\u52a0\u4e00\u7ec4\u53c2\u6570\u5230 Optimizer \u7684 <cite>param_groups</cite> \u91cc\u9762. \u5f53\u5fae\u8c03\u4e00\u4e2a\u9884\u8bad\u7ec3\u597d\u7684\u7f51\u7edc\u4f5c\u4e3a\u51bb\u7ed3\u5c42\u65f6\u662f\u6709\u7528\u7684, \u5b83\u80fd\u591f\u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u548c\u53ef\u589e\u52a0\u7684\u53c2\u6570\u5230 Optimizer \u4f5c\u4e3a\u4e00\u4e2a\u8bad\u7ec3\u9884\u5904\u7406. Parameters: param_group ( dict ) \u2013 \u6307\u5b9a\u8fd9\u4e00\u7ec4\u4e2d\u5177\u6709\u7279\u6b8a\u4f18\u5316\u9009\u9879\u7684\u90a3\u4e9b Variables \u80fd\u591f\u88ab\u4f18\u5316. load_state_dict(state_dict) \u52a0\u8f7d\u4f18\u5316\u5668\u72b6\u6001. Parameters: state_dict ( dict ) \u2013 \u4f18\u5316\u5668\u72b6\u6001. \u662f\u8c03\u7528 state_dict() \u65f6\u6240\u8fd4\u56de\u7684\u5bf9\u8c61. state_dict() \u4ee5 dict \u7684\u5f62\u5f0f\u8fd4\u56de\u4f18\u5316\u5668\u7684\u72b6\u6001. \u5b83\u5305\u542b\u4e24\u90e8\u5206\u5185\u5bb9: state - \u4e00\u4e2a\u5305\u542b\u5f53\u524d\u4f18\u5316\u72b6\u6001\u7684\u5b57\u5178\uff08dict\uff09, \u5b57\u5178\u91cc\u7684\u5185\u5bb9\u56e0\u4f18\u5316\u5668\u7684\u4e0d\u540c\u800c\u53d8\u6362. param_groups - \u4e00\u4e2a\u5305\u542b\u6240\u6709\u53c2\u6570\u7ec4\u7684\u5b57\u5178\uff08dict\uff09. step(closure) \u8fdb\u884c\u5355\u6b21\u4f18\u5316(\u53c2\u6570\u66f4\u65b0). Parameters: closure ( callable ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de loss \u7684\u95ed\u5305\u5927\u591a\u6570\u4f18\u5316\u5668\u53ef\u9009\u62e9. zero_grad() Clears the gradients of all optimized Variable s. class torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0) \u5b9e\u65bd Adadelta \u7b97\u6cd5. \u5b83\u5728 ADADELTA: \u4e00\u79cd\u53ef\u8c03\u8282\u5b66\u4e60\u7387\u7684\u65b9\u6cd5 \u4e2d\u63d0\u51fa Args: * params (iterable): \u901a\u8fc7\u53c2\u6570\u8fed\u4ee3\u53bb\u4f18\u5316\u6216\u8005\u5b57\u5178\u7684\u5f62\u5f0f\u5b9a\u4e49\u53c2\u6570\u7ec4. * rho (float, optional): \u7528\u6765\u8ba1\u7b97\u5e73\u5747\u5e73\u65b9\u68af\u5ea6\u7684\u7cfb\u6570(\u9ed8\u8ba4\u503c: 0.9) * eps (float, optional): \u589e\u52a0\u5206\u6bcd\u6765\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027(\u9ed8\u8ba4\u503c: 1e-6) * lr (float, optional): \u5728\u5c06 delta \u5e94\u7528\u4e8e\u53c2\u6570\u4e4b\u524d\u5bf9\u5176\u8fdb\u884c\u7cfb\u6570\u7684\u7f29\u653e(\u9ed8\u8ba4\u503c: 1.0) * weight_decay (float, optional): \u6743\u91cd\u8870\u51cf (L2\u6b63\u5219\u5316) (\u9ed8\u8ba4\u503c: 0) step(closure=None) \u5b9e\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u5e76\u8fd4\u56de\u8bef\u5dee\u635f\u5931\u7684\u95ed\u5305. class torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0) \u5b9e\u73b0 Adagrad \u7b97\u6cd5. \u5b83\u5728 Adaptive Subgradient Methods for Online Learning and Stochastic Optimization \u4e2d\u88ab\u63d0\u51fa. Args: * params (iterable): \u8fed\u4ee3\u7684\u4f18\u5316\u53c2\u6570\u6216\u8005\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b9a\u4e49\u53c2\u6570\u7ec4 * lr (float, optional): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 1e-2) * lr_decay (float, optional): \u5b66\u4e60\u7387\u8870\u51cf (\u9ed8\u8ba4\u503c: 0) * weight_decay (float, optional): \u6743\u91cd\u8870\u51cf (L2\u6b63\u5219\u5316) (\u9ed8\u8ba4\u503c: 0) step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de\u8bef\u5dee\u7684\u95ed\u5305. class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) \u5b9e\u73b0 Adam \u7b97\u6cd5. \u5b83\u5728 Adam: A Method for Stochastic Optimization \u4e2d\u88ab\u63d0\u51fa. Args: * params (iterable): \u8fed\u4ee3\u7684\u4f18\u5316\u53c2\u6570\u6216\u8005\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b9a\u4e49\u53c2\u6570\u7ec4. * lr (float, optional): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 1e-3) * betas (Tuple[float, float], optional): \u7528\u6765\u8ba1\u7b97\u68af\u5ea6\u548c\u5e73\u65b9\u68af\u5ea6\u7684\u7cfb\u6570 (\u9ed8\u8ba4\u503c: (0.9, 0.999)) * eps (float, optional): \u589e\u52a0\u5206\u6bcd\u6765\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027 (\u9ed8\u8ba4\u503c: 1e-8) * weight_decay (float, optional): \u6743\u91cd\u8870\u51cf (L2 \u6b63\u5219\u5316) (\u9ed8\u8ba4\u503c: 0) step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de\u8bef\u5dee\u7684\u95ed\u5305. class torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08) \u5b9e\u73b0\u4e0a\u4e00\u7248\u672c Adam \u7b97\u6cd5\u6765\u9002\u7528\u4e8e sparse tensors. \u5728\u8fd9\u4e2a\u53d8\u5316\u4e0b,\u53ea\u5c06\u663e\u793a\u51fa\u6765\u7684\u68af\u5ea6\u8fdb\u884c\u66f4\u65b0\u5b58\u50a8\u5e76\u4e14\u53ea\u5c06\u8fd9\u90e8\u5206\u68af\u5ea6\u5e94\u7528\u5230\u53c2\u6570\u4e2d. Args: * params (iterable): \u5f85\u4f18\u5316\u7684\u8fed\u4ee3\u53c2\u6570\u6216\u8005\u662f\u5b9a\u4e49\u4e86\u53c2\u6570\u7ec4\u7684 dict * lr (float, optional): \u5b66\u4e60\u7387 (default: 1e-3) * betas (Tuple[float, float], optional): \u7528\u6765\u8ba1\u7b97\u68af\u5ea6\u548c\u5e73\u65b9\u68af\u5ea6\u7684\u7cfb\u6570 (\u9ed8\u8ba4\u503c: (0.9, 0.999)) * eps (float, optional): \u589e\u52a0\u5206\u6bcd\u6765\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027 (\u9ed8\u8ba4\u503c: 1e-8) step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de loss \u7684\u95ed\u5305, \u5bf9\u4e8e\u5927\u591a\u6570\u53c2\u6570\u6765\u8bf4\u662f\u53ef\u9009\u7684. class torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) \u5b9e\u73b0 Adamax \u7b97\u6cd5 ( Adam \u7684\u4e00\u79cd\u57fa\u4e8e\u65e0\u7a77\u8303\u6570\u7684\u53d8\u79cd). \u5b83\u5728 Adam: A Method for Stochastic Optimization \u4e2d\u88ab\u63d0\u51fa. Args: * params (iterable): \u8fed\u4ee3\u7684\u4f18\u5316\u53c2\u6570\u6216\u8005\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b9a\u4e49\u53c2\u6570\u7ec4. * lr (float, optional): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 2e-3) * betas (Tuple[float, float], optional): \u7528\u6765\u8ba1\u7b97\u68af\u5ea6\u548c\u5e73\u65b9\u68af\u5ea6\u7684\u7cfb\u6570 * eps (float, optional): \u589e\u52a0\u5206\u6bcd\u6765\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027 (\u9ed8\u8ba4\u503c: 1e-8) * weight_decay (float, optional): \u6743\u91cd\u8870\u51cf (L2 \u6b63\u5219\u5316) (\u9ed8\u8ba4\u503c: 0) step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de\u8bef\u5dee\u7684\u95ed\u5305. class torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0) \u5b9e\u73b0\u5e73\u5747\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5. \u5b83\u5728 Acceleration of stochastic approximation by averaging \u4e2d\u88ab\u63d0\u51fa Args: * params (iterable): \u8fed\u4ee3\u7684\u4f18\u5316\u53c2\u6570\u6216\u8005\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b9a\u4e49\u53c2\u6570\u7ec4 * lr (float, optional): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 1e-2) * lambd (float, optional): \u8870\u51cf\u671f (\u9ed8\u8ba4\u503c: 1e-4) * alpha (float, optional): eta \u66f4\u65b0\u7684\u6743\u91cd (\u9ed8\u8ba4\u503c: 0.75) * t0 (float, optional): \u6307\u660e\u5728\u54ea\u4e00\u6b21\u5f00\u59cb\u5e73\u5747\u5316 (\u9ed8\u8ba4\u503c: 1e6) * weight_decay (float, optional): \u6743\u91cd\u8870\u51cf (L2 \u6b63\u5219\u5316) (\u9ed8\u8ba4\u503c: 0) step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de\u8bef\u5dee\u7684\u95ed\u5305. class torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None) \u5b9e\u73b0 L-BFGS \u7b97\u6cd5. Warning \u8fd9\u4e2a optimizer \u4e0d\u652f\u6301\u4e3a\u6bcf\u4e2a\u53c2\u6570\u5355\u72ec\u8bbe\u7f6e\u9009\u9879\u4ee5\u53ca\u4e0d\u652f\u6301\u53c2\u6570\u7ec4\uff08\u53ea\u80fd\u6709\u4e00\u4e2a\uff09. Warning \u76ee\u524d\u6240\u6709\u7684\u53c2\u6570\u4e0d\u5f97\u4e0d\u90fd\u5728\u540c\u4e00\u8bbe\u5907\u4e0a. \u8fd9\u5728\u5c06\u6765\u4f1a\u5f97\u5230\u6539\u8fdb. Note \u8fd9\u662f\u4e00\u4e2a\u5185\u5b58\u9ad8\u5ea6\u5bc6\u96c6\u7684 optimizer (\u5b83\u8981\u6c42\u989d\u5916\u7684 param_bytes * (history_size + 1) \u4e2a\u5b57\u8282). \u5982\u679c\u5b83\u4e0d\u9002\u5e94\u5185\u5b58, \u5c1d\u8bd5\u51cf\u5c0f\u5386\u53f2\u89c4\u683c, \u6216\u8005\u4f7f\u7528\u4e0d\u540c\u7684\u7b97\u6cd5. Args: * lr (float): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 1) * max_iter (int): \u6bcf\u4e00\u6b65\u4f18\u5316\u7684\u6700\u5927\u8fed\u4ee3\u6b21\u6570 (\u9ed8\u8ba4\u503c: 20) * max_eval (int): \u6bcf\u4e00\u6b65\u4f18\u5316\u7684\u6700\u5927\u51fd\u6570\u8bc4\u4f30\u6b21\u6570 (\u9ed8\u8ba4\u503c: max_iter * 1.25). * tolerance_grad (float): \u4e00\u9636\u6700\u4f18\u7684\u7ec8\u6b62\u5bb9\u5fcd\u5ea6 (\u9ed8\u8ba4\u503c: 1e-5). * tolerance_change (float): \u5728\u51fd\u6570\u503c/\u53c2\u6570\u53d8\u5316\u91cf\u4e0a\u7684\u7ec8\u6b62\u5bb9\u5fcd\u5ea6 (\u9ed8\u8ba4\u503c: 1e-9). * history_size (int): \u66f4\u65b0\u5386\u53f2\u5c3a\u5bf8 (\u9ed8\u8ba4\u503c: 100). step(closure) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de loss \u7684\u95ed\u5305, \u5bf9\u4e8e\u5927\u591a\u6570\u53c2\u6570\u6765\u8bf4\u662f\u53ef\u9009\u7684. class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False) \u5b9e\u73b0 RMSprop \u7b97\u6cd5. \u7531 G. Hinton \u5728\u6b64\u63d0\u51fa course . \u4e2d\u5fc3\u7248\u672c\u9996\u6b21\u51fa\u73b0\u5728 Generating Sequences With Recurrent Neural Networks . \u7b97\u6cd5: * params (iterable): \u5f85\u4f18\u5316\u7684\u8fed\u4ee3\u53c2\u6570\u6216\u8005\u662f\u5b9a\u4e49\u4e86\u53c2\u6570\u7ec4\u7684 dict * lr (float, optional): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 1e-2) * momentum (float, optional): \u52a8\u91cf\u56e0\u5b50 (\u9ed8\u8ba4\u503c: 0) * alpha (float, optional): \u5e73\u6ed1\u5e38\u91cf (default: 0.99) * eps (float, optional): \u4e3a\u4e86\u589e\u52a0\u6570\u503c\u8ba1\u7b97\u7684\u7a33\u5b9a\u6027\u800c\u52a0\u5230\u5206\u6bcd\u91cc\u7684\u9879 (\u9ed8\u8ba4\u503c: 1e-8) * centered (bool, optional) : \u5982\u679c\u4e3a True , \u8ba1\u7b97 RMSProp \u7684\u4e2d\u503c, \u5e76\u4e14\u7528\u5b83\u7684\u65b9\u5dee\u9884\u6d4b\u503c\u5bf9\u68af\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316 * weight_decay (float, optional): weight decay (L2 penalty) (default: 0) step(closure=None) Performs a single optimization step. Parameters: closure ( callable , optional ) \u2013 A closure that reevaluates the model and returns the loss. class torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50)) \u5b9e\u73b0\u5f39\u6027\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5. Args: * params (iterable): \u5f85\u4f18\u5316\u7684\u8fed\u4ee3\u53c2\u6570\u6216\u8005\u662f\u5b9a\u4e49\u4e86\u53c2\u6570\u7ec4\u7684 dict * lr (float, optional): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 1e-2) * etas (Tuple[float, float], optional): \u4e00\u5bf9 (etaminus, etaplis), t\u5b83\u4eec\u5206\u522b\u662f\u4e58\u6cd5 \u7684\u589e\u52a0\u548c\u51cf\u5c0f\u7684\u56e0\u5b50 (\u9ed8\u8ba4\u503c: (0.5, 1.2)) step_sizes (Tuple[float, float], optional): \u5141\u8bb8\u7684\u4e00\u5bf9\u6700\u5c0f\u548c\u6700\u5927\u7684\u6b65\u957f (\u9ed8\u8ba4\u503c: (1e-6, 50)) step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de loss \u7684\u95ed\u5305, \u5bf9\u4e8e\u5927\u591a\u6570\u53c2\u6570\u6765\u8bf4\u662f\u53ef\u9009\u7684. class torch.optim.SGD(params, lr=<object object>, momentum=0, dampening=0, weight_decay=0, nesterov=False) \u5b9e\u73b0\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff08 momentum \u53ef\u9009\uff09. Nesterov \u52a8\u91cf\u57fa\u4e8e On the importance of initialization and momentum in deep learning \u4e2d\u7684\u516c\u5f0f. Args: * params (iterable): \u5f85\u4f18\u5316\u7684\u8fed\u4ee3\u53c2\u6570\u6216\u8005\u662f\u5b9a\u4e49\u4e86\u53c2\u6570\u7ec4\u7684 dict * lr (float): \u5b66\u4e60\u7387 * momentum (float, optional): \u52a8\u91cf\u56e0\u5b50 (\u9ed8\u8ba4\u503c: 0) * weight_decay (float, optional): \u6743\u91cd\u8870\u51cf (L2 \u6b63\u5219\u5316) (\u9ed8\u8ba4\u503c: 0) * dampening (float, optional): \u52a8\u91cf\u7684\u6291\u5236\u56e0\u5b50 (\u9ed8\u8ba4\u503c: 0) * nesterov (bool, optional): \u4f7f\u7528 Nesterov \u52a8\u91cf (\u9ed8\u8ba4\u503c: False) Example >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) >>> optimizer.zero_grad() >>> loss_fn(model(input), target).backward() >>> optimizer.step() Note \u5e26\u6709\u52a8\u91cf /Nesterov \u7684 SGD \u7684\u5b9e\u73b0\u7a0d\u5fae\u4e0d\u540c\u4e8e Sutskever \u7b49\u4eba\u4ee5\u53ca\u5176\u4ed6\u6846\u67b6\u4e2d\u7684\u5b9e\u73b0. \u8003\u8651\u52a8\u91cf\u7684\u5177\u4f53\u60c5\u51b5, \u66f4\u65b0\u53ef\u4ee5\u5199\u6210 \\[\\begin{split}v = \\rho * v + g \\\\ p = p - lr * v\\end{split}\\] \u5176\u4e2d p, g, v \u548c \\(\\rho\\) \u5206\u522b\u662f\u53c2\u6570\u3001\u68af\u5ea6\u3001\u901f\u5ea6\u548c\u52a8\u91cf. \u8fd9\u8ddf Sutskever \u7b49\u4eba\u4ee5\u53ca\u5176\u4ed6\u6846\u67b6\u7684\u5b9e\u73b0\u662f\u76f8\u53cd\u7684, \u5b83\u4eec\u91c7\u7528\u8fd9\u6837\u7684\u66f4\u65b0. \\[\\begin{split}v = \\rho * v + lr * g \\\\ p = p - v\\end{split}\\] Nesterov \u7684\u7248\u672c\u4e5f\u76f8\u5e94\u7684\u88ab\u4fee\u6539\u4e86. step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de loss \u7684\u95ed\u5305, \u5bf9\u4e8e\u5927\u591a\u6570\u53c2\u6570\u6765\u8bf4\u662f\u53ef\u9009\u7684. \u5982\u4f55\u8c03\u6574\u5b66\u4e60\u7387 mod: <cite>torch.optim.lr_scheduler</cite> \u57fa\u4e8e\u5faa\u73af\u7684\u6b21\u6570\u63d0\u4f9b\u4e86\u4e00\u4e9b\u65b9\u6cd5\u6765\u8c03\u8282\u5b66\u4e60\u7387. class: <cite>torch.optim.lr_scheduler.ReduceLROnPlateau</cite> \u57fa\u4e8e\u9a8c\u8bc1\u6d4b\u91cf\u7ed3\u679c\u6765\u8bbe\u7f6e\u4e0d\u540c\u7684\u5b66\u4e60\u7387. --- --- class torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1) \u5c06\u6bcf\u4e2a\u53c2\u6570\u7ec4\u7684\u5b66\u4e60\u901f\u7387\u8bbe\u7f6e\u4e3a\u7ed9\u5b9a\u51fd\u6570\u7684\u521d\u59cbLR. \u5f53 last_epoch=-1, \u8bbe\u7f6e\u51fa\u4e8b\u7684 lr \u4f5c\u4e3a lr. Args: * optimizer (Optimizer): \u5c01\u88c5\u597d\u7684\u4f18\u5316\u5668. * lr_lambda (function or list): \u8ba1\u7b97\u7ed9\u5b9a\u6574\u6570\u53c2\u6570\u5386\u5143\u7684\u4e58\u6cd5\u56e0\u5b50\u7684\u51fd\u6570, \u6216\u8005\u4e00\u7cfb\u5217\u7684\u6b64\u7c7b\u51fd\u6570, \u6bcf\u7ec4\u7684\u4e00\u4e2a\u90fd\u5728 optimizer.param_groups \u4e2d. * last_epoch (int): \u6700\u540e\u4e00\u4e2a epoch \u7684\u7d22\u5f15. \u9ed8\u8ba4\u503c: -1. Example >>> # Assuming optimizer has two groups. >>> lambda1 = lambda epoch: epoch // 30 >>> lambda2 = lambda epoch: 0.95 ** epoch >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2]) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1) \u901a\u8fc7 gamma \u5728\u6bcf\u4e00\u4e2a epoch \u91cc\u9762\u7684 step_size \u8bbe\u7f6e\u6bcf\u4e2a\u53c2\u6570\u7ec4\u7684\u521d\u59cb\u5b66\u4e60\u7387\u8870\u51cf\u53d8\u91cf. \u5f53 last_epoch=-1, \u8bbe\u7f6e\u521d\u59cb lr \u4e3a lr. Args: * optimizer (Optimizer): \u5c01\u88c5\u597d\u7684\u4f18\u5316\u5668. * step_size (int): \u5b66\u4e60\u7387\u8870\u51cf\u5468\u671f. * gamma (float): \u5b66\u4e60\u7387\u8870\u51cf\u7684\u4e58\u6cd5\u56e0\u5b50. \u9ed8\u8ba4\u503c: 0.1. * last_epoch (int): \u6700\u540e\u4e00\u4e2a epoch \u7684\u7d22\u5f15. \u9ed8\u8ba4\u503c: -1. Example >>> # Assuming optimizer uses lr = 0.5 for all groups >>> # lr = 0.05 if epoch < 30 >>> # lr = 0.005 if 30 <= epoch < 60 >>> # lr = 0.0005 if 60 <= epoch < 90 >>> # ... >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) class torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1) \u4e00\u65e6 epoch \u7684\u6570\u91cf\u8fbe\u5230\u4e86\u4e00\u4e2a\u4e34\u754c\u70b9\u901a\u8fc7 gamma \u5728\u6bcf\u4e00\u4e2a epoch \u91cc\u9762\u7684 step_size \u8bbe\u7f6e\u6bcf\u4e2a\u53c2\u6570 \u7ec4\u7684\u521d\u59cb\u5b66\u4e60\u7387\u8870\u51cf\u53d8\u91cf.\u5f53 last_epoch=-1, \u8bbe\u7f6e\u521d\u59cb lr \u4f5c\u4e3a lr. Args: * optimizer (Optimizer): \u5c01\u88c5\u597d\u7684\u4f18\u5316\u5668. * milestones (list): epoch \u7d22\u5f15\u5217\u8868. \u5fc5\u987b\u4e3a\u9012\u589e\u7684. * gamma (float): \u5b66\u4e60\u7387\u8870\u51cf\u7684\u4e58\u6cd5\u56e0\u5b50. \u9ed8\u8ba4\u503c: 0.1. last_epoch (int): \u6700\u540e\u4e00\u4e2a epoch \u7684\u7d22\u5f15. \u9ed8\u8ba4\u503c: -1. Example >>> # Assuming optimizer uses lr = 0.5 for all groups >>> # lr = 0.05 if epoch < 30 >>> # lr = 0.005 if 30 <= epoch < 80 >>> # lr = 0.0005 if epoch >= 80 >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) class torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1) \u901a\u8fc7 gamma \u5728\u6bcf\u4e00\u4e2a epoch \u91cc\u9762\u7684 step_size \u8bbe\u7f6e\u6bcf\u4e2a\u53c2\u6570\u7ec4\u7684\u521d\u59cb\u5b66\u4e60\u7387\u8870\u51cf\u53d8\u91cf . \u5f53 last_epoch=-1, \u8bbe\u7f6e\u521d\u59cb lr \u4f5c\u4e3a lr. :param optimizer: \u5c01\u88c5\u597d\u7684\u4f18\u5316\u5668. :type optimizer: Optimizer :param gamma: \u5b66\u4e60\u7387\u8870\u51cf\u7684\u4e58\u6cd5\u56e0\u5b50. :type gamma: float :param last_epoch: \u6700\u540e\u4e00\u4e2a epoch \u7684\u7d22\u5f15. \u9ed8\u8ba4\u503c: -1. :type last_epoch: int class torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08) \u5f53\u4e00\u4e2a\u6307\u6807\u5df2\u7ecf\u505c\u6b62\u63d0\u5347\u65f6\u51cf\u5c11\u5b66\u4e60\u7387.\u6a21\u578b\u901a\u5e38\u53d7\u76ca\u4e8e\u901a\u8fc7\u4e00\u6b212-10\u7684\u5b66\u4e60\u505c\u6b62\u56e0\u7d20\u51cf\u5c11\u5b66\u4e60\u7387 \u8fd9\u4e2a\u8c03\u5ea6\u7a0b\u5e8f\u8bfb\u53d6\u4e00\u4e2a\u6307\u6807\u8d28\u91cf \u4ee5\u53ca\u770b\u5230 \u2018patience\u2019 \u7684\u6570\u91cf\u5728\u4e00\u4e2a epoch \u91cc\u9762\u5982\u679c\u6ca1\u6709\u63d0\u5347, \u8fd9\u65f6\u5b66\u4e60\u7387\u5df2\u7ecf\u51cf\u5c0f. Args: * optimizer (Optimizer): \u5c01\u88c5\u597d\u7684\u4f18\u5316\u5668. mode (str): <cite>min</cite>, <cite>max</cite> \u5176\u4e2d\u4e00\u4e2a. \u5728 <cite>min</cite> \u6a21\u5757\u4e0b,\u5f53\u8d28\u91cf\u76d1\u6d4b\u5df2\u7ecf \u505c\u6b62\u4e0b\u964d\u65f6 lr \u5c06\u88ab\u51cf\u5c11; \u5728 <cite>max</cite> \u6a21\u5757\u4e0b \u5f53\u8d28\u91cf\u76d1\u6d4b\u5df2\u7ecf\u505c\u6b62\u4e0a\u5347\u65f6 lr \u5c06 \u88ab\u51cf\u5c11. \u9ed8\u8ba4\u503c: \u2018min\u2019. factor (float): \u54ea\u4e2a\u5b66\u4e60\u7387\u5c06\u4f1a\u88ab\u51cf\u5c11\u7684\u5f71\u54cd\u56e0\u5b50 . new_lr = lr * factor. \u9ed8\u8ba4\u503c: 0.1. patience (int): epoch \u4e2d\u6ca1\u6709\u6539\u5584\u7684\u6b21\u6570, \u5b66\u4e60\u7387\u5c06\u4f1a\u964d\u4f4e. . \u9ed8\u8ba4\u503c: 10. verbose (bool): \u82e5\u4e3a True , \u6bcf\u6b21\u66f4\u65b0\u6253\u5370\u4fe1\u606f\u5230\u63a7\u5236\u53f0\u8f93\u51fa. \u9ed8\u8ba4\u503c: False . threshold (float): \u6d4b\u91cf\u65b0\u7684\u6700\u4f73\u9608\u503c, \u53ea\u5173\u6ce8\u6709\u91cd\u5927\u610f\u4e49\u7684\u6539\u53d8. \u9ed8\u8ba4\u503c: 1e-4. threshold_mode (str): <cite>rel</cite>, <cite>abs</cite> \u4e2d\u7684\u4e00\u4e2a. \u5728 <cite>rel</cite> \u6a21\u5f0f\u4e0b, dynamic_threshold = best * ( 1 + threshold ) \u5728 \u2018max\u2019 \u6a21\u5f0f\u4e0b\u6216\u8005\u5728 <cite>min</cite> \u6a21\u5f0f\u4e0b best * ( 1 - threshold ) . \u5728 <cite>abs</cite> \u6a21\u5f0f\u4e0b, dynamic_threshold = best + threshold \u5728 <cite>max</cite> \u6a21\u5f0f\u4e0b\u6216\u8005\u5728 <cite>min</cite> \u6a21\u5f0f\u4e0b best - threshold . \u9ed8\u8ba4\u503c: \u2018rel\u2019. cooldown (int): lr \u5df2\u7ecf\u51cf\u5c11\u4e4b\u540e\u53bb\u7b49\u5f85\u6700\u4f73\u7684\u6b63\u5e38\u64cd\u4f5c\u4e4b\u524d\u7684 epoch \u6570\u76ee. \u9ed8\u8ba4\u503c: 0. min_lr (float or list): \u4e00\u4e2a\u5217\u8868\u7684\u6807\u91cf.\u6240\u6709\u53c2\u6570\u7ec4\u6216\u6bcf\u4e2a\u7ec4\u7684\u5b66\u4e60\u7387\u4e0b\u9650. \u9ed8\u8ba4\u503c: 0. eps (float): lr \u6700\u5c0f\u7684\u8870\u51cf\u503c\u9002\u5e94\u4e8e. \u5982\u679c\u65b0 lr \u548c\u65e7 lr \u4e4b\u95f4\u7684\u5dee\u5f02\u5c0f\u4e8e eps,\u66f4\u65b0\u53ef\u4ee5\u5ffd\u7565. \u9ed8\u8ba4\u503c: 1e-8. Example >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) >>> scheduler = ReduceLROnPlateau(optimizer, 'min') >>> for epoch in range(10): >>> train(...) >>> val_loss = validate(...) >>> # Note that step should be called after validate() >>> scheduler.step(val_loss)","title":"torch.optim"},{"location":"58/#torchoptim","text":"torch.optim is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.","title":"torch.optim"},{"location":"58/#optimizer","text":"\u4e3a\u4e86\u4f7f\u7528 torch.optim \u4f60\u9700\u8981\u521b\u5efa\u4e00\u4e2a optimizer \u5bf9\u8c61, \u8fd9\u4e2a\u5bf9\u8c61\u80fd\u591f\u4fdd\u6301\u5f53\u524d\u7684\u72b6\u6001\u4ee5\u53ca\u4f9d\u9760\u68af\u5ea6\u8ba1\u7b97 \u6765\u5b8c\u6210\u53c2\u6570\u66f4\u65b0.","title":"\u5982\u4f55\u4f7f\u7528 optimizer (\u4f18\u5316\u5668)"},{"location":"58/#_1","text":"\u8981\u6784\u5efa\u4e00\u4e2a Optimizer \u4f60\u9700\u8981\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u53c2\u6570 (\u5168\u90e8\u90fd\u5e94\u8be5\u662f Variable ) \u8fdb\u884c\u4f18\u5316. \u7136\u540e, \u4f60\u80fd\u591f\u8bbe\u7f6e\u4f18\u5316\u5668\u7684\u53c2\u6570\u9009\u9879, \u4f8b\u5982\u5b66\u4e60\u7387, \u6743\u91cd\u8870\u51cf\u7b49. Note \u5982\u679c\u4f60\u9700\u8981\u901a\u8fc7 <cite>.cuda()</cite> \u5c06\u6a21\u578b\u79fb\u52a8\u5230 GPU \u4e0a, \u8bf7\u5728\u6784\u5efa\u4f18\u5316\u5668\u4e4b\u524d\u6765\u79fb\u52a8. \u6a21\u578b\u7684\u53c2\u6570\u5728\u8fdb\u884c <cite>.cuda()</cite> \u4e4b\u540e\u5c06\u53d8\u6210\u4e0d\u540c\u7684\u5bf9\u8c61,\u8be5\u5bf9\u8c61\u4e0e\u4e4b\u524d\u8c03\u7528\u7684\u53c2\u6570\u4e0d\u540c. \u901a\u5e38\u6765\u8bf4, \u5728\u5bf9\u4f18\u5316\u5668\u8fdb\u884c\u6784\u5efa\u548c\u8c03\u7528\u7684\u65f6\u5019, \u4f60\u5e94\u8be5\u8981\u786e\u4fdd\u4f18\u5316\u53c2\u6570\u4f4d\u4e8e\u76f8\u540c\u7684 \u5730\u70b9. \u4f8b\u5b50 optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) optimizer = optim.Adam([var1, var2], lr = 0.0001)","title":"\u6784\u5efa"},{"location":"58/#_2","text":"Optimizer \u4e5f\u652f\u6301\u4e3a\u6bcf\u4e2a\u53c2\u6570\u5355\u72ec\u8bbe\u7f6e\u9009\u9879. \u82e5\u8981\u8fd9\u4e48\u505a, \u4e0d\u8981\u76f4\u63a5\u4f7f\u7528 :class: <cite>~torch.autograd.Variable</cite> \u7684\u8fed\u4ee3, \u800c\u662f\u4f7f\u7528 dict \u7684\u8fed\u4ee3. \u6bcf\u4e00\u4e2a dict \u90fd\u5206\u522b\u5b9a\u4e49\u4e86\u4e00\u7ec4\u53c2\u6570, \u5e76\u4e14\u5e94\u8be5\u8981\u5305\u542b params \u952e,\u8fd9\u4e2a\u952e\u5bf9\u5e94\u5217\u8868\u7684\u53c2\u6570. \u5176\u4ed6\u7684\u952e\u5e94\u8be5\u4e0e optimizer \u6240\u63a5\u53d7\u7684\u5176\u4ed6\u53c2\u6570\u7684\u5173\u952e\u5b57\u76f8\u5339\u914d, \u5e76\u4e14\u4f1a\u88ab\u7528\u4e8e\u5bf9\u8fd9\u7ec4\u53c2\u6570\u7684\u4f18\u5316. Note \u4f60\u4ecd\u7136\u80fd\u591f\u4f20\u9012\u9009\u9879\u4f5c\u4e3a\u5173\u952e\u5b57\u53c2\u6570.\u5728\u672a\u91cd\u5199\u8fd9\u4e9b\u9009\u9879\u7684\u7ec4\u4e2d, \u5b83\u4eec\u4f1a\u88ab\u7528\u4f5c\u9ed8\u8ba4\u503c. \u8fd9\u975e\u5e38\u9002\u7528\u4e8e\u5f53\u4f60\u53ea\u60f3\u6539\u52a8\u4e00\u4e2a\u53c2\u6570\u7ec4\u7684\u9009\u9879, \u4f46\u5176\u4ed6\u53c2\u6570\u7ec4\u7684\u9009\u9879\u4e0d\u53d8\u7684\u60c5\u51b5. \u4f8b\u5982, \u5f53\u6211\u4eec\u60f3\u6307\u5b9a\u6bcf\u4e00\u5c42\u7684\u5b66\u4e60\u7387\u65f6, \u8fd9\u662f\u975e\u5e38\u6709\u7528\u7684: optim.SGD([ {'params': model.base.parameters()}, {'params': model.classifier.parameters(), 'lr': 1e-3} ], lr=1e-2, momentum=0.9) \u8fd9\u610f\u5473\u7740 model.base \u7684\u53c2\u6570\u5c06\u4f1a\u4f7f\u7528 1e-2 \u7684\u5b66\u4e60\u7387, model.classifier \u7684\u53c2\u6570\u5c06\u4f1a\u4f7f\u7528 1e-3 \u7684\u5b66\u4e60\u7387, \u5e76\u4e14 0.9 \u7684 momentum \u5c06\u5e94\u7528\u4e8e\u6240\u6709\u53c2\u6570.","title":"\u4e3a\u6bcf\u4e2a\u53c2\u6570\u5355\u72ec\u8bbe\u7f6e\u9009\u9879"},{"location":"58/#_3","text":"\u6240\u6709\u7684\u4f18\u5316\u5668\u90fd\u5b9e\u73b0\u4e86 step() \u65b9\u6cd5, \u4e14\u66f4\u65b0\u5230\u6240\u6709\u7684\u53c2\u6570. \u5b83\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u6765\u4f7f\u7528:","title":"\u8fdb\u884c\u5355\u6b65\u4f18\u5316"},{"location":"58/#optimizerstep","text":"\u8fd9\u662f\u5927\u591a\u6570 optimizer \u6240\u652f\u6301\u7684\u7b80\u5316\u7248\u672c. \u4e00\u65e6\u4f7f\u7528 backward() \u4e4b\u7c7b\u7684\u51fd\u6570\u8ba1\u7b97\u51fa\u6765\u68af\u5ea6\u4e4b\u540e\u6211\u4eec\u5c31\u53ef\u4ee5\u8c03\u7528\u8fd9\u4e2a\u51fd\u6570\u4e86. \u4f8b\u5b50 for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step()","title":"optimizer.step()"},{"location":"58/#optimizerstepclosure","text":"\u4e00\u4e9b\u4f18\u5316\u7b97\u6cd5\u4f8b\u5982 Conjugate Gradient \u548c LBFGS \u9700\u8981\u91cd\u590d\u591a\u6b21\u8ba1\u7b97\u51fd\u6570, \u56e0\u6b64\u4f60\u9700\u8981\u4f20\u5165\u4e00\u4e2a\u95ed\u5305\u53bb\u5141\u8bb8\u5b83\u4eec\u91cd\u65b0\u8ba1\u7b97\u4f60\u7684\u6a21\u578b. \u8fd9\u4e2a\u95ed\u5305\u5e94\u5f53\u6e05\u7a7a\u68af\u5ea6, \u8ba1\u7b97\u635f\u5931, \u7136\u540e\u8fd4\u56de. \u4f8b\u5b50 for input, target in dataset: def closure(): optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() return loss optimizer.step(closure)","title":"optimizer.step(closure)"},{"location":"58/#_4","text":"class torch.optim.Optimizer(params, defaults) \u4f18\u5316\u5668\u7684\u57fa\u7c7b. Args: * params (iterable): :class: ](#id5)Variable \u6216 :class:[ dict \u7684\u8fed\u4ee3, \u6307\u5b9a\u4e86\u5e94\u8be5\u4f18\u5316\u54ea\u4e9b\u53c2\u6570. * defaults: (dict): \u5305\u542b\u4e86\u4f18\u5316\u9009\u9879\u9ed8\u8ba4\u503c\u7684\u5b57\u5178(\u4e00\u4e2a\u53c2\u6570\u7ec4\u6ca1\u6709\u6307\u5b9a\u7684\u53c2\u6570\u9009\u9879\u5c06\u4f1a\u4f7f\u7528\u9ed8\u8ba4\u503c). add_param_group(param_group) \u589e\u52a0\u4e00\u7ec4\u53c2\u6570\u5230 Optimizer \u7684 <cite>param_groups</cite> \u91cc\u9762. \u5f53\u5fae\u8c03\u4e00\u4e2a\u9884\u8bad\u7ec3\u597d\u7684\u7f51\u7edc\u4f5c\u4e3a\u51bb\u7ed3\u5c42\u65f6\u662f\u6709\u7528\u7684, \u5b83\u80fd\u591f\u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u548c\u53ef\u589e\u52a0\u7684\u53c2\u6570\u5230 Optimizer \u4f5c\u4e3a\u4e00\u4e2a\u8bad\u7ec3\u9884\u5904\u7406. Parameters: param_group ( dict ) \u2013 \u6307\u5b9a\u8fd9\u4e00\u7ec4\u4e2d\u5177\u6709\u7279\u6b8a\u4f18\u5316\u9009\u9879\u7684\u90a3\u4e9b Variables \u80fd\u591f\u88ab\u4f18\u5316. load_state_dict(state_dict) \u52a0\u8f7d\u4f18\u5316\u5668\u72b6\u6001. Parameters: state_dict ( dict ) \u2013 \u4f18\u5316\u5668\u72b6\u6001. \u662f\u8c03\u7528 state_dict() \u65f6\u6240\u8fd4\u56de\u7684\u5bf9\u8c61. state_dict() \u4ee5 dict \u7684\u5f62\u5f0f\u8fd4\u56de\u4f18\u5316\u5668\u7684\u72b6\u6001. \u5b83\u5305\u542b\u4e24\u90e8\u5206\u5185\u5bb9: state - \u4e00\u4e2a\u5305\u542b\u5f53\u524d\u4f18\u5316\u72b6\u6001\u7684\u5b57\u5178\uff08dict\uff09, \u5b57\u5178\u91cc\u7684\u5185\u5bb9\u56e0\u4f18\u5316\u5668\u7684\u4e0d\u540c\u800c\u53d8\u6362. param_groups - \u4e00\u4e2a\u5305\u542b\u6240\u6709\u53c2\u6570\u7ec4\u7684\u5b57\u5178\uff08dict\uff09. step(closure) \u8fdb\u884c\u5355\u6b21\u4f18\u5316(\u53c2\u6570\u66f4\u65b0). Parameters: closure ( callable ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de loss \u7684\u95ed\u5305\u5927\u591a\u6570\u4f18\u5316\u5668\u53ef\u9009\u62e9. zero_grad() Clears the gradients of all optimized Variable s. class torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0) \u5b9e\u65bd Adadelta \u7b97\u6cd5. \u5b83\u5728 ADADELTA: \u4e00\u79cd\u53ef\u8c03\u8282\u5b66\u4e60\u7387\u7684\u65b9\u6cd5 \u4e2d\u63d0\u51fa Args: * params (iterable): \u901a\u8fc7\u53c2\u6570\u8fed\u4ee3\u53bb\u4f18\u5316\u6216\u8005\u5b57\u5178\u7684\u5f62\u5f0f\u5b9a\u4e49\u53c2\u6570\u7ec4. * rho (float, optional): \u7528\u6765\u8ba1\u7b97\u5e73\u5747\u5e73\u65b9\u68af\u5ea6\u7684\u7cfb\u6570(\u9ed8\u8ba4\u503c: 0.9) * eps (float, optional): \u589e\u52a0\u5206\u6bcd\u6765\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027(\u9ed8\u8ba4\u503c: 1e-6) * lr (float, optional): \u5728\u5c06 delta \u5e94\u7528\u4e8e\u53c2\u6570\u4e4b\u524d\u5bf9\u5176\u8fdb\u884c\u7cfb\u6570\u7684\u7f29\u653e(\u9ed8\u8ba4\u503c: 1.0) * weight_decay (float, optional): \u6743\u91cd\u8870\u51cf (L2\u6b63\u5219\u5316) (\u9ed8\u8ba4\u503c: 0) step(closure=None) \u5b9e\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u5e76\u8fd4\u56de\u8bef\u5dee\u635f\u5931\u7684\u95ed\u5305. class torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0) \u5b9e\u73b0 Adagrad \u7b97\u6cd5. \u5b83\u5728 Adaptive Subgradient Methods for Online Learning and Stochastic Optimization \u4e2d\u88ab\u63d0\u51fa. Args: * params (iterable): \u8fed\u4ee3\u7684\u4f18\u5316\u53c2\u6570\u6216\u8005\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b9a\u4e49\u53c2\u6570\u7ec4 * lr (float, optional): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 1e-2) * lr_decay (float, optional): \u5b66\u4e60\u7387\u8870\u51cf (\u9ed8\u8ba4\u503c: 0) * weight_decay (float, optional): \u6743\u91cd\u8870\u51cf (L2\u6b63\u5219\u5316) (\u9ed8\u8ba4\u503c: 0) step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de\u8bef\u5dee\u7684\u95ed\u5305. class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) \u5b9e\u73b0 Adam \u7b97\u6cd5. \u5b83\u5728 Adam: A Method for Stochastic Optimization \u4e2d\u88ab\u63d0\u51fa. Args: * params (iterable): \u8fed\u4ee3\u7684\u4f18\u5316\u53c2\u6570\u6216\u8005\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b9a\u4e49\u53c2\u6570\u7ec4. * lr (float, optional): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 1e-3) * betas (Tuple[float, float], optional): \u7528\u6765\u8ba1\u7b97\u68af\u5ea6\u548c\u5e73\u65b9\u68af\u5ea6\u7684\u7cfb\u6570 (\u9ed8\u8ba4\u503c: (0.9, 0.999)) * eps (float, optional): \u589e\u52a0\u5206\u6bcd\u6765\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027 (\u9ed8\u8ba4\u503c: 1e-8) * weight_decay (float, optional): \u6743\u91cd\u8870\u51cf (L2 \u6b63\u5219\u5316) (\u9ed8\u8ba4\u503c: 0) step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de\u8bef\u5dee\u7684\u95ed\u5305. class torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08) \u5b9e\u73b0\u4e0a\u4e00\u7248\u672c Adam \u7b97\u6cd5\u6765\u9002\u7528\u4e8e sparse tensors. \u5728\u8fd9\u4e2a\u53d8\u5316\u4e0b,\u53ea\u5c06\u663e\u793a\u51fa\u6765\u7684\u68af\u5ea6\u8fdb\u884c\u66f4\u65b0\u5b58\u50a8\u5e76\u4e14\u53ea\u5c06\u8fd9\u90e8\u5206\u68af\u5ea6\u5e94\u7528\u5230\u53c2\u6570\u4e2d. Args: * params (iterable): \u5f85\u4f18\u5316\u7684\u8fed\u4ee3\u53c2\u6570\u6216\u8005\u662f\u5b9a\u4e49\u4e86\u53c2\u6570\u7ec4\u7684 dict * lr (float, optional): \u5b66\u4e60\u7387 (default: 1e-3) * betas (Tuple[float, float], optional): \u7528\u6765\u8ba1\u7b97\u68af\u5ea6\u548c\u5e73\u65b9\u68af\u5ea6\u7684\u7cfb\u6570 (\u9ed8\u8ba4\u503c: (0.9, 0.999)) * eps (float, optional): \u589e\u52a0\u5206\u6bcd\u6765\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027 (\u9ed8\u8ba4\u503c: 1e-8) step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de loss \u7684\u95ed\u5305, \u5bf9\u4e8e\u5927\u591a\u6570\u53c2\u6570\u6765\u8bf4\u662f\u53ef\u9009\u7684. class torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) \u5b9e\u73b0 Adamax \u7b97\u6cd5 ( Adam \u7684\u4e00\u79cd\u57fa\u4e8e\u65e0\u7a77\u8303\u6570\u7684\u53d8\u79cd). \u5b83\u5728 Adam: A Method for Stochastic Optimization \u4e2d\u88ab\u63d0\u51fa. Args: * params (iterable): \u8fed\u4ee3\u7684\u4f18\u5316\u53c2\u6570\u6216\u8005\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b9a\u4e49\u53c2\u6570\u7ec4. * lr (float, optional): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 2e-3) * betas (Tuple[float, float], optional): \u7528\u6765\u8ba1\u7b97\u68af\u5ea6\u548c\u5e73\u65b9\u68af\u5ea6\u7684\u7cfb\u6570 * eps (float, optional): \u589e\u52a0\u5206\u6bcd\u6765\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027 (\u9ed8\u8ba4\u503c: 1e-8) * weight_decay (float, optional): \u6743\u91cd\u8870\u51cf (L2 \u6b63\u5219\u5316) (\u9ed8\u8ba4\u503c: 0) step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de\u8bef\u5dee\u7684\u95ed\u5305. class torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0) \u5b9e\u73b0\u5e73\u5747\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5. \u5b83\u5728 Acceleration of stochastic approximation by averaging \u4e2d\u88ab\u63d0\u51fa Args: * params (iterable): \u8fed\u4ee3\u7684\u4f18\u5316\u53c2\u6570\u6216\u8005\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b9a\u4e49\u53c2\u6570\u7ec4 * lr (float, optional): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 1e-2) * lambd (float, optional): \u8870\u51cf\u671f (\u9ed8\u8ba4\u503c: 1e-4) * alpha (float, optional): eta \u66f4\u65b0\u7684\u6743\u91cd (\u9ed8\u8ba4\u503c: 0.75) * t0 (float, optional): \u6307\u660e\u5728\u54ea\u4e00\u6b21\u5f00\u59cb\u5e73\u5747\u5316 (\u9ed8\u8ba4\u503c: 1e6) * weight_decay (float, optional): \u6743\u91cd\u8870\u51cf (L2 \u6b63\u5219\u5316) (\u9ed8\u8ba4\u503c: 0) step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de\u8bef\u5dee\u7684\u95ed\u5305. class torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None) \u5b9e\u73b0 L-BFGS \u7b97\u6cd5. Warning \u8fd9\u4e2a optimizer \u4e0d\u652f\u6301\u4e3a\u6bcf\u4e2a\u53c2\u6570\u5355\u72ec\u8bbe\u7f6e\u9009\u9879\u4ee5\u53ca\u4e0d\u652f\u6301\u53c2\u6570\u7ec4\uff08\u53ea\u80fd\u6709\u4e00\u4e2a\uff09. Warning \u76ee\u524d\u6240\u6709\u7684\u53c2\u6570\u4e0d\u5f97\u4e0d\u90fd\u5728\u540c\u4e00\u8bbe\u5907\u4e0a. \u8fd9\u5728\u5c06\u6765\u4f1a\u5f97\u5230\u6539\u8fdb. Note \u8fd9\u662f\u4e00\u4e2a\u5185\u5b58\u9ad8\u5ea6\u5bc6\u96c6\u7684 optimizer (\u5b83\u8981\u6c42\u989d\u5916\u7684 param_bytes * (history_size + 1) \u4e2a\u5b57\u8282). \u5982\u679c\u5b83\u4e0d\u9002\u5e94\u5185\u5b58, \u5c1d\u8bd5\u51cf\u5c0f\u5386\u53f2\u89c4\u683c, \u6216\u8005\u4f7f\u7528\u4e0d\u540c\u7684\u7b97\u6cd5. Args: * lr (float): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 1) * max_iter (int): \u6bcf\u4e00\u6b65\u4f18\u5316\u7684\u6700\u5927\u8fed\u4ee3\u6b21\u6570 (\u9ed8\u8ba4\u503c: 20) * max_eval (int): \u6bcf\u4e00\u6b65\u4f18\u5316\u7684\u6700\u5927\u51fd\u6570\u8bc4\u4f30\u6b21\u6570 (\u9ed8\u8ba4\u503c: max_iter * 1.25). * tolerance_grad (float): \u4e00\u9636\u6700\u4f18\u7684\u7ec8\u6b62\u5bb9\u5fcd\u5ea6 (\u9ed8\u8ba4\u503c: 1e-5). * tolerance_change (float): \u5728\u51fd\u6570\u503c/\u53c2\u6570\u53d8\u5316\u91cf\u4e0a\u7684\u7ec8\u6b62\u5bb9\u5fcd\u5ea6 (\u9ed8\u8ba4\u503c: 1e-9). * history_size (int): \u66f4\u65b0\u5386\u53f2\u5c3a\u5bf8 (\u9ed8\u8ba4\u503c: 100). step(closure) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de loss \u7684\u95ed\u5305, \u5bf9\u4e8e\u5927\u591a\u6570\u53c2\u6570\u6765\u8bf4\u662f\u53ef\u9009\u7684. class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False) \u5b9e\u73b0 RMSprop \u7b97\u6cd5. \u7531 G. Hinton \u5728\u6b64\u63d0\u51fa course . \u4e2d\u5fc3\u7248\u672c\u9996\u6b21\u51fa\u73b0\u5728 Generating Sequences With Recurrent Neural Networks . \u7b97\u6cd5: * params (iterable): \u5f85\u4f18\u5316\u7684\u8fed\u4ee3\u53c2\u6570\u6216\u8005\u662f\u5b9a\u4e49\u4e86\u53c2\u6570\u7ec4\u7684 dict * lr (float, optional): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 1e-2) * momentum (float, optional): \u52a8\u91cf\u56e0\u5b50 (\u9ed8\u8ba4\u503c: 0) * alpha (float, optional): \u5e73\u6ed1\u5e38\u91cf (default: 0.99) * eps (float, optional): \u4e3a\u4e86\u589e\u52a0\u6570\u503c\u8ba1\u7b97\u7684\u7a33\u5b9a\u6027\u800c\u52a0\u5230\u5206\u6bcd\u91cc\u7684\u9879 (\u9ed8\u8ba4\u503c: 1e-8) * centered (bool, optional) : \u5982\u679c\u4e3a True , \u8ba1\u7b97 RMSProp \u7684\u4e2d\u503c, \u5e76\u4e14\u7528\u5b83\u7684\u65b9\u5dee\u9884\u6d4b\u503c\u5bf9\u68af\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316 * weight_decay (float, optional): weight decay (L2 penalty) (default: 0) step(closure=None) Performs a single optimization step. Parameters: closure ( callable , optional ) \u2013 A closure that reevaluates the model and returns the loss. class torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50)) \u5b9e\u73b0\u5f39\u6027\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5. Args: * params (iterable): \u5f85\u4f18\u5316\u7684\u8fed\u4ee3\u53c2\u6570\u6216\u8005\u662f\u5b9a\u4e49\u4e86\u53c2\u6570\u7ec4\u7684 dict * lr (float, optional): \u5b66\u4e60\u7387 (\u9ed8\u8ba4\u503c: 1e-2) * etas (Tuple[float, float], optional): \u4e00\u5bf9 (etaminus, etaplis), t\u5b83\u4eec\u5206\u522b\u662f\u4e58\u6cd5 \u7684\u589e\u52a0\u548c\u51cf\u5c0f\u7684\u56e0\u5b50 (\u9ed8\u8ba4\u503c: (0.5, 1.2)) step_sizes (Tuple[float, float], optional): \u5141\u8bb8\u7684\u4e00\u5bf9\u6700\u5c0f\u548c\u6700\u5927\u7684\u6b65\u957f (\u9ed8\u8ba4\u503c: (1e-6, 50)) step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de loss \u7684\u95ed\u5305, \u5bf9\u4e8e\u5927\u591a\u6570\u53c2\u6570\u6765\u8bf4\u662f\u53ef\u9009\u7684. class torch.optim.SGD(params, lr=<object object>, momentum=0, dampening=0, weight_decay=0, nesterov=False) \u5b9e\u73b0\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff08 momentum \u53ef\u9009\uff09. Nesterov \u52a8\u91cf\u57fa\u4e8e On the importance of initialization and momentum in deep learning \u4e2d\u7684\u516c\u5f0f. Args: * params (iterable): \u5f85\u4f18\u5316\u7684\u8fed\u4ee3\u53c2\u6570\u6216\u8005\u662f\u5b9a\u4e49\u4e86\u53c2\u6570\u7ec4\u7684 dict * lr (float): \u5b66\u4e60\u7387 * momentum (float, optional): \u52a8\u91cf\u56e0\u5b50 (\u9ed8\u8ba4\u503c: 0) * weight_decay (float, optional): \u6743\u91cd\u8870\u51cf (L2 \u6b63\u5219\u5316) (\u9ed8\u8ba4\u503c: 0) * dampening (float, optional): \u52a8\u91cf\u7684\u6291\u5236\u56e0\u5b50 (\u9ed8\u8ba4\u503c: 0) * nesterov (bool, optional): \u4f7f\u7528 Nesterov \u52a8\u91cf (\u9ed8\u8ba4\u503c: False) Example >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) >>> optimizer.zero_grad() >>> loss_fn(model(input), target).backward() >>> optimizer.step() Note \u5e26\u6709\u52a8\u91cf /Nesterov \u7684 SGD \u7684\u5b9e\u73b0\u7a0d\u5fae\u4e0d\u540c\u4e8e Sutskever \u7b49\u4eba\u4ee5\u53ca\u5176\u4ed6\u6846\u67b6\u4e2d\u7684\u5b9e\u73b0. \u8003\u8651\u52a8\u91cf\u7684\u5177\u4f53\u60c5\u51b5, \u66f4\u65b0\u53ef\u4ee5\u5199\u6210 \\[\\begin{split}v = \\rho * v + g \\\\ p = p - lr * v\\end{split}\\] \u5176\u4e2d p, g, v \u548c \\(\\rho\\) \u5206\u522b\u662f\u53c2\u6570\u3001\u68af\u5ea6\u3001\u901f\u5ea6\u548c\u52a8\u91cf. \u8fd9\u8ddf Sutskever \u7b49\u4eba\u4ee5\u53ca\u5176\u4ed6\u6846\u67b6\u7684\u5b9e\u73b0\u662f\u76f8\u53cd\u7684, \u5b83\u4eec\u91c7\u7528\u8fd9\u6837\u7684\u66f4\u65b0. \\[\\begin{split}v = \\rho * v + lr * g \\\\ p = p - v\\end{split}\\] Nesterov \u7684\u7248\u672c\u4e5f\u76f8\u5e94\u7684\u88ab\u4fee\u6539\u4e86. step(closure=None) \u8fdb\u884c\u5355\u6b65\u4f18\u5316. Parameters: closure ( callable , optional ) \u2013 \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4ef7\u6a21\u578b\u5e76\u8fd4\u56de loss \u7684\u95ed\u5305, \u5bf9\u4e8e\u5927\u591a\u6570\u53c2\u6570\u6765\u8bf4\u662f\u53ef\u9009\u7684.","title":"\u7b97\u6cd5"},{"location":"58/#_5","text":"mod: <cite>torch.optim.lr_scheduler</cite> \u57fa\u4e8e\u5faa\u73af\u7684\u6b21\u6570\u63d0\u4f9b\u4e86\u4e00\u4e9b\u65b9\u6cd5\u6765\u8c03\u8282\u5b66\u4e60\u7387. class: <cite>torch.optim.lr_scheduler.ReduceLROnPlateau</cite> \u57fa\u4e8e\u9a8c\u8bc1\u6d4b\u91cf\u7ed3\u679c\u6765\u8bbe\u7f6e\u4e0d\u540c\u7684\u5b66\u4e60\u7387. --- --- class torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1) \u5c06\u6bcf\u4e2a\u53c2\u6570\u7ec4\u7684\u5b66\u4e60\u901f\u7387\u8bbe\u7f6e\u4e3a\u7ed9\u5b9a\u51fd\u6570\u7684\u521d\u59cbLR. \u5f53 last_epoch=-1, \u8bbe\u7f6e\u51fa\u4e8b\u7684 lr \u4f5c\u4e3a lr. Args: * optimizer (Optimizer): \u5c01\u88c5\u597d\u7684\u4f18\u5316\u5668. * lr_lambda (function or list): \u8ba1\u7b97\u7ed9\u5b9a\u6574\u6570\u53c2\u6570\u5386\u5143\u7684\u4e58\u6cd5\u56e0\u5b50\u7684\u51fd\u6570, \u6216\u8005\u4e00\u7cfb\u5217\u7684\u6b64\u7c7b\u51fd\u6570, \u6bcf\u7ec4\u7684\u4e00\u4e2a\u90fd\u5728 optimizer.param_groups \u4e2d. * last_epoch (int): \u6700\u540e\u4e00\u4e2a epoch \u7684\u7d22\u5f15. \u9ed8\u8ba4\u503c: -1. Example >>> # Assuming optimizer has two groups. >>> lambda1 = lambda epoch: epoch // 30 >>> lambda2 = lambda epoch: 0.95 ** epoch >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2]) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1) \u901a\u8fc7 gamma \u5728\u6bcf\u4e00\u4e2a epoch \u91cc\u9762\u7684 step_size \u8bbe\u7f6e\u6bcf\u4e2a\u53c2\u6570\u7ec4\u7684\u521d\u59cb\u5b66\u4e60\u7387\u8870\u51cf\u53d8\u91cf. \u5f53 last_epoch=-1, \u8bbe\u7f6e\u521d\u59cb lr \u4e3a lr. Args: * optimizer (Optimizer): \u5c01\u88c5\u597d\u7684\u4f18\u5316\u5668. * step_size (int): \u5b66\u4e60\u7387\u8870\u51cf\u5468\u671f. * gamma (float): \u5b66\u4e60\u7387\u8870\u51cf\u7684\u4e58\u6cd5\u56e0\u5b50. \u9ed8\u8ba4\u503c: 0.1. * last_epoch (int): \u6700\u540e\u4e00\u4e2a epoch \u7684\u7d22\u5f15. \u9ed8\u8ba4\u503c: -1. Example >>> # Assuming optimizer uses lr = 0.5 for all groups >>> # lr = 0.05 if epoch < 30 >>> # lr = 0.005 if 30 <= epoch < 60 >>> # lr = 0.0005 if 60 <= epoch < 90 >>> # ... >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) class torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1) \u4e00\u65e6 epoch \u7684\u6570\u91cf\u8fbe\u5230\u4e86\u4e00\u4e2a\u4e34\u754c\u70b9\u901a\u8fc7 gamma \u5728\u6bcf\u4e00\u4e2a epoch \u91cc\u9762\u7684 step_size \u8bbe\u7f6e\u6bcf\u4e2a\u53c2\u6570 \u7ec4\u7684\u521d\u59cb\u5b66\u4e60\u7387\u8870\u51cf\u53d8\u91cf.\u5f53 last_epoch=-1, \u8bbe\u7f6e\u521d\u59cb lr \u4f5c\u4e3a lr. Args: * optimizer (Optimizer): \u5c01\u88c5\u597d\u7684\u4f18\u5316\u5668. * milestones (list): epoch \u7d22\u5f15\u5217\u8868. \u5fc5\u987b\u4e3a\u9012\u589e\u7684. * gamma (float): \u5b66\u4e60\u7387\u8870\u51cf\u7684\u4e58\u6cd5\u56e0\u5b50. \u9ed8\u8ba4\u503c: 0.1. last_epoch (int): \u6700\u540e\u4e00\u4e2a epoch \u7684\u7d22\u5f15. \u9ed8\u8ba4\u503c: -1. Example >>> # Assuming optimizer uses lr = 0.5 for all groups >>> # lr = 0.05 if epoch < 30 >>> # lr = 0.005 if 30 <= epoch < 80 >>> # lr = 0.0005 if epoch >= 80 >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) class torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1) \u901a\u8fc7 gamma \u5728\u6bcf\u4e00\u4e2a epoch \u91cc\u9762\u7684 step_size \u8bbe\u7f6e\u6bcf\u4e2a\u53c2\u6570\u7ec4\u7684\u521d\u59cb\u5b66\u4e60\u7387\u8870\u51cf\u53d8\u91cf . \u5f53 last_epoch=-1, \u8bbe\u7f6e\u521d\u59cb lr \u4f5c\u4e3a lr. :param optimizer: \u5c01\u88c5\u597d\u7684\u4f18\u5316\u5668. :type optimizer: Optimizer :param gamma: \u5b66\u4e60\u7387\u8870\u51cf\u7684\u4e58\u6cd5\u56e0\u5b50. :type gamma: float :param last_epoch: \u6700\u540e\u4e00\u4e2a epoch \u7684\u7d22\u5f15. \u9ed8\u8ba4\u503c: -1. :type last_epoch: int class torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08) \u5f53\u4e00\u4e2a\u6307\u6807\u5df2\u7ecf\u505c\u6b62\u63d0\u5347\u65f6\u51cf\u5c11\u5b66\u4e60\u7387.\u6a21\u578b\u901a\u5e38\u53d7\u76ca\u4e8e\u901a\u8fc7\u4e00\u6b212-10\u7684\u5b66\u4e60\u505c\u6b62\u56e0\u7d20\u51cf\u5c11\u5b66\u4e60\u7387 \u8fd9\u4e2a\u8c03\u5ea6\u7a0b\u5e8f\u8bfb\u53d6\u4e00\u4e2a\u6307\u6807\u8d28\u91cf \u4ee5\u53ca\u770b\u5230 \u2018patience\u2019 \u7684\u6570\u91cf\u5728\u4e00\u4e2a epoch \u91cc\u9762\u5982\u679c\u6ca1\u6709\u63d0\u5347, \u8fd9\u65f6\u5b66\u4e60\u7387\u5df2\u7ecf\u51cf\u5c0f. Args: * optimizer (Optimizer): \u5c01\u88c5\u597d\u7684\u4f18\u5316\u5668. mode (str): <cite>min</cite>, <cite>max</cite> \u5176\u4e2d\u4e00\u4e2a. \u5728 <cite>min</cite> \u6a21\u5757\u4e0b,\u5f53\u8d28\u91cf\u76d1\u6d4b\u5df2\u7ecf \u505c\u6b62\u4e0b\u964d\u65f6 lr \u5c06\u88ab\u51cf\u5c11; \u5728 <cite>max</cite> \u6a21\u5757\u4e0b \u5f53\u8d28\u91cf\u76d1\u6d4b\u5df2\u7ecf\u505c\u6b62\u4e0a\u5347\u65f6 lr \u5c06 \u88ab\u51cf\u5c11. \u9ed8\u8ba4\u503c: \u2018min\u2019. factor (float): \u54ea\u4e2a\u5b66\u4e60\u7387\u5c06\u4f1a\u88ab\u51cf\u5c11\u7684\u5f71\u54cd\u56e0\u5b50 . new_lr = lr * factor. \u9ed8\u8ba4\u503c: 0.1. patience (int): epoch \u4e2d\u6ca1\u6709\u6539\u5584\u7684\u6b21\u6570, \u5b66\u4e60\u7387\u5c06\u4f1a\u964d\u4f4e. . \u9ed8\u8ba4\u503c: 10. verbose (bool): \u82e5\u4e3a True , \u6bcf\u6b21\u66f4\u65b0\u6253\u5370\u4fe1\u606f\u5230\u63a7\u5236\u53f0\u8f93\u51fa. \u9ed8\u8ba4\u503c: False . threshold (float): \u6d4b\u91cf\u65b0\u7684\u6700\u4f73\u9608\u503c, \u53ea\u5173\u6ce8\u6709\u91cd\u5927\u610f\u4e49\u7684\u6539\u53d8. \u9ed8\u8ba4\u503c: 1e-4. threshold_mode (str): <cite>rel</cite>, <cite>abs</cite> \u4e2d\u7684\u4e00\u4e2a. \u5728 <cite>rel</cite> \u6a21\u5f0f\u4e0b, dynamic_threshold = best * ( 1 + threshold ) \u5728 \u2018max\u2019 \u6a21\u5f0f\u4e0b\u6216\u8005\u5728 <cite>min</cite> \u6a21\u5f0f\u4e0b best * ( 1 - threshold ) . \u5728 <cite>abs</cite> \u6a21\u5f0f\u4e0b, dynamic_threshold = best + threshold \u5728 <cite>max</cite> \u6a21\u5f0f\u4e0b\u6216\u8005\u5728 <cite>min</cite> \u6a21\u5f0f\u4e0b best - threshold . \u9ed8\u8ba4\u503c: \u2018rel\u2019. cooldown (int): lr \u5df2\u7ecf\u51cf\u5c11\u4e4b\u540e\u53bb\u7b49\u5f85\u6700\u4f73\u7684\u6b63\u5e38\u64cd\u4f5c\u4e4b\u524d\u7684 epoch \u6570\u76ee. \u9ed8\u8ba4\u503c: 0. min_lr (float or list): \u4e00\u4e2a\u5217\u8868\u7684\u6807\u91cf.\u6240\u6709\u53c2\u6570\u7ec4\u6216\u6bcf\u4e2a\u7ec4\u7684\u5b66\u4e60\u7387\u4e0b\u9650. \u9ed8\u8ba4\u503c: 0. eps (float): lr \u6700\u5c0f\u7684\u8870\u51cf\u503c\u9002\u5e94\u4e8e. \u5982\u679c\u65b0 lr \u548c\u65e7 lr \u4e4b\u95f4\u7684\u5dee\u5f02\u5c0f\u4e8e eps,\u66f4\u65b0\u53ef\u4ee5\u5ffd\u7565. \u9ed8\u8ba4\u503c: 1e-8. Example >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) >>> scheduler = ReduceLROnPlateau(optimizer, 'min') >>> for epoch in range(10): >>> train(...) >>> val_loss = validate(...) >>> # Note that step should be called after validate() >>> scheduler.step(val_loss)","title":"\u5982\u4f55\u8c03\u6574\u5b66\u4e60\u7387"},{"location":"59/","text":"Automatic differentiation package - torch.autograd torch.autograd \u63d0\u4f9b\u4e86\u7c7b\u548c\u51fd\u6570\u7528\u6765\u5bf9\u4efb\u610f\u6807\u91cf\u51fd\u6570\u8fdb\u884c\u6c42\u5bfc.\u53ea\u9700\u8981\u5bf9\u5df2\u6709\u7684\u4ee3\u7801\u8fdb\u884c\u5fae\u5c0f\u7684\u6539\u53d8-\u53ea\u9700\u8981\u5c06\u6240\u6709\u7684 tensors \u5305\u542b\u5728 Variable \u5bf9\u8c61\u4e2d\u5373\u53ef. torch.autograd.backward(variables, grad_variables=None, retain_graph=None, create_graph=None, retain_variables=None) \u7ed9\u5b9a\u56fe\u67d0\u4e00\u4e2a\u7684\u8282\u70b9\u53d8\u91cfvariables,\u8ba1\u7b97\u5bf9\u8be5\u53d8\u91cf\u6c42\u5bfc\u7684\u68af\u5ea6\u548c. \u8ba1\u7b97\u56fe\u53ef\u4ee5\u901a\u8fc7\u94fe\u5f0f\u6cd5\u5219\u6c42\u5bfc.\u5982\u679c\u4efb\u4f55 variables \u90fd\u662f\u975e\u6807\u91cf(\u6bd4\u5982 \u4ed6\u4eec\u7684 data \u5c5e\u6027\u4e2d\u6709\u591a\u4e2a\u5143\u7d20)\u5e76\u4e14\u9700\u8981\u6c42\u5bfc, \u90a3\u4e48\u6b64\u51fd\u6570\u9700\u8981\u6307\u5b9a grad_variables . \u5b83\u7684\u957f\u5ea6\u5e94\u8be5\u548cvariables\u7684\u957f\u5ea6\u5339\u914d,\u91cc\u9762\u4fdd\u5b58\u4e86\u76f8\u5173 variable \u7684\u68af\u5ea6 (\u5bf9\u4e8e\u4e0d\u9700\u8981 gradient tensor \u7684 variable, \u5e94\u5236\u5b9a\u4e3a None). \u6b64\u51fd\u6570\u7d2f\u79ef\u53f6\u5b50\u8282\u70b9 variables \u8ba1\u7b97\u7684\u68af\u5ea6 - \u8c03\u7528\u6b64\u51fd\u6570\u4e4b\u524d\u5e94\u5148\u5c06\u53f6\u5b50\u8282\u70b9 variables \u68af\u5ea6\u7f6e\u96f6. \u53c2\u6570\u8bf4\u660e: * variables(Variable \u5217\u8868): \u88ab\u6c42\u5fae\u5206\u7684\u53f6\u5b50\u8282\u70b9. * grad_variables ((Tensor,Variable or None)\u5217\u8868):\u5bf9\u5e94 variable \u7684\u68af\u5ea6.\u4efb\u4f55\u5f20\u91cf\u5c06\u81ea\u52a8\u8f6c\u6362\u4e3a\u53d8\u91cf\u9664\u975e create_graph \u662f True . \u6ca1\u6709\u503c\u53ef\u4ee5\u88ab\u6307\u5b9a\u4e3a\u6807\u91cf\u53d8\u91cf\u6216\u8005\u4e0d\u9700\u8981\u88ab\u6c42\u5bfc. \u5982\u679c\u6ca1\u6709\u503c\u88ab\u6240\u6709\u7684grad_variables\u63a5\u53d7, \u90a3\u4e48\u8be5\u53c2\u6570\u662f\u53ef\u4ee5\u88ab\u7701\u7565\u7684. retain_graph (bool, \u53ef\u9009): \u5982\u679c\u662f False , \u8be5\u56fe\u8ba1\u7b97\u8fc7\u7684\u68af\u5ea6\u88ab\u91ca\u653e\u6389.\u6ce8\u610f\u7684\u662f,\u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u90fd\u8bbe\u7f6e\u4e3a True \u5e76\u4e0d\u662f\u5fc5\u987b\u7684\u5e76\u4e14\u80fd\u591f\u9ad8\u6548\u7684\u8ba1\u7b97.\u5c06\u8be5 create_graph \u53c2\u6570\u503c\u8bbe\u7f6e\u4e3a\u9ed8\u8ba4\u5373\u53ef. create_graph (bool, \u53ef\u9009): \u5982\u679c\u662f True , \u5c06\u4f1a\u5efa\u7acb\u4e00\u4e2a\u68af\u5ea6\u56fe, \u7528\u6765\u6c42\u89e3\u9ad8\u9636\u5bfc\u6570. \u9ed8\u8ba4\u4e3a False , \u9664\u975e grad_variables \u62e5\u6709\u4e0d\u6b62\u4e00\u4e2a \u6613\u53d8\u7684 Variable. torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=None, only_inputs=True, allow_unused=False) \u8ba1\u7b97\u5e76\u8fd4\u56de\u7ed9\u5b9a\u503c\u7684\u68af\u5ea6\u7684\u548c. grad_outputs \u662f\u4e00\u4e2a\u5217\u8868\u540c\u65f6\u957f\u5ea6\u4e0e output \u4e00\u6837, \u5b58\u653e\u4e86\u9884\u5148\u8ba1\u7b97 input \u7684\u68af\u5ea6\u7684\u548c. \u5982\u679c output \u4e0d\u9700\u8981\u88ab\u6c42\u5bfc, \u90a3\u4e48\u68af\u5ea6\u5c06\u4e3a None ). \u5f53\u4e0d\u9700\u8981\u6d3e\u751f\u56fe\u65f6,\u53ef\u4ee5\u5c06\u68af\u5ea6\u4f5c\u4e3a\u5f20\u91cf,\u6216\u8005\u4f5c\u4e3a\u53d8\u91cf,\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u56fe\u5c06\u88ab\u521b\u5efa. \u5982\u679c\u53c2\u6570 only_inputs \u4e3a True , \u8be5\u65b9\u6cd5\u5c06\u4f1a\u8fd4\u56de\u7ed9\u5b9a\u8f93\u5165\u7684\u68af\u5ea6\u503c\u5217\u8868.\u5982\u679c\u4e3a False , \u90a3\u4e48\u9057\u7559\u4e0b\u6765\u7684\u6240\u6709\u53f6\u5b50\u8282\u70b9\u7684\u68af\u5ea6\u90fd\u4f1a\u88ab\u8ba1\u7b97, \u88ab\u4e14\u4f1a\u88ab\u5217\u52a0\u5230 .grad \u53c2\u6570\u4e2d. \u53c2\u6570\u8bf4\u660e: * outputs (\u53d8\u91cf\u5e8f\u5217): \u68af\u5ea6\u51fd\u6570\u7684\u8fd4\u56de\u503c. * inputs (\u53d8\u91cf\u5e8f\u5217): \u9700\u8981\u8ba1\u7b97\u7684\u68af\u5ea6\u7684\u8f93\u5165 (\u5e76\u4e14\u4e0d\u4f1a\u88ab\u7d2f\u52a0\u5230 .grad \u53c2\u6570\u4e2d). * grad_outputs (\u5f20\u91cf\u6216\u53d8\u91cf\u5e8f\u5217): \u6bcf\u4e00\u4e2a\u8f93\u51fa\u7684\u68af\u5ea6. \u6240\u6709\u7684\u5f20\u91cf\u90fd\u4f1a\u53d8\u6210\u53d8\u91cf\u5e76\u4e14\u662f\u53ef\u53d8\u7684\u9664\u975e\u53c2\u6570 create_graph \u4e3a True . \u6ca1\u6709\u503c\u53ef\u4ee5\u88ab\u6307\u5b9a\u4e3a\u6807\u91cf\u53d8\u91cf\u6216\u8005\u4e0d\u9700\u8981\u53d8\u5316\u7684\u503c. \u5982\u679c\u6240\u6709 grad_variabls \u90fd\u53ef\u4ee5\u63a5\u53d7 None \u503c,\u90a3\u4e48\u8fd9\u4e2a\u53c2\u6570\u662f\u53ef\u9009\u7684. retain_graph (bool, \u53ef\u9009): \u5982\u679c\u662f False , \u7528\u4e8e\u8ba1\u7b97 grad \u7684\u56fe\u5c06\u88ab\u91ca\u653e. \u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u90fd\u8bbe\u7f6e\u4e3a True \u5e76\u4e0d\u662f\u5fc5\u987b\u7684\u5e76\u4e14\u80fd\u591f\u9ad8\u6548\u5730\u8fd0\u884c. \u9ed8\u8ba4\u4e0e create_graph \u53c2\u6570\u4e00\u6837. create_graph (bool, \u53ef\u9009): \u5982\u679c\u662f True , \u68af\u5ea6\u56fe\u5c06\u4f1a\u88ab\u5efa\u7acb,\u7528\u6765\u6c42\u89e3\u9ad8\u9636\u5bfc\u6570. \u9ed8\u8ba4\u4e3a <cite>False &lt;/cite&gt; , \u9664\u975e\u53c2\u6570 grad_variables` \u5305\u542b\u4e0d\u53ea\u4e00\u4e2a\u53d8\u91cf. only_inputs (bool, \u53ef\u9009): \u5982\u679c\u662f True , \u53f6\u5b50\u8282\u70b9\u7684\u5bfc\u6570\u5c06\u4f1a\u5728\u56fe\u4e2d, \u4f46\u662f\u4e0d\u4f1a\u51fa\u73b0\u5728\u53c2\u6570 inputs \u4e5f\u4e0d\u4f1a\u88ab\u8ba1\u7b97\u4ee5\u53ca\u7d2f\u52a0. \u9ed8\u8ba4\u4e3a True . allow_unused (bool, \u53ef\u9009): \u5982\u679c\u662f False , \u6307\u5b9a\u8ba1\u7b97\u8f93\u51fa\u65f6\u672a\u4f7f\u7528\u7684\u8f93\u5165\uff08\u56e0\u6b64\u5b83\u4eec\u7684 grad \u59cb\u7ec8\u4e3a\u96f6\uff09\u662f\u9519\u8bef\u7684. \u9ed8\u8ba4\u4e3a False . Variable (\u53d8\u91cf) API compatibility Variable API \u51e0\u4e4e\u4e0e\u5e38\u89c4 Tensor API \u76f8\u540c(\u4e00\u4e9b\u4f1a\u8986\u76d6\u68af\u5ea6\u8ba1\u7b97\u8f93\u5165\u7684\u5185\u7f6e\u65b9\u6cd5\u9664\u5916). \u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b, \u53d8\u91cf\u91cf\u53ef\u4ee5\u5b89\u5168\u5730\u66ff\u6362\u5f20\u91cf\u5e76\u4e14\u4ee3\u7801\u5c06\u4fdd\u6301\u6b63\u5e38\u5de5\u4f5c. \u56e0\u4e3a\u8fd9\u4e2a, \u6211\u4eec\u6ca1\u6709\u8bb0\u5f55\u53d8\u91cf\u4e0a\u7684\u6240\u6709\u64cd\u4f5c, \u4f60\u5e94\u8be5\u53c2\u9605 torch.Tensor \u6587\u6863\u6765\u67e5\u770b\u53d8\u91cf\u4e0a\u7684\u6240\u6709\u64cd\u4f5c. In-place operations on Variables \u5728 autograd \u652f\u6301\u5c31\u5730\u64cd\u4f5c\u662f\u4e00\u4ef6\u56f0\u96be\u7684\u4e8b\u60c5, \u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6211\u4eec\u4e0d\u9f13\u52b1\u4f7f\u7528. Autograd \u79ef\u6781\u7684\u7f13\u51b2\u533a\u91ca\u653e\u548c\u91cd\u7528\u4f7f\u5f97\u5b83\u975e\u5e38\u9ad8\u6548, \u800c\u4e14\u5f88\u5c11\u6709\u5c31\u5730\u64cd\u4f5c\u5b9e\u9645\u4e0a\u5927\u91cf\u5730\u964d\u4f4e\u4e86\u5185\u5b58\u4f7f\u7528\u91cf\u7684\u60c5\u51b5. \u9664\u975e\u4f60\u6b63\u5728\u5927\u91cf\u7684\u7684\u5185\u5b58\u538b\u529b\u4e0b\u8fd0\u884c, \u5426\u5219\u4f60\u53ef\u80fd\u6c38\u8fdc\u4e0d\u9700\u8981\u4f7f\u7528\u5b83\u4eec. In-place correctness checks \u6240\u6709\u7684 Variable \u8ddf\u8e2a\u9002\u7528\u4e8e\u5b83\u4eec\u7684\u5c31\u5730\u64cd\u4f5c, \u5e76\u4e14\u5982\u679c\u5b9e\u73b0\u68c0\u6d4b\u5230\u4e00\u4e2a\u53d8\u91cf\u662f\u5426\u88ab\u5176\u4e2d\u4e00\u4e2a\u51fd\u6570\u540e\u53f0\u4fdd\u5b58, \u4f46\u662f\u4e4b\u540e\u5b83\u88ab\u5c31\u5730\u4fee\u6539\u4e86, \u4f1a\u5728\u5f00\u59cb\u6c42\u5bfc\u65f6\u4f1a\u62a5\u51fa\u5f02\u5e38. \u8fd9\u786e\u4fdd\u4e86\u5982\u679c\u4f60\u5728\u5c31\u5730\u4f7f\u7528\u51fd\u6570\u5e76\u6ca1\u6709\u770b\u5230\u4efb\u4f55\u9519\u8bef, \u4f60\u53ef\u4ee5\u80af\u5b9a\u7684\u662f\u8ba1\u7b97\u53d8\u91cf\u662f\u6b63\u786e\u7684. class torch.autograd.Variable \u5c01\u88c5\u4e00\u4e2a\u5f20\u91cf\u7528\u6765\u5404\u79cd\u64cd\u4f5c. \u53d8\u91cf\u662f\u5f20\u91cf\u5bf9\u8c61\u5468\u56f4\u7684\u8f7b\u5305\u88c5,\u80fd\u591f\u62e5\u6709\u5bfc\u6570\u7b49\u6570\u636e, \u8fd9\u4e2a\u5f15\u7528\u5141\u8bb8\u56de\u6eaf\u6574\u4e2a\u64cd\u4f5c\u94fe\u521b\u5efa\u6570\u636e. \u5982\u679c\u53d8\u91cf\u5df2\u7ecf\u7531\u7528\u6237\u521b\u5efa, \u5b83\u7684 grad_fn \u4e3a None \u6211\u4eec\u79f0\u4e4b\u4e3a\u53f6\u5b50\u8282\u70b9. \u7531\u4e8e autograd \u53ea\u652f\u6301\u6807\u91cf\u503c\u51fd\u6570\u5fae\u5206, grad \u5927\u5c0f\u59cb\u7ec8\u4e0e\u6570\u636e\u5927\u5c0f\u5339\u914d. \u6b64\u5916,\u5bfc\u6570\u901a\u5e38\u53ea\u5206\u914d \u53f6\u53d8\u91cf,\u5426\u5219\u5c06\u59cb\u7ec8\u4e3a\u96f6. \u53c2\u6570\u8bf4\u660e: * data: \u5305\u88f9\u4efb\u4f55\u7c7b\u578b\u7684\u5f20\u91cf. * grad: \u53d8\u91cf\u4fdd\u6301\u7c7b\u578b\u548c\u4f4d\u7f6e\u5339\u914d\u7684\u53d8\u91cf .data . \u8fd9\u4e2a\u5c5e\u6027\u662f\u61d2\u60f0\u7684\u5206\u914d,\u4e0d\u80fd\u88ab\u91cd\u65b0\u5206\u914d. * requires_grad: \u6307\u793a\u53d8\u91cf\u662f\u5426\u5df2\u88ab\u4f7f\u7528\u7684\u5e03\u5c14\u503c\u7531\u5305\u542b\u4efb\u4f55\u53d8\u91cf\u7684\u5b50\u56fe\u521b\u5efa,\u9700\u8981\u5b83. \u6709\u5173\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f,\u8bf7\u53c2\u9605 excluded-subgraphs.\u53ea\u80fd\u5728\u53f6\u53d8\u91cf\u4e0a\u8fdb\u884c\u66f4\u6539. * volatile: \u5e03\u5c14\u503c\u8868\u793a\u5e94\u8be5\u4f7f\u7528\u53d8\u91cf\u63a8\u7406\u6a21\u5f0f,\u5373\u4e0d\u4fdd\u5b58\u5386\u53f2. \u67e5\u770b \u53cd\u5411\u6392\u9664 subgraphs (\u5b50\u56fe) \u66f4\u591a\u7ec6\u8282. \u53ea\u80fd\u5728\u53f6\u53d8\u91cf\u4e0a\u8fdb\u884c\u66f4\u6539. * is_leaf: \u6307\u793a\u662f\u5426\u4e3a\u53f6\u5b50\u8282\u70b9,\u5373\u662f\u5426\u7531\u7528\u6237\u521b\u5efa\u7684\u8282\u70b9. * grad_fn: \u5bfc\u6570\u51fd\u6570\u8ddf\u8e2a. \u53c2\u6570: * data (any tensor class): \u7528\u6765\u5305\u88c5\u7684\u5f20\u91cf. * requires_grad (bool): \u6307\u793a\u662f\u5426\u8981\u88ab\u6c42\u5bfc. Keyword only. * volatile (bool): \u6307\u793a\u662f\u5426\u53ef\u53d8. Keyword only. backward(gradient=None, retain_graph=None, create_graph=None, retain_variables=None) \u7ed9\u5b9a\u56fe\u53f6\u5b50\u8282\u70b9\u8ba1\u7b97\u5bfc\u6570. \u8be5\u56fe\u4f7f\u7528\u94fe\u5f0f\u89c4\u5219\u8fdb\u884c\u8ba1\u7b97. \u5982\u679c\u53d8\u91cf\u662f\u975e\u6807\u91cf\uff08\u5373\u5176\u6570\u636e\u5177\u6709\u591a\u4e2a\u5143\u7d20\uff09\u5e76\u4e14\u9700\u8981 \u6539\u53d8,\u8be5\u529f\u80fd\u53e6\u5916\u9700\u8981\u6307\u5b9a\u201c\u68af\u5ea6\u201d.\u5b83\u5e94\u8be5\u662f\u4e00\u4e2a\u5305\u542b\u5339\u914d\u7c7b\u578b\u548c\u4f4d\u7f6e\u7684\u5f20\u91cf \u5fae\u5206\u51fd\u6570\u7684\u68af\u5ea6w.r.t. self . \u8fd9\u4e2a\u529f\u80fd\u5728\u53f6\u5b50\u4e0a\u7d2f\u79ef\u6e10\u53d8 - \u4f60\u53ef\u80fd\u9700\u8981\u8c03\u7528\u4e4b\u524d\u5c06\u5b83\u4eec\u7f6e\u96f6. \u53c2\u6570: * gradient (Tensor, Variable or None): \u8ba1\u7b97\u53d8\u91cf\u7684\u68af\u5ea6. \u5982\u679c\u662f\u5f20\u91cf,\u5219\u4f1a\u81ea\u52a8\u8f6c\u6362 > \u5230\u4e00\u4e2a\u53d8\u91cf,\u8fd9\u662f\u6325\u53d1\u6027\u7684,\u9664\u975e create_graph \u4e3a\u771f.\u6ca1\u6709\u503c\u53ef\u4ee5\u88ab\u6307\u5b9a\u4e3a\u6807\u91cf\u53d8\u91cf\u6216\u90a3\u4e9b \u4e0d\u8981\u6c42\u6bd5\u4e1a. \u5982\u679c\u4e00\u4e2aNone\u503c\u662f\u53ef\u4ee5\u63a5\u53d7\u7684\u8fd9\u4e2a\u53c2\u6570\u662f\u53ef\u9009\u7684. retain_graph (bool, \u53ef\u9009): \u5982\u679c \u201cFalse\u201d ,\u5219\u7528\u4e8e\u8ba1\u7b97\u7684\u56fe\u5f62\u5bfc\u6570\u5c06\u88ab\u91ca\u653e. \u8bf7\u6ce8\u610f,\u5728\u51e0 > \u4e4e\u6240\u6709\u60c5\u51b5\u4e0b\u8bbe\u7f6e\u8fd9\u4e2a\u9009\u9879\u4e3a True \u662f\u4e0d\u9700\u8981\u7684,\u901a\u5e38\u53ef\u4ee5\u89e3\u51b3\u5728\u4e00\u4e2a\u66f4\u6709\u6548\u7684\u65b9\u5f0f. \u9ed8\u8ba4\u503c\u4e3a create_graph . create_graph (bool, optional): \u5982\u679c\u201c\u771f\u201d,\u6d3e\u751f\u56fe\u5c06\u4f1a\u88ab\u6784\u9020,\u5141\u8bb8\u8ba1\u7b97\u66f4\u9ad8\u9636\u7684\u5bfc\u6570. \u9ed8\u8ba4\u4e3a False ,\u9664\u975e gradient \u662f\u4e00\u4e2avolatile\u53d8\u91cf. detach() \u5c06\u4e00\u4e2aVariable\u4ece\u521b\u5efa\u5b83\u7684\u56fe\u4e2d\u5206\u79bb,\u5e76\u628a\u5b83\u8bbe\u7f6e\u6210 leaf variable. \u8fd4\u56de\u53d8\u91cf\u4f7f\u7528\u4e0e\u539f\u59cb\u6570\u636e\u5f20\u91cf\u76f8\u540c\u7684\u6570\u636e\u5f20\u91cf,\u5176\u4e2d\u4efb\u4f55\u4e00\u4e2a\u7684\u5c31\u5730\u4fee\u6539\u90fd\u5c06\u88ab\u770b\u5230,\u5e76\u53ef\u80fd\u89e6\u53d1 \u9519\u8bef\u5728\u6b63\u786e\u6027\u68c0\u67e5. detach_() \u5c06\u4e00\u4e2a Variable \u4ece\u521b\u5efa\u5b83\u7684\u56fe\u4e2d\u5206\u79bb,\u5e76\u628a\u5b83\u8bbe\u7f6e\u6210 leaf variable. register_hook(hook) \u6ce8\u518c\u4e00\u4e2abackward\u94a9\u5b50. \u6bcf\u6b21gradients\u88ab\u8ba1\u7b97\u7684\u65f6\u5019,\u8fd9\u4e2a hook \u90fd\u88ab\u8c03\u7528 .hook \u5e94\u8be5\u62e5\u6709\u4ee5\u4e0b\u7b7e\u540d: hook(grad) -> Variable or None hook\u4e0d\u5e94\u8be5\u4fee\u6539\u5b83\u7684\u8f93\u5165,\u4f46\u662f\u5b83\u53ef\u4ee5\u9009\u62e9\u6027\u7684\u8fd4\u56de\u4e00\u4e2a\u66ff\u4ee3\u5f53\u524d\u68af\u5ea6\u7684\u65b0\u68af\u5ea6. \u8fd9\u4e2a\u51fd\u6570\u8fd4\u56de\u4e00\u4e2a \u53e5\u67c4 (handle).\u5b83\u6709\u4e00\u4e2a\u65b9\u6cd5 handle.remove(),\u53ef\u4ee5\u7528\u8fd9\u4e2a\u65b9\u6cd5\u5c06 hook \u4ece module \u79fb\u9664. Example >>> v = Variable(torch.Tensor([0, 0, 0]), requires_grad=True) >>> h = v.register_hook(lambda grad: grad * 2) # double the gradient >>> v.backward(torch.Tensor([1, 1, 1])) >>> v.grad.data 2 2 2 [torch.FloatTensor of size 3] >>> h.remove() # removes the hook retain_grad() \u4e3a\u975e\u53f6\u53d8\u91cf\u542f\u7528 .grad \u5c5e\u6027. :hidden:\u2019Function(\u51fd\u6570)\u2019 class torch.autograd.Function \u8bb0\u5f55\u64cd\u4f5c\u5386\u53f2\u8bb0\u5f55\u5e76\u5b9a\u4e49\u533a\u5206\u64cd\u4f5c\u7684\u65b9\u6cd5. \u6bcf\u4e2a\u6267\u884c\u5728 Varaibles \u4e0a\u7684 operation \u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a Function \u5bf9\u8c61,\u8fd9\u4e2a Function \u5bf9\u8c61\u6267\u884c\u8ba1\u7b97\u5de5\u4f5c,\u540c\u65f6\u8bb0\u5f55\u4e0b\u6765.\u8fd9\u4e2a\u5386\u53f2\u4ee5\u6709\u5411\u65e0\u73af\u56fe\u7684\u5f62\u5f0f\u4fdd\u5b58\u4e0b\u6765, \u6709\u5411\u56fe\u7684\u8282\u70b9\u4e3a functions ,\u6709\u5411\u56fe\u7684\u8fb9\u4ee3\u8868\u6570\u636e\u4f9d\u8d56\u5173\u7cfb (input<-output).\u4e4b\u540e,\u5f53 backward \u88ab\u8c03\u7528\u7684\u65f6\u5019,\u8ba1\u7b97\u56fe\u4ee5\u62d3\u6251\u987a\u5e8f\u5904\u7406,\u901a\u8fc7\u8c03\u7528\u6bcf\u4e2a Function \u5bf9\u8c61\u7684 backward(), \u540c\u65f6\u5c06\u8fd4\u56de\u7684\u68af\u5ea6\u4f20\u9012\u7ed9\u4e0b\u4e00\u4e2a Function. \u901a\u5e38\u60c5\u51b5\u4e0b,\u7528\u6237\u80fd\u548c Functions \u4ea4\u4e92\u7684\u552f\u4e00\u65b9\u6cd5\u5c31\u662f\u521b\u5efa Function \u7684\u5b50\u7c7b,\u5b9a\u4e49\u65b0\u7684 operation. \u8fd9\u662f\u6269\u5c55 torch.autograd \u7684\u63a8\u8350\u65b9\u6cd5. \u6bcf\u4e2a Function \u53ea\u88ab\u4f7f\u7528\u4e00\u6b21(\u5728forward\u8fc7\u7a0b\u4e2d). \u53c2\u6570\u8bf4\u660e: requires_grad: \u5e03\u5c14\u7c7b\u578b\u4f9d\u8d56\u4e8e\u65b9\u6cd5 backward() \u4f1a\u4e0d\u4f1a\u8fd8\u4f1a\u88ab\u4f7f\u7528. \u6bd4\u5982: >>> class Exp(Function): >>> >>> @staticmethod >>> def forward(ctx, i): >>> result = i.exp() >>> ctx.save_for_backward(result) >>> return result >>> >>> @staticmethod >>> def backward(ctx, grad_output): >>> result, = ctx.saved_variables >>> return grad_output * result static backward ( ctx , *grad_outputs ) [source] \u5b9a\u4e49\u53cd\u5411\u4f20\u64ad\u64cd\u4f5c \u8fd9\u4e2a\u65b9\u6cd5\u5c06\u4f1a\u88ab\u7ee7\u627f\u4ed6\u7684\u6240\u6709\u5b50\u7c7b\u8986\u76d6. \u7b2c\u4e00\u4e2a\u53c2\u6570\u4e3a\u4e0a\u4e0b\u6587\u53c2\u6570, \u63a5\u4e0b\u6765\u53ef\u4ee5\u8f93\u5165\u4efb\u4f55\u5f20\u91cf\u6216\u53d8\u91cf (\u5f20\u91cf\u6216\u5176\u4ed6\u7c7b\u578b), \u5e76\u4e14\u6709\u591a\u4e2a\u8fd4\u56de\u503c, \u5e76\u4e14\u4e3a\u51fd\u6570 forward() \u7684\u8f93\u5165. \u6bcf\u4e2a\u53c2\u6570\u90fd\u662f\u7ed9\u5b9a\u8f93\u51fa\u7684\u5bfc\u6570, \u5e76\u4e14\u6bcf\u4e00\u4e2a\u8f93\u51fa\u90fd\u662f\u8f93\u5165\u7684\u5bfc\u6570. \u4e0a\u4e0b\u6587\u53ef\u4ee5\u7528\u6765\u68c0\u7d22\u8f6c\u53d1\u8fc7\u7a0b\u4e2d\u4fdd\u5b58\u7684\u53d8\u91cf. static forward ( ctx , args_, _ *kwargs ) [source] \u8fdb\u884c\u64cd\u4f5c. \u8fd9\u4e2a\u65b9\u6cd5\u5c06\u4f1a\u88ab\u7ee7\u627f\u4ed6\u7684\u6240\u6709\u5b50\u7c7b\u8986\u76d6. \u7b2c\u4e00\u4e2a\u53c2\u6570\u4e3a\u4e0a\u4e0b\u6587\u53c2\u6570,\u63a5\u4e0b\u6765\u53ef\u4ee5\u8f93\u5165\u4efb\u4f55\u5f20\u91cf\u6216\u53d8\u91cf (\u5f20\u91cf\u6216\u5176\u4ed6\u7c7b\u578b). \u4e0a\u4e0b\u6587\u53ef\u4ee5\u7528\u6765\u5b58\u50a8\u53ef\u4ee5\u5728\u56de\u4f20\u671f\u95f4\u68c0\u7d22\u7684\u53d8\u91cf. Profiler(\u5206\u6790\u5668) Autograd \u5305\u542b\u4e00\u4e2a\u5206\u6790\u5668, \u53ef\u4ee5\u8ba9\u4f60\u68c0\u67e5\u4f60\u7684\u6a21\u578b\u5728CPU \u548c GPU \u4e0a\u4e0d\u540c\u8fd0\u7b97\u7684\u6210\u672c. \u76ee\u524d\u5b9e\u73b0\u6709\u4e24\u79cd\u6a21\u5f0f - \u53ea\u4f7f\u7528 CPU \u7684 profile . \u548c\u57fa\u4e8e nvprof (\u6ce8\u518c CPU \u548c GPU \u6d3b\u52a8) \u7684\u65b9\u5f0f\u4f7f\u7528 emit_nvtx . class torch.autograd.profiler.profile(enabled=True) \u7ed3\u679c\u7684\u8bc4\u4ef7\u6307\u6807. Parameters: enabled ( bool , \u53ef\u9009 ) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a False ,\u5219\u6ca1\u6709\u8bc4\u4ef7\u6307\u6807. Default: True . Example >>> x = Variable(torch.randn(1, 1), requires_grad=True) >>> with torch.autograd.profiler.profile() as prof: ... y = x ** 2 ... y.backward() >>> # NOTE: some columns were removed for brevity ... print(prof) ------------------------------------- --------------- --------------- Name CPU time CUDA time ------------------------------------- --------------- --------------- PowConstant 142.036us 0.000us N5torch8autograd9GraphRootE 63.524us 0.000us PowConstantBackward 184.228us 0.000us MulConstant 50.288us 0.000us PowConstant 28.439us 0.000us Mul 20.154us 0.000us N5torch8autograd14AccumulateGradE 13.790us 0.000us N5torch8autograd5CloneE 4.088us 0.000us export_chrome_trace(path) \u5c06EventList\u5bfc\u51fa\u4e3aChrome\u8ddf\u8e2a\u5de5\u5177\u6587\u4ef6. \u65ad\u70b9\u80fd\u591f\u901a\u8fc7 chrome://tracing URL\u6765\u8bfb\u53d6. Parameters: path ( str ) \u2013 \u5236\u5b9a\u65ad\u70b9\u5199\u7684\u8def\u5f84. key_averages() \u5e73\u5747\u6240\u6709\u7684\u529f\u80fd\u6307\u6807\u901a\u8fc7\u4ed6\u4eec\u7684\u952e. Returns: \u5305\u542b FunctionEventAvg \u5bf9\u8c61\u7684 EventList. table(sort_by=None) \u6253\u5370\u64cd\u4f5c\u8868 Parameters: sort_by ( str , \u53ef\u9009 ) \u2013 \u7528\u6765\u5bf9\u53c2\u6570\u8fdb\u884c\u6392\u5e8f. \u9ed8\u8ba4\u60c5\u51b5\u4e0b,\u5b83\u4eec\u4ee5\u4e0e\u767b\u8bb0\u76f8\u540c\u7684\u987a\u5e8f\u6253\u5370. \u6709\u6548\u7684\u952e: cpu_time , cuda_time , cpu_time_total , cuda_time_total , count . Returns: \u5305\u542b\u8868\u7684\u5b57\u7b26\u4e32. --- --- total_average() \u6240\u6709\u4e8b\u4ef6\u7684\u5e73\u5747\u6307\u6807. Returns: \u4e00\u4e2a FunctionEventAvg \u5bf9\u8c61. class torch.autograd.profiler.emit_nvtx(enabled=True) \u4f7f\u6bcf\u4e2aautograd\u64cd\u4f5c\u90fd\u53d1\u51fa\u4e00\u4e2aNVTX\u8303\u56f4\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668. \u5982\u4e0b\u4f7f\u7528\u662f\u6b63\u786e\u7684: nvprof --profile-from-start off -o trace_name.prof -- <regular command here> \u4e0d\u5e78\u7684\u662f,\u6ca1\u6709\u529e\u6cd5\u5f3a\u5236nvprof\u5237\u65b0\u6536\u96c6\u5230\u7684\u6570\u636e\u5230\u78c1\u76d8,\u56e0\u6b64\u5bf9\u4e8e CUDA \u5206\u6790,\u5fc5\u987b\u4f7f\u7528\u6b64\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u8fdb\u884c\u6ce8\u91ca nvprof \u8ddf\u8e2a\u5e76\u7b49\u5f85\u8fdb\u7a0b\u5728\u68c0\u67e5\u4e4b\u524d\u9000\u51fa. \u7136\u540e,\u53ef\u4ee5\u4f7f\u7528NVIDIA Visual Profiler\uff08nvvp\uff09\u6765\u663e\u793a\u65f6\u95f4\u8f74,\u6216\u8005 torch.autograd.profiler.load_nvprof() \u53ef\u4ee5\u52a0\u8f7d\u68c0\u67e5\u7ed3\u679c. Parameters: enabled ( bool , \u53ef\u9009 ) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a False ,\u5219\u6ca1\u6709\u8bc4\u4ef7\u6307\u6807. \u9ed8\u8ba4: True . Example >>> with torch.cuda.profiler.profile(): ... model(x) # Warmup CUDA memory allocator and profiler ... with torch.autograd.profiler.emit_nvtx(): ... model(x) torch.autograd.profiler.load_nvprof(path) \u6253\u5f00 nvprof trace \u6587\u4ef6. Parameters: path ( str ) \u2013 nvprof trace \u6587\u4ef6\u8def\u5f84.","title":"Automatic differentiation package - torch.autograd"},{"location":"59/#automatic-differentiation-package-torchautograd","text":"torch.autograd \u63d0\u4f9b\u4e86\u7c7b\u548c\u51fd\u6570\u7528\u6765\u5bf9\u4efb\u610f\u6807\u91cf\u51fd\u6570\u8fdb\u884c\u6c42\u5bfc.\u53ea\u9700\u8981\u5bf9\u5df2\u6709\u7684\u4ee3\u7801\u8fdb\u884c\u5fae\u5c0f\u7684\u6539\u53d8-\u53ea\u9700\u8981\u5c06\u6240\u6709\u7684 tensors \u5305\u542b\u5728 Variable \u5bf9\u8c61\u4e2d\u5373\u53ef. torch.autograd.backward(variables, grad_variables=None, retain_graph=None, create_graph=None, retain_variables=None) \u7ed9\u5b9a\u56fe\u67d0\u4e00\u4e2a\u7684\u8282\u70b9\u53d8\u91cfvariables,\u8ba1\u7b97\u5bf9\u8be5\u53d8\u91cf\u6c42\u5bfc\u7684\u68af\u5ea6\u548c. \u8ba1\u7b97\u56fe\u53ef\u4ee5\u901a\u8fc7\u94fe\u5f0f\u6cd5\u5219\u6c42\u5bfc.\u5982\u679c\u4efb\u4f55 variables \u90fd\u662f\u975e\u6807\u91cf(\u6bd4\u5982 \u4ed6\u4eec\u7684 data \u5c5e\u6027\u4e2d\u6709\u591a\u4e2a\u5143\u7d20)\u5e76\u4e14\u9700\u8981\u6c42\u5bfc, \u90a3\u4e48\u6b64\u51fd\u6570\u9700\u8981\u6307\u5b9a grad_variables . \u5b83\u7684\u957f\u5ea6\u5e94\u8be5\u548cvariables\u7684\u957f\u5ea6\u5339\u914d,\u91cc\u9762\u4fdd\u5b58\u4e86\u76f8\u5173 variable \u7684\u68af\u5ea6 (\u5bf9\u4e8e\u4e0d\u9700\u8981 gradient tensor \u7684 variable, \u5e94\u5236\u5b9a\u4e3a None). \u6b64\u51fd\u6570\u7d2f\u79ef\u53f6\u5b50\u8282\u70b9 variables \u8ba1\u7b97\u7684\u68af\u5ea6 - \u8c03\u7528\u6b64\u51fd\u6570\u4e4b\u524d\u5e94\u5148\u5c06\u53f6\u5b50\u8282\u70b9 variables \u68af\u5ea6\u7f6e\u96f6. \u53c2\u6570\u8bf4\u660e: * variables(Variable \u5217\u8868): \u88ab\u6c42\u5fae\u5206\u7684\u53f6\u5b50\u8282\u70b9. * grad_variables ((Tensor,Variable or None)\u5217\u8868):\u5bf9\u5e94 variable \u7684\u68af\u5ea6.\u4efb\u4f55\u5f20\u91cf\u5c06\u81ea\u52a8\u8f6c\u6362\u4e3a\u53d8\u91cf\u9664\u975e create_graph \u662f True . \u6ca1\u6709\u503c\u53ef\u4ee5\u88ab\u6307\u5b9a\u4e3a\u6807\u91cf\u53d8\u91cf\u6216\u8005\u4e0d\u9700\u8981\u88ab\u6c42\u5bfc. \u5982\u679c\u6ca1\u6709\u503c\u88ab\u6240\u6709\u7684grad_variables\u63a5\u53d7, \u90a3\u4e48\u8be5\u53c2\u6570\u662f\u53ef\u4ee5\u88ab\u7701\u7565\u7684. retain_graph (bool, \u53ef\u9009): \u5982\u679c\u662f False , \u8be5\u56fe\u8ba1\u7b97\u8fc7\u7684\u68af\u5ea6\u88ab\u91ca\u653e\u6389.\u6ce8\u610f\u7684\u662f,\u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u90fd\u8bbe\u7f6e\u4e3a True \u5e76\u4e0d\u662f\u5fc5\u987b\u7684\u5e76\u4e14\u80fd\u591f\u9ad8\u6548\u7684\u8ba1\u7b97.\u5c06\u8be5 create_graph \u53c2\u6570\u503c\u8bbe\u7f6e\u4e3a\u9ed8\u8ba4\u5373\u53ef. create_graph (bool, \u53ef\u9009): \u5982\u679c\u662f True , \u5c06\u4f1a\u5efa\u7acb\u4e00\u4e2a\u68af\u5ea6\u56fe, \u7528\u6765\u6c42\u89e3\u9ad8\u9636\u5bfc\u6570. \u9ed8\u8ba4\u4e3a False , \u9664\u975e grad_variables \u62e5\u6709\u4e0d\u6b62\u4e00\u4e2a \u6613\u53d8\u7684 Variable. torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=None, only_inputs=True, allow_unused=False) \u8ba1\u7b97\u5e76\u8fd4\u56de\u7ed9\u5b9a\u503c\u7684\u68af\u5ea6\u7684\u548c. grad_outputs \u662f\u4e00\u4e2a\u5217\u8868\u540c\u65f6\u957f\u5ea6\u4e0e output \u4e00\u6837, \u5b58\u653e\u4e86\u9884\u5148\u8ba1\u7b97 input \u7684\u68af\u5ea6\u7684\u548c. \u5982\u679c output \u4e0d\u9700\u8981\u88ab\u6c42\u5bfc, \u90a3\u4e48\u68af\u5ea6\u5c06\u4e3a None ). \u5f53\u4e0d\u9700\u8981\u6d3e\u751f\u56fe\u65f6,\u53ef\u4ee5\u5c06\u68af\u5ea6\u4f5c\u4e3a\u5f20\u91cf,\u6216\u8005\u4f5c\u4e3a\u53d8\u91cf,\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u56fe\u5c06\u88ab\u521b\u5efa. \u5982\u679c\u53c2\u6570 only_inputs \u4e3a True , \u8be5\u65b9\u6cd5\u5c06\u4f1a\u8fd4\u56de\u7ed9\u5b9a\u8f93\u5165\u7684\u68af\u5ea6\u503c\u5217\u8868.\u5982\u679c\u4e3a False , \u90a3\u4e48\u9057\u7559\u4e0b\u6765\u7684\u6240\u6709\u53f6\u5b50\u8282\u70b9\u7684\u68af\u5ea6\u90fd\u4f1a\u88ab\u8ba1\u7b97, \u88ab\u4e14\u4f1a\u88ab\u5217\u52a0\u5230 .grad \u53c2\u6570\u4e2d. \u53c2\u6570\u8bf4\u660e: * outputs (\u53d8\u91cf\u5e8f\u5217): \u68af\u5ea6\u51fd\u6570\u7684\u8fd4\u56de\u503c. * inputs (\u53d8\u91cf\u5e8f\u5217): \u9700\u8981\u8ba1\u7b97\u7684\u68af\u5ea6\u7684\u8f93\u5165 (\u5e76\u4e14\u4e0d\u4f1a\u88ab\u7d2f\u52a0\u5230 .grad \u53c2\u6570\u4e2d). * grad_outputs (\u5f20\u91cf\u6216\u53d8\u91cf\u5e8f\u5217): \u6bcf\u4e00\u4e2a\u8f93\u51fa\u7684\u68af\u5ea6. \u6240\u6709\u7684\u5f20\u91cf\u90fd\u4f1a\u53d8\u6210\u53d8\u91cf\u5e76\u4e14\u662f\u53ef\u53d8\u7684\u9664\u975e\u53c2\u6570 create_graph \u4e3a True . \u6ca1\u6709\u503c\u53ef\u4ee5\u88ab\u6307\u5b9a\u4e3a\u6807\u91cf\u53d8\u91cf\u6216\u8005\u4e0d\u9700\u8981\u53d8\u5316\u7684\u503c. \u5982\u679c\u6240\u6709 grad_variabls \u90fd\u53ef\u4ee5\u63a5\u53d7 None \u503c,\u90a3\u4e48\u8fd9\u4e2a\u53c2\u6570\u662f\u53ef\u9009\u7684. retain_graph (bool, \u53ef\u9009): \u5982\u679c\u662f False , \u7528\u4e8e\u8ba1\u7b97 grad \u7684\u56fe\u5c06\u88ab\u91ca\u653e. \u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u90fd\u8bbe\u7f6e\u4e3a True \u5e76\u4e0d\u662f\u5fc5\u987b\u7684\u5e76\u4e14\u80fd\u591f\u9ad8\u6548\u5730\u8fd0\u884c. \u9ed8\u8ba4\u4e0e create_graph \u53c2\u6570\u4e00\u6837. create_graph (bool, \u53ef\u9009): \u5982\u679c\u662f True , \u68af\u5ea6\u56fe\u5c06\u4f1a\u88ab\u5efa\u7acb,\u7528\u6765\u6c42\u89e3\u9ad8\u9636\u5bfc\u6570. \u9ed8\u8ba4\u4e3a <cite>False &lt;/cite&gt; , \u9664\u975e\u53c2\u6570 grad_variables` \u5305\u542b\u4e0d\u53ea\u4e00\u4e2a\u53d8\u91cf. only_inputs (bool, \u53ef\u9009): \u5982\u679c\u662f True , \u53f6\u5b50\u8282\u70b9\u7684\u5bfc\u6570\u5c06\u4f1a\u5728\u56fe\u4e2d, \u4f46\u662f\u4e0d\u4f1a\u51fa\u73b0\u5728\u53c2\u6570 inputs \u4e5f\u4e0d\u4f1a\u88ab\u8ba1\u7b97\u4ee5\u53ca\u7d2f\u52a0. \u9ed8\u8ba4\u4e3a True . allow_unused (bool, \u53ef\u9009): \u5982\u679c\u662f False , \u6307\u5b9a\u8ba1\u7b97\u8f93\u51fa\u65f6\u672a\u4f7f\u7528\u7684\u8f93\u5165\uff08\u56e0\u6b64\u5b83\u4eec\u7684 grad \u59cb\u7ec8\u4e3a\u96f6\uff09\u662f\u9519\u8bef\u7684. \u9ed8\u8ba4\u4e3a False .","title":"Automatic differentiation package - torch.autograd"},{"location":"59/#variable","text":"","title":"Variable (\u53d8\u91cf)"},{"location":"59/#api-compatibility","text":"Variable API \u51e0\u4e4e\u4e0e\u5e38\u89c4 Tensor API \u76f8\u540c(\u4e00\u4e9b\u4f1a\u8986\u76d6\u68af\u5ea6\u8ba1\u7b97\u8f93\u5165\u7684\u5185\u7f6e\u65b9\u6cd5\u9664\u5916). \u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b, \u53d8\u91cf\u91cf\u53ef\u4ee5\u5b89\u5168\u5730\u66ff\u6362\u5f20\u91cf\u5e76\u4e14\u4ee3\u7801\u5c06\u4fdd\u6301\u6b63\u5e38\u5de5\u4f5c. \u56e0\u4e3a\u8fd9\u4e2a, \u6211\u4eec\u6ca1\u6709\u8bb0\u5f55\u53d8\u91cf\u4e0a\u7684\u6240\u6709\u64cd\u4f5c, \u4f60\u5e94\u8be5\u53c2\u9605 torch.Tensor \u6587\u6863\u6765\u67e5\u770b\u53d8\u91cf\u4e0a\u7684\u6240\u6709\u64cd\u4f5c.","title":"API compatibility"},{"location":"59/#in-place-operations-on-variables","text":"\u5728 autograd \u652f\u6301\u5c31\u5730\u64cd\u4f5c\u662f\u4e00\u4ef6\u56f0\u96be\u7684\u4e8b\u60c5, \u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6211\u4eec\u4e0d\u9f13\u52b1\u4f7f\u7528. Autograd \u79ef\u6781\u7684\u7f13\u51b2\u533a\u91ca\u653e\u548c\u91cd\u7528\u4f7f\u5f97\u5b83\u975e\u5e38\u9ad8\u6548, \u800c\u4e14\u5f88\u5c11\u6709\u5c31\u5730\u64cd\u4f5c\u5b9e\u9645\u4e0a\u5927\u91cf\u5730\u964d\u4f4e\u4e86\u5185\u5b58\u4f7f\u7528\u91cf\u7684\u60c5\u51b5. \u9664\u975e\u4f60\u6b63\u5728\u5927\u91cf\u7684\u7684\u5185\u5b58\u538b\u529b\u4e0b\u8fd0\u884c, \u5426\u5219\u4f60\u53ef\u80fd\u6c38\u8fdc\u4e0d\u9700\u8981\u4f7f\u7528\u5b83\u4eec.","title":"In-place operations on Variables"},{"location":"59/#in-place-correctness-checks","text":"\u6240\u6709\u7684 Variable \u8ddf\u8e2a\u9002\u7528\u4e8e\u5b83\u4eec\u7684\u5c31\u5730\u64cd\u4f5c, \u5e76\u4e14\u5982\u679c\u5b9e\u73b0\u68c0\u6d4b\u5230\u4e00\u4e2a\u53d8\u91cf\u662f\u5426\u88ab\u5176\u4e2d\u4e00\u4e2a\u51fd\u6570\u540e\u53f0\u4fdd\u5b58, \u4f46\u662f\u4e4b\u540e\u5b83\u88ab\u5c31\u5730\u4fee\u6539\u4e86, \u4f1a\u5728\u5f00\u59cb\u6c42\u5bfc\u65f6\u4f1a\u62a5\u51fa\u5f02\u5e38. \u8fd9\u786e\u4fdd\u4e86\u5982\u679c\u4f60\u5728\u5c31\u5730\u4f7f\u7528\u51fd\u6570\u5e76\u6ca1\u6709\u770b\u5230\u4efb\u4f55\u9519\u8bef, \u4f60\u53ef\u4ee5\u80af\u5b9a\u7684\u662f\u8ba1\u7b97\u53d8\u91cf\u662f\u6b63\u786e\u7684. class torch.autograd.Variable \u5c01\u88c5\u4e00\u4e2a\u5f20\u91cf\u7528\u6765\u5404\u79cd\u64cd\u4f5c. \u53d8\u91cf\u662f\u5f20\u91cf\u5bf9\u8c61\u5468\u56f4\u7684\u8f7b\u5305\u88c5,\u80fd\u591f\u62e5\u6709\u5bfc\u6570\u7b49\u6570\u636e, \u8fd9\u4e2a\u5f15\u7528\u5141\u8bb8\u56de\u6eaf\u6574\u4e2a\u64cd\u4f5c\u94fe\u521b\u5efa\u6570\u636e. \u5982\u679c\u53d8\u91cf\u5df2\u7ecf\u7531\u7528\u6237\u521b\u5efa, \u5b83\u7684 grad_fn \u4e3a None \u6211\u4eec\u79f0\u4e4b\u4e3a\u53f6\u5b50\u8282\u70b9. \u7531\u4e8e autograd \u53ea\u652f\u6301\u6807\u91cf\u503c\u51fd\u6570\u5fae\u5206, grad \u5927\u5c0f\u59cb\u7ec8\u4e0e\u6570\u636e\u5927\u5c0f\u5339\u914d. \u6b64\u5916,\u5bfc\u6570\u901a\u5e38\u53ea\u5206\u914d \u53f6\u53d8\u91cf,\u5426\u5219\u5c06\u59cb\u7ec8\u4e3a\u96f6. \u53c2\u6570\u8bf4\u660e: * data: \u5305\u88f9\u4efb\u4f55\u7c7b\u578b\u7684\u5f20\u91cf. * grad: \u53d8\u91cf\u4fdd\u6301\u7c7b\u578b\u548c\u4f4d\u7f6e\u5339\u914d\u7684\u53d8\u91cf .data . \u8fd9\u4e2a\u5c5e\u6027\u662f\u61d2\u60f0\u7684\u5206\u914d,\u4e0d\u80fd\u88ab\u91cd\u65b0\u5206\u914d. * requires_grad: \u6307\u793a\u53d8\u91cf\u662f\u5426\u5df2\u88ab\u4f7f\u7528\u7684\u5e03\u5c14\u503c\u7531\u5305\u542b\u4efb\u4f55\u53d8\u91cf\u7684\u5b50\u56fe\u521b\u5efa,\u9700\u8981\u5b83. \u6709\u5173\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f,\u8bf7\u53c2\u9605 excluded-subgraphs.\u53ea\u80fd\u5728\u53f6\u53d8\u91cf\u4e0a\u8fdb\u884c\u66f4\u6539. * volatile: \u5e03\u5c14\u503c\u8868\u793a\u5e94\u8be5\u4f7f\u7528\u53d8\u91cf\u63a8\u7406\u6a21\u5f0f,\u5373\u4e0d\u4fdd\u5b58\u5386\u53f2. \u67e5\u770b \u53cd\u5411\u6392\u9664 subgraphs (\u5b50\u56fe) \u66f4\u591a\u7ec6\u8282. \u53ea\u80fd\u5728\u53f6\u53d8\u91cf\u4e0a\u8fdb\u884c\u66f4\u6539. * is_leaf: \u6307\u793a\u662f\u5426\u4e3a\u53f6\u5b50\u8282\u70b9,\u5373\u662f\u5426\u7531\u7528\u6237\u521b\u5efa\u7684\u8282\u70b9. * grad_fn: \u5bfc\u6570\u51fd\u6570\u8ddf\u8e2a. \u53c2\u6570: * data (any tensor class): \u7528\u6765\u5305\u88c5\u7684\u5f20\u91cf. * requires_grad (bool): \u6307\u793a\u662f\u5426\u8981\u88ab\u6c42\u5bfc. Keyword only. * volatile (bool): \u6307\u793a\u662f\u5426\u53ef\u53d8. Keyword only. backward(gradient=None, retain_graph=None, create_graph=None, retain_variables=None) \u7ed9\u5b9a\u56fe\u53f6\u5b50\u8282\u70b9\u8ba1\u7b97\u5bfc\u6570. \u8be5\u56fe\u4f7f\u7528\u94fe\u5f0f\u89c4\u5219\u8fdb\u884c\u8ba1\u7b97. \u5982\u679c\u53d8\u91cf\u662f\u975e\u6807\u91cf\uff08\u5373\u5176\u6570\u636e\u5177\u6709\u591a\u4e2a\u5143\u7d20\uff09\u5e76\u4e14\u9700\u8981 \u6539\u53d8,\u8be5\u529f\u80fd\u53e6\u5916\u9700\u8981\u6307\u5b9a\u201c\u68af\u5ea6\u201d.\u5b83\u5e94\u8be5\u662f\u4e00\u4e2a\u5305\u542b\u5339\u914d\u7c7b\u578b\u548c\u4f4d\u7f6e\u7684\u5f20\u91cf \u5fae\u5206\u51fd\u6570\u7684\u68af\u5ea6w.r.t. self . \u8fd9\u4e2a\u529f\u80fd\u5728\u53f6\u5b50\u4e0a\u7d2f\u79ef\u6e10\u53d8 - \u4f60\u53ef\u80fd\u9700\u8981\u8c03\u7528\u4e4b\u524d\u5c06\u5b83\u4eec\u7f6e\u96f6. \u53c2\u6570: * gradient (Tensor, Variable or None): \u8ba1\u7b97\u53d8\u91cf\u7684\u68af\u5ea6. \u5982\u679c\u662f\u5f20\u91cf,\u5219\u4f1a\u81ea\u52a8\u8f6c\u6362 > \u5230\u4e00\u4e2a\u53d8\u91cf,\u8fd9\u662f\u6325\u53d1\u6027\u7684,\u9664\u975e create_graph \u4e3a\u771f.\u6ca1\u6709\u503c\u53ef\u4ee5\u88ab\u6307\u5b9a\u4e3a\u6807\u91cf\u53d8\u91cf\u6216\u90a3\u4e9b \u4e0d\u8981\u6c42\u6bd5\u4e1a. \u5982\u679c\u4e00\u4e2aNone\u503c\u662f\u53ef\u4ee5\u63a5\u53d7\u7684\u8fd9\u4e2a\u53c2\u6570\u662f\u53ef\u9009\u7684. retain_graph (bool, \u53ef\u9009): \u5982\u679c \u201cFalse\u201d ,\u5219\u7528\u4e8e\u8ba1\u7b97\u7684\u56fe\u5f62\u5bfc\u6570\u5c06\u88ab\u91ca\u653e. \u8bf7\u6ce8\u610f,\u5728\u51e0 > \u4e4e\u6240\u6709\u60c5\u51b5\u4e0b\u8bbe\u7f6e\u8fd9\u4e2a\u9009\u9879\u4e3a True \u662f\u4e0d\u9700\u8981\u7684,\u901a\u5e38\u53ef\u4ee5\u89e3\u51b3\u5728\u4e00\u4e2a\u66f4\u6709\u6548\u7684\u65b9\u5f0f. \u9ed8\u8ba4\u503c\u4e3a create_graph . create_graph (bool, optional): \u5982\u679c\u201c\u771f\u201d,\u6d3e\u751f\u56fe\u5c06\u4f1a\u88ab\u6784\u9020,\u5141\u8bb8\u8ba1\u7b97\u66f4\u9ad8\u9636\u7684\u5bfc\u6570. \u9ed8\u8ba4\u4e3a False ,\u9664\u975e gradient \u662f\u4e00\u4e2avolatile\u53d8\u91cf. detach() \u5c06\u4e00\u4e2aVariable\u4ece\u521b\u5efa\u5b83\u7684\u56fe\u4e2d\u5206\u79bb,\u5e76\u628a\u5b83\u8bbe\u7f6e\u6210 leaf variable. \u8fd4\u56de\u53d8\u91cf\u4f7f\u7528\u4e0e\u539f\u59cb\u6570\u636e\u5f20\u91cf\u76f8\u540c\u7684\u6570\u636e\u5f20\u91cf,\u5176\u4e2d\u4efb\u4f55\u4e00\u4e2a\u7684\u5c31\u5730\u4fee\u6539\u90fd\u5c06\u88ab\u770b\u5230,\u5e76\u53ef\u80fd\u89e6\u53d1 \u9519\u8bef\u5728\u6b63\u786e\u6027\u68c0\u67e5. detach_() \u5c06\u4e00\u4e2a Variable \u4ece\u521b\u5efa\u5b83\u7684\u56fe\u4e2d\u5206\u79bb,\u5e76\u628a\u5b83\u8bbe\u7f6e\u6210 leaf variable. register_hook(hook) \u6ce8\u518c\u4e00\u4e2abackward\u94a9\u5b50. \u6bcf\u6b21gradients\u88ab\u8ba1\u7b97\u7684\u65f6\u5019,\u8fd9\u4e2a hook \u90fd\u88ab\u8c03\u7528 .hook \u5e94\u8be5\u62e5\u6709\u4ee5\u4e0b\u7b7e\u540d: hook(grad) -> Variable or None hook\u4e0d\u5e94\u8be5\u4fee\u6539\u5b83\u7684\u8f93\u5165,\u4f46\u662f\u5b83\u53ef\u4ee5\u9009\u62e9\u6027\u7684\u8fd4\u56de\u4e00\u4e2a\u66ff\u4ee3\u5f53\u524d\u68af\u5ea6\u7684\u65b0\u68af\u5ea6. \u8fd9\u4e2a\u51fd\u6570\u8fd4\u56de\u4e00\u4e2a \u53e5\u67c4 (handle).\u5b83\u6709\u4e00\u4e2a\u65b9\u6cd5 handle.remove(),\u53ef\u4ee5\u7528\u8fd9\u4e2a\u65b9\u6cd5\u5c06 hook \u4ece module \u79fb\u9664. Example >>> v = Variable(torch.Tensor([0, 0, 0]), requires_grad=True) >>> h = v.register_hook(lambda grad: grad * 2) # double the gradient >>> v.backward(torch.Tensor([1, 1, 1])) >>> v.grad.data 2 2 2 [torch.FloatTensor of size 3] >>> h.remove() # removes the hook retain_grad() \u4e3a\u975e\u53f6\u53d8\u91cf\u542f\u7528 .grad \u5c5e\u6027.","title":"In-place correctness checks"},{"location":"59/#hiddenfunction","text":"class torch.autograd.Function \u8bb0\u5f55\u64cd\u4f5c\u5386\u53f2\u8bb0\u5f55\u5e76\u5b9a\u4e49\u533a\u5206\u64cd\u4f5c\u7684\u65b9\u6cd5. \u6bcf\u4e2a\u6267\u884c\u5728 Varaibles \u4e0a\u7684 operation \u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a Function \u5bf9\u8c61,\u8fd9\u4e2a Function \u5bf9\u8c61\u6267\u884c\u8ba1\u7b97\u5de5\u4f5c,\u540c\u65f6\u8bb0\u5f55\u4e0b\u6765.\u8fd9\u4e2a\u5386\u53f2\u4ee5\u6709\u5411\u65e0\u73af\u56fe\u7684\u5f62\u5f0f\u4fdd\u5b58\u4e0b\u6765, \u6709\u5411\u56fe\u7684\u8282\u70b9\u4e3a functions ,\u6709\u5411\u56fe\u7684\u8fb9\u4ee3\u8868\u6570\u636e\u4f9d\u8d56\u5173\u7cfb (input<-output).\u4e4b\u540e,\u5f53 backward \u88ab\u8c03\u7528\u7684\u65f6\u5019,\u8ba1\u7b97\u56fe\u4ee5\u62d3\u6251\u987a\u5e8f\u5904\u7406,\u901a\u8fc7\u8c03\u7528\u6bcf\u4e2a Function \u5bf9\u8c61\u7684 backward(), \u540c\u65f6\u5c06\u8fd4\u56de\u7684\u68af\u5ea6\u4f20\u9012\u7ed9\u4e0b\u4e00\u4e2a Function. \u901a\u5e38\u60c5\u51b5\u4e0b,\u7528\u6237\u80fd\u548c Functions \u4ea4\u4e92\u7684\u552f\u4e00\u65b9\u6cd5\u5c31\u662f\u521b\u5efa Function \u7684\u5b50\u7c7b,\u5b9a\u4e49\u65b0\u7684 operation. \u8fd9\u662f\u6269\u5c55 torch.autograd \u7684\u63a8\u8350\u65b9\u6cd5. \u6bcf\u4e2a Function \u53ea\u88ab\u4f7f\u7528\u4e00\u6b21(\u5728forward\u8fc7\u7a0b\u4e2d). \u53c2\u6570\u8bf4\u660e: requires_grad: \u5e03\u5c14\u7c7b\u578b\u4f9d\u8d56\u4e8e\u65b9\u6cd5 backward() \u4f1a\u4e0d\u4f1a\u8fd8\u4f1a\u88ab\u4f7f\u7528. \u6bd4\u5982: >>> class Exp(Function): >>> >>> @staticmethod >>> def forward(ctx, i): >>> result = i.exp() >>> ctx.save_for_backward(result) >>> return result >>> >>> @staticmethod >>> def backward(ctx, grad_output): >>> result, = ctx.saved_variables >>> return grad_output * result static backward ( ctx , *grad_outputs ) [source] \u5b9a\u4e49\u53cd\u5411\u4f20\u64ad\u64cd\u4f5c \u8fd9\u4e2a\u65b9\u6cd5\u5c06\u4f1a\u88ab\u7ee7\u627f\u4ed6\u7684\u6240\u6709\u5b50\u7c7b\u8986\u76d6. \u7b2c\u4e00\u4e2a\u53c2\u6570\u4e3a\u4e0a\u4e0b\u6587\u53c2\u6570, \u63a5\u4e0b\u6765\u53ef\u4ee5\u8f93\u5165\u4efb\u4f55\u5f20\u91cf\u6216\u53d8\u91cf (\u5f20\u91cf\u6216\u5176\u4ed6\u7c7b\u578b), \u5e76\u4e14\u6709\u591a\u4e2a\u8fd4\u56de\u503c, \u5e76\u4e14\u4e3a\u51fd\u6570 forward() \u7684\u8f93\u5165. \u6bcf\u4e2a\u53c2\u6570\u90fd\u662f\u7ed9\u5b9a\u8f93\u51fa\u7684\u5bfc\u6570, \u5e76\u4e14\u6bcf\u4e00\u4e2a\u8f93\u51fa\u90fd\u662f\u8f93\u5165\u7684\u5bfc\u6570. \u4e0a\u4e0b\u6587\u53ef\u4ee5\u7528\u6765\u68c0\u7d22\u8f6c\u53d1\u8fc7\u7a0b\u4e2d\u4fdd\u5b58\u7684\u53d8\u91cf. static forward ( ctx , args_, _ *kwargs ) [source] \u8fdb\u884c\u64cd\u4f5c. \u8fd9\u4e2a\u65b9\u6cd5\u5c06\u4f1a\u88ab\u7ee7\u627f\u4ed6\u7684\u6240\u6709\u5b50\u7c7b\u8986\u76d6. \u7b2c\u4e00\u4e2a\u53c2\u6570\u4e3a\u4e0a\u4e0b\u6587\u53c2\u6570,\u63a5\u4e0b\u6765\u53ef\u4ee5\u8f93\u5165\u4efb\u4f55\u5f20\u91cf\u6216\u53d8\u91cf (\u5f20\u91cf\u6216\u5176\u4ed6\u7c7b\u578b). \u4e0a\u4e0b\u6587\u53ef\u4ee5\u7528\u6765\u5b58\u50a8\u53ef\u4ee5\u5728\u56de\u4f20\u671f\u95f4\u68c0\u7d22\u7684\u53d8\u91cf.","title":":hidden:\u2019Function(\u51fd\u6570)\u2019"},{"location":"59/#profiler","text":"Autograd \u5305\u542b\u4e00\u4e2a\u5206\u6790\u5668, \u53ef\u4ee5\u8ba9\u4f60\u68c0\u67e5\u4f60\u7684\u6a21\u578b\u5728CPU \u548c GPU \u4e0a\u4e0d\u540c\u8fd0\u7b97\u7684\u6210\u672c. \u76ee\u524d\u5b9e\u73b0\u6709\u4e24\u79cd\u6a21\u5f0f - \u53ea\u4f7f\u7528 CPU \u7684 profile . \u548c\u57fa\u4e8e nvprof (\u6ce8\u518c CPU \u548c GPU \u6d3b\u52a8) \u7684\u65b9\u5f0f\u4f7f\u7528 emit_nvtx . class torch.autograd.profiler.profile(enabled=True) \u7ed3\u679c\u7684\u8bc4\u4ef7\u6307\u6807. Parameters: enabled ( bool , \u53ef\u9009 ) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a False ,\u5219\u6ca1\u6709\u8bc4\u4ef7\u6307\u6807. Default: True . Example >>> x = Variable(torch.randn(1, 1), requires_grad=True) >>> with torch.autograd.profiler.profile() as prof: ... y = x ** 2 ... y.backward() >>> # NOTE: some columns were removed for brevity ... print(prof) ------------------------------------- --------------- --------------- Name CPU time CUDA time ------------------------------------- --------------- --------------- PowConstant 142.036us 0.000us N5torch8autograd9GraphRootE 63.524us 0.000us PowConstantBackward 184.228us 0.000us MulConstant 50.288us 0.000us PowConstant 28.439us 0.000us Mul 20.154us 0.000us N5torch8autograd14AccumulateGradE 13.790us 0.000us N5torch8autograd5CloneE 4.088us 0.000us export_chrome_trace(path) \u5c06EventList\u5bfc\u51fa\u4e3aChrome\u8ddf\u8e2a\u5de5\u5177\u6587\u4ef6. \u65ad\u70b9\u80fd\u591f\u901a\u8fc7 chrome://tracing URL\u6765\u8bfb\u53d6. Parameters: path ( str ) \u2013 \u5236\u5b9a\u65ad\u70b9\u5199\u7684\u8def\u5f84. key_averages() \u5e73\u5747\u6240\u6709\u7684\u529f\u80fd\u6307\u6807\u901a\u8fc7\u4ed6\u4eec\u7684\u952e. Returns: \u5305\u542b FunctionEventAvg \u5bf9\u8c61\u7684 EventList. table(sort_by=None) \u6253\u5370\u64cd\u4f5c\u8868 Parameters: sort_by ( str , \u53ef\u9009 ) \u2013 \u7528\u6765\u5bf9\u53c2\u6570\u8fdb\u884c\u6392\u5e8f. \u9ed8\u8ba4\u60c5\u51b5\u4e0b,\u5b83\u4eec\u4ee5\u4e0e\u767b\u8bb0\u76f8\u540c\u7684\u987a\u5e8f\u6253\u5370. \u6709\u6548\u7684\u952e: cpu_time , cuda_time , cpu_time_total , cuda_time_total , count . Returns: \u5305\u542b\u8868\u7684\u5b57\u7b26\u4e32. --- --- total_average() \u6240\u6709\u4e8b\u4ef6\u7684\u5e73\u5747\u6307\u6807. Returns: \u4e00\u4e2a FunctionEventAvg \u5bf9\u8c61. class torch.autograd.profiler.emit_nvtx(enabled=True) \u4f7f\u6bcf\u4e2aautograd\u64cd\u4f5c\u90fd\u53d1\u51fa\u4e00\u4e2aNVTX\u8303\u56f4\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668. \u5982\u4e0b\u4f7f\u7528\u662f\u6b63\u786e\u7684: nvprof --profile-from-start off -o trace_name.prof -- <regular command here> \u4e0d\u5e78\u7684\u662f,\u6ca1\u6709\u529e\u6cd5\u5f3a\u5236nvprof\u5237\u65b0\u6536\u96c6\u5230\u7684\u6570\u636e\u5230\u78c1\u76d8,\u56e0\u6b64\u5bf9\u4e8e CUDA \u5206\u6790,\u5fc5\u987b\u4f7f\u7528\u6b64\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u8fdb\u884c\u6ce8\u91ca nvprof \u8ddf\u8e2a\u5e76\u7b49\u5f85\u8fdb\u7a0b\u5728\u68c0\u67e5\u4e4b\u524d\u9000\u51fa. \u7136\u540e,\u53ef\u4ee5\u4f7f\u7528NVIDIA Visual Profiler\uff08nvvp\uff09\u6765\u663e\u793a\u65f6\u95f4\u8f74,\u6216\u8005 torch.autograd.profiler.load_nvprof() \u53ef\u4ee5\u52a0\u8f7d\u68c0\u67e5\u7ed3\u679c. Parameters: enabled ( bool , \u53ef\u9009 ) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a False ,\u5219\u6ca1\u6709\u8bc4\u4ef7\u6307\u6807. \u9ed8\u8ba4: True . Example >>> with torch.cuda.profiler.profile(): ... model(x) # Warmup CUDA memory allocator and profiler ... with torch.autograd.profiler.emit_nvtx(): ... model(x) torch.autograd.profiler.load_nvprof(path) \u6253\u5f00 nvprof trace \u6587\u4ef6. Parameters: path ( str ) \u2013 nvprof trace \u6587\u4ef6\u8def\u5f84.","title":"Profiler(\u5206\u6790\u5668)"},{"location":"6/","text":"\u53ef\u9009: \u6570\u636e\u5e76\u884c \u4f5c\u8005 : Sung Kim \u548c Jenny Kang \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u4f1a\u5b66\u4e60\u5982\u4f55\u5728\u591a\u4e2aGPU\u4e0a\u4f7f\u7528 DataParallel . \u5728 PyTorch \u4e2d\u4f7f\u7528 GPU \u662f\u4e00\u4ef6\u5f88\u5bb9\u6613\u7684\u4e8b\u60c5.\u4f60\u53ef\u4ee5\u50cf\u4e0b\u9762\u8fd9\u6837\u8f7b\u677e\u7684\u5c06\u4e00\u4e2a\u6a21\u578b\u5206\u914d\u5230\u4e00\u4e2a GPU \u4e0a. model.gpu() \u968f\u540e, \u4f60\u53ef\u4ee5\u5c06\u4f60\u7684\u6240\u6709\u5f20\u91cf\u62f7\u8d1d\u5230\u4e0a\u9762\u7684GPU: mytensor = my_tensor.gpu() \u6b64\u5904\u8bf7\u6ce8\u610f: \u5982\u679c\u53ea\u662f\u8c03\u7528 mytensor.gpu() \u662f\u4e0d\u4f1a\u5c06\u5f20\u91cf\u62f7\u8d1d\u5230 GPU \u7684.\u4f60\u9700\u8981\u5c06\u5b83\u8d4b\u7ed9\u4e00\u4e2a \u65b0\u7684\u5f20\u91cf, \u8fd9\u4e2a\u5f20\u91cf\u5c31\u80fd\u5728 GPU \u4e0a\u4f7f\u7528\u4e86. \u5728\u591a\u4e2a GPU \u4e0a\u8fd0\u884c\u524d\u5411\u3001\u53cd\u5411\u4f20\u64ad\u662f\u4e00\u4ef6\u5f88\u81ea\u7136\u7684\u4e8b\u60c5, \u7136\u800c, PyTorch \u9ed8\u8ba4\u60c5\u51b5\u4e0b\u53ea\u4f1a\u7528\u5230\u4e00\u4e2aGPU, \u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 DataParallel \u4f7f\u4f60\u7684\u6a21\u578b\u5e76\u884c\u8fd0\u884c, \u5728\u591a\u4e2aGPU\u4e0a\u8fd0\u884c\u8fd9\u4e9b\u64cd\u4f5c\u4e5f\u5c06\u53d8\u5f97\u975e\u5e38\u7b80\u5355: model = nn.DataParallel(model) \u8fd9\u662f\u6559\u7a0b\u7684\u6838\u5fc3\u5185\u5bb9, \u6211\u4eec\u5c06\u5728\u968f\u540e\u8fdb\u884c\u8be6\u7ec6\u8bb2\u89e3 \u5bfc\u5165\u548c\u53c2\u6570 \u5bfc\u5165PyTorch\u6a21\u5757\u548c\u53c2\u6570\u5b9a\u4e49 import torch import torch.nn as nn from torch.autograd import Variable from torch.utils.data import Dataset, DataLoader # \u53c2\u6570\u548c\u6570\u636e\u52a0\u8f7d input_size = 5 output_size = 2 batch_size = 30 data_size = 100 \u4f2a\u6570\u636e\u96c6 \u53ea\u9700\u8981\u5b9e\u73b0 getitem \u5c31\u53ef\u4ee5\u8f7b\u677e\u7684\u751f\u6210\u4e00\u4e2a\uff08\u968f\u673a\uff09\u4f2a\u6570\u636e\u96c6, \u5982\u4e0b\u4ee3\u7801\u6240\u793a: class RandomDataset(Dataset): def __init__(self, size, length): self.len = length self.data = torch.randn(length, size) def __getitem__(self, index): return self.data[index] def __len__(self): return self.len rand_loader = DataLoader(dataset=RandomDataset(input_size, 100), batch_size=batch_size, shuffle=True) \u7b80\u5355\u6a21\u578b \u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d, \u6211\u4eec\u7684\u6a21\u578b\u53ea\u9700\u8981\u4e00\u4e2a\u8f93\u5165\u5e76\u4e14\u5b8c\u6210\u4e00\u4e2a\u7ebf\u6027\u64cd\u4f5c, \u6700\u540e\u5f97 \u5230\u4e00\u4e2a\u8f93\u51fa.\u5f53\u7136, \u4f60\u53ef\u4ee5\u5728\u4efb\u610f\u6a21\u578b (CNN,RNN,Capsule Net\u7b49) \u8fd0\u7528 DataParallel \u6211\u4eec\u5728\u6a21\u578b\u4e2d\u8bbe\u7f6e\u4e86\u6253\u5370\u6307\u4ee4\u6765\u76d1\u63a7\u8f93\u5165\u548c\u8f93\u51fa\u7684\u5f20\u91cf\u5927\u5c0f, \u8bf7\u6ce8\u610f\u6279\u6570\u636e\u6b21\u5e8f\u4e3a0\u65f6\u7684\u8f93\u51fa. class Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(\" In Model: input size\", input.size(), \"output size\", output.size()) return output \u521b\u5efa\u6a21\u578b\u548c DataParallel \u8fd9\u662f\u672c\u6559\u7a0b\u7684\u6838\u5fc3\u90e8\u5206. \u9996\u5148, \u6211\u4eec\u9700\u8981\u751f\u6210\u4e00\u4e2a\u6a21\u578b\u7684\u5b9e\u4f8b\u5e76\u4e14\u68c0\u6d4b\u6211\u4eec\u662f\u5426\u62e5\u6709\u591a\u4e2a GPU.\u5982\u679c\u6709\u591a\u4e2aGPU , \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 nn.DataParallel \u6765\u5305\u88c5\u6211\u4eec\u7684\u6a21\u578b, \u7136\u540e\u6211\u4eec \u5c31\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7 model.gpu() \u65bd\u52a0\u4e8e\u8fd9\u4e9bGPU\u4e0a. model = Model(input_size, output_size) if torch.cuda.device_count() > 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn.DataParallel(model) if torch.cuda.is_available(): model.cuda() \u8fd0\u884c\u6a21\u578b \u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e86. for data in rand_loader: if torch.cuda.is_available(): input_var = Variable(data.cuda()) else: input_var = Variable(data) output = model(input_var) print(\"Outside: input size\", input_var.size(), \"output_size\", output.size()) \u7ed3\u679c \u5f53\u6211\u4eec\u5c06\u8f93\u5165\u8bbe\u7f6e\u4e3a30\u6279, \u6a21\u578b\u4e5f\u4ea7\u751f\u4e8630\u6279\u7684\u8f93\u51fa.\u4f46\u662f\u5f53\u6211\u4eec\u4f7f\u7528\u591a\u4e2aGPU, \u7136\u540e\u4f60 \u4f1a\u5f97\u5230\u7c7b\u4f3c\u4e0b\u9762\u8fd9\u6837\u7684\u8f93\u51fa. 2 GPUs \u5982\u679c\u67092\u4e2aGPU, \u6211\u4eec\u5c06\u4f1a\u770b\u5230\u8fd9\u6837\u7684\u7ed3\u679c: # on 2 GPUs Let's use 2 GPUs! In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 3 GPUs \u5982\u679c\u67093\u4e2aGPU, \u6211\u4eec\u5c06\u4f1a\u770b\u5230\u8fd9\u6837\u7684\u7ed3\u679c: Let's use 3 GPUs! In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 8 GPUs \u5982\u679c\u67098\u4e2aGPU, \u6211\u4eec\u5c06\u4f1a\u770b\u5230\u8fd9\u6837\u7684\u7ed3\u679c: Let's use 8 GPUs! In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) \u603b\u7ed3 DataParallel \u81ea\u52a8\u5730\u5c06\u6570\u636e\u5206\u5272\u5e76\u4e14\u5c06\u4efb\u52a1\u9001\u5165\u591a\u4e2aGPU\u4e0a\u7684\u591a\u4e2a\u6a21\u578b\u4e2d\u8fdb\u884c\u5904\u7406. \u5728\u6bcf\u4e2a\u6a21\u578b\u5b8c\u6210\u4efb\u52a1\u540e, DataParallel \u91c7\u96c6\u548c\u5408\u5e76\u6240\u6709\u7ed3\u679c, \u5e76\u5c06\u6700\u540e\u7684\u7ed3\u679c\u5448\u73b0\u7ed9\u4f60. \u60f3\u4e86\u89e3\u66f4\u591a\u4fe1\u606f, \u8bf7\u70b9\u51fb: http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html . Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: data_parallel_tutorial.py Download Jupyter notebook: data_parallel_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u53ef\u9009: \u6570\u636e\u5e76\u884c"},{"location":"6/#_1","text":"\u4f5c\u8005 : Sung Kim \u548c Jenny Kang \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u4f1a\u5b66\u4e60\u5982\u4f55\u5728\u591a\u4e2aGPU\u4e0a\u4f7f\u7528 DataParallel . \u5728 PyTorch \u4e2d\u4f7f\u7528 GPU \u662f\u4e00\u4ef6\u5f88\u5bb9\u6613\u7684\u4e8b\u60c5.\u4f60\u53ef\u4ee5\u50cf\u4e0b\u9762\u8fd9\u6837\u8f7b\u677e\u7684\u5c06\u4e00\u4e2a\u6a21\u578b\u5206\u914d\u5230\u4e00\u4e2a GPU \u4e0a. model.gpu() \u968f\u540e, \u4f60\u53ef\u4ee5\u5c06\u4f60\u7684\u6240\u6709\u5f20\u91cf\u62f7\u8d1d\u5230\u4e0a\u9762\u7684GPU: mytensor = my_tensor.gpu() \u6b64\u5904\u8bf7\u6ce8\u610f: \u5982\u679c\u53ea\u662f\u8c03\u7528 mytensor.gpu() \u662f\u4e0d\u4f1a\u5c06\u5f20\u91cf\u62f7\u8d1d\u5230 GPU \u7684.\u4f60\u9700\u8981\u5c06\u5b83\u8d4b\u7ed9\u4e00\u4e2a \u65b0\u7684\u5f20\u91cf, \u8fd9\u4e2a\u5f20\u91cf\u5c31\u80fd\u5728 GPU \u4e0a\u4f7f\u7528\u4e86. \u5728\u591a\u4e2a GPU \u4e0a\u8fd0\u884c\u524d\u5411\u3001\u53cd\u5411\u4f20\u64ad\u662f\u4e00\u4ef6\u5f88\u81ea\u7136\u7684\u4e8b\u60c5, \u7136\u800c, PyTorch \u9ed8\u8ba4\u60c5\u51b5\u4e0b\u53ea\u4f1a\u7528\u5230\u4e00\u4e2aGPU, \u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 DataParallel \u4f7f\u4f60\u7684\u6a21\u578b\u5e76\u884c\u8fd0\u884c, \u5728\u591a\u4e2aGPU\u4e0a\u8fd0\u884c\u8fd9\u4e9b\u64cd\u4f5c\u4e5f\u5c06\u53d8\u5f97\u975e\u5e38\u7b80\u5355: model = nn.DataParallel(model) \u8fd9\u662f\u6559\u7a0b\u7684\u6838\u5fc3\u5185\u5bb9, \u6211\u4eec\u5c06\u5728\u968f\u540e\u8fdb\u884c\u8be6\u7ec6\u8bb2\u89e3","title":"\u53ef\u9009: \u6570\u636e\u5e76\u884c"},{"location":"6/#_2","text":"\u5bfc\u5165PyTorch\u6a21\u5757\u548c\u53c2\u6570\u5b9a\u4e49 import torch import torch.nn as nn from torch.autograd import Variable from torch.utils.data import Dataset, DataLoader # \u53c2\u6570\u548c\u6570\u636e\u52a0\u8f7d input_size = 5 output_size = 2 batch_size = 30 data_size = 100","title":"\u5bfc\u5165\u548c\u53c2\u6570"},{"location":"6/#_3","text":"\u53ea\u9700\u8981\u5b9e\u73b0 getitem \u5c31\u53ef\u4ee5\u8f7b\u677e\u7684\u751f\u6210\u4e00\u4e2a\uff08\u968f\u673a\uff09\u4f2a\u6570\u636e\u96c6, \u5982\u4e0b\u4ee3\u7801\u6240\u793a: class RandomDataset(Dataset): def __init__(self, size, length): self.len = length self.data = torch.randn(length, size) def __getitem__(self, index): return self.data[index] def __len__(self): return self.len rand_loader = DataLoader(dataset=RandomDataset(input_size, 100), batch_size=batch_size, shuffle=True)","title":"\u4f2a\u6570\u636e\u96c6"},{"location":"6/#_4","text":"\u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d, \u6211\u4eec\u7684\u6a21\u578b\u53ea\u9700\u8981\u4e00\u4e2a\u8f93\u5165\u5e76\u4e14\u5b8c\u6210\u4e00\u4e2a\u7ebf\u6027\u64cd\u4f5c, \u6700\u540e\u5f97 \u5230\u4e00\u4e2a\u8f93\u51fa.\u5f53\u7136, \u4f60\u53ef\u4ee5\u5728\u4efb\u610f\u6a21\u578b (CNN,RNN,Capsule Net\u7b49) \u8fd0\u7528 DataParallel \u6211\u4eec\u5728\u6a21\u578b\u4e2d\u8bbe\u7f6e\u4e86\u6253\u5370\u6307\u4ee4\u6765\u76d1\u63a7\u8f93\u5165\u548c\u8f93\u51fa\u7684\u5f20\u91cf\u5927\u5c0f, \u8bf7\u6ce8\u610f\u6279\u6570\u636e\u6b21\u5e8f\u4e3a0\u65f6\u7684\u8f93\u51fa. class Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(\" In Model: input size\", input.size(), \"output size\", output.size()) return output","title":"\u7b80\u5355\u6a21\u578b"},{"location":"6/#dataparallel","text":"\u8fd9\u662f\u672c\u6559\u7a0b\u7684\u6838\u5fc3\u90e8\u5206. \u9996\u5148, \u6211\u4eec\u9700\u8981\u751f\u6210\u4e00\u4e2a\u6a21\u578b\u7684\u5b9e\u4f8b\u5e76\u4e14\u68c0\u6d4b\u6211\u4eec\u662f\u5426\u62e5\u6709\u591a\u4e2a GPU.\u5982\u679c\u6709\u591a\u4e2aGPU , \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 nn.DataParallel \u6765\u5305\u88c5\u6211\u4eec\u7684\u6a21\u578b, \u7136\u540e\u6211\u4eec \u5c31\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7 model.gpu() \u65bd\u52a0\u4e8e\u8fd9\u4e9bGPU\u4e0a. model = Model(input_size, output_size) if torch.cuda.device_count() > 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn.DataParallel(model) if torch.cuda.is_available(): model.cuda()","title":"\u521b\u5efa\u6a21\u578b\u548c DataParallel"},{"location":"6/#_5","text":"\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u4e86. for data in rand_loader: if torch.cuda.is_available(): input_var = Variable(data.cuda()) else: input_var = Variable(data) output = model(input_var) print(\"Outside: input size\", input_var.size(), \"output_size\", output.size())","title":"\u8fd0\u884c\u6a21\u578b"},{"location":"6/#_6","text":"\u5f53\u6211\u4eec\u5c06\u8f93\u5165\u8bbe\u7f6e\u4e3a30\u6279, \u6a21\u578b\u4e5f\u4ea7\u751f\u4e8630\u6279\u7684\u8f93\u51fa.\u4f46\u662f\u5f53\u6211\u4eec\u4f7f\u7528\u591a\u4e2aGPU, \u7136\u540e\u4f60 \u4f1a\u5f97\u5230\u7c7b\u4f3c\u4e0b\u9762\u8fd9\u6837\u7684\u8f93\u51fa.","title":"\u7ed3\u679c"},{"location":"6/#2-gpus","text":"\u5982\u679c\u67092\u4e2aGPU, \u6211\u4eec\u5c06\u4f1a\u770b\u5230\u8fd9\u6837\u7684\u7ed3\u679c: # on 2 GPUs Let's use 2 GPUs! In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"2 GPUs"},{"location":"6/#3-gpus","text":"\u5982\u679c\u67093\u4e2aGPU, \u6211\u4eec\u5c06\u4f1a\u770b\u5230\u8fd9\u6837\u7684\u7ed3\u679c: Let's use 3 GPUs! In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"3 GPUs"},{"location":"6/#8-gpus","text":"\u5982\u679c\u67098\u4e2aGPU, \u6211\u4eec\u5c06\u4f1a\u770b\u5230\u8fd9\u6837\u7684\u7ed3\u679c: Let's use 8 GPUs! In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"8 GPUs"},{"location":"6/#_7","text":"DataParallel \u81ea\u52a8\u5730\u5c06\u6570\u636e\u5206\u5272\u5e76\u4e14\u5c06\u4efb\u52a1\u9001\u5165\u591a\u4e2aGPU\u4e0a\u7684\u591a\u4e2a\u6a21\u578b\u4e2d\u8fdb\u884c\u5904\u7406. \u5728\u6bcf\u4e2a\u6a21\u578b\u5b8c\u6210\u4efb\u52a1\u540e, DataParallel \u91c7\u96c6\u548c\u5408\u5e76\u6240\u6709\u7ed3\u679c, \u5e76\u5c06\u6700\u540e\u7684\u7ed3\u679c\u5448\u73b0\u7ed9\u4f60. \u60f3\u4e86\u89e3\u66f4\u591a\u4fe1\u606f, \u8bf7\u70b9\u51fb: http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html . Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: data_parallel_tutorial.py Download Jupyter notebook: data_parallel_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u603b\u7ed3"},{"location":"60/","text":"Probability distributions - torch.distributions \u8be5 distributions \u7edf\u8ba1\u5206\u5e03\u5305\u4e2d\u542b\u6709\u53ef\u81ea\u5b9a\u4e49\u53c2\u6570\u7684\u6982\u7387\u5206\u5e03\u548c\u91c7\u6837\u51fd\u6570. \u5f53\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u5bf9\u5176\u53c2\u6570\u53ef\u5fae\u65f6, \u53ef\u4ee5\u4f7f\u7528 log_prob() \u65b9\u6cd5\u6765\u5b9e\u65bd\u68af\u5ea6\u65b9\u6cd5 Policy Gradient. \u5b83\u7684\u4e00\u4e2a\u57fa\u672c\u65b9\u6cd5\u662fREINFORCE\u89c4\u5219: \\[\\Delta\\theta = \\alpha r \\frac{\\partial\\log p(a|\\pi^\\theta(s))}{\\partial\\theta}\\] \u8fd9\u5176\u4e2d \\(\\theta\\) \u662f\u53c2\u6570, \\(\\alpha\\) \u662f\u5b66\u4e60\u7387, \\(r\\) \u662f\u5956\u60e9, \\(p(a|\\pi^\\theta(s))\\) \u662f\u5728\u7b56\u7565 \\(\\pi^\\theta\\) \u4e2d\u4ece \\(s\\) \u72b6\u6001\u4e0b\u91c7\u53d6 \\(a\\) \u884c\u52a8\u7684\u6982\u7387. \u5728\u5b9e\u8df5\u4e2d, \u6211\u4eec\u8981\u4ece\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u4e2d\u91c7\u6837\u9009\u51fa\u4e00\u4e2a\u884c\u52a8, \u5728\u67d0\u4e2a\u73af\u5883\u4e2d\u5e94\u7528\u8be5\u884c\u52a8, \u7136\u540e \u4f7f\u7528 log_prob \u51fd\u6570\u6765\u6784\u9020\u4e00\u4e2a\u7b49\u4ef7\u7684\u635f\u5931\u51fd\u6570. \u8bf7\u6ce8\u610f, \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u4e86\u8d1f\u53f7, \u56e0\u4e3a\u4f18\u5316\u5668\u4f7f\u7528 \u662f\u662f\u68af\u5ea6\u4e0b\u964d\u6cd5, \u7136\u800c\u4e0a\u9762\u7684REINFORCE\u89c4\u5219\u662f\u5047\u8bbe\u4e86\u68af\u5ea6\u4e0a\u5347\u60c5\u5f62. \u5982\u4e0b\u6240\u793a\u662f\u5728\u591a\u9879\u5f0f\u5206\u5e03\u4e0b \u5b9e\u73b0REINFORCE\u7684\u4ee3\u7801: probs = policy_network(state) # NOTE: \u7b49\u540c\u4e8e\u591a\u9879\u5f0f\u5206\u5e03 m = Categorical(probs) action = m.sample() next_state, reward = env.step(action) loss = -m.log_prob(action) * reward loss.backward() Distribution (\u6982\u7387\u5206\u5e03) class torch.distributions.Distribution Distribution\u662f\u6982\u7387\u5206\u5e03\u7684\u62bd\u8c61\u57fa\u7c7b. log_prob(value) \u8fd4\u56de\u5728 value \u5904\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u7684\u5bf9\u6570. Parameters: value ( Tensor or Variable ) \u2013 \uff08\u57fa\u7c7b\u7684\u53c2\u6570,\u6ca1\u6709\u5b9e\u9645\u7528\u5904\uff09 sample() \u751f\u6210\u4e00\u4e2a\u6837\u672c, \u5982\u679c\u5206\u5e03\u53c2\u6570\u6709\u591a\u4e2a, \u5c31\u751f\u6210\u4e00\u6279\u6837\u672c. sample_n(n) \u751f\u6210n\u4e2a\u6837\u672c, \u5982\u679c\u5206\u5e03\u53c2\u6570\u6709\u591a\u4e2a, \u5c31\u751f\u6210n\u6279\u6837\u672c. Bernoulli (\u4f2f\u52aa\u5229\u5206\u5e03) class torch.distributions.Bernoulli(probs) \u521b\u5efa\u4ee5 <cite>probs</cite> \u4e3a\u53c2\u6570\u7684\u4f2f\u52aa\u5229\u5206\u5e03. \u6837\u672c\u662f\u4e8c\u8fdb\u5236\u7684 (0\u62161). \u4ed6\u4eec\u4ee5p\u7684\u6982\u7387\u53d6\u503c\u4e3a1, \u4ee5 (1 - p) \u7684\u6982\u7387\u53d6\u503c\u4e3a0. \u4f8b: >>> m = Bernoulli(torch.Tensor([0.3])) >>> m.sample() # 30% chance 1; 70% chance 0 0.0 [torch.FloatTensor of size 1] Parameters: probs ( Tensor or Variable ) \u2013 \u91c7\u6837\u5230 <cite>1</cite> \u7684\u6982\u7387 Categorical (\u7c7b\u522b\u5206\u5e03) class torch.distributions.Categorical(probs) \u521b\u5efa\u4ee5 <cite>probs</cite> \u4e3a\u53c2\u6570\u7684\u7c7b\u522b\u5206\u5e03. \u5b83\u548c multinomial() \u91c7\u6837\u7684\u5206\u5e03\u662f\u4e00\u6837\u7684. \u6837\u672c\u662f\u6765\u81ea \u201c0 \u2026 K-1\u201d \u7684\u6574\u6570,\u5176\u4e2d \u201cK\u201d \u662fprobs.size(-1). \u5982\u679c <cite>probs</cite> \u662f\u957f\u5ea6\u4e3a <cite>K</cite> \u7684\u4e00\u7ef4\u5217\u8868,\u5219\u6bcf\u4e2a\u5143\u7d20\u662f\u5bf9\u8be5\u7d22\u5f15\u5904\u7684\u7c7b\u8fdb\u884c\u62bd\u6837\u7684\u76f8\u5bf9\u6982\u7387. \u5982\u679c <cite>probs</cite> \u662f\u4e8c\u7ef4\u7684,\u5b83\u88ab\u89c6\u4e3a\u4e00\u6279\u6982\u7387\u5411\u91cf. \u53e6\u89c1: torch.multinomial() \u4f8b: >>> m = Categorical(torch.Tensor([ 0.25, 0.25, 0.25, 0.25 ])) >>> m.sample() # equal probability of 0, 1, 2, 3 3 [torch.LongTensor of size 1] Parameters: probs ( Tensor or Variable ) \u2013 \u4e8b\u4ef6\u6982\u7387 Normal (\u6b63\u6001\u5206\u5e03) class torch.distributions.Normal(mean, std) \u521b\u5efa\u4ee5 <cite>mean</cite> \u548c <cite>std</cite> \u4e3a\u53c2\u6570\u7684\u6b63\u6001\u5206\u5e03\uff08\u4e5f\u79f0\u4e3a\u9ad8\u65af\u5206\u5e03\uff09. \u4f8b: >>> m = Normal(torch.Tensor([0.0]), torch.Tensor([1.0])) >>> m.sample() # normally distributed with mean=0 and stddev=1 0.1046 [torch.FloatTensor of size 1] | Parameters: | mean ( float or Tensor or Variable ) \u2013 \u5206\u5e03\u7684\u5747\u503c std ( float or Tensor or Variable ) \u2013 \u5206\u5e03\u7684\u6807\u51c6\u5dee | | --- | --- |","title":"Probability distributions - torch.distributions"},{"location":"60/#probability-distributions-torchdistributions","text":"\u8be5 distributions \u7edf\u8ba1\u5206\u5e03\u5305\u4e2d\u542b\u6709\u53ef\u81ea\u5b9a\u4e49\u53c2\u6570\u7684\u6982\u7387\u5206\u5e03\u548c\u91c7\u6837\u51fd\u6570. \u5f53\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u5bf9\u5176\u53c2\u6570\u53ef\u5fae\u65f6, \u53ef\u4ee5\u4f7f\u7528 log_prob() \u65b9\u6cd5\u6765\u5b9e\u65bd\u68af\u5ea6\u65b9\u6cd5 Policy Gradient. \u5b83\u7684\u4e00\u4e2a\u57fa\u672c\u65b9\u6cd5\u662fREINFORCE\u89c4\u5219: \\[\\Delta\\theta = \\alpha r \\frac{\\partial\\log p(a|\\pi^\\theta(s))}{\\partial\\theta}\\] \u8fd9\u5176\u4e2d \\(\\theta\\) \u662f\u53c2\u6570, \\(\\alpha\\) \u662f\u5b66\u4e60\u7387, \\(r\\) \u662f\u5956\u60e9, \\(p(a|\\pi^\\theta(s))\\) \u662f\u5728\u7b56\u7565 \\(\\pi^\\theta\\) \u4e2d\u4ece \\(s\\) \u72b6\u6001\u4e0b\u91c7\u53d6 \\(a\\) \u884c\u52a8\u7684\u6982\u7387. \u5728\u5b9e\u8df5\u4e2d, \u6211\u4eec\u8981\u4ece\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u4e2d\u91c7\u6837\u9009\u51fa\u4e00\u4e2a\u884c\u52a8, \u5728\u67d0\u4e2a\u73af\u5883\u4e2d\u5e94\u7528\u8be5\u884c\u52a8, \u7136\u540e \u4f7f\u7528 log_prob \u51fd\u6570\u6765\u6784\u9020\u4e00\u4e2a\u7b49\u4ef7\u7684\u635f\u5931\u51fd\u6570. \u8bf7\u6ce8\u610f, \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u4e86\u8d1f\u53f7, \u56e0\u4e3a\u4f18\u5316\u5668\u4f7f\u7528 \u662f\u662f\u68af\u5ea6\u4e0b\u964d\u6cd5, \u7136\u800c\u4e0a\u9762\u7684REINFORCE\u89c4\u5219\u662f\u5047\u8bbe\u4e86\u68af\u5ea6\u4e0a\u5347\u60c5\u5f62. \u5982\u4e0b\u6240\u793a\u662f\u5728\u591a\u9879\u5f0f\u5206\u5e03\u4e0b \u5b9e\u73b0REINFORCE\u7684\u4ee3\u7801: probs = policy_network(state) # NOTE: \u7b49\u540c\u4e8e\u591a\u9879\u5f0f\u5206\u5e03 m = Categorical(probs) action = m.sample() next_state, reward = env.step(action) loss = -m.log_prob(action) * reward loss.backward()","title":"Probability distributions - torch.distributions"},{"location":"60/#distribution","text":"class torch.distributions.Distribution Distribution\u662f\u6982\u7387\u5206\u5e03\u7684\u62bd\u8c61\u57fa\u7c7b. log_prob(value) \u8fd4\u56de\u5728 value \u5904\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u7684\u5bf9\u6570. Parameters: value ( Tensor or Variable ) \u2013 \uff08\u57fa\u7c7b\u7684\u53c2\u6570,\u6ca1\u6709\u5b9e\u9645\u7528\u5904\uff09 sample() \u751f\u6210\u4e00\u4e2a\u6837\u672c, \u5982\u679c\u5206\u5e03\u53c2\u6570\u6709\u591a\u4e2a, \u5c31\u751f\u6210\u4e00\u6279\u6837\u672c. sample_n(n) \u751f\u6210n\u4e2a\u6837\u672c, \u5982\u679c\u5206\u5e03\u53c2\u6570\u6709\u591a\u4e2a, \u5c31\u751f\u6210n\u6279\u6837\u672c.","title":"Distribution (\u6982\u7387\u5206\u5e03)"},{"location":"60/#bernoulli","text":"class torch.distributions.Bernoulli(probs) \u521b\u5efa\u4ee5 <cite>probs</cite> \u4e3a\u53c2\u6570\u7684\u4f2f\u52aa\u5229\u5206\u5e03. \u6837\u672c\u662f\u4e8c\u8fdb\u5236\u7684 (0\u62161). \u4ed6\u4eec\u4ee5p\u7684\u6982\u7387\u53d6\u503c\u4e3a1, \u4ee5 (1 - p) \u7684\u6982\u7387\u53d6\u503c\u4e3a0. \u4f8b: >>> m = Bernoulli(torch.Tensor([0.3])) >>> m.sample() # 30% chance 1; 70% chance 0 0.0 [torch.FloatTensor of size 1] Parameters: probs ( Tensor or Variable ) \u2013 \u91c7\u6837\u5230 <cite>1</cite> \u7684\u6982\u7387","title":"Bernoulli (\u4f2f\u52aa\u5229\u5206\u5e03)"},{"location":"60/#categorical","text":"class torch.distributions.Categorical(probs) \u521b\u5efa\u4ee5 <cite>probs</cite> \u4e3a\u53c2\u6570\u7684\u7c7b\u522b\u5206\u5e03. \u5b83\u548c multinomial() \u91c7\u6837\u7684\u5206\u5e03\u662f\u4e00\u6837\u7684. \u6837\u672c\u662f\u6765\u81ea \u201c0 \u2026 K-1\u201d \u7684\u6574\u6570,\u5176\u4e2d \u201cK\u201d \u662fprobs.size(-1). \u5982\u679c <cite>probs</cite> \u662f\u957f\u5ea6\u4e3a <cite>K</cite> \u7684\u4e00\u7ef4\u5217\u8868,\u5219\u6bcf\u4e2a\u5143\u7d20\u662f\u5bf9\u8be5\u7d22\u5f15\u5904\u7684\u7c7b\u8fdb\u884c\u62bd\u6837\u7684\u76f8\u5bf9\u6982\u7387. \u5982\u679c <cite>probs</cite> \u662f\u4e8c\u7ef4\u7684,\u5b83\u88ab\u89c6\u4e3a\u4e00\u6279\u6982\u7387\u5411\u91cf. \u53e6\u89c1: torch.multinomial() \u4f8b: >>> m = Categorical(torch.Tensor([ 0.25, 0.25, 0.25, 0.25 ])) >>> m.sample() # equal probability of 0, 1, 2, 3 3 [torch.LongTensor of size 1] Parameters: probs ( Tensor or Variable ) \u2013 \u4e8b\u4ef6\u6982\u7387","title":"Categorical (\u7c7b\u522b\u5206\u5e03)"},{"location":"60/#normal","text":"class torch.distributions.Normal(mean, std) \u521b\u5efa\u4ee5 <cite>mean</cite> \u548c <cite>std</cite> \u4e3a\u53c2\u6570\u7684\u6b63\u6001\u5206\u5e03\uff08\u4e5f\u79f0\u4e3a\u9ad8\u65af\u5206\u5e03\uff09. \u4f8b: >>> m = Normal(torch.Tensor([0.0]), torch.Tensor([1.0])) >>> m.sample() # normally distributed with mean=0 and stddev=1 0.1046 [torch.FloatTensor of size 1] | Parameters: | mean ( float or Tensor or Variable ) \u2013 \u5206\u5e03\u7684\u5747\u503c std ( float or Tensor or Variable ) \u2013 \u5206\u5e03\u7684\u6807\u51c6\u5dee | | --- | --- |","title":"Normal (\u6b63\u6001\u5206\u5e03)"},{"location":"61/","text":"Multiprocessing package - torch.multiprocessing torch.multiprocessing \u662f\u672c\u5730 multiprocessing \u591a\u8fdb\u7a0b\u5904\u7406\u6a21\u5757\u7684\u4e00\u4e2a wrapper\uff08\u5305\u88c5\u5668\uff09. \u5b83\u901a\u8fc7\u6ce8\u518c\u81ea\u5b9a\u4e49\u7684 reducers\uff08\u7f29\u51cf\u5668\uff09, \u4f7f\u7528\u5171\u4eab\u5185\u5b58\u6765\u63d0\u4f9b\u4e0d\u540c\u8fdb\u7a0b\u4e2d\u76f8\u540c\u6570\u636e\u7684\u5171\u4eab\u89c6\u56fe. \u4e00\u65e6 tensor/storage\uff08\u5f20\u91cf/\u5b58\u50a8\uff09\u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58 (\u8bf7\u53c2\u9605 share_memory_() ), \u5c31\u53ef\u4ee5\u5c06\u5176\u53d1\u9001\u5230\u5176\u4ed6\u8fdb\u7a0b\u800c\u4e0d\u505a\u4efb\u4f55\u590d\u5236. \u8be5 API \u4e0e\u539f\u59cb\u6a21\u5757 100% \u517c\u5bb9 - \u53ea\u9700\u5c06 import multiprocessing \u66f4\u6539\u4e3a import torch.multiprocessing \u5c31 \u53ef\u4ee5\u5c06\u6240\u6709\u5f20\u91cf\u901a\u8fc7\u961f\u5217\u53d1\u9001, \u6216\u901a\u8fc7\u5176\u5b83\u673a\u5236\u5171\u4eab, \u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58. \u7531\u4e8e API \u7684\u76f8\u4f3c\u6027, \u6211\u4eec\u6ca1\u6709\u8bb0\u5f55\u5927\u90e8\u5206\u8fd9\u4e2a\u5305\u7684\u5185\u5bb9, \u6211\u4eec\u53c2\u8003\u5f15\u7528\u539f\u59cb\u6a21\u5757\u4e2d\u975e\u5e38\u4f18\u79c0\u7684\u6587\u6863. Warning \u5982\u679c\u4e3b\u8fdb\u7a0b\u7a81\u7136\u9000\u51fa (\u4f8b\u5982, \u7531\u4e8e\u4f20\u5165\u7684\u4fe1\u53f7) , Python \u7684\u591a\u8fdb\u7a0b\u6709\u65f6\u65e0\u6cd5\u6e05\u7406\u5176\u5b50\u8fdb\u7a0b. \u8fd9\u662f\u4e00\u4e2a\u5df2\u77e5\u7684\u8b66\u544a, \u6240\u4ee5\u5982\u679c\u4f60\u5728\u4e2d\u65ad\u89e3\u91ca\u5668\u4e4b\u540e\u53d1\u73b0\u4efb\u4f55\u8d44\u6e90\u6cc4\u6f0f, \u8fd9\u53ef\u80fd\u610f\u5473\u7740\u8fd9\u53ea\u662f\u53d1\u751f\u5728\u4f60\u8eab\u4e0a. \u7ba1\u7406\u7b56\u7565 torch.multiprocessing.get_all_sharing_strategies() \u8fd4\u56de\u5f53\u524d\u7cfb\u7edf\u652f\u6301\u7684\u4e00\u7ec4\u5171\u4eab\u7b56\u7565. torch.multiprocessing.get_sharing_strategy() \u8fd4\u56de\u7528\u4e8e\u5171\u4eab CPU \u5f20\u91cf\u7684\u5f53\u524d\u7b56\u7565. torch.multiprocessing.set_sharing_strategy(new_strategy) \u4e3a\u5171\u4eab\u7684 CPU \u5f20\u91cf\u6765\u8bbe\u7f6e\u7b56\u7565. Parameters: new_strategy ( str ) \u2013 \u6240\u9009\u7b56\u7565\u7684\u540d\u79f0. \u5fc5\u987b\u662f\u51fd\u6570 get_all_sharing_strategies() \u6240\u8fd4\u56de\u7684\u503c\u4e4b\u4e00. \u5171\u4eab CUDA \u5f20\u91cf \u5728\u8fdb\u7a0b\u4e4b\u95f4\u5171\u4eab CUDA \u5f20\u91cf\u4ec5\u5728 Python 3 \u4e2d\u652f\u6301, \u4f7f\u7528 spawn \u6216 forkserver \u542f\u52a8\u65b9\u6cd5. Python 2 \u4e2d\u7684 multiprocessing \u53ea\u80fd\u4f7f\u7528 fork \u521b\u5efa\u5b50\u8fdb\u7a0b, \u800c\u4e2d CUDA \u8fd0\u884c\u65f6\u662f\u4e0d\u652f\u6301\u7684. Warning CUDA API \u8981\u6c42\u8f93\u51fa\u5230\u5176\u4ed6\u8fdb\u7a0b\u7684\u5206\u914d\u4fdd\u6301\u6709\u6548, \u53ea\u8981\u5b83\u4eec\u88ab\u5b83\u4eec\u4f7f\u7528. \u60a8\u5e94\u8be5\u6ce8\u610f, \u5e76\u786e\u4fdd\u60a8\u5171\u4eab\u7684 CUDA \u5f20\u91cf\u4e0d\u4f1a\u8d85\u51fa\u8303\u56f4, \u5728\u6709\u5fc5\u8981\u7684\u60c5\u51b5\u4e0b. \u8fd9\u4e0d\u5e94\u8be5\u662f\u5171\u4eab\u6a21\u578b\u53c2\u6570\u7684\u95ee\u9898, \u800c\u662f\u5e94\u8be5\u5c0f\u5fc3\u5730\u4f20\u9012\u5176\u4ed6\u7c7b\u578b\u7684\u6570\u636e. \u8bf7\u6ce8\u610f, \u6b64\u9650\u5236\u4e0d\u9002\u7528\u4e8e\u5171\u4eab CPU \u5185\u5b58. \u5171\u4eab\u7b56\u7565 \u672c\u8282\u7b80\u8981\u4ecb\u7ecd\u4e0d\u540c\u5206\u4eab\u7b56\u7565\u7684\u5de5\u4f5c\u539f\u7406. \u8bf7\u6ce8\u610f, \u5b83\u4ec5\u9002\u7528\u4e8e CPU \u5f20\u91cf - CUDA \u5f20\u91cf\u5c06\u59cb\u7ec8\u4f7f\u7528 CUDA API, \u56e0\u4e3a\u8fd9\u662f\u5b83\u4eec\u53ef\u4ee5\u5171\u4eab\u7684\u552f\u4e00\u65b9\u5f0f. File descriptor - file_descriptor Note \u8fd9\u662f\u9ed8\u8ba4\u7684\u7b56\u7565 (\u9664\u4e86\u4e0d\u652f\u6301\u7684 macOS \u548c OS X\u4e4b\u5916) This is the default strategy (except for macOS and OS X where it\u2019s not supported). \u8fd9\u4e2a\u7b56\u7565\u5c06\u4f7f\u7528\u6587\u4ef6\u63cf\u8ff0\u7b26\u4f5c\u4e3a\u5171\u4eab\u5185\u5b58\u53e5\u67c4. \u65e0\u8bba\u4f55\u65f6\u5c06\u5b58\u50a8\u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58, \u4ece shm_open \u83b7\u53d6\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26\u90fd\u5c06\u4e0e\u8be5\u5bf9\u8c61\u4e00\u8d77\u7f13\u5b58, \u5e76\u4e14\u5f53\u5c06\u8981\u5c06\u5176\u53d1\u9001\u5230\u5176\u4ed6\u8fdb\u7a0b\u65f6, \u6587\u4ef6\u63cf\u8ff0\u7b26\u5c06\u88ab\u4f20\u9001 (\u4f8b\u5982, \u901a\u8fc7 UNIX sockets) \u5230\u5176\u4e2d. \u63a5\u6536\u5668\u8fd8\u5c06\u7f13\u5b58\u6587\u4ef6\u63cf\u8ff0\u7b26\u5e76\u5bf9\u5176\u8fdb\u884c mmap , \u4ee5\u83b7\u5f97\u5b58\u50a8\u6570\u636e\u7684\u5171\u4eab\u89c6\u56fe. \u8bf7\u6ce8\u610f, \u5982\u679c\u5171\u4eab\u5f20\u91cf\u5f88\u5927, \u8fd9\u4e2a\u7b56\u7565\u4f1a\u5728\u5927\u90e8\u5206\u65f6\u95f4\u4fdd\u6301\u5927\u91cf\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26. \u5982\u679c\u60a8\u7684\u7cfb\u7edf\u5bf9\u6253\u5f00\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26\u7684\u6570\u91cf\u6709\u9650\u5236, \u5e76\u4e14\u4e0d\u80fd\u63d0\u9ad8\u5b83\u4eec, \u5219\u5e94\u8be5\u4f7f\u7528 file_system \u7b56\u7565. File system - file_system \u8be5\u7b56\u7565\u5c06\u4f7f\u7528\u7ed9 shm_open \u7684\u6587\u4ef6\u540d\u6765\u6807\u8bc6\u5171\u4eab\u5185\u5b58\u533a. \u8fd9\u6837\u505a\u7684\u597d\u5904\u662f\u4e0d\u9700\u8981\u7f13\u5b58\u4ece\u4e2d\u83b7\u53d6\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26, \u4f46\u540c\u65f6\u4e5f\u5bb9\u6613\u53d1\u751f\u5171\u4eab\u5185\u5b58\u6cc4\u6f0f. \u8be5\u6587\u4ef6\u521b\u5efa\u540e\u4e0d\u80fd\u88ab\u5220\u9664, \u56e0\u4e3a\u5176\u4ed6\u8fdb\u7a0b\u9700\u8981\u8bbf\u95ee\u5b83\u6765\u6253\u5f00\u5b83\u4eec\u5404\u81ea\u7684\u89c6\u56fe. \u5982\u679c\u8be5\u8fdb\u7a0b\u5d29\u6e83, \u5e76\u4e14\u4e0d\u8c03\u7528\u5b58\u50a8\u6790\u6784\u51fd\u6570, \u5219\u8fd9\u4e9b\u6587\u4ef6\u5c06\u4fdd\u7559\u5728\u7cfb\u7edf\u4e2d. \u8fd9\u79cd\u60c5\u51b5\u975e\u5e38\u4e25\u91cd, \u56e0\u4e3a\u5b83\u4eec\u4f1a\u4e00\u76f4\u4f7f\u7528\u5185\u5b58, \u76f4\u5230\u7cfb\u7edf\u91cd\u65b0\u542f\u52a8, \u6216\u8005\u88ab\u624b\u52a8\u91ca\u653e. \u4e3a\u4e86\u89e3\u51b3\u5171\u4eab\u5185\u5b58\u6587\u4ef6\u6cc4\u6f0f\u7684\u95ee\u9898, torch.multiprocessing \u6a21\u5757\u4f1a\u4ea7\u751f\u4e00\u4e2a\u540d\u4e3a torch_shm_manager \u7684\u5b88\u62a4\u8fdb\u7a0b, \u5b83\u5c06\u628a\u81ea\u5df1\u4ece\u5f53\u524d\u8fdb\u7a0b\u7ec4\u4e2d\u5206\u79bb\u51fa\u6765, \u5e76\u8ddf\u8e2a\u6240\u6709\u7684\u5171\u4eab\u5185\u5b58\u5206\u914d. \u4e00\u65e6\u8fde\u63a5\u5230\u5b83\u7684\u6240\u6709\u8fdb\u7a0b\u9000\u51fa, \u5b83\u5c06\u7b49\u5f85\u4e00\u4f1a\u513f, \u4ee5\u786e\u4fdd\u4e0d\u4f1a\u6709\u65b0\u7684\u8fde\u63a5. \u5e76\u5c06\u8fed\u4ee3\u7ec4\u4e2d\u5df2\u5206\u914d\u7684\u6240\u6709\u5171\u4eab\u5185\u5b58\u6587\u4ef6. \u5982\u679c\u53d1\u73b0\u5176\u4e2d\u4efb\u4f55\u4e00\u4e2a\u4ecd\u7136\u5b58\u5728, \u5b83\u4eec\u5c06\u88ab\u91ca\u653e. \u6211\u4eec\u5df2\u7ecf\u6d4b\u8bd5\u4e86\u8fd9\u4e2a\u65b9\u6cd5, \u5e76\u8bc1\u660e\u5b83\u5bf9\u5404\u79cd\u5931\u8d25\u90fd\u662f\u6709\u6548\u7684. \u4e0d\u8fc7, \u5982\u679c\u4f60\u7684\u7cfb\u7edf\u6709\u8db3\u591f\u9ad8\u7684\u9650\u5236, file_descriptor \u662f\u4e00\u4e2a\u6240\u652f\u6301\u7684\u7b56\u7565, \u867d\u7136\u6211\u4eec\u4e0d\u5efa\u8bae\u5207\u6362\u5230\u8fd9\u4e2a\u7b56\u7565\u4e0a.","title":"Multiprocessing package - torch.multiprocessing"},{"location":"61/#multiprocessing-package-torchmultiprocessing","text":"torch.multiprocessing \u662f\u672c\u5730 multiprocessing \u591a\u8fdb\u7a0b\u5904\u7406\u6a21\u5757\u7684\u4e00\u4e2a wrapper\uff08\u5305\u88c5\u5668\uff09. \u5b83\u901a\u8fc7\u6ce8\u518c\u81ea\u5b9a\u4e49\u7684 reducers\uff08\u7f29\u51cf\u5668\uff09, \u4f7f\u7528\u5171\u4eab\u5185\u5b58\u6765\u63d0\u4f9b\u4e0d\u540c\u8fdb\u7a0b\u4e2d\u76f8\u540c\u6570\u636e\u7684\u5171\u4eab\u89c6\u56fe. \u4e00\u65e6 tensor/storage\uff08\u5f20\u91cf/\u5b58\u50a8\uff09\u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58 (\u8bf7\u53c2\u9605 share_memory_() ), \u5c31\u53ef\u4ee5\u5c06\u5176\u53d1\u9001\u5230\u5176\u4ed6\u8fdb\u7a0b\u800c\u4e0d\u505a\u4efb\u4f55\u590d\u5236. \u8be5 API \u4e0e\u539f\u59cb\u6a21\u5757 100% \u517c\u5bb9 - \u53ea\u9700\u5c06 import multiprocessing \u66f4\u6539\u4e3a import torch.multiprocessing \u5c31 \u53ef\u4ee5\u5c06\u6240\u6709\u5f20\u91cf\u901a\u8fc7\u961f\u5217\u53d1\u9001, \u6216\u901a\u8fc7\u5176\u5b83\u673a\u5236\u5171\u4eab, \u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58. \u7531\u4e8e API \u7684\u76f8\u4f3c\u6027, \u6211\u4eec\u6ca1\u6709\u8bb0\u5f55\u5927\u90e8\u5206\u8fd9\u4e2a\u5305\u7684\u5185\u5bb9, \u6211\u4eec\u53c2\u8003\u5f15\u7528\u539f\u59cb\u6a21\u5757\u4e2d\u975e\u5e38\u4f18\u79c0\u7684\u6587\u6863. Warning \u5982\u679c\u4e3b\u8fdb\u7a0b\u7a81\u7136\u9000\u51fa (\u4f8b\u5982, \u7531\u4e8e\u4f20\u5165\u7684\u4fe1\u53f7) , Python \u7684\u591a\u8fdb\u7a0b\u6709\u65f6\u65e0\u6cd5\u6e05\u7406\u5176\u5b50\u8fdb\u7a0b. \u8fd9\u662f\u4e00\u4e2a\u5df2\u77e5\u7684\u8b66\u544a, \u6240\u4ee5\u5982\u679c\u4f60\u5728\u4e2d\u65ad\u89e3\u91ca\u5668\u4e4b\u540e\u53d1\u73b0\u4efb\u4f55\u8d44\u6e90\u6cc4\u6f0f, \u8fd9\u53ef\u80fd\u610f\u5473\u7740\u8fd9\u53ea\u662f\u53d1\u751f\u5728\u4f60\u8eab\u4e0a.","title":"Multiprocessing package - torch.multiprocessing"},{"location":"61/#_1","text":"torch.multiprocessing.get_all_sharing_strategies() \u8fd4\u56de\u5f53\u524d\u7cfb\u7edf\u652f\u6301\u7684\u4e00\u7ec4\u5171\u4eab\u7b56\u7565. torch.multiprocessing.get_sharing_strategy() \u8fd4\u56de\u7528\u4e8e\u5171\u4eab CPU \u5f20\u91cf\u7684\u5f53\u524d\u7b56\u7565. torch.multiprocessing.set_sharing_strategy(new_strategy) \u4e3a\u5171\u4eab\u7684 CPU \u5f20\u91cf\u6765\u8bbe\u7f6e\u7b56\u7565. Parameters: new_strategy ( str ) \u2013 \u6240\u9009\u7b56\u7565\u7684\u540d\u79f0. \u5fc5\u987b\u662f\u51fd\u6570 get_all_sharing_strategies() \u6240\u8fd4\u56de\u7684\u503c\u4e4b\u4e00.","title":"\u7ba1\u7406\u7b56\u7565"},{"location":"61/#cuda","text":"\u5728\u8fdb\u7a0b\u4e4b\u95f4\u5171\u4eab CUDA \u5f20\u91cf\u4ec5\u5728 Python 3 \u4e2d\u652f\u6301, \u4f7f\u7528 spawn \u6216 forkserver \u542f\u52a8\u65b9\u6cd5. Python 2 \u4e2d\u7684 multiprocessing \u53ea\u80fd\u4f7f\u7528 fork \u521b\u5efa\u5b50\u8fdb\u7a0b, \u800c\u4e2d CUDA \u8fd0\u884c\u65f6\u662f\u4e0d\u652f\u6301\u7684. Warning CUDA API \u8981\u6c42\u8f93\u51fa\u5230\u5176\u4ed6\u8fdb\u7a0b\u7684\u5206\u914d\u4fdd\u6301\u6709\u6548, \u53ea\u8981\u5b83\u4eec\u88ab\u5b83\u4eec\u4f7f\u7528. \u60a8\u5e94\u8be5\u6ce8\u610f, \u5e76\u786e\u4fdd\u60a8\u5171\u4eab\u7684 CUDA \u5f20\u91cf\u4e0d\u4f1a\u8d85\u51fa\u8303\u56f4, \u5728\u6709\u5fc5\u8981\u7684\u60c5\u51b5\u4e0b. \u8fd9\u4e0d\u5e94\u8be5\u662f\u5171\u4eab\u6a21\u578b\u53c2\u6570\u7684\u95ee\u9898, \u800c\u662f\u5e94\u8be5\u5c0f\u5fc3\u5730\u4f20\u9012\u5176\u4ed6\u7c7b\u578b\u7684\u6570\u636e. \u8bf7\u6ce8\u610f, \u6b64\u9650\u5236\u4e0d\u9002\u7528\u4e8e\u5171\u4eab CPU \u5185\u5b58.","title":"\u5171\u4eab CUDA \u5f20\u91cf"},{"location":"61/#_2","text":"\u672c\u8282\u7b80\u8981\u4ecb\u7ecd\u4e0d\u540c\u5206\u4eab\u7b56\u7565\u7684\u5de5\u4f5c\u539f\u7406. \u8bf7\u6ce8\u610f, \u5b83\u4ec5\u9002\u7528\u4e8e CPU \u5f20\u91cf - CUDA \u5f20\u91cf\u5c06\u59cb\u7ec8\u4f7f\u7528 CUDA API, \u56e0\u4e3a\u8fd9\u662f\u5b83\u4eec\u53ef\u4ee5\u5171\u4eab\u7684\u552f\u4e00\u65b9\u5f0f.","title":"\u5171\u4eab\u7b56\u7565"},{"location":"61/#file-descriptor-file_descriptor","text":"Note \u8fd9\u662f\u9ed8\u8ba4\u7684\u7b56\u7565 (\u9664\u4e86\u4e0d\u652f\u6301\u7684 macOS \u548c OS X\u4e4b\u5916) This is the default strategy (except for macOS and OS X where it\u2019s not supported). \u8fd9\u4e2a\u7b56\u7565\u5c06\u4f7f\u7528\u6587\u4ef6\u63cf\u8ff0\u7b26\u4f5c\u4e3a\u5171\u4eab\u5185\u5b58\u53e5\u67c4. \u65e0\u8bba\u4f55\u65f6\u5c06\u5b58\u50a8\u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58, \u4ece shm_open \u83b7\u53d6\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26\u90fd\u5c06\u4e0e\u8be5\u5bf9\u8c61\u4e00\u8d77\u7f13\u5b58, \u5e76\u4e14\u5f53\u5c06\u8981\u5c06\u5176\u53d1\u9001\u5230\u5176\u4ed6\u8fdb\u7a0b\u65f6, \u6587\u4ef6\u63cf\u8ff0\u7b26\u5c06\u88ab\u4f20\u9001 (\u4f8b\u5982, \u901a\u8fc7 UNIX sockets) \u5230\u5176\u4e2d. \u63a5\u6536\u5668\u8fd8\u5c06\u7f13\u5b58\u6587\u4ef6\u63cf\u8ff0\u7b26\u5e76\u5bf9\u5176\u8fdb\u884c mmap , \u4ee5\u83b7\u5f97\u5b58\u50a8\u6570\u636e\u7684\u5171\u4eab\u89c6\u56fe. \u8bf7\u6ce8\u610f, \u5982\u679c\u5171\u4eab\u5f20\u91cf\u5f88\u5927, \u8fd9\u4e2a\u7b56\u7565\u4f1a\u5728\u5927\u90e8\u5206\u65f6\u95f4\u4fdd\u6301\u5927\u91cf\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26. \u5982\u679c\u60a8\u7684\u7cfb\u7edf\u5bf9\u6253\u5f00\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26\u7684\u6570\u91cf\u6709\u9650\u5236, \u5e76\u4e14\u4e0d\u80fd\u63d0\u9ad8\u5b83\u4eec, \u5219\u5e94\u8be5\u4f7f\u7528 file_system \u7b56\u7565.","title":"File descriptor - file_descriptor"},{"location":"61/#file-system-file_system","text":"\u8be5\u7b56\u7565\u5c06\u4f7f\u7528\u7ed9 shm_open \u7684\u6587\u4ef6\u540d\u6765\u6807\u8bc6\u5171\u4eab\u5185\u5b58\u533a. \u8fd9\u6837\u505a\u7684\u597d\u5904\u662f\u4e0d\u9700\u8981\u7f13\u5b58\u4ece\u4e2d\u83b7\u53d6\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26, \u4f46\u540c\u65f6\u4e5f\u5bb9\u6613\u53d1\u751f\u5171\u4eab\u5185\u5b58\u6cc4\u6f0f. \u8be5\u6587\u4ef6\u521b\u5efa\u540e\u4e0d\u80fd\u88ab\u5220\u9664, \u56e0\u4e3a\u5176\u4ed6\u8fdb\u7a0b\u9700\u8981\u8bbf\u95ee\u5b83\u6765\u6253\u5f00\u5b83\u4eec\u5404\u81ea\u7684\u89c6\u56fe. \u5982\u679c\u8be5\u8fdb\u7a0b\u5d29\u6e83, \u5e76\u4e14\u4e0d\u8c03\u7528\u5b58\u50a8\u6790\u6784\u51fd\u6570, \u5219\u8fd9\u4e9b\u6587\u4ef6\u5c06\u4fdd\u7559\u5728\u7cfb\u7edf\u4e2d. \u8fd9\u79cd\u60c5\u51b5\u975e\u5e38\u4e25\u91cd, \u56e0\u4e3a\u5b83\u4eec\u4f1a\u4e00\u76f4\u4f7f\u7528\u5185\u5b58, \u76f4\u5230\u7cfb\u7edf\u91cd\u65b0\u542f\u52a8, \u6216\u8005\u88ab\u624b\u52a8\u91ca\u653e. \u4e3a\u4e86\u89e3\u51b3\u5171\u4eab\u5185\u5b58\u6587\u4ef6\u6cc4\u6f0f\u7684\u95ee\u9898, torch.multiprocessing \u6a21\u5757\u4f1a\u4ea7\u751f\u4e00\u4e2a\u540d\u4e3a torch_shm_manager \u7684\u5b88\u62a4\u8fdb\u7a0b, \u5b83\u5c06\u628a\u81ea\u5df1\u4ece\u5f53\u524d\u8fdb\u7a0b\u7ec4\u4e2d\u5206\u79bb\u51fa\u6765, \u5e76\u8ddf\u8e2a\u6240\u6709\u7684\u5171\u4eab\u5185\u5b58\u5206\u914d. \u4e00\u65e6\u8fde\u63a5\u5230\u5b83\u7684\u6240\u6709\u8fdb\u7a0b\u9000\u51fa, \u5b83\u5c06\u7b49\u5f85\u4e00\u4f1a\u513f, \u4ee5\u786e\u4fdd\u4e0d\u4f1a\u6709\u65b0\u7684\u8fde\u63a5. \u5e76\u5c06\u8fed\u4ee3\u7ec4\u4e2d\u5df2\u5206\u914d\u7684\u6240\u6709\u5171\u4eab\u5185\u5b58\u6587\u4ef6. \u5982\u679c\u53d1\u73b0\u5176\u4e2d\u4efb\u4f55\u4e00\u4e2a\u4ecd\u7136\u5b58\u5728, \u5b83\u4eec\u5c06\u88ab\u91ca\u653e. \u6211\u4eec\u5df2\u7ecf\u6d4b\u8bd5\u4e86\u8fd9\u4e2a\u65b9\u6cd5, \u5e76\u8bc1\u660e\u5b83\u5bf9\u5404\u79cd\u5931\u8d25\u90fd\u662f\u6709\u6548\u7684. \u4e0d\u8fc7, \u5982\u679c\u4f60\u7684\u7cfb\u7edf\u6709\u8db3\u591f\u9ad8\u7684\u9650\u5236, file_descriptor \u662f\u4e00\u4e2a\u6240\u652f\u6301\u7684\u7b56\u7565, \u867d\u7136\u6211\u4eec\u4e0d\u5efa\u8bae\u5207\u6362\u5230\u8fd9\u4e2a\u7b56\u7565\u4e0a.","title":"File system - file_system"},{"location":"62/","text":"Distributed communication package - torch.distributed torch.distributed \u63d0\u4f9b\u7c7b\u4f3c MPI \u7684\u524d\u5411\u8fd0\u7b97\u673a\u5236, \u652f\u6301\u5728\u591a\u53f0\u673a\u7684\u7f51\u7edc\u4e2d\u4ea4\u6362\u6570\u636e. \u652f\u6301\u4e0d\u540c\u7684\u540e\u6bb5\u548c\u521d\u59cb\u5316\u65b9\u6cd5. \u76ee\u524dtorch.distributed\u652f\u6301\u4e09\u4e2a\u540e\u7aef, \u6bcf\u4e2a\u90fd\u6709\u4e0d\u540c\u7684\u529f\u80fd. \u4e0b\u8868\u663e\u793a\u54ea\u4e9b\u529f\u80fd\u53ef\u7528\u4e8e CPU/CUDA \u5f20\u91cf. \u53ea\u6709\u5728\u8bbe\u5907\u4e0a\u7f16\u8bd1\u5b89\u88c5PyTorch, \u624d\u80fd\u5728MPI\u7684\u8bbe\u5907\u4e0a\u652f\u6301cuda. Backend tcp gloo mpi Device CPU GPU CPU --- --- --- --- send \u2713 \u2718 \u2718 recv \u2713 \u2718 \u2718 broadcast \u2713 \u2718 \u2713 all_reduce \u2713 \u2718 \u2713 reduce \u2713 \u2718 \u2718 all_gather \u2713 \u2718 \u2718 gather \u2713 \u2718 \u2718 scatter \u2713 \u2718 \u2718 barrier \u2713 \u2718 \u2713 Basics <cite>torch.distributed</cite> \u4e3a\u5728\u4e00\u53f0\u6216\u591a\u53f0\u673a\u5668\u4e0a\u8fd0\u884c\u7684\u591a\u4e2a\u8ba1\u7b97\u8282\u70b9\u63d0\u4f9b\u591a\u8fdb\u7a0b\u5e76\u884c\u7684\u901a\u4fe1\u6a21\u5757\u548cPyTorch\u7684\u652f\u6301. \u7c7b torch.nn.parallel.DistributedDataParallel() \u5efa\u7acb\u5728\u8fd9\u4e2a\u529f\u80fd\u4e4b\u4e0a, \u4ee5\u63d0\u4f9b\u4efb\u4f55PyTorch\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u88c5\u9970\u5668. \u8fd9\u4e2a\u7c7b\u548c Multiprocessing package - torch.multiprocessing \u548c torch.nn.DataParallel() \u5e76\u4e0d\u76f8\u540c, PyTorch\u96c6\u7fa4\u5206\u5e03\u5f0f\u8ba1\u7b97\u652f\u6301\u591a\u53f0\u673a\u5668, \u4f7f\u7528\u65f6\u7528\u6237\u5fc5\u987b\u5728\u4e3b\u8981\u8bad\u7ec3\u7684\u811a\u672c\u4e2d, \u660e\u786e\u5730\u5c06\u6bcf\u4e2a\u8fdb\u7a0b\u590d\u5236\u5230\u6bcf\u53f0\u673a\u5668\u4e2d. \u5728\u5355\u673a\u591a\u8282\u70b9\u8ba1\u7b97\u7684\u60c5\u51b5\u4e0b, \u4f7f\u7528 <cite>torch.distributed</cite> \u548c torch.nn.parallel.DistributedDataParallel() \u4f5c\u4e3a \u8bad\u7ec3\u7684\u88c5\u9970\u5668, \u76f8\u6bd4\u4e8e torch.nn.DataParallel() \u4e4b\u7c7b\u7684\u6570\u636e\u5e76\u884c\u8ba1\u7b97, \u4efb\u7136\u5177\u6709\u4f18\u52bf: \u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d, \u6bcf\u4e2a\u8fdb\u7a0b\u7ef4\u62a4\u81ea\u5df1\u7684\u4f18\u5316\u5668, \u6267\u884c\u5b8c\u6574\u7684\u4f18\u5316\u6b65\u9aa4. \u867d\u7136\u8fd9\u770b\u8d77\u6765\u53ef\u80fd\u662f\u591a\u4f59\u7684, \u4f46\u662f\u56e0\u4e3a\u68af\u5ea6\u5df2\u7ecf\u88ab\u6536\u96c6\u5728 \u4e00\u8d77, \u5e76\u4e14\u8ba1\u7b97\u4e86\u68af\u5ea6\u7684\u5e73\u5747\u503c, \u56e0\u6b64\u5bf9\u4e8e\u6bcf\u4e2a\u8fdb\u7a0b\u68af\u5ea6\u662f\u76f8\u540c\u7684, \u8fd9\u53ef\u4ee5\u51cf\u5c11\u5728\u8282\u70b9\u4e4b\u95f4\u4f20\u9012\u5f20\u91cf, \u518d\u8ba1\u7b97\u53c2\u6570\u7684\u65f6\u95f4. \u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u5305\u542b\u4e00\u4e2a\u72ec\u7acb\u7684Python\u89e3\u91ca\u5668, \u6d88\u9664\u4e86Python\u89e3\u91ca\u5668\u7684\u989d\u5916\u5f00\u9500, \u4ee5\u53ca\u7531\u4e8e\u9a71\u52a8\u591a\u7ebf\u7a0b, \u6a21\u578b\u526f\u672c\u548cGPU\u9020\u6210 \u201cGIL-thrashing\u201d . \u5bf9\u4e8e\u9700\u8981\u6d88\u8017\u5927\u91cfPython\u89e3\u91ca\u5668\u8fd0\u884c\u65f6\u95f4 (\u5305\u62ec\u5177\u6709\u5faa\u73af\u56fe\u5c42\u6216\u8bb8\u591a\u5c0f\u7ec4\u4ef6\u7684\u6a21\u578b) \u7684\u6a21\u578b\u6765\u8bf4\u662f\u975e\u5e38\u91cd\u8981\u7684. Initialization \u5728\u8c03\u7528\u5176\u4ed6\u6a21\u578b\u4e4b\u524d, \u8fd9\u4e2a\u5305\u9700\u8981\u4f7f\u7528 torch.distributed.init_process_group() \u51fd\u6570\u8fdb\u884c\u521d\u59cb\u5316. \u5728\u521d\u59cb\u5316\u5355\u5143\u4e2d, \u6240\u6709\u8fdb\u7a0b\u90fd\u4f1a\u53c2\u4e0e. torch.distributed.init_process_group(backend, init_method='env://', **kwargs) \u521d\u59cb\u5316\u65b9\u6cd5. | Parameters: | backend ( str ) \u2013 \u4f7f\u7528\u540e\u7aef\u7684\u540d\u5b57. \u8f93\u5165\u7684\u6709\u6548\u503c\u5305\u62ec: tcp , mpi and gloo . init_method ( str , optional ) \u2013 \u6307\u5b9a\u5982\u4f55\u521d\u59cb\u5316\u7684URL. world_size ( int , optional ) \u2013 \u53c2\u4e0e\u5de5\u4f5c\u7684\u8fdb\u7a0b\u6570\u91cf. rank ( int , optional ) \u2013 \u5f53\u524d\u8fdb\u7a0b\u7684\u6392\u540d. group_name ( str , optional ) \u2013 \u96c6\u7fa4\u7684\u540d\u5b57. \u8bf7\u53c2\u9605init\u65b9\u6cd5\u7684\u63cf\u8ff0. | | --- | --- | \u4e3a\u4e86\u652f\u6301 backend == mpi , PyTorch \u9700\u8981\u5728\u652f\u6301 MPI \u7684\u7cfb\u7edf\u4e0a\u7528\u8fdb\u884c\u6e90\u7801\u7f16\u8bd1\u5b89\u88c5 torch.distributed.get_rank() \u8fd4\u56de\u5f53\u524d\u8fdb\u7a0b\u7684\u6392\u540d. \u6392\u540d\u662f\u72ec\u4e00\u65e0\u4e8c\u7684 Rank\uff08\u6392\u540d\uff09\u662f\u5206\u914d\u7ed9\u5206\u5e03\u5f0f\u96c6\u7fa4\u4e2d\u6bcf\u4e2a\u8fdb\u7a0b\u7684\u552f\u4e00\u6807\u8bc6\u7b26. \u5b83\u4eec\u603b\u662f\u8fde\u7eed\u7684\u6574\u6570, \u8303\u56f4\u4ece0\u5230 world_size . torch.distributed.get_world_size() \u8fd4\u56de\u5728\u5206\u5e03\u5f0f\u96c6\u7fa4\u4e2d\u7684\u8fdb\u7a0b\u6570\u76ee. \u76ee\u524d\u652f\u6301\u4e09\u79cd\u521d\u59cb\u5316\u7684\u65b9\u6cd5: TCP initialization \u63d0\u4f9b\u4e24\u79cdTCP\u7684\u521d\u59cb\u5316\u7684\u65b9\u6cd5, \u4e24\u79cd\u65b9\u6cd5\u90fd\u9700\u8981\u5404\u53f0\u673a\u5668\u7684\u7f51\u7edc\u5730\u5740\u548c\u96c6\u7fa4\u673a\u5668\u6570\u76ee world_size . \u7b2c\u4e00\u79cd\u65b9\u6cd5\u9700\u8981\u6307\u5b9a\u5c5e\u4e8e0\u7ea7\u8fdb\u7a0b\u7684\u5730\u5740, \u5e76\u4e14\u521d\u59cb\u5316\u65f6\u6240\u6709\u8fdb\u7a0b\u7684\u7b49\u7ea7\u90fd\u7531\u624b\u52a8\u6307\u5b9a. \u7b2c\u4e8c\u79cd\u65b9\u6cd5\u662f, \u5730\u5740\u5fc5\u987b\u662f\u6709\u6548\u7684IP\u591a\u64ad\u5730\u5740, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u53ef\u4ee5\u81ea\u52a8\u5206\u914d\u7b49\u7ea7. \u591a\u8def\u901a\u4fe1\u7684\u521d\u59cb\u5316\u4e5f\u652f\u6301 group_name \u53c2\u6570, \u5b83\u5141\u8bb8\u4f60\u4e3a\u591a\u4e2a\u4f5c\u4e1a\u4f7f\u7528\u76f8\u540c\u7684\u5730\u5740, \u53ea\u8981\u5b83\u4eec\u4f7f\u7528\u4e0d\u540c\u7684\u5c0f\u7ec4\u540d\u5373\u53ef. import torch.distributed as dist # Use address of one of the machines dist.init_process_group(init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4) # or a multicast address - rank will be assigned automatically if unspecified dist.init_process_group(init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456', world_size=4) Shared file-system initialization \u53e6\u4e00\u4e2a\u521d\u59cb\u5316\u65b9\u6cd5\u4f7f\u7528\u4e00\u4e2a\u6587\u4ef6\u7cfb\u7edf, \u8fd9\u4e2a\u6587\u4ef6\u7cfb\u7edf\u5728\u4e00\u4e2a\u7ec4\u4e2d\u7684\u6240\u6709\u673a\u5668\u4e0a\u5171\u4eab\u548c\u53ef\u89c1, \u4ee5\u53ca\u4e00\u4e2a\u6240\u9700\u7684 world_size \u53c2\u6570. URL\u5e94\u8be5\u4ee5 file:// \u5f00\u5934, \u5e76\u5305\u542b\u4e00\u4e2a\u53ef\u4ee5\u548c\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\u6240\u6709\u73b0\u6709\u76ee\u5f55\u4e2d\u7684\u8def\u5f84\u76f8\u533a\u522b\u7684\u8def\u5f84, \u4f5c\u4e3aURL. \u8fd9\u4e2a\u521d\u59cb\u5316\u65b9\u6cd5\u4e5f \u652f\u6301 group_name \u53c2\u6570, \u5b83\u5141\u8bb8\u4f60\u4e3a\u591a\u4e2a\u4f5c\u4e1a\u4f7f\u7528\u76f8\u540c\u7684\u5171\u4eab\u6587\u4ef6\u8def\u5f84, \u53ea\u8981\u5b83\u4eec\u4f7f\u7528\u4e0d\u540c\u7684\u5c0f\u7ec4\u540d. Warning \u8fd9\u79cd\u65b9\u6cd5\u5047\u8bbe\u6587\u4ef6\u7cfb\u7edf\u652f\u6301\u4f7f\u7528 fcntl \u8fdb\u884c\u9501\u5b9a -\u5927\u591a\u6570\u672c\u5730\u7cfb\u7edf\u548cNFS\u90fd\u652f\u6301\u5b83. import torch.distributed as dist # Rank will be assigned automatically if unspecified dist.init_process_group(init_method='file:///mnt/nfs/sharedfile', world_size=4, group_name=args.group) Environment variable initialization \u6b64\u65b9\u6cd5\u5c06\u4ece\u73af\u5883\u53d8\u91cf\u4e2d\u8bfb\u53d6\u914d\u7f6e, \u4ece\u800c\u53ef\u4ee5\u5b8c\u5168\u81ea\u5b9a\u4e49\u5982\u4f55\u83b7\u53d6\u4fe1\u606f. \u8981\u8bbe\u7f6e\u7684\u53d8\u91cf\u662f: MASTER_PORT - \u9700\u8981; \u5fc5\u987b\u662f0\u7ea7\u673a\u5668\u4e0a\u7684\u81ea\u7531\u7aef\u53e3 MASTER_ADDR - \u9700\u8981 (\u9664\u4e86\u7b49\u7ea70) ; \u7b49\u7ea70\u8282\u70b9\u7684\u5730\u5740 WORLD_SIZE - \u9700\u8981; \u53ef\u4ee5\u5728\u8fd9\u91cc\u8bbe\u7f6e, \u6216\u8005\u5728\u8c03\u7528init\u51fd\u6570 RANK - \u9700\u8981; \u53ef\u4ee5\u5728\u8fd9\u91cc\u8bbe\u7f6e, \u6216\u8005\u5728\u8c03\u7528init\u51fd\u6570 \u7b49\u7ea7\u4e3a0\u7684\u673a\u5668\u5c06\u7528\u4e8e\u8bbe\u7f6e\u6240\u6709\u8fde\u63a5. \u8fd9\u662f\u9ed8\u8ba4\u7684\u65b9\u6cd5, \u8fd9\u610f\u5473\u7740 init_method \u4e0d\u5fc5\u88ab\u7279\u522b\u6307\u5b9a(\u6216\u8005\u53ef\u4ee5\u662f env:// ) Groups \u9ed8\u8ba4\u7684\u96c6\u7fa4 (collectives) \u64cd\u4f5c\u9ed8\u8ba4\u7684\u5c0f\u7ec4 (group), \u8981\u6c42\u6240\u6709\u7684\u8fdb\u7a0b\u8fdb\u5165\u5206\u5e03\u5f0f\u51fd\u6570\u4e2d\u8c03\u7528. \u4e00\u4e9b\u5de5\u4f5c\u8d1f\u8f7d\u53ef\u4ee5\u4ece\u53ef\u4ee5\u4ece\u66f4\u7ec6\u7c92\u5ea6\u7684\u901a\u4fe1\u4e2d\u53d7\u76ca \u8fd9\u662f\u5206\u5e03\u5f0f\u96c6\u7fa4\u53d1\u6325\u4f5c\u7528\u7684\u5730\u65b9. new_group() \u51fd\u6570\u53ef\u4ee5\u7528\u6765\u521b\u5efa\u65b0\u7684\u7ec4, \u5e76\u4e14\u5305\u542b\u6240\u6709\u8fdb\u7a0b\u7684\u4efb\u610f\u5b50\u96c6. \u5b83\u8fd4\u56de\u4e00\u4e2a\u4e0d\u900f\u660e\u7684\u7ec4\u53e5\u67c4, \u5b83\u53ef\u4ee5\u4f5c\u4e3a\u96c6\u7fa4\u7684 group \u53c2\u6570 (\u96c6\u7fa4 collectives \u662f\u4e00\u822c\u7684\u7f16\u7a0b\u6a21\u5f0f\u4e2d\u7684\u4ea4\u6362\u4fe1\u606f\u7684\u5206\u5e03\u5f0f\u51fd\u6570) . torch.distributed.new_group(ranks=None) \u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5206\u5e03\u5f0f\u5c0f\u7ec4 \u6b64\u51fd\u6570\u8981\u6c42\u4e3b\u7ec4\u4e2d\u7684\u6240\u6709\u8fdb\u7a0b\uff08\u5373\u4f5c\u4e3a\u5206\u5e03\u5f0f\u4f5c\u4e1a\u4e00\u90e8\u5206\u7684\u6240\u6709\u8fdb\u7a0b\uff09\u90fd\u4f1a\u8f93\u5165\u6b64\u51fd\u6570, \u5373\u4f7f\u5b83\u4eec\u4e0d\u662f\u8be5\u5c0f\u7ec4\u7684\u6210\u5458. \u6b64\u5916, \u5e94\u8be5\u5728\u6240\u6709\u7684\u8fdb\u7a0b\u4e2d\u4ee5\u76f8\u540c\u7684\u987a\u5e8f\u521b\u5efa\u65b0\u7684\u5c0f\u7ec4. Parameters: ranks ( list [ int ] ) \u2013 \u5c0f\u7ec4\u5185\u6210\u5458\u7684 Rank \u7684\u5217\u8868. Returns: \u5206\u914d\u7ec4\u7684\u53e5\u67c4, \u4ee5\u4fbf\u5728\u96c6\u7fa4\u4e2d\u8c03\u7528. --- --- Point-to-point communication torch.distributed.send(tensor, dst) \u540c\u6b65\u53d1\u9001\u5f20\u91cf. | Parameters: | tensor ( Tensor ) \u2013 \u53d1\u9001\u7684\u5f20\u91cf. dst ( int ) \u2013 \u6307\u5b9a\u53d1\u9001\u7684\u76ee\u7684\u5730\u7684 Rank. | | --- | --- | torch.distributed.recv(tensor, src=None) \u540c\u6b65\u63a5\u6536\u5f20\u91cf. | Parameters: | tensor ( Tensor ) \u2013 \u7528\u6536\u5230\u7684\u6570\u636e\u586b\u5145\u5f20\u91cf. src ( int , optional ) \u2013 \u53d1\u9001\u7aef\u7684Rank, \u5982\u679c\u6ca1\u6709\u6307\u5b9a, \u5c06\u4f1a\u63a5\u6536\u4efb\u4f55\u53d1\u9001\u7684\u6570\u636e. | | --- | --- | | Returns: | \u53d1\u9001\u7aef\u7684Rank. | | --- | --- | isend() \u548c irecv() \u4f7f\u7528\u65f6\u8fd4\u56de\u5206\u5e03\u5f0f\u8bf7\u6c42\u5bf9\u8c61. \u901a\u5e38, \u8fd9\u4e2a\u5bf9\u8c61\u7684\u7c7b\u578b\u662f\u672a\u6307\u5b9a\u7684, \u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u4f7f\u7528\u624b\u52a8\u521b\u5efa, \u4f46\u662f\u5b83\u4eec\u652f\u6301\u4e24\u79cd\u65b9\u6cd5\u6307\u5b9a: is_completed() - \u5982\u679c\u64cd\u4f5c\u5b8c\u6210\u8fd4\u56deTrue wait() - \u5982\u679c\u64cd\u4f5c\u5b8c\u6210\u4f1a\u963b\u585e\u6240\u6709\u7684\u8fdb\u7a0b. is_completed() \u5982\u679c\u7ed3\u679c\u8fd4\u56de, \u4fdd\u8bc1\u51fd\u6570\u8fd4\u56deTrue. \u5f53\u4f7f\u7528MPI\u4f5c\u4e3a\u540e\u7aef, isend() \u548c irecv() \u652f\u6301 \u201c\u4e0d\u8d85\u8f66\u201d \u5f0f\u7684\u5de5\u4f5c\u65b9\u5f0f, \u8fd9\u79cd\u65b9\u5f0f\u53ef\u4ee5\u4fdd\u8bc1\u6d88\u606f\u7684\u987a\u5e8f. \u66f4\u591a\u7684\u7ec6\u8282\u53ef\u4ee5\u770b http://mpi-forum.org/docs/mpi-2.2/mpi22-report/node54.htm#Node54 torch.distributed.isend(tensor, dst) \u5f02\u6b65\u53d1\u9001\u5f20\u91cf\u6570\u636e. | Parameters: | tensor ( Tensor ) \u2013 \u53d1\u9001\u7684\u5f20\u91cf\u7684\u6570\u636e. dst ( int ) \u2013 \u6307\u5b9a\u53d1\u9001\u5230\u7684 Rank. | | --- | --- | | Returns: | \u5206\u5e03\u5f0f\u8bf7\u6c42\u5bf9\u8c61. | | --- | --- | torch.distributed.irecv(tensor, src) \u5f02\u6b65\u63a5\u6536\u5f20\u91cf. | Parameters: | tensor ( Tensor ) \u2013 \u7528\u6536\u5230\u7684\u6570\u636e\u586b\u5145\u5f20\u91cf. src ( int ) \u2013 \u6307\u5b9a\u53d1\u9001\u5f20\u91cf\u7684 Rank. | | --- | --- | | Returns: | \u4e00\u4e2a\u5206\u5e03\u5f0f\u8bf7\u6c42\u5bf9\u8c61. | | --- | --- | Collective functions torch.distributed.broadcast(tensor, src, group=<object object>) \u5411\u67d0\u4e2a\u5c0f\u7ec4\u5185\u7684\u5f20\u91cf\u5e7f\u64ad\u7684\u65b9\u6cd5. tensor \u5728\u8be5\u5c0f\u7ec4\u5904\u7406\u6570\u636e\u7684\u6240\u6709\u8fc7\u7a0b\u4e2d\u5143\u7d20\u7684\u6570\u76ee\u5fc5\u987b\u76f8\u540c. | Parameters: | tensor ( Tensor ) \u2013 \u5982\u679c\u53d1\u9001\u7aef src \u662f\u5f53\u524d\u8fdb\u7a0b\u7684 Rank, \u5219\u53d1\u9001\u6570\u636e, \u5426\u5219\u4f7f\u7528\u5f20\u91cf\u4fdd\u5b58\u63a5\u6536\u7684\u6570\u636e. src ( int ) \u2013 \u53d1\u9001\u7aef\u7684 Rank. group ( optional ) \u2013 \u96c6\u7fa4\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57. | | --- | --- | torch.distributed.all_reduce(tensor, op=<object object>, group=<object object>) \u5904\u7406\u6240\u6709\u673a\u5668\u4e0a\u7684\u5904\u7406\u7684\u5f20\u91cf\u6570\u636e, \u8ba1\u7b97\u6700\u7ec8\u7684\u7ed3\u679c. \u5728\u6240\u6709\u8fdb\u7a0b\u4e2d\u8c03\u7528 tensor \u5c06\u6309\u4f4d\u76f8\u540c. | Parameters: | tensor ( Tensor ) \u2013 \u96c6\u7fa4\u7684\u8f93\u5165\u548c\u8f93\u51fa. op ( optional ) \u2013 \u201ctorch.distributed.reduce_op\u201d \u679a\u4e3e\u503c\u4e4b\u4e00. \u6307\u5b9a\u7528\u4e8e\u5143\u7d20\u51cf\u5c11\u7684\u64cd\u4f5c. group ( optional ) \u2013 \u96c6\u7fa4\u7684\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57. | | --- | --- | torch.distributed.reduce(tensor, dst, op=<object object>, group=<object object>) \u51cf\u5c11\u6240\u6709\u673a\u5668\u4e0a\u7684\u5f20\u91cf\u6570\u636e. \u53ea\u6709\u7ea7\u522b\u4e3a dst \u7684\u8fdb\u7a0b\u624d\u4f1a\u6536\u5230\u6700\u7ec8\u7ed3\u679c. | Parameters: | tensor ( Tensor ) \u2013 \u96c6\u7fa4\u7684\u8f93\u5165\u548c\u8f93\u51fa\u6570\u636e. \u5206\u522b\u5728\u6bcf\u53f0\u673a\u5668\u4e0a\u672c\u5730\u5904\u7406. op ( optional ) \u2013 \u201ctorch.distributed.reduce_op\u201d \u679a\u4e3e\u503c\u4e4b\u4e00. \u6307\u5b9a\u7528\u4e8e\u5143\u7d20\u51cf\u5c11\u7684\u64cd\u4f5c. group ( optional ) \u2013 \u96c6\u7fa4\u7684\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57. | | --- | --- | torch.distributed.all_gather(tensor_list, tensor, group=<object object>) \u5728\u6574\u4e2a\u96c6\u7fa4\u4e2d\u6536\u96c6list\u8868\u683c\u4e2d\u7684\u5f20\u91cf. | Parameters: | tensor_list ( list [ Tensor ] ) \u2013 \u8f93\u51fa\u5217\u8868. \u5b83\u5e94\u8be5\u5305\u542b\u6b63\u786e\u5927\u5c0f\u7684\u5f20\u91cf\u4ee5\u7528\u4e8e\u96c6\u4f53\u7684\u8f93\u51fa. tensor ( Tensor ) \u2013 \u5f20\u91cf\u4ece\u5f53\u524d\u8fdb\u7a0b\u4e2d\u8fdb\u884c\u5e7f\u64ad. group ( optional ) \u2013 \u96c6\u7fa4\u7684\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57. | | --- | --- | torch.distributed.gather(tensor, **kwargs) \u6536\u96c6\u4e00\u4e2a\u5f20\u91cf\u5217\u8868\u4ece\u4e00\u4e2a\u5355\u4e00\u8fdb\u7a0b\u4e2d. | Parameters: | tensor ( Tensor ) \u2013 \u8f93\u5165\u7684\u6570\u636e. dst ( int ) \u2013 \u76ee\u7684\u5730\u7684 Rank. \u5305\u62ec\u9664\u4e86\u6b63\u5728\u63a5\u6536\u6570\u636e\u7684\u8fdb\u7a0b\u7684\u6240\u6709\u8fdb\u7a0b. gather_list ( list [ Tensor ] ) \u2013 \u7528\u4e8e\u63a5\u6536\u6570\u636e\u7684\u9002\u5f53\u5927\u5c0f\u7684\u5f20\u91cf\u5217\u8868. \u53ea\u5728\u63a5\u6536\u8fc7\u7a0b\u4e2d\u9700\u8981. group ( optional ) \u2013 \u96c6\u7fa4\u7684\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57. | | --- | --- | torch.distributed.scatter(tensor, **kwargs) \u5c06\u5f20\u91cf\u5217\u8868\u6563\u5e03\u5230\u5c0f\u7ec4\u4e2d\u7684\u6240\u6709\u8fdb\u7a0b. \u6bcf\u4e2a\u8fdb\u7a0b\u53ea\u4f1a\u6536\u5230\u4e00\u4e2a\u5f20\u91cf, \u5e76\u5c06\u5176\u6570\u636e\u5b58\u50a8\u5728 tensor \u7684\u53c2\u6570\u4e2d. | Parameters: | tensor ( Tensor ) \u2013 \u8f93\u51fa\u7684\u5f20\u91cf. src ( int ) \u2013 \u53d1\u9001\u7aef\u7684 Rank. \u5305\u62ec\u9664\u4e86\u6b63\u5728\u63a5\u6536\u6570\u636e\u7684\u8fdb\u7a0b\u7684\u6240\u6709\u8fdb\u7a0b. scatter_list ( list [ Tensor ] ) \u2013 \u5f20\u91cf\u5206\u6563\u7684\u5217\u8868. \u4ec5\u5728\u53d1\u9001\u6570\u636e\u7684\u8fc7\u7a0b\u4e2d\u9700\u8981. group ( optional ) \u2013 \u96c6\u7fa4\u7684\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57. | | --- | --- | torch.distributed.barrier(group=<object object>) \u540c\u6b65\u6240\u6709\u8fdb\u7a0b. \u8fd9\u4e2a\u96c6\u7fa4\u963b\u585e\u8fdb\u7a0b, \u76f4\u5230\u5168\u90e8\u7684\u5c0f\u7ec4\u7684\u8ba1\u7b97\u7ed3\u679c\u90fd\u8f93\u5165\u8fdb\u8fd9\u4e2a\u51fd\u6570\u4e2d. Parameters: group ( optional ) \u2013 \u96c6\u7fa4\u7684\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57.","title":"Distributed communication package - torch.distributed"},{"location":"62/#distributed-communication-package-torchdistributed","text":"torch.distributed \u63d0\u4f9b\u7c7b\u4f3c MPI \u7684\u524d\u5411\u8fd0\u7b97\u673a\u5236, \u652f\u6301\u5728\u591a\u53f0\u673a\u7684\u7f51\u7edc\u4e2d\u4ea4\u6362\u6570\u636e. \u652f\u6301\u4e0d\u540c\u7684\u540e\u6bb5\u548c\u521d\u59cb\u5316\u65b9\u6cd5. \u76ee\u524dtorch.distributed\u652f\u6301\u4e09\u4e2a\u540e\u7aef, \u6bcf\u4e2a\u90fd\u6709\u4e0d\u540c\u7684\u529f\u80fd. \u4e0b\u8868\u663e\u793a\u54ea\u4e9b\u529f\u80fd\u53ef\u7528\u4e8e CPU/CUDA \u5f20\u91cf. \u53ea\u6709\u5728\u8bbe\u5907\u4e0a\u7f16\u8bd1\u5b89\u88c5PyTorch, \u624d\u80fd\u5728MPI\u7684\u8bbe\u5907\u4e0a\u652f\u6301cuda. Backend tcp gloo mpi Device CPU GPU CPU --- --- --- --- send \u2713 \u2718 \u2718 recv \u2713 \u2718 \u2718 broadcast \u2713 \u2718 \u2713 all_reduce \u2713 \u2718 \u2713 reduce \u2713 \u2718 \u2718 all_gather \u2713 \u2718 \u2718 gather \u2713 \u2718 \u2718 scatter \u2713 \u2718 \u2718 barrier \u2713 \u2718 \u2713","title":"Distributed communication package - torch.distributed"},{"location":"62/#basics","text":"<cite>torch.distributed</cite> \u4e3a\u5728\u4e00\u53f0\u6216\u591a\u53f0\u673a\u5668\u4e0a\u8fd0\u884c\u7684\u591a\u4e2a\u8ba1\u7b97\u8282\u70b9\u63d0\u4f9b\u591a\u8fdb\u7a0b\u5e76\u884c\u7684\u901a\u4fe1\u6a21\u5757\u548cPyTorch\u7684\u652f\u6301. \u7c7b torch.nn.parallel.DistributedDataParallel() \u5efa\u7acb\u5728\u8fd9\u4e2a\u529f\u80fd\u4e4b\u4e0a, \u4ee5\u63d0\u4f9b\u4efb\u4f55PyTorch\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u88c5\u9970\u5668. \u8fd9\u4e2a\u7c7b\u548c Multiprocessing package - torch.multiprocessing \u548c torch.nn.DataParallel() \u5e76\u4e0d\u76f8\u540c, PyTorch\u96c6\u7fa4\u5206\u5e03\u5f0f\u8ba1\u7b97\u652f\u6301\u591a\u53f0\u673a\u5668, \u4f7f\u7528\u65f6\u7528\u6237\u5fc5\u987b\u5728\u4e3b\u8981\u8bad\u7ec3\u7684\u811a\u672c\u4e2d, \u660e\u786e\u5730\u5c06\u6bcf\u4e2a\u8fdb\u7a0b\u590d\u5236\u5230\u6bcf\u53f0\u673a\u5668\u4e2d. \u5728\u5355\u673a\u591a\u8282\u70b9\u8ba1\u7b97\u7684\u60c5\u51b5\u4e0b, \u4f7f\u7528 <cite>torch.distributed</cite> \u548c torch.nn.parallel.DistributedDataParallel() \u4f5c\u4e3a \u8bad\u7ec3\u7684\u88c5\u9970\u5668, \u76f8\u6bd4\u4e8e torch.nn.DataParallel() \u4e4b\u7c7b\u7684\u6570\u636e\u5e76\u884c\u8ba1\u7b97, \u4efb\u7136\u5177\u6709\u4f18\u52bf: \u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d, \u6bcf\u4e2a\u8fdb\u7a0b\u7ef4\u62a4\u81ea\u5df1\u7684\u4f18\u5316\u5668, \u6267\u884c\u5b8c\u6574\u7684\u4f18\u5316\u6b65\u9aa4. \u867d\u7136\u8fd9\u770b\u8d77\u6765\u53ef\u80fd\u662f\u591a\u4f59\u7684, \u4f46\u662f\u56e0\u4e3a\u68af\u5ea6\u5df2\u7ecf\u88ab\u6536\u96c6\u5728 \u4e00\u8d77, \u5e76\u4e14\u8ba1\u7b97\u4e86\u68af\u5ea6\u7684\u5e73\u5747\u503c, \u56e0\u6b64\u5bf9\u4e8e\u6bcf\u4e2a\u8fdb\u7a0b\u68af\u5ea6\u662f\u76f8\u540c\u7684, \u8fd9\u53ef\u4ee5\u51cf\u5c11\u5728\u8282\u70b9\u4e4b\u95f4\u4f20\u9012\u5f20\u91cf, \u518d\u8ba1\u7b97\u53c2\u6570\u7684\u65f6\u95f4. \u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u5305\u542b\u4e00\u4e2a\u72ec\u7acb\u7684Python\u89e3\u91ca\u5668, \u6d88\u9664\u4e86Python\u89e3\u91ca\u5668\u7684\u989d\u5916\u5f00\u9500, \u4ee5\u53ca\u7531\u4e8e\u9a71\u52a8\u591a\u7ebf\u7a0b, \u6a21\u578b\u526f\u672c\u548cGPU\u9020\u6210 \u201cGIL-thrashing\u201d . \u5bf9\u4e8e\u9700\u8981\u6d88\u8017\u5927\u91cfPython\u89e3\u91ca\u5668\u8fd0\u884c\u65f6\u95f4 (\u5305\u62ec\u5177\u6709\u5faa\u73af\u56fe\u5c42\u6216\u8bb8\u591a\u5c0f\u7ec4\u4ef6\u7684\u6a21\u578b) \u7684\u6a21\u578b\u6765\u8bf4\u662f\u975e\u5e38\u91cd\u8981\u7684.","title":"Basics"},{"location":"62/#initialization","text":"\u5728\u8c03\u7528\u5176\u4ed6\u6a21\u578b\u4e4b\u524d, \u8fd9\u4e2a\u5305\u9700\u8981\u4f7f\u7528 torch.distributed.init_process_group() \u51fd\u6570\u8fdb\u884c\u521d\u59cb\u5316. \u5728\u521d\u59cb\u5316\u5355\u5143\u4e2d, \u6240\u6709\u8fdb\u7a0b\u90fd\u4f1a\u53c2\u4e0e. torch.distributed.init_process_group(backend, init_method='env://', **kwargs) \u521d\u59cb\u5316\u65b9\u6cd5. | Parameters: | backend ( str ) \u2013 \u4f7f\u7528\u540e\u7aef\u7684\u540d\u5b57. \u8f93\u5165\u7684\u6709\u6548\u503c\u5305\u62ec: tcp , mpi and gloo . init_method ( str , optional ) \u2013 \u6307\u5b9a\u5982\u4f55\u521d\u59cb\u5316\u7684URL. world_size ( int , optional ) \u2013 \u53c2\u4e0e\u5de5\u4f5c\u7684\u8fdb\u7a0b\u6570\u91cf. rank ( int , optional ) \u2013 \u5f53\u524d\u8fdb\u7a0b\u7684\u6392\u540d. group_name ( str , optional ) \u2013 \u96c6\u7fa4\u7684\u540d\u5b57. \u8bf7\u53c2\u9605init\u65b9\u6cd5\u7684\u63cf\u8ff0. | | --- | --- | \u4e3a\u4e86\u652f\u6301 backend == mpi , PyTorch \u9700\u8981\u5728\u652f\u6301 MPI \u7684\u7cfb\u7edf\u4e0a\u7528\u8fdb\u884c\u6e90\u7801\u7f16\u8bd1\u5b89\u88c5 torch.distributed.get_rank() \u8fd4\u56de\u5f53\u524d\u8fdb\u7a0b\u7684\u6392\u540d. \u6392\u540d\u662f\u72ec\u4e00\u65e0\u4e8c\u7684 Rank\uff08\u6392\u540d\uff09\u662f\u5206\u914d\u7ed9\u5206\u5e03\u5f0f\u96c6\u7fa4\u4e2d\u6bcf\u4e2a\u8fdb\u7a0b\u7684\u552f\u4e00\u6807\u8bc6\u7b26. \u5b83\u4eec\u603b\u662f\u8fde\u7eed\u7684\u6574\u6570, \u8303\u56f4\u4ece0\u5230 world_size . torch.distributed.get_world_size() \u8fd4\u56de\u5728\u5206\u5e03\u5f0f\u96c6\u7fa4\u4e2d\u7684\u8fdb\u7a0b\u6570\u76ee. \u76ee\u524d\u652f\u6301\u4e09\u79cd\u521d\u59cb\u5316\u7684\u65b9\u6cd5:","title":"Initialization"},{"location":"62/#tcp-initialization","text":"\u63d0\u4f9b\u4e24\u79cdTCP\u7684\u521d\u59cb\u5316\u7684\u65b9\u6cd5, \u4e24\u79cd\u65b9\u6cd5\u90fd\u9700\u8981\u5404\u53f0\u673a\u5668\u7684\u7f51\u7edc\u5730\u5740\u548c\u96c6\u7fa4\u673a\u5668\u6570\u76ee world_size . \u7b2c\u4e00\u79cd\u65b9\u6cd5\u9700\u8981\u6307\u5b9a\u5c5e\u4e8e0\u7ea7\u8fdb\u7a0b\u7684\u5730\u5740, \u5e76\u4e14\u521d\u59cb\u5316\u65f6\u6240\u6709\u8fdb\u7a0b\u7684\u7b49\u7ea7\u90fd\u7531\u624b\u52a8\u6307\u5b9a. \u7b2c\u4e8c\u79cd\u65b9\u6cd5\u662f, \u5730\u5740\u5fc5\u987b\u662f\u6709\u6548\u7684IP\u591a\u64ad\u5730\u5740, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u53ef\u4ee5\u81ea\u52a8\u5206\u914d\u7b49\u7ea7. \u591a\u8def\u901a\u4fe1\u7684\u521d\u59cb\u5316\u4e5f\u652f\u6301 group_name \u53c2\u6570, \u5b83\u5141\u8bb8\u4f60\u4e3a\u591a\u4e2a\u4f5c\u4e1a\u4f7f\u7528\u76f8\u540c\u7684\u5730\u5740, \u53ea\u8981\u5b83\u4eec\u4f7f\u7528\u4e0d\u540c\u7684\u5c0f\u7ec4\u540d\u5373\u53ef. import torch.distributed as dist # Use address of one of the machines dist.init_process_group(init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4) # or a multicast address - rank will be assigned automatically if unspecified dist.init_process_group(init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456', world_size=4)","title":"TCP initialization"},{"location":"62/#shared-file-system-initialization","text":"\u53e6\u4e00\u4e2a\u521d\u59cb\u5316\u65b9\u6cd5\u4f7f\u7528\u4e00\u4e2a\u6587\u4ef6\u7cfb\u7edf, \u8fd9\u4e2a\u6587\u4ef6\u7cfb\u7edf\u5728\u4e00\u4e2a\u7ec4\u4e2d\u7684\u6240\u6709\u673a\u5668\u4e0a\u5171\u4eab\u548c\u53ef\u89c1, \u4ee5\u53ca\u4e00\u4e2a\u6240\u9700\u7684 world_size \u53c2\u6570. URL\u5e94\u8be5\u4ee5 file:// \u5f00\u5934, \u5e76\u5305\u542b\u4e00\u4e2a\u53ef\u4ee5\u548c\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\u6240\u6709\u73b0\u6709\u76ee\u5f55\u4e2d\u7684\u8def\u5f84\u76f8\u533a\u522b\u7684\u8def\u5f84, \u4f5c\u4e3aURL. \u8fd9\u4e2a\u521d\u59cb\u5316\u65b9\u6cd5\u4e5f \u652f\u6301 group_name \u53c2\u6570, \u5b83\u5141\u8bb8\u4f60\u4e3a\u591a\u4e2a\u4f5c\u4e1a\u4f7f\u7528\u76f8\u540c\u7684\u5171\u4eab\u6587\u4ef6\u8def\u5f84, \u53ea\u8981\u5b83\u4eec\u4f7f\u7528\u4e0d\u540c\u7684\u5c0f\u7ec4\u540d. Warning \u8fd9\u79cd\u65b9\u6cd5\u5047\u8bbe\u6587\u4ef6\u7cfb\u7edf\u652f\u6301\u4f7f\u7528 fcntl \u8fdb\u884c\u9501\u5b9a -\u5927\u591a\u6570\u672c\u5730\u7cfb\u7edf\u548cNFS\u90fd\u652f\u6301\u5b83. import torch.distributed as dist # Rank will be assigned automatically if unspecified dist.init_process_group(init_method='file:///mnt/nfs/sharedfile', world_size=4, group_name=args.group)","title":"Shared file-system initialization"},{"location":"62/#environment-variable-initialization","text":"\u6b64\u65b9\u6cd5\u5c06\u4ece\u73af\u5883\u53d8\u91cf\u4e2d\u8bfb\u53d6\u914d\u7f6e, \u4ece\u800c\u53ef\u4ee5\u5b8c\u5168\u81ea\u5b9a\u4e49\u5982\u4f55\u83b7\u53d6\u4fe1\u606f. \u8981\u8bbe\u7f6e\u7684\u53d8\u91cf\u662f: MASTER_PORT - \u9700\u8981; \u5fc5\u987b\u662f0\u7ea7\u673a\u5668\u4e0a\u7684\u81ea\u7531\u7aef\u53e3 MASTER_ADDR - \u9700\u8981 (\u9664\u4e86\u7b49\u7ea70) ; \u7b49\u7ea70\u8282\u70b9\u7684\u5730\u5740 WORLD_SIZE - \u9700\u8981; \u53ef\u4ee5\u5728\u8fd9\u91cc\u8bbe\u7f6e, \u6216\u8005\u5728\u8c03\u7528init\u51fd\u6570 RANK - \u9700\u8981; \u53ef\u4ee5\u5728\u8fd9\u91cc\u8bbe\u7f6e, \u6216\u8005\u5728\u8c03\u7528init\u51fd\u6570 \u7b49\u7ea7\u4e3a0\u7684\u673a\u5668\u5c06\u7528\u4e8e\u8bbe\u7f6e\u6240\u6709\u8fde\u63a5. \u8fd9\u662f\u9ed8\u8ba4\u7684\u65b9\u6cd5, \u8fd9\u610f\u5473\u7740 init_method \u4e0d\u5fc5\u88ab\u7279\u522b\u6307\u5b9a(\u6216\u8005\u53ef\u4ee5\u662f env:// )","title":"Environment variable initialization"},{"location":"62/#groups","text":"\u9ed8\u8ba4\u7684\u96c6\u7fa4 (collectives) \u64cd\u4f5c\u9ed8\u8ba4\u7684\u5c0f\u7ec4 (group), \u8981\u6c42\u6240\u6709\u7684\u8fdb\u7a0b\u8fdb\u5165\u5206\u5e03\u5f0f\u51fd\u6570\u4e2d\u8c03\u7528. \u4e00\u4e9b\u5de5\u4f5c\u8d1f\u8f7d\u53ef\u4ee5\u4ece\u53ef\u4ee5\u4ece\u66f4\u7ec6\u7c92\u5ea6\u7684\u901a\u4fe1\u4e2d\u53d7\u76ca \u8fd9\u662f\u5206\u5e03\u5f0f\u96c6\u7fa4\u53d1\u6325\u4f5c\u7528\u7684\u5730\u65b9. new_group() \u51fd\u6570\u53ef\u4ee5\u7528\u6765\u521b\u5efa\u65b0\u7684\u7ec4, \u5e76\u4e14\u5305\u542b\u6240\u6709\u8fdb\u7a0b\u7684\u4efb\u610f\u5b50\u96c6. \u5b83\u8fd4\u56de\u4e00\u4e2a\u4e0d\u900f\u660e\u7684\u7ec4\u53e5\u67c4, \u5b83\u53ef\u4ee5\u4f5c\u4e3a\u96c6\u7fa4\u7684 group \u53c2\u6570 (\u96c6\u7fa4 collectives \u662f\u4e00\u822c\u7684\u7f16\u7a0b\u6a21\u5f0f\u4e2d\u7684\u4ea4\u6362\u4fe1\u606f\u7684\u5206\u5e03\u5f0f\u51fd\u6570) . torch.distributed.new_group(ranks=None) \u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5206\u5e03\u5f0f\u5c0f\u7ec4 \u6b64\u51fd\u6570\u8981\u6c42\u4e3b\u7ec4\u4e2d\u7684\u6240\u6709\u8fdb\u7a0b\uff08\u5373\u4f5c\u4e3a\u5206\u5e03\u5f0f\u4f5c\u4e1a\u4e00\u90e8\u5206\u7684\u6240\u6709\u8fdb\u7a0b\uff09\u90fd\u4f1a\u8f93\u5165\u6b64\u51fd\u6570, \u5373\u4f7f\u5b83\u4eec\u4e0d\u662f\u8be5\u5c0f\u7ec4\u7684\u6210\u5458. \u6b64\u5916, \u5e94\u8be5\u5728\u6240\u6709\u7684\u8fdb\u7a0b\u4e2d\u4ee5\u76f8\u540c\u7684\u987a\u5e8f\u521b\u5efa\u65b0\u7684\u5c0f\u7ec4. Parameters: ranks ( list [ int ] ) \u2013 \u5c0f\u7ec4\u5185\u6210\u5458\u7684 Rank \u7684\u5217\u8868. Returns: \u5206\u914d\u7ec4\u7684\u53e5\u67c4, \u4ee5\u4fbf\u5728\u96c6\u7fa4\u4e2d\u8c03\u7528. --- ---","title":"Groups"},{"location":"62/#point-to-point-communication","text":"torch.distributed.send(tensor, dst) \u540c\u6b65\u53d1\u9001\u5f20\u91cf. | Parameters: | tensor ( Tensor ) \u2013 \u53d1\u9001\u7684\u5f20\u91cf. dst ( int ) \u2013 \u6307\u5b9a\u53d1\u9001\u7684\u76ee\u7684\u5730\u7684 Rank. | | --- | --- | torch.distributed.recv(tensor, src=None) \u540c\u6b65\u63a5\u6536\u5f20\u91cf. | Parameters: | tensor ( Tensor ) \u2013 \u7528\u6536\u5230\u7684\u6570\u636e\u586b\u5145\u5f20\u91cf. src ( int , optional ) \u2013 \u53d1\u9001\u7aef\u7684Rank, \u5982\u679c\u6ca1\u6709\u6307\u5b9a, \u5c06\u4f1a\u63a5\u6536\u4efb\u4f55\u53d1\u9001\u7684\u6570\u636e. | | --- | --- | | Returns: | \u53d1\u9001\u7aef\u7684Rank. | | --- | --- | isend() \u548c irecv() \u4f7f\u7528\u65f6\u8fd4\u56de\u5206\u5e03\u5f0f\u8bf7\u6c42\u5bf9\u8c61. \u901a\u5e38, \u8fd9\u4e2a\u5bf9\u8c61\u7684\u7c7b\u578b\u662f\u672a\u6307\u5b9a\u7684, \u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u4f7f\u7528\u624b\u52a8\u521b\u5efa, \u4f46\u662f\u5b83\u4eec\u652f\u6301\u4e24\u79cd\u65b9\u6cd5\u6307\u5b9a: is_completed() - \u5982\u679c\u64cd\u4f5c\u5b8c\u6210\u8fd4\u56deTrue wait() - \u5982\u679c\u64cd\u4f5c\u5b8c\u6210\u4f1a\u963b\u585e\u6240\u6709\u7684\u8fdb\u7a0b. is_completed() \u5982\u679c\u7ed3\u679c\u8fd4\u56de, \u4fdd\u8bc1\u51fd\u6570\u8fd4\u56deTrue. \u5f53\u4f7f\u7528MPI\u4f5c\u4e3a\u540e\u7aef, isend() \u548c irecv() \u652f\u6301 \u201c\u4e0d\u8d85\u8f66\u201d \u5f0f\u7684\u5de5\u4f5c\u65b9\u5f0f, \u8fd9\u79cd\u65b9\u5f0f\u53ef\u4ee5\u4fdd\u8bc1\u6d88\u606f\u7684\u987a\u5e8f. \u66f4\u591a\u7684\u7ec6\u8282\u53ef\u4ee5\u770b http://mpi-forum.org/docs/mpi-2.2/mpi22-report/node54.htm#Node54 torch.distributed.isend(tensor, dst) \u5f02\u6b65\u53d1\u9001\u5f20\u91cf\u6570\u636e. | Parameters: | tensor ( Tensor ) \u2013 \u53d1\u9001\u7684\u5f20\u91cf\u7684\u6570\u636e. dst ( int ) \u2013 \u6307\u5b9a\u53d1\u9001\u5230\u7684 Rank. | | --- | --- | | Returns: | \u5206\u5e03\u5f0f\u8bf7\u6c42\u5bf9\u8c61. | | --- | --- | torch.distributed.irecv(tensor, src) \u5f02\u6b65\u63a5\u6536\u5f20\u91cf. | Parameters: | tensor ( Tensor ) \u2013 \u7528\u6536\u5230\u7684\u6570\u636e\u586b\u5145\u5f20\u91cf. src ( int ) \u2013 \u6307\u5b9a\u53d1\u9001\u5f20\u91cf\u7684 Rank. | | --- | --- | | Returns: | \u4e00\u4e2a\u5206\u5e03\u5f0f\u8bf7\u6c42\u5bf9\u8c61. | | --- | --- |","title":"Point-to-point communication"},{"location":"62/#collective-functions","text":"torch.distributed.broadcast(tensor, src, group=<object object>) \u5411\u67d0\u4e2a\u5c0f\u7ec4\u5185\u7684\u5f20\u91cf\u5e7f\u64ad\u7684\u65b9\u6cd5. tensor \u5728\u8be5\u5c0f\u7ec4\u5904\u7406\u6570\u636e\u7684\u6240\u6709\u8fc7\u7a0b\u4e2d\u5143\u7d20\u7684\u6570\u76ee\u5fc5\u987b\u76f8\u540c. | Parameters: | tensor ( Tensor ) \u2013 \u5982\u679c\u53d1\u9001\u7aef src \u662f\u5f53\u524d\u8fdb\u7a0b\u7684 Rank, \u5219\u53d1\u9001\u6570\u636e, \u5426\u5219\u4f7f\u7528\u5f20\u91cf\u4fdd\u5b58\u63a5\u6536\u7684\u6570\u636e. src ( int ) \u2013 \u53d1\u9001\u7aef\u7684 Rank. group ( optional ) \u2013 \u96c6\u7fa4\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57. | | --- | --- | torch.distributed.all_reduce(tensor, op=<object object>, group=<object object>) \u5904\u7406\u6240\u6709\u673a\u5668\u4e0a\u7684\u5904\u7406\u7684\u5f20\u91cf\u6570\u636e, \u8ba1\u7b97\u6700\u7ec8\u7684\u7ed3\u679c. \u5728\u6240\u6709\u8fdb\u7a0b\u4e2d\u8c03\u7528 tensor \u5c06\u6309\u4f4d\u76f8\u540c. | Parameters: | tensor ( Tensor ) \u2013 \u96c6\u7fa4\u7684\u8f93\u5165\u548c\u8f93\u51fa. op ( optional ) \u2013 \u201ctorch.distributed.reduce_op\u201d \u679a\u4e3e\u503c\u4e4b\u4e00. \u6307\u5b9a\u7528\u4e8e\u5143\u7d20\u51cf\u5c11\u7684\u64cd\u4f5c. group ( optional ) \u2013 \u96c6\u7fa4\u7684\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57. | | --- | --- | torch.distributed.reduce(tensor, dst, op=<object object>, group=<object object>) \u51cf\u5c11\u6240\u6709\u673a\u5668\u4e0a\u7684\u5f20\u91cf\u6570\u636e. \u53ea\u6709\u7ea7\u522b\u4e3a dst \u7684\u8fdb\u7a0b\u624d\u4f1a\u6536\u5230\u6700\u7ec8\u7ed3\u679c. | Parameters: | tensor ( Tensor ) \u2013 \u96c6\u7fa4\u7684\u8f93\u5165\u548c\u8f93\u51fa\u6570\u636e. \u5206\u522b\u5728\u6bcf\u53f0\u673a\u5668\u4e0a\u672c\u5730\u5904\u7406. op ( optional ) \u2013 \u201ctorch.distributed.reduce_op\u201d \u679a\u4e3e\u503c\u4e4b\u4e00. \u6307\u5b9a\u7528\u4e8e\u5143\u7d20\u51cf\u5c11\u7684\u64cd\u4f5c. group ( optional ) \u2013 \u96c6\u7fa4\u7684\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57. | | --- | --- | torch.distributed.all_gather(tensor_list, tensor, group=<object object>) \u5728\u6574\u4e2a\u96c6\u7fa4\u4e2d\u6536\u96c6list\u8868\u683c\u4e2d\u7684\u5f20\u91cf. | Parameters: | tensor_list ( list [ Tensor ] ) \u2013 \u8f93\u51fa\u5217\u8868. \u5b83\u5e94\u8be5\u5305\u542b\u6b63\u786e\u5927\u5c0f\u7684\u5f20\u91cf\u4ee5\u7528\u4e8e\u96c6\u4f53\u7684\u8f93\u51fa. tensor ( Tensor ) \u2013 \u5f20\u91cf\u4ece\u5f53\u524d\u8fdb\u7a0b\u4e2d\u8fdb\u884c\u5e7f\u64ad. group ( optional ) \u2013 \u96c6\u7fa4\u7684\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57. | | --- | --- | torch.distributed.gather(tensor, **kwargs) \u6536\u96c6\u4e00\u4e2a\u5f20\u91cf\u5217\u8868\u4ece\u4e00\u4e2a\u5355\u4e00\u8fdb\u7a0b\u4e2d. | Parameters: | tensor ( Tensor ) \u2013 \u8f93\u5165\u7684\u6570\u636e. dst ( int ) \u2013 \u76ee\u7684\u5730\u7684 Rank. \u5305\u62ec\u9664\u4e86\u6b63\u5728\u63a5\u6536\u6570\u636e\u7684\u8fdb\u7a0b\u7684\u6240\u6709\u8fdb\u7a0b. gather_list ( list [ Tensor ] ) \u2013 \u7528\u4e8e\u63a5\u6536\u6570\u636e\u7684\u9002\u5f53\u5927\u5c0f\u7684\u5f20\u91cf\u5217\u8868. \u53ea\u5728\u63a5\u6536\u8fc7\u7a0b\u4e2d\u9700\u8981. group ( optional ) \u2013 \u96c6\u7fa4\u7684\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57. | | --- | --- | torch.distributed.scatter(tensor, **kwargs) \u5c06\u5f20\u91cf\u5217\u8868\u6563\u5e03\u5230\u5c0f\u7ec4\u4e2d\u7684\u6240\u6709\u8fdb\u7a0b. \u6bcf\u4e2a\u8fdb\u7a0b\u53ea\u4f1a\u6536\u5230\u4e00\u4e2a\u5f20\u91cf, \u5e76\u5c06\u5176\u6570\u636e\u5b58\u50a8\u5728 tensor \u7684\u53c2\u6570\u4e2d. | Parameters: | tensor ( Tensor ) \u2013 \u8f93\u51fa\u7684\u5f20\u91cf. src ( int ) \u2013 \u53d1\u9001\u7aef\u7684 Rank. \u5305\u62ec\u9664\u4e86\u6b63\u5728\u63a5\u6536\u6570\u636e\u7684\u8fdb\u7a0b\u7684\u6240\u6709\u8fdb\u7a0b. scatter_list ( list [ Tensor ] ) \u2013 \u5f20\u91cf\u5206\u6563\u7684\u5217\u8868. \u4ec5\u5728\u53d1\u9001\u6570\u636e\u7684\u8fc7\u7a0b\u4e2d\u9700\u8981. group ( optional ) \u2013 \u96c6\u7fa4\u7684\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57. | | --- | --- | torch.distributed.barrier(group=<object object>) \u540c\u6b65\u6240\u6709\u8fdb\u7a0b. \u8fd9\u4e2a\u96c6\u7fa4\u963b\u585e\u8fdb\u7a0b, \u76f4\u5230\u5168\u90e8\u7684\u5c0f\u7ec4\u7684\u8ba1\u7b97\u7ed3\u679c\u90fd\u8f93\u5165\u8fdb\u8fd9\u4e2a\u51fd\u6570\u4e2d. Parameters: group ( optional ) \u2013 \u96c6\u7fa4\u7684\u5185\u7684\u5c0f\u7ec4\u7684\u540d\u5b57.","title":"Collective functions"},{"location":"63/","text":"Legacy package - torch.legacy \u5305\u542b\u4ece Lua torch \u79fb\u690d\u7684\u4ee3\u7801\u7684\u5305. \u4e3a\u4e86\u80fd\u591f\u4e0e\u73b0\u6709\u7684\u6a21\u578b\u4e00\u8d77\u5de5\u4f5c, \u5e76\u7b80\u5316\u5f53\u524d Lua torch \u7528\u6237\u7684\u8fc7\u6e21, \u6211\u4eec\u7279\u610f\u521b\u5efa\u4e86\u8fd9\u4e2a\u5305. \u60a8\u53ef\u4ee5\u5728 torch.legacy.nn \u4e2d\u627e\u5230 nn \u4ee3\u7801, \u5e76\u5728 torch.legacy.optim \u4e2d\u8fdb\u884c optim \u4f18\u5316. \u8be5 API \u5e94\u8be5\u5b8c\u9002\u914d Lua torch.","title":"Legacy package - torch.legacy"},{"location":"63/#legacy-package-torchlegacy","text":"\u5305\u542b\u4ece Lua torch \u79fb\u690d\u7684\u4ee3\u7801\u7684\u5305. \u4e3a\u4e86\u80fd\u591f\u4e0e\u73b0\u6709\u7684\u6a21\u578b\u4e00\u8d77\u5de5\u4f5c, \u5e76\u7b80\u5316\u5f53\u524d Lua torch \u7528\u6237\u7684\u8fc7\u6e21, \u6211\u4eec\u7279\u610f\u521b\u5efa\u4e86\u8fd9\u4e2a\u5305. \u60a8\u53ef\u4ee5\u5728 torch.legacy.nn \u4e2d\u627e\u5230 nn \u4ee3\u7801, \u5e76\u5728 torch.legacy.optim \u4e2d\u8fdb\u884c optim \u4f18\u5316. \u8be5 API \u5e94\u8be5\u5b8c\u9002\u914d Lua torch.","title":"Legacy package - torch.legacy"},{"location":"64/","text":"torch.cuda \u8fd9\u4e2a\u5305\u589e\u52a0\u4e86\u5bf9 CUDA tensor (\u5f20\u91cf) \u7c7b\u578b\u7684\u652f\u6301,\u5229\u7528 GPUs \u8ba1\u7b97\u5b9e\u73b0\u4e86\u4e0e CPU tensors \u76f8\u540c\u7684\u7c7b\u578b. \u8fd9\u4e2a\u662f lazily initialized (\u61d2\u52a0\u8f7d,\u5ef6\u8fdf\u52a0\u8f7d), \u6240\u4ee5\u4f60\u53ef\u4ee5\u4e00\u76f4\u5bfc\u5165\u5b83,\u5e76\u4e14\u53ef\u4ee5\u7528 is_available() \u6765\u5224\u65ad \u4f60\u7684\u7cfb\u7edf\u662f\u5426\u652f\u6301 CUDA. CUDA \u8bed\u4e49 \u6709\u66f4\u591a\u5173\u4e8e\u4f7f\u7528 CUDA \u7684\u7ec6\u8282. torch.cuda.current_blas_handle() \u8fd4\u56de\u6307\u5411\u5f53\u524d cuBLAS \u53e5\u67c4\u7684 cublasHandle_t \u6307\u9488 torch.cuda.current_device() \u8fd4\u56de\u5f53\u524d\u9009\u62e9\u7684\u8bbe\u5907\u7684\u7d22\u5f15. torch.cuda.current_stream() \u8fd4\u56de\u5f53\u524d\u9009\u62e9\u7684 Stream . class torch.cuda.device(idx) \u66f4\u6539\u9009\u5b9a\u8bbe\u5907\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668. Parameters: idx ( int ) \u2013 \u9009\u62e9\u8bbe\u5907\u7f16\u53f7. \u5982\u679c\u53c2\u6570\u65e0\u6548,\u5219\u662f\u65e0\u6548\u64cd\u4f5c. torch.cuda.device_count() \u8fd4\u56de\u53ef\u7528\u7684 GPU \u6570\u91cf. torch.cuda.``device_ctx_manager alias of device class torch.cuda.device_of(obj) \u5c06\u5f53\u524d\u8bbe\u5907\u66f4\u6539\u4e3a\u7ed9\u5b9a\u5bf9\u8c61\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668. \u53ef\u4ee5\u4f7f\u7528\u5f20\u91cf\u548c\u5b58\u50a8\u4f5c\u4e3a\u53c2\u6570,\u5982\u679c\u7ed9\u5b9a\u7684\u5bf9\u8c61\u4e0d\u662f\u5728 GPU \u4e0a\u5206\u914d\u7684,\u8fd9\u662f\u4e00\u4e2a\u65e0\u6548\u64cd\u4f5c. Parameters: obj ( Tensor or Storage ) \u2013 \u5728\u9009\u5b9a\u8bbe\u5907\u4e0a\u5206\u914d\u7684\u5bf9\u8c61. torch.cuda.empty_cache() \u91ca\u653e\u5f53\u524d\u7531\u7f13\u5b58\u6301\u6709\u7684\u6240\u6709\u672a\u5360\u7528\u7f13\u5b58\u5185\u5b58\u5206\u914d\u5668,\u4ee5\u4fbf\u53ef\u4ee5\u5728\u5176\u4ed6GPU\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u5e76\u5728 <cite>nvidia-smi</cite> \u4e2d\u53ef\u89c1. torch.cuda.get_device_capability(device) \u83b7\u53d6\u8bbe\u5907\u7684 CUDA \u7b97\u529b. Parameters: device ( int ) \u2013 \u8fd4\u56de\u8bbe\u5907\u540d, \u53c2\u6570\u65e0\u6548\u65f6, \u65b9\u6cd5\u5931\u6548. Returns: \u8bbe\u5907\u7684\u4e3b\u6b21\u8981 CUDA \u7b97\u529b. --- --- Return type: tuple ( int , int ) --- --- torch.cuda.get_device_name(device) \u83b7\u53d6\u8bbe\u5907\u540d. Parameters: device ( int ) \u2013 \u8fd4\u56de\u8bbe\u5907\u540d. \u53c2\u6570\u65e0\u6548\u65f6,\u5219\u662f\u65e0\u6548\u64cd\u4f5c. torch.cuda.is_available() \u8fd4\u56de\u4e00\u4e2a bool \u503c\u8868\u793a CUDA \u76ee\u524d\u662f\u5426\u53ef\u7528. torch.cuda.set_device(device) \u8bbe\u7f6e\u5f53\u524d\u8bbe\u5907. \u4e0d\u9f13\u52b1\u4f7f\u7528\u8fd9\u4e2a\u51fd\u6570 device . \u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b,\u6700\u597d\u4f7f\u7528 CUDA_VISIBLE_DEVICES \u73af\u5883\u53d8\u91cf. Parameters: device ( int ) \u2013 \u9009\u62e9\u8bbe\u5907. \u53c2\u6570\u65e0\u6548\u65f6,\u5219\u662f\u65e0\u6548\u64cd\u4f5c. torch.cuda.stream(*args, **kwds) \u9009\u62e9\u7ed9\u5b9a\u6d41\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668. \u5728\u9009\u5b9a\u7684\u6d41\u4e0a, \u6240\u6709\u7684CUDA\u5185\u6838\u5728\u5176\u4e0a\u4e0b\u6587\u5185\u6392\u961f. Parameters: stream ( Stream ) \u2013 \u9009\u62e9\u6d41. \u5982\u679c\u662f None , \u7ba1\u7406\u5668\u65e0\u6548. torch.cuda.synchronize() \u7b49\u5f85\u5f53\u524d\u8bbe\u5907\u4e0a\u6240\u6709\u6d41\u4e2d\u7684\u6240\u6709\u5185\u6838\u5b8c\u6210. Random Number Generator torch.cuda.get_rng_state(device=-1) \u5c06\u5f53\u524d GPU \u7684\u968f\u673a\u6570\u751f\u6210\u5668\u72b6\u6001\u4f5c\u4e3a ByteTensor \u8fd4\u56de. Parameters: device ( int , optional ) \u2013 \u8bbe\u5907\u7684 RNG \u72b6\u6001. Default: -1 (i.e., \u4f7f\u7528\u5f53\u524d\u8bbe\u5907). Warning \u51fd\u6570\u9700\u8981\u63d0\u524d\u521d\u59cb\u5316 CUDA . torch.cuda.set_rng_state(new_state, device=-1) \u8bbe\u7f6e\u5f53\u524d GPU \u7684\u968f\u673a\u6570\u53d1\u751f\u5668\u72b6\u6001. Parameters: new_state ( torch.ByteTensor ) \u2013 \u6240\u9700\u7684\u72b6\u6001 torch.cuda.manual_seed(seed) \u8bbe\u7f6e\u7528\u4e8e\u5f53\u524d GPU \u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50. \u5982\u679c CUDA \u4e0d\u53ef\u7528,\u8c03\u7528\u8fd9\u4e2a\u51fd\u6570\u662f\u5b89\u5168\u7684;\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u5b83\u5c06\u88ab\u5ffd\u7565. Parameters: seed ( int or long ) \u2013 \u6240\u9700\u7684\u79cd\u5b50. Warning \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528\u591a GPU \u6a21\u578b,\u5219\u6b64\u529f\u80fd\u4e0d\u8db3\u4ee5\u83b7\u5f97\u786e\u5b9a\u6027. seef\u4f5c\u7528\u4e8e\u6240\u6709 GPUs , \u4f7f\u7528 manual_seed_all() . torch.cuda.manual_seed_all(seed) \u8bbe\u7f6e\u5728\u6240\u6709 GPU \u4e0a\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50. \u5982\u679c CUDA \u4e0d\u53ef\u7528, \u8c03\u7528\u6b64\u51fd\u6570\u662f\u5b89\u5168\u7684; \u8fd9\u79cd\u60c5\u51b5\u4e0b,\u4f1a\u88ab\u5ffd\u7565. Parameters: seed ( int or long ) \u2013 \u6240\u9700\u7684\u79cd\u5b50. torch.cuda.seed() \u5c06\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50\u8bbe\u7f6e\u4e3a\u5f53\u524d GPU \u7684\u968f\u673a\u6570. \u5982\u679c CUDA \u4e0d\u53ef\u7528,\u5219\u8c03\u7528\u6b64\u51fd\u6570\u662f\u5b89\u5168\u7684. \u5728\u90a3\u79cd\u60c5\u51b5\u4e0b,\u4f1a\u88ab\u5ffd\u7565. Warning \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528\u591a GPU \u6a21\u578b, \u5219\u6b64\u529f\u80fd\u4e0d\u8db3\u4ee5\u83b7\u5f97\u786e\u5b9a\u6027. seef\u4f5c\u7528\u4e8e\u6240\u6709 GPUs , \u4f7f\u7528 seed_all() . torch.cuda.seed_all() \u5728\u6240\u6709 GPU \u4e0a\u5c06\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50\u8bbe\u7f6e\u4e3a\u968f\u673a\u6570. \u5982\u679c CUDA \u4e0d\u53ef\u7528,\u5219\u8c03\u7528\u6b64\u51fd\u6570\u662f\u5b89\u5168\u7684. \u5728\u90a3\u79cd\u60c5\u51b5\u4e0b,\u4f1a\u88ab\u5ffd\u7565. torch.cuda.initial_seed() \u8fd4\u56de\u5f53\u524d GPU \u7684\u5f53\u524d\u968f\u673a\u79cd\u5b50. Warning \u51fd\u6570\u63d0\u524d\u521d\u59cb\u5316 CUDA . Communication collectives torch.cuda.comm.broadcast(tensor, devices) \u5c06\u5f20\u91cf\u5e7f\u64ad\u7ed9\u591a\u4e2a GPU . | Parameters: | tensor ( Tensor ) \u2013 \u9700\u8981\u5e7f\u64ad\u7684\u5f20\u91cf. devices ( Iterable ) \u2013 \u5728\u4e00\u4e2a\u53ef\u8fed\u4ee3\u8bbe\u5907\u4e2d\u5e7f\u64ad. \u8bf7\u6ce8\u610f, \u5b83\u5e94\u8be5\u50cf (src, dst1, dst2, \u2026), \u5176\u4e2d\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u662f\u6765\u81f3\u5176\u5e7f\u64ad\u7684\u6e90\u8bbe\u5907. | | --- | --- | | Returns: | \u4e00\u4e2a\u5143\u7ec4, \u5305\u542b tensor \u526f\u672c,\u653e\u7f6e\u5728\u4e0e\u8bbe\u5907\u7684\u7d22\u5f15\u76f8\u5bf9\u5e94\u7684 \u8bbe\u5907 \u4e0a. | | --- | --- | torch.cuda.comm.reduce_add(inputs, destination=None) \u4ece\u591a\u4e2a GPU \u4e2d\u6536\u96c6\u5f20\u91cf. \u6240\u6709\u7684\u8f93\u5165\u5e94\u8be5\u6709\u5339\u914d\u7684 shapes (\u5f62\u72b6). | Parameters: | inputs ( Iterable__[ Tensor ] ) \u2013 \u6dfb\u52a0\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u5f20\u91cf. destination ( int , optional ) \u2013 \u653e\u7f6e\u8f93\u51fa\u7684\u8bbe\u5907 (\u9ed8\u8ba4: \u5f53\u524d\u8bbe\u5907). | | --- | --- | | Returns: | \u5305\u542b\u6240\u6709\u8f93\u5165\u7684\u5143\u7d20\u548c\u7684\u5f20\u91cf, \u5b58\u653e\u5728 destination(\u76ee\u6807) \u8bbe\u5907. | | --- | --- | torch.cuda.comm.scatter(tensor, devices, chunk_sizes=None, dim=0, streams=None) \u5206\u6563\u5f20\u91cf\u5230\u591a\u4e2a GPU. | Parameters: | tensor ( Tensor ) \u2013 \u9700\u8981\u5206\u6563\u7684\u5f20\u91cf. devices ( Iterable__[ int ] ) \u2013 \u6574\u6570\u7684\u8fed\u4ee3,\u6307\u5b9a\u5f20\u91cf\u5e94\u5206\u6563\u5728\u54ea\u4e9b\u8bbe\u5907\u4e4b\u95f4. chunk_sizes ( Iterable__[ int ]__, optional ) \u2013 \u8981\u653e\u5728\u6bcf\u4e2a\u8bbe\u5907\u4e0a\u7684\u5757\u7684\u5927\u5c0f. \u5e94\u8be5\u5339\u914d \u8bbe\u5907 \u957f\u5ea6\u548c tensor.size(dim) \u7684\u548c. \u5982\u679c\u672a\u6307\u5b9a,\u5f20\u91cf\u5c06\u88ab\u5212\u5206\u6210\u76f8\u7b49\u7684\u5757. dim ( int , optional ) \u2013 \u5206\u5757\u5f20\u91cf\u6cbf\u7740\u7684\u7ef4\u5ea6 | | --- | --- | | Returns: | \u4e00\u4e2a\u5143\u7ec4\u5305\u542b tensor \u5757, \u4f20\u9012\u7ed9 devices . | | --- | --- | torch.cuda.comm.gather(tensors, dim=0, destination=None) \u4ece\u591a\u4e2a GPU \u6536\u96c6\u5f20\u91cf. \u5f20\u91cf\u5c3a\u5bf8\u5728\u4e0d\u540c\u4e8e dim \u7684\u7ef4\u5ea6\u4e0a\u90fd\u5e94\u8be5\u5339\u914d. | Parameters: | tensors ( Iterable__[ Tensor ] ) \u2013 \u5f20\u91cf\u96c6\u5408\u7684\u8fed\u4ee3\u5668. dim ( int ) \u2013 \u5f20\u91cf\u88ab\u8fde\u63a5\u7684\u7ef4\u5ea6. destination ( int , optional ) \u2013 \u8f93\u51fa\u8bbe\u5907 (-1 \u4ee3\u8868 CPU, \u9ed8\u8ba4: \u5f53\u524d\u8bbe\u5907) | | --- | --- | | Returns: | \u4e00\u4e2a\u4f4d\u4e8e \u76ee\u6807 \u8bbe\u5907\u4e0a\u7684\u5f20\u91cf, \u5c06 tensors \u6cbf\u7740 dim \u8fde\u63a5\u8d77\u6765\u7684\u7ed3\u679c. | | --- | --- | Streams and events class torch.cuda.Stream CUDA \u6d41\u7684\u5305\u88c5. | Parameters: | device ( int , optional ) \u2013 \u5206\u914d\u6d41\u7684\u8bbe\u5907. priority ( int , optional ) \u2013 \u6d41\u7684\u4f18\u5148\u7ea7. \u8f83\u4f4e\u7684\u6570\u5b57\u4ee3\u8868\u8f83\u9ad8\u7684\u4f18\u5148\u7ea7. | | --- | --- | query() \u68c0\u67e5\u4e8b\u4ef6\u662f\u5426\u5df2\u88ab\u8bb0\u5f55. Returns: \u4e00\u4e2a BOOL \u503c, \u6307\u793a\u4e8b\u4ef6\u662f\u5426\u5df2\u88ab\u8bb0\u5f55. record_event(event=None) \u8bb0\u5f55\u4e00\u4e2a\u4e8b\u4ef6. Parameters: event ( Event , optional ) \u2013 \u8981\u8bb0\u5f55\u7684\u4e8b\u4ef6.\u5982\u679c\u6ca1\u6709\u7ed9\u51fa,\u5c06\u5206\u914d\u4e00\u4e2a\u65b0\u7684. Returns: \u8bb0\u5f55\u7684\u4e8b\u4ef6. --- --- synchronize() \u7b49\u5f85\u6d41\u4e2d\u7684\u6240\u6709\u5185\u6838\u5b8c\u6210. wait_event(event) \u5c06\u6240\u6709\u672a\u6765\u7684\u5de5\u4f5c\u63d0\u4ea4\u5230\u6d41\u7b49\u5f85\u4e8b\u4ef6. Parameters: event ( Event ) \u2013 \u7b49\u5f85\u7684\u4e8b\u4ef6. wait_stream(stream) \u4e0e\u53e6\u4e00\u4e2a\u6d41\u540c\u6b65. \u63d0\u4ea4\u5230\u6b64\u6d41\u7684\u6240\u6709\u672a\u6765\u5de5\u4f5c\u5c06\u7b49\u5f85\u76f4\u5230\u6240\u6709\u6838\u5fc3\u5728\u8c03\u7528\u5b8c\u6210\u65f6\u63d0\u4ea4\u7ed9\u7ed9\u5b9a\u7684\u6d41. Parameters: stream ( Stream ) \u2013 \u540c\u6b65\u6d41. class torch.cuda.Event(enable_timing=False, blocking=False, interprocess=False, _handle=None) CUDA \u4e8b\u4ef6\u5305\u88c5\u5668. | Parameters: | enable_timing ( bool ) \u2013 \u6307\u793a\u4e8b\u4ef6\u662f\u5426\u5e94\u6d4b\u91cf\u65f6\u95f4 (\u9ed8\u8ba4: False ) blocking ( bool ) \u2013 \u5982\u679c True , wait() \u5c06\u963b\u585e (\u9ed8\u8ba4: False ) interprocess ( bool ) \u2013 \u5982\u679c True , \u4e8b\u4ef6\u53ef\u4ee5\u5728\u8fdb\u7a0b\u4e4b\u95f4\u5171\u4eab (\u9ed8\u8ba4: False ) | | --- | --- | elapsed_time(end_event) \u8fd4\u56de\u8bb0\u5f55\u4e8b\u4ef6\u4e4b\u524d\u6240\u7ecf\u8fc7\u7684\u65f6\u95f4. ipc_handle() \u8fd4\u56de\u6b64\u4e8b\u4ef6\u7684 IPC \u53e5\u67c4. query() \u68c0\u67e5\u4e8b\u4ef6\u662f\u5426\u5df2\u8bb0\u5f55. Returns: \u4e00\u4e2a BOOL \u503c, \u6307\u793a\u4e8b\u4ef6\u662f\u5426\u5df2\u88ab\u8bb0\u5f55. record(stream=None) \u8bb0\u5f55\u7ed9\u5b9a\u6d41\u4e2d\u7684\u4e8b\u4ef6. synchronize() \u4e0e\u4e8b\u4ef6\u540c\u6b65. wait(stream=None) \u4f7f\u7ed9\u5b9a\u6d41\u7b49\u5f85\u4e8b\u4ef6\u53d1\u751f. Memory management torch.cuda.empty_cache() \u91ca\u653e\u5f53\u524d\u7531\u7f13\u5b58\u6301\u6709\u7684\u6240\u6709\u672a\u5360\u7528\u7f13\u5b58\u5185\u5b58\u5206\u914d\u5668,\u4ee5\u4fbf\u53ef\u4ee5\u5728\u5176\u4ed6GPU\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u5e76\u5728 <cite>nvidia-smi</cite> \u4e2d\u53ef\u89c1. NVIDIA Tools Extension (NVTX) torch.cuda.nvtx.mark(msg) \u63cf\u8ff0\u5728\u67d0\u4e2a\u65f6\u523b\u53d1\u751f\u7684\u77ac\u95f4\u4e8b\u4ef6. Parameters: msg ( string ) \u2013 \u4e8b\u4ef6(\u7528 ASCII \u7f16\u7801\u8868\u793a). torch.cuda.nvtx.range_push(msg) \u8bbe\u7f6e\u4e00\u4e2a\u56fa\u5b9a\u8303\u56f4\u7684\u5806\u6808,\u8fd4\u56de\u7684\u5806\u6808\u8303\u56f4\u6df1\u5ea6\u4ece0\u5f00\u59cb. Parameters: msg ( string ) \u2013 \u8303\u56f4(\u7528 ASCII \u7f16\u7801\u8bbe\u7f6e) torch.cuda.nvtx.range_pop() \u5f39\u51fa\u4e00\u4e2a\u56fa\u5b9a\u8303\u56f4\u7684\u5806\u6808,\u8fd4\u56de\u7684\u5806\u6808\u8303\u56f4\u6df1\u5ea6\u4ece0\u7ed3\u675f.","title":"torch.cuda"},{"location":"64/#torchcuda","text":"\u8fd9\u4e2a\u5305\u589e\u52a0\u4e86\u5bf9 CUDA tensor (\u5f20\u91cf) \u7c7b\u578b\u7684\u652f\u6301,\u5229\u7528 GPUs \u8ba1\u7b97\u5b9e\u73b0\u4e86\u4e0e CPU tensors \u76f8\u540c\u7684\u7c7b\u578b. \u8fd9\u4e2a\u662f lazily initialized (\u61d2\u52a0\u8f7d,\u5ef6\u8fdf\u52a0\u8f7d), \u6240\u4ee5\u4f60\u53ef\u4ee5\u4e00\u76f4\u5bfc\u5165\u5b83,\u5e76\u4e14\u53ef\u4ee5\u7528 is_available() \u6765\u5224\u65ad \u4f60\u7684\u7cfb\u7edf\u662f\u5426\u652f\u6301 CUDA. CUDA \u8bed\u4e49 \u6709\u66f4\u591a\u5173\u4e8e\u4f7f\u7528 CUDA \u7684\u7ec6\u8282. torch.cuda.current_blas_handle() \u8fd4\u56de\u6307\u5411\u5f53\u524d cuBLAS \u53e5\u67c4\u7684 cublasHandle_t \u6307\u9488 torch.cuda.current_device() \u8fd4\u56de\u5f53\u524d\u9009\u62e9\u7684\u8bbe\u5907\u7684\u7d22\u5f15. torch.cuda.current_stream() \u8fd4\u56de\u5f53\u524d\u9009\u62e9\u7684 Stream . class torch.cuda.device(idx) \u66f4\u6539\u9009\u5b9a\u8bbe\u5907\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668. Parameters: idx ( int ) \u2013 \u9009\u62e9\u8bbe\u5907\u7f16\u53f7. \u5982\u679c\u53c2\u6570\u65e0\u6548,\u5219\u662f\u65e0\u6548\u64cd\u4f5c. torch.cuda.device_count() \u8fd4\u56de\u53ef\u7528\u7684 GPU \u6570\u91cf. torch.cuda.``device_ctx_manager alias of device class torch.cuda.device_of(obj) \u5c06\u5f53\u524d\u8bbe\u5907\u66f4\u6539\u4e3a\u7ed9\u5b9a\u5bf9\u8c61\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668. \u53ef\u4ee5\u4f7f\u7528\u5f20\u91cf\u548c\u5b58\u50a8\u4f5c\u4e3a\u53c2\u6570,\u5982\u679c\u7ed9\u5b9a\u7684\u5bf9\u8c61\u4e0d\u662f\u5728 GPU \u4e0a\u5206\u914d\u7684,\u8fd9\u662f\u4e00\u4e2a\u65e0\u6548\u64cd\u4f5c. Parameters: obj ( Tensor or Storage ) \u2013 \u5728\u9009\u5b9a\u8bbe\u5907\u4e0a\u5206\u914d\u7684\u5bf9\u8c61. torch.cuda.empty_cache() \u91ca\u653e\u5f53\u524d\u7531\u7f13\u5b58\u6301\u6709\u7684\u6240\u6709\u672a\u5360\u7528\u7f13\u5b58\u5185\u5b58\u5206\u914d\u5668,\u4ee5\u4fbf\u53ef\u4ee5\u5728\u5176\u4ed6GPU\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u5e76\u5728 <cite>nvidia-smi</cite> \u4e2d\u53ef\u89c1. torch.cuda.get_device_capability(device) \u83b7\u53d6\u8bbe\u5907\u7684 CUDA \u7b97\u529b. Parameters: device ( int ) \u2013 \u8fd4\u56de\u8bbe\u5907\u540d, \u53c2\u6570\u65e0\u6548\u65f6, \u65b9\u6cd5\u5931\u6548. Returns: \u8bbe\u5907\u7684\u4e3b\u6b21\u8981 CUDA \u7b97\u529b. --- --- Return type: tuple ( int , int ) --- --- torch.cuda.get_device_name(device) \u83b7\u53d6\u8bbe\u5907\u540d. Parameters: device ( int ) \u2013 \u8fd4\u56de\u8bbe\u5907\u540d. \u53c2\u6570\u65e0\u6548\u65f6,\u5219\u662f\u65e0\u6548\u64cd\u4f5c. torch.cuda.is_available() \u8fd4\u56de\u4e00\u4e2a bool \u503c\u8868\u793a CUDA \u76ee\u524d\u662f\u5426\u53ef\u7528. torch.cuda.set_device(device) \u8bbe\u7f6e\u5f53\u524d\u8bbe\u5907. \u4e0d\u9f13\u52b1\u4f7f\u7528\u8fd9\u4e2a\u51fd\u6570 device . \u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b,\u6700\u597d\u4f7f\u7528 CUDA_VISIBLE_DEVICES \u73af\u5883\u53d8\u91cf. Parameters: device ( int ) \u2013 \u9009\u62e9\u8bbe\u5907. \u53c2\u6570\u65e0\u6548\u65f6,\u5219\u662f\u65e0\u6548\u64cd\u4f5c. torch.cuda.stream(*args, **kwds) \u9009\u62e9\u7ed9\u5b9a\u6d41\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668. \u5728\u9009\u5b9a\u7684\u6d41\u4e0a, \u6240\u6709\u7684CUDA\u5185\u6838\u5728\u5176\u4e0a\u4e0b\u6587\u5185\u6392\u961f. Parameters: stream ( Stream ) \u2013 \u9009\u62e9\u6d41. \u5982\u679c\u662f None , \u7ba1\u7406\u5668\u65e0\u6548. torch.cuda.synchronize() \u7b49\u5f85\u5f53\u524d\u8bbe\u5907\u4e0a\u6240\u6709\u6d41\u4e2d\u7684\u6240\u6709\u5185\u6838\u5b8c\u6210.","title":"torch.cuda"},{"location":"64/#random-number-generator","text":"torch.cuda.get_rng_state(device=-1) \u5c06\u5f53\u524d GPU \u7684\u968f\u673a\u6570\u751f\u6210\u5668\u72b6\u6001\u4f5c\u4e3a ByteTensor \u8fd4\u56de. Parameters: device ( int , optional ) \u2013 \u8bbe\u5907\u7684 RNG \u72b6\u6001. Default: -1 (i.e., \u4f7f\u7528\u5f53\u524d\u8bbe\u5907). Warning \u51fd\u6570\u9700\u8981\u63d0\u524d\u521d\u59cb\u5316 CUDA . torch.cuda.set_rng_state(new_state, device=-1) \u8bbe\u7f6e\u5f53\u524d GPU \u7684\u968f\u673a\u6570\u53d1\u751f\u5668\u72b6\u6001. Parameters: new_state ( torch.ByteTensor ) \u2013 \u6240\u9700\u7684\u72b6\u6001 torch.cuda.manual_seed(seed) \u8bbe\u7f6e\u7528\u4e8e\u5f53\u524d GPU \u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50. \u5982\u679c CUDA \u4e0d\u53ef\u7528,\u8c03\u7528\u8fd9\u4e2a\u51fd\u6570\u662f\u5b89\u5168\u7684;\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u5b83\u5c06\u88ab\u5ffd\u7565. Parameters: seed ( int or long ) \u2013 \u6240\u9700\u7684\u79cd\u5b50. Warning \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528\u591a GPU \u6a21\u578b,\u5219\u6b64\u529f\u80fd\u4e0d\u8db3\u4ee5\u83b7\u5f97\u786e\u5b9a\u6027. seef\u4f5c\u7528\u4e8e\u6240\u6709 GPUs , \u4f7f\u7528 manual_seed_all() . torch.cuda.manual_seed_all(seed) \u8bbe\u7f6e\u5728\u6240\u6709 GPU \u4e0a\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50. \u5982\u679c CUDA \u4e0d\u53ef\u7528, \u8c03\u7528\u6b64\u51fd\u6570\u662f\u5b89\u5168\u7684; \u8fd9\u79cd\u60c5\u51b5\u4e0b,\u4f1a\u88ab\u5ffd\u7565. Parameters: seed ( int or long ) \u2013 \u6240\u9700\u7684\u79cd\u5b50. torch.cuda.seed() \u5c06\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50\u8bbe\u7f6e\u4e3a\u5f53\u524d GPU \u7684\u968f\u673a\u6570. \u5982\u679c CUDA \u4e0d\u53ef\u7528,\u5219\u8c03\u7528\u6b64\u51fd\u6570\u662f\u5b89\u5168\u7684. \u5728\u90a3\u79cd\u60c5\u51b5\u4e0b,\u4f1a\u88ab\u5ffd\u7565. Warning \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528\u591a GPU \u6a21\u578b, \u5219\u6b64\u529f\u80fd\u4e0d\u8db3\u4ee5\u83b7\u5f97\u786e\u5b9a\u6027. seef\u4f5c\u7528\u4e8e\u6240\u6709 GPUs , \u4f7f\u7528 seed_all() . torch.cuda.seed_all() \u5728\u6240\u6709 GPU \u4e0a\u5c06\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50\u8bbe\u7f6e\u4e3a\u968f\u673a\u6570. \u5982\u679c CUDA \u4e0d\u53ef\u7528,\u5219\u8c03\u7528\u6b64\u51fd\u6570\u662f\u5b89\u5168\u7684. \u5728\u90a3\u79cd\u60c5\u51b5\u4e0b,\u4f1a\u88ab\u5ffd\u7565. torch.cuda.initial_seed() \u8fd4\u56de\u5f53\u524d GPU \u7684\u5f53\u524d\u968f\u673a\u79cd\u5b50. Warning \u51fd\u6570\u63d0\u524d\u521d\u59cb\u5316 CUDA .","title":"Random Number Generator"},{"location":"64/#communication-collectives","text":"torch.cuda.comm.broadcast(tensor, devices) \u5c06\u5f20\u91cf\u5e7f\u64ad\u7ed9\u591a\u4e2a GPU . | Parameters: | tensor ( Tensor ) \u2013 \u9700\u8981\u5e7f\u64ad\u7684\u5f20\u91cf. devices ( Iterable ) \u2013 \u5728\u4e00\u4e2a\u53ef\u8fed\u4ee3\u8bbe\u5907\u4e2d\u5e7f\u64ad. \u8bf7\u6ce8\u610f, \u5b83\u5e94\u8be5\u50cf (src, dst1, dst2, \u2026), \u5176\u4e2d\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u662f\u6765\u81f3\u5176\u5e7f\u64ad\u7684\u6e90\u8bbe\u5907. | | --- | --- | | Returns: | \u4e00\u4e2a\u5143\u7ec4, \u5305\u542b tensor \u526f\u672c,\u653e\u7f6e\u5728\u4e0e\u8bbe\u5907\u7684\u7d22\u5f15\u76f8\u5bf9\u5e94\u7684 \u8bbe\u5907 \u4e0a. | | --- | --- | torch.cuda.comm.reduce_add(inputs, destination=None) \u4ece\u591a\u4e2a GPU \u4e2d\u6536\u96c6\u5f20\u91cf. \u6240\u6709\u7684\u8f93\u5165\u5e94\u8be5\u6709\u5339\u914d\u7684 shapes (\u5f62\u72b6). | Parameters: | inputs ( Iterable__[ Tensor ] ) \u2013 \u6dfb\u52a0\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u5f20\u91cf. destination ( int , optional ) \u2013 \u653e\u7f6e\u8f93\u51fa\u7684\u8bbe\u5907 (\u9ed8\u8ba4: \u5f53\u524d\u8bbe\u5907). | | --- | --- | | Returns: | \u5305\u542b\u6240\u6709\u8f93\u5165\u7684\u5143\u7d20\u548c\u7684\u5f20\u91cf, \u5b58\u653e\u5728 destination(\u76ee\u6807) \u8bbe\u5907. | | --- | --- | torch.cuda.comm.scatter(tensor, devices, chunk_sizes=None, dim=0, streams=None) \u5206\u6563\u5f20\u91cf\u5230\u591a\u4e2a GPU. | Parameters: | tensor ( Tensor ) \u2013 \u9700\u8981\u5206\u6563\u7684\u5f20\u91cf. devices ( Iterable__[ int ] ) \u2013 \u6574\u6570\u7684\u8fed\u4ee3,\u6307\u5b9a\u5f20\u91cf\u5e94\u5206\u6563\u5728\u54ea\u4e9b\u8bbe\u5907\u4e4b\u95f4. chunk_sizes ( Iterable__[ int ]__, optional ) \u2013 \u8981\u653e\u5728\u6bcf\u4e2a\u8bbe\u5907\u4e0a\u7684\u5757\u7684\u5927\u5c0f. \u5e94\u8be5\u5339\u914d \u8bbe\u5907 \u957f\u5ea6\u548c tensor.size(dim) \u7684\u548c. \u5982\u679c\u672a\u6307\u5b9a,\u5f20\u91cf\u5c06\u88ab\u5212\u5206\u6210\u76f8\u7b49\u7684\u5757. dim ( int , optional ) \u2013 \u5206\u5757\u5f20\u91cf\u6cbf\u7740\u7684\u7ef4\u5ea6 | | --- | --- | | Returns: | \u4e00\u4e2a\u5143\u7ec4\u5305\u542b tensor \u5757, \u4f20\u9012\u7ed9 devices . | | --- | --- | torch.cuda.comm.gather(tensors, dim=0, destination=None) \u4ece\u591a\u4e2a GPU \u6536\u96c6\u5f20\u91cf. \u5f20\u91cf\u5c3a\u5bf8\u5728\u4e0d\u540c\u4e8e dim \u7684\u7ef4\u5ea6\u4e0a\u90fd\u5e94\u8be5\u5339\u914d. | Parameters: | tensors ( Iterable__[ Tensor ] ) \u2013 \u5f20\u91cf\u96c6\u5408\u7684\u8fed\u4ee3\u5668. dim ( int ) \u2013 \u5f20\u91cf\u88ab\u8fde\u63a5\u7684\u7ef4\u5ea6. destination ( int , optional ) \u2013 \u8f93\u51fa\u8bbe\u5907 (-1 \u4ee3\u8868 CPU, \u9ed8\u8ba4: \u5f53\u524d\u8bbe\u5907) | | --- | --- | | Returns: | \u4e00\u4e2a\u4f4d\u4e8e \u76ee\u6807 \u8bbe\u5907\u4e0a\u7684\u5f20\u91cf, \u5c06 tensors \u6cbf\u7740 dim \u8fde\u63a5\u8d77\u6765\u7684\u7ed3\u679c. | | --- | --- |","title":"Communication collectives"},{"location":"64/#streams-and-events","text":"class torch.cuda.Stream CUDA \u6d41\u7684\u5305\u88c5. | Parameters: | device ( int , optional ) \u2013 \u5206\u914d\u6d41\u7684\u8bbe\u5907. priority ( int , optional ) \u2013 \u6d41\u7684\u4f18\u5148\u7ea7. \u8f83\u4f4e\u7684\u6570\u5b57\u4ee3\u8868\u8f83\u9ad8\u7684\u4f18\u5148\u7ea7. | | --- | --- | query() \u68c0\u67e5\u4e8b\u4ef6\u662f\u5426\u5df2\u88ab\u8bb0\u5f55. Returns: \u4e00\u4e2a BOOL \u503c, \u6307\u793a\u4e8b\u4ef6\u662f\u5426\u5df2\u88ab\u8bb0\u5f55. record_event(event=None) \u8bb0\u5f55\u4e00\u4e2a\u4e8b\u4ef6. Parameters: event ( Event , optional ) \u2013 \u8981\u8bb0\u5f55\u7684\u4e8b\u4ef6.\u5982\u679c\u6ca1\u6709\u7ed9\u51fa,\u5c06\u5206\u914d\u4e00\u4e2a\u65b0\u7684. Returns: \u8bb0\u5f55\u7684\u4e8b\u4ef6. --- --- synchronize() \u7b49\u5f85\u6d41\u4e2d\u7684\u6240\u6709\u5185\u6838\u5b8c\u6210. wait_event(event) \u5c06\u6240\u6709\u672a\u6765\u7684\u5de5\u4f5c\u63d0\u4ea4\u5230\u6d41\u7b49\u5f85\u4e8b\u4ef6. Parameters: event ( Event ) \u2013 \u7b49\u5f85\u7684\u4e8b\u4ef6. wait_stream(stream) \u4e0e\u53e6\u4e00\u4e2a\u6d41\u540c\u6b65. \u63d0\u4ea4\u5230\u6b64\u6d41\u7684\u6240\u6709\u672a\u6765\u5de5\u4f5c\u5c06\u7b49\u5f85\u76f4\u5230\u6240\u6709\u6838\u5fc3\u5728\u8c03\u7528\u5b8c\u6210\u65f6\u63d0\u4ea4\u7ed9\u7ed9\u5b9a\u7684\u6d41. Parameters: stream ( Stream ) \u2013 \u540c\u6b65\u6d41. class torch.cuda.Event(enable_timing=False, blocking=False, interprocess=False, _handle=None) CUDA \u4e8b\u4ef6\u5305\u88c5\u5668. | Parameters: | enable_timing ( bool ) \u2013 \u6307\u793a\u4e8b\u4ef6\u662f\u5426\u5e94\u6d4b\u91cf\u65f6\u95f4 (\u9ed8\u8ba4: False ) blocking ( bool ) \u2013 \u5982\u679c True , wait() \u5c06\u963b\u585e (\u9ed8\u8ba4: False ) interprocess ( bool ) \u2013 \u5982\u679c True , \u4e8b\u4ef6\u53ef\u4ee5\u5728\u8fdb\u7a0b\u4e4b\u95f4\u5171\u4eab (\u9ed8\u8ba4: False ) | | --- | --- | elapsed_time(end_event) \u8fd4\u56de\u8bb0\u5f55\u4e8b\u4ef6\u4e4b\u524d\u6240\u7ecf\u8fc7\u7684\u65f6\u95f4. ipc_handle() \u8fd4\u56de\u6b64\u4e8b\u4ef6\u7684 IPC \u53e5\u67c4. query() \u68c0\u67e5\u4e8b\u4ef6\u662f\u5426\u5df2\u8bb0\u5f55. Returns: \u4e00\u4e2a BOOL \u503c, \u6307\u793a\u4e8b\u4ef6\u662f\u5426\u5df2\u88ab\u8bb0\u5f55. record(stream=None) \u8bb0\u5f55\u7ed9\u5b9a\u6d41\u4e2d\u7684\u4e8b\u4ef6. synchronize() \u4e0e\u4e8b\u4ef6\u540c\u6b65. wait(stream=None) \u4f7f\u7ed9\u5b9a\u6d41\u7b49\u5f85\u4e8b\u4ef6\u53d1\u751f.","title":"Streams and events"},{"location":"64/#memory-management","text":"torch.cuda.empty_cache() \u91ca\u653e\u5f53\u524d\u7531\u7f13\u5b58\u6301\u6709\u7684\u6240\u6709\u672a\u5360\u7528\u7f13\u5b58\u5185\u5b58\u5206\u914d\u5668,\u4ee5\u4fbf\u53ef\u4ee5\u5728\u5176\u4ed6GPU\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u5e76\u5728 <cite>nvidia-smi</cite> \u4e2d\u53ef\u89c1.","title":"Memory management"},{"location":"64/#nvidia-tools-extension-nvtx","text":"torch.cuda.nvtx.mark(msg) \u63cf\u8ff0\u5728\u67d0\u4e2a\u65f6\u523b\u53d1\u751f\u7684\u77ac\u95f4\u4e8b\u4ef6. Parameters: msg ( string ) \u2013 \u4e8b\u4ef6(\u7528 ASCII \u7f16\u7801\u8868\u793a). torch.cuda.nvtx.range_push(msg) \u8bbe\u7f6e\u4e00\u4e2a\u56fa\u5b9a\u8303\u56f4\u7684\u5806\u6808,\u8fd4\u56de\u7684\u5806\u6808\u8303\u56f4\u6df1\u5ea6\u4ece0\u5f00\u59cb. Parameters: msg ( string ) \u2013 \u8303\u56f4(\u7528 ASCII \u7f16\u7801\u8bbe\u7f6e) torch.cuda.nvtx.range_pop() \u5f39\u51fa\u4e00\u4e2a\u56fa\u5b9a\u8303\u56f4\u7684\u5806\u6808,\u8fd4\u56de\u7684\u5806\u6808\u8303\u56f4\u6df1\u5ea6\u4ece0\u7ed3\u675f.","title":"NVIDIA Tools Extension (NVTX)"},{"location":"65/","text":"torch.utils.ffi torch.utils.ffi.create_extension(name, headers, sources, verbose=True, with_cuda=False, package=False, relative_to='.', **kwargs) \u521b\u5efa\u5e76\u914d\u7f6e\u4e00\u4e2a cffi.FFI \u5bf9\u8c61, \u7528\u4e8e\u6784\u5efa PyTorch \u7684\u6269\u5c55. | Parameters: | name ( str ) \u2013 \u5305\u540d. \u53ef\u4ee5\u662f\u5d4c\u5957\u6a21\u5757, \u4f8b\u5982. .ext.my_lib . headers ( str or List__[ str ] ) \u2013 \u53ea\u5305\u542b\u5bfc\u51fa\u51fd\u6570\u7684\u5934\u6587\u4ef6\u5217\u8868. sources ( List__[ str ] ) \u2013 \u7528\u4e8e\u7f16\u8bd1\u7684sources\u5217\u8868. verbose ( bool , optional ) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a False , \u5219\u4e0d\u4f1a\u6253\u5370\u8f93\u51fa (\u9ed8\u8ba4\u503c: True). with_cuda ( bool , optional ) \u2013 \u8bbe\u7f6e\u4e3a True \u4ee5\u4f7f\u7528 CUDA \u5934\u6587\u4ef6\u8fdb\u884c\u7f16\u8bd1 (\u9ed8\u8ba4\u503c: False) package ( bool , optional ) \u2013 \u8bbe\u7f6e\u4e3a True \u4ee5\u5728\u5305\u6a21\u5f0f\u4e0b\u6784\u5efa (\u5bf9\u4e8e\u8981\u4f5c\u4e3a pip \u7a0b\u5e8f\u5305\u5b89\u88c5\u7684\u6a21\u5757) (\u9ed8\u8ba4\u503c: False). relative_to ( str , optional ) \u2013 \u6784\u5efa\u6587\u4ef6\u7684\u8def\u5f84. \u5f53 package \u4e3a True \u65f6\u9700\u8981. \u6700\u597d\u4f7f\u7528 __file__ \u4f5c\u4e3a\u53c2\u6570. kwargs \u2013 \u4f20\u9012\u7ed9 ffi \u4ee5\u58f0\u660e\u6269\u5c55\u7684\u9644\u4ef6\u53c2\u6570. \u53c2\u8003 Extension API reference \u67e5\u9605\u66f4\u8be6\u7ec6\u5185\u5bb9. | | --- | --- |","title":"torch.utils.ffi"},{"location":"65/#torchutilsffi","text":"torch.utils.ffi.create_extension(name, headers, sources, verbose=True, with_cuda=False, package=False, relative_to='.', **kwargs) \u521b\u5efa\u5e76\u914d\u7f6e\u4e00\u4e2a cffi.FFI \u5bf9\u8c61, \u7528\u4e8e\u6784\u5efa PyTorch \u7684\u6269\u5c55. | Parameters: | name ( str ) \u2013 \u5305\u540d. \u53ef\u4ee5\u662f\u5d4c\u5957\u6a21\u5757, \u4f8b\u5982. .ext.my_lib . headers ( str or List__[ str ] ) \u2013 \u53ea\u5305\u542b\u5bfc\u51fa\u51fd\u6570\u7684\u5934\u6587\u4ef6\u5217\u8868. sources ( List__[ str ] ) \u2013 \u7528\u4e8e\u7f16\u8bd1\u7684sources\u5217\u8868. verbose ( bool , optional ) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a False , \u5219\u4e0d\u4f1a\u6253\u5370\u8f93\u51fa (\u9ed8\u8ba4\u503c: True). with_cuda ( bool , optional ) \u2013 \u8bbe\u7f6e\u4e3a True \u4ee5\u4f7f\u7528 CUDA \u5934\u6587\u4ef6\u8fdb\u884c\u7f16\u8bd1 (\u9ed8\u8ba4\u503c: False) package ( bool , optional ) \u2013 \u8bbe\u7f6e\u4e3a True \u4ee5\u5728\u5305\u6a21\u5f0f\u4e0b\u6784\u5efa (\u5bf9\u4e8e\u8981\u4f5c\u4e3a pip \u7a0b\u5e8f\u5305\u5b89\u88c5\u7684\u6a21\u5757) (\u9ed8\u8ba4\u503c: False). relative_to ( str , optional ) \u2013 \u6784\u5efa\u6587\u4ef6\u7684\u8def\u5f84. \u5f53 package \u4e3a True \u65f6\u9700\u8981. \u6700\u597d\u4f7f\u7528 __file__ \u4f5c\u4e3a\u53c2\u6570. kwargs \u2013 \u4f20\u9012\u7ed9 ffi \u4ee5\u58f0\u660e\u6269\u5c55\u7684\u9644\u4ef6\u53c2\u6570. \u53c2\u8003 Extension API reference \u67e5\u9605\u66f4\u8be6\u7ec6\u5185\u5bb9. | | --- | --- |","title":"torch.utils.ffi"},{"location":"66/","text":"torch.utils.data class torch.utils.data.Dataset \u8868\u793a Dataset \u7684\u62bd\u8c61\u7c7b. \u6240\u6709\u5176\u5b83\u6570\u636e\u96c6\u90fd\u5e94\u7ee7\u627f\u8be5\u7c7b. \u6240\u6709\u5b50\u7c7b\u90fd\u5e94\u8be5\u91cd\u5199 __len__ , \u63d0\u4f9b\u6570\u636e\u96c6\u5927\u5c0f\u7684\u65b9\u6cd5, \u548c __getitem__ , \u652f\u6301\u4ece 0 \u5230 len(self) \u6574\u6570\u7d22\u5f15\u7684\u65b9\u6cd5. class torch.utils.data.TensorDataset(data_tensor, target_tensor) \u5305\u88c5\u6570\u636e\u548c\u76ee\u6807\u5f20\u91cf\u7684\u6570\u636e\u96c6. \u901a\u8fc7\u6cbf\u7740\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u7d22\u5f15\u4e24\u4e2a\u5f20\u91cf\u6765\u6062\u590d\u6bcf\u4e2a\u6837\u672c. | Parameters: | data_tensor ( Tensor ) \u2013 \u5305\u542b\u6837\u672c\u6570\u636e. target_tensor ( Tensor ) \u2013 \u5305\u542b\u6837\u672c\u76ee\u6807 (\u6807\u7b7e). | | --- | --- | class torch.utils.data.ConcatDataset(datasets) \u7528\u4ee5\u8fde\u7ed3\u591a\u4e2a\u6570\u636e\u96c6\u7684\u6570\u636e\u96c6. \u76ee\u7684: \u5bf9\u4e8e\u7ec4\u88c5\u4e0d\u540c\u7684\u73b0\u6709\u6570\u636e\u96c6\u975e\u5e38\u6709\u5e2e\u52a9, \u53ef\u80fd\u662f \u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6, \u56e0\u4e3a\u4e32\u8054\u64cd\u4f5c\u662f\u4ee5\u5373\u65f6\u65b9\u5f0f\u5b8c\u6210\u7684. Parameters: datasets ( iterable ) \u2013 \u9700\u8981\u8fde\u7ed3\u7684\u6570\u636e\u96c6\u5217\u8868 class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=<function default_collate at 0x4316c08>, pin_memory=False, drop_last=False) \u6570\u636e\u52a0\u8f7d\u5668. \u7ec4\u5408\u6570\u636e\u96c6\u548c\u91c7\u6837\u5668,\u5e76\u5728\u6570\u636e\u96c6\u4e0a\u63d0\u4f9b\u5355\u8fdb\u7a0b\u6216\u591a\u8fdb\u7a0b\u8fed\u4ee3\u5668. | Parameters: | dataset ( Dataset ) \u2013 \u4ece\u8be5\u6570\u636e\u96c6\u4e2d\u52a0\u8f7d\u6570\u636e. batch_size ( int , optional ) \u2013 \u6bcf\u4e2a batch \u52a0\u8f7d\u591a\u5c11\u4e2a\u6837\u672c (\u9ed8\u8ba4\u503c: 1). shuffle ( bool , optional ) \u2013 \u8bbe\u7f6e\u4e3a True \u65f6, \u4f1a\u5728\u6bcf\u4e2a epoch \u91cd\u65b0\u6253\u4e71\u6570\u636e (\u9ed8\u8ba4\u503c: False). sampler ( Sampler , optional ) \u2013 \u5b9a\u4e49\u4ece\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u6837\u672c\u7684\u7b56\u7565. \u5982\u679c\u6307\u5b9a, shuffle \u503c\u5fc5\u987b\u4e3a False. batch_sampler ( Sampler , optional ) \u2013 \u4e0e sampler \u76f8\u4f3c, \u4f46\u4e00\u6b21\u8fd4\u56de\u4e00\u6279\u6307\u6807. \u4e0e batch_size, shuffle, sampler, and drop_last \u4e92\u65a5. num_workers ( int , optional ) \u2013 \u7528\u591a\u5c11\u4e2a\u5b50\u8fdb\u7a0b\u52a0\u8f7d\u6570\u636e. 0\u8868\u793a\u6570\u636e\u5c06\u5728\u4e3b\u8fdb\u7a0b\u4e2d\u52a0\u8f7d (\u9ed8\u8ba4\u503c: 0) collate_fn ( callable , optional ) \u2013 \u5408\u5e76\u6837\u672c\u5217\u8868\u4ee5\u5f62\u6210\u4e00\u4e2a mini-batch. pin_memory ( bool , optional ) \u2013 \u5982\u679c\u4e3a True , \u6570\u636e\u52a0\u8f7d\u5668\u4f1a\u5c06\u5f20\u91cf\u590d\u5236\u5230 CUDA \u56fa\u5b9a\u5185\u5b58\u4e2d, \u7136\u540e\u518d\u8fd4\u56de\u5b83\u4eec. drop_last ( bool , optional ) \u2013 \u8bbe\u5b9a\u4e3a True \u4ee5\u4e22\u6389\u6700\u540e\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684 batch, \u5982\u679c\u6570\u636e\u96c6\u5927\u5c0f\u4e0d\u80fd\u88ab batch size\u6574\u9664. \u8bbe\u5b9a\u4e3a False \u5e76\u4e14\u6570\u636e\u96c6\u7684\u5927\u5c0f\u4e0d\u80fd\u88ab batch size\u6574\u9664, \u5219\u6700\u540e\u4e00\u4e2a batch \u5c06\u4f1a\u66f4\u5c0f. (default: False) | | --- | --- | class torch.utils.data.sampler.Sampler(data_source) \u6240\u6709\u91c7\u6837\u5668\u7684\u57fa\u7c7b. \u6bcf\u4e00\u4e2a Sampler \u7684\u5b50\u7c7b\u90fd\u5fc5\u987b\u63d0\u4f9b\u4e00\u4e2a iter \u65b9\u6cd5, \u63d0\u4f9b\u4e00\u79cd \u8fed\u4ee3\u6570\u636e\u96c6\u5143\u7d20\u7684\u7d22\u5f15\u7684\u65b9\u6cd5, \u4ee5\u53ca\u4e00\u4e2a len \u65b9\u6cd5, \u7528\u6765\u8fd4\u56de \u8fed\u4ee3\u5668\u7684\u957f\u5ea6. class torch.utils.data.sampler.SequentialSampler(data_source) \u603b\u662f\u4ee5\u76f8\u540c\u7684\u987a\u5e8f, \u4f9d\u6b21\u5bf9\u5143\u7d20\u8fdb\u884c\u91c7\u6837. Parameters: data_source ( Dataset ) \u2013 \u91c7\u6837\u7684\u6570\u636e\u96c6 class torch.utils.data.sampler.RandomSampler(data_source) \u91c7\u7528\u65e0\u653e\u56de\u91c7\u6837\u6cd5, \u968f\u673a\u5bf9\u6837\u672c\u5143\u7d20\u91c7\u6837. Parameters: data_source ( Dataset ) \u2013 \u91c7\u6837\u7684\u6570\u636e\u96c6 class torch.utils.data.sampler.SubsetRandomSampler(indices) \u91c7\u7528\u65e0\u653e\u56de\u91c7\u6837\u6cd5, \u6837\u672c\u5143\u7d20\u4ece\u6307\u5b9a\u7684\u7d22\u5f15\u5217\u8868\u4e2d\u968f\u673a\u62bd\u53d6. Parameters: indices ( list ) \u2013 \u7d22\u5f15\u7684\u5217\u8868 class torch.utils.data.sampler.WeightedRandomSampler(weights, num_samples, replacement=True) \u4f7f\u7528\u7ed9\u5b9a\u7684\u6982\u7387 (\u6743\u91cd) \u5bf9 [0,..,len(weights)-1] \u8303\u56f4\u7684\u5143\u7d20\u8fdb\u884c\u91c7\u6837. | Parameters: | weights ( list ) \u2013 \u6743\u91cd\u5217\u8868, \u6ca1\u5fc5\u8981\u52a0\u8d77\u6765\u7b49\u4e8e 1 num_samples ( int ) \u2013 \u62bd\u6837\u6570\u91cf replacement ( bool ) \u2013 \u8bbe\u5b9a\u4e3a True , \u4f7f\u7528\u6709\u653e\u56de\u91c7\u6837\u6cd5. \u8bbe\u5b9a\u4e3a False , \u91c7\u7528\u65e0\u653e\u56de\u91c7\u6837\u6cd5, \u8fd9\u610f\u5473\u7740\u5bf9\u4e8e\u4e00\u884c\u6765\u8bf4,\u5f53\u4e00\u4e2a \u6837\u672c\u7d22\u5f15\u88ab\u53d6\u5230\u540e, \u5bf9\u4e8e\u6539\u884c, \u8fd9\u4e2a\u6837\u672c\u7d22\u5f15\u4e0d\u80fd\u518d\u6b21\u88ab\u53d6\u5230. | | --- | --- | class torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=None, rank=None) \u5c06\u6570\u636e\u52a0\u8f7d\u9650\u5236\u4e3a\u6570\u636e\u96c6\u5b50\u96c6\u7684\u91c7\u6837\u5668. \u5f53\u4e0e:class:<cite>torch.nn.parallel.DistributedDataParallel</cite> \u7ec4\u5408\u4f7f\u7528\u65f6, \u7279\u522b\u6709\u7528. \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u53ef\u4ee5\u5c06\u5206\u5e03\u5f0f\u91c7\u6837\u5668\u5b9e\u4f8b\u4f5c\u4e3aData Loader\u91c7\u6837\u5668, \u5e76\u4e14\u52a0\u8f7d\u4e00\u4e2a\u539f\u59cb\u6570\u636e\u96c6\u7684\u5b50\u96c6\u5e76\u72ec\u5360\u8be5\u6570\u636e\u5b50\u96c6. Note \u6570\u636e\u96c6\u88ab\u5047\u5b9a\u4e3a\u4e0d\u53d8\u7684\u5927\u5c0f. | Parameters: | dataset \u2013 \u91c7\u6837\u7684\u6570\u636e\u96c6. num_replicas ( optional ) \u2013 \u53c2\u4e0e\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u8fdb\u7a0b\u6570\u91cf. rank ( optional ) \u2013 \u5728 num_replicas \u4e2d, \u5f53\u524d\u8fdb\u7a0b\u7684\u7b49\u7ea7. | | --- | --- |","title":"torch.utils.data"},{"location":"66/#torchutilsdata","text":"class torch.utils.data.Dataset \u8868\u793a Dataset \u7684\u62bd\u8c61\u7c7b. \u6240\u6709\u5176\u5b83\u6570\u636e\u96c6\u90fd\u5e94\u7ee7\u627f\u8be5\u7c7b. \u6240\u6709\u5b50\u7c7b\u90fd\u5e94\u8be5\u91cd\u5199 __len__ , \u63d0\u4f9b\u6570\u636e\u96c6\u5927\u5c0f\u7684\u65b9\u6cd5, \u548c __getitem__ , \u652f\u6301\u4ece 0 \u5230 len(self) \u6574\u6570\u7d22\u5f15\u7684\u65b9\u6cd5. class torch.utils.data.TensorDataset(data_tensor, target_tensor) \u5305\u88c5\u6570\u636e\u548c\u76ee\u6807\u5f20\u91cf\u7684\u6570\u636e\u96c6. \u901a\u8fc7\u6cbf\u7740\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u7d22\u5f15\u4e24\u4e2a\u5f20\u91cf\u6765\u6062\u590d\u6bcf\u4e2a\u6837\u672c. | Parameters: | data_tensor ( Tensor ) \u2013 \u5305\u542b\u6837\u672c\u6570\u636e. target_tensor ( Tensor ) \u2013 \u5305\u542b\u6837\u672c\u76ee\u6807 (\u6807\u7b7e). | | --- | --- | class torch.utils.data.ConcatDataset(datasets) \u7528\u4ee5\u8fde\u7ed3\u591a\u4e2a\u6570\u636e\u96c6\u7684\u6570\u636e\u96c6. \u76ee\u7684: \u5bf9\u4e8e\u7ec4\u88c5\u4e0d\u540c\u7684\u73b0\u6709\u6570\u636e\u96c6\u975e\u5e38\u6709\u5e2e\u52a9, \u53ef\u80fd\u662f \u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6, \u56e0\u4e3a\u4e32\u8054\u64cd\u4f5c\u662f\u4ee5\u5373\u65f6\u65b9\u5f0f\u5b8c\u6210\u7684. Parameters: datasets ( iterable ) \u2013 \u9700\u8981\u8fde\u7ed3\u7684\u6570\u636e\u96c6\u5217\u8868 class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=<function default_collate at 0x4316c08>, pin_memory=False, drop_last=False) \u6570\u636e\u52a0\u8f7d\u5668. \u7ec4\u5408\u6570\u636e\u96c6\u548c\u91c7\u6837\u5668,\u5e76\u5728\u6570\u636e\u96c6\u4e0a\u63d0\u4f9b\u5355\u8fdb\u7a0b\u6216\u591a\u8fdb\u7a0b\u8fed\u4ee3\u5668. | Parameters: | dataset ( Dataset ) \u2013 \u4ece\u8be5\u6570\u636e\u96c6\u4e2d\u52a0\u8f7d\u6570\u636e. batch_size ( int , optional ) \u2013 \u6bcf\u4e2a batch \u52a0\u8f7d\u591a\u5c11\u4e2a\u6837\u672c (\u9ed8\u8ba4\u503c: 1). shuffle ( bool , optional ) \u2013 \u8bbe\u7f6e\u4e3a True \u65f6, \u4f1a\u5728\u6bcf\u4e2a epoch \u91cd\u65b0\u6253\u4e71\u6570\u636e (\u9ed8\u8ba4\u503c: False). sampler ( Sampler , optional ) \u2013 \u5b9a\u4e49\u4ece\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u6837\u672c\u7684\u7b56\u7565. \u5982\u679c\u6307\u5b9a, shuffle \u503c\u5fc5\u987b\u4e3a False. batch_sampler ( Sampler , optional ) \u2013 \u4e0e sampler \u76f8\u4f3c, \u4f46\u4e00\u6b21\u8fd4\u56de\u4e00\u6279\u6307\u6807. \u4e0e batch_size, shuffle, sampler, and drop_last \u4e92\u65a5. num_workers ( int , optional ) \u2013 \u7528\u591a\u5c11\u4e2a\u5b50\u8fdb\u7a0b\u52a0\u8f7d\u6570\u636e. 0\u8868\u793a\u6570\u636e\u5c06\u5728\u4e3b\u8fdb\u7a0b\u4e2d\u52a0\u8f7d (\u9ed8\u8ba4\u503c: 0) collate_fn ( callable , optional ) \u2013 \u5408\u5e76\u6837\u672c\u5217\u8868\u4ee5\u5f62\u6210\u4e00\u4e2a mini-batch. pin_memory ( bool , optional ) \u2013 \u5982\u679c\u4e3a True , \u6570\u636e\u52a0\u8f7d\u5668\u4f1a\u5c06\u5f20\u91cf\u590d\u5236\u5230 CUDA \u56fa\u5b9a\u5185\u5b58\u4e2d, \u7136\u540e\u518d\u8fd4\u56de\u5b83\u4eec. drop_last ( bool , optional ) \u2013 \u8bbe\u5b9a\u4e3a True \u4ee5\u4e22\u6389\u6700\u540e\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684 batch, \u5982\u679c\u6570\u636e\u96c6\u5927\u5c0f\u4e0d\u80fd\u88ab batch size\u6574\u9664. \u8bbe\u5b9a\u4e3a False \u5e76\u4e14\u6570\u636e\u96c6\u7684\u5927\u5c0f\u4e0d\u80fd\u88ab batch size\u6574\u9664, \u5219\u6700\u540e\u4e00\u4e2a batch \u5c06\u4f1a\u66f4\u5c0f. (default: False) | | --- | --- | class torch.utils.data.sampler.Sampler(data_source) \u6240\u6709\u91c7\u6837\u5668\u7684\u57fa\u7c7b. \u6bcf\u4e00\u4e2a Sampler \u7684\u5b50\u7c7b\u90fd\u5fc5\u987b\u63d0\u4f9b\u4e00\u4e2a iter \u65b9\u6cd5, \u63d0\u4f9b\u4e00\u79cd \u8fed\u4ee3\u6570\u636e\u96c6\u5143\u7d20\u7684\u7d22\u5f15\u7684\u65b9\u6cd5, \u4ee5\u53ca\u4e00\u4e2a len \u65b9\u6cd5, \u7528\u6765\u8fd4\u56de \u8fed\u4ee3\u5668\u7684\u957f\u5ea6. class torch.utils.data.sampler.SequentialSampler(data_source) \u603b\u662f\u4ee5\u76f8\u540c\u7684\u987a\u5e8f, \u4f9d\u6b21\u5bf9\u5143\u7d20\u8fdb\u884c\u91c7\u6837. Parameters: data_source ( Dataset ) \u2013 \u91c7\u6837\u7684\u6570\u636e\u96c6 class torch.utils.data.sampler.RandomSampler(data_source) \u91c7\u7528\u65e0\u653e\u56de\u91c7\u6837\u6cd5, \u968f\u673a\u5bf9\u6837\u672c\u5143\u7d20\u91c7\u6837. Parameters: data_source ( Dataset ) \u2013 \u91c7\u6837\u7684\u6570\u636e\u96c6 class torch.utils.data.sampler.SubsetRandomSampler(indices) \u91c7\u7528\u65e0\u653e\u56de\u91c7\u6837\u6cd5, \u6837\u672c\u5143\u7d20\u4ece\u6307\u5b9a\u7684\u7d22\u5f15\u5217\u8868\u4e2d\u968f\u673a\u62bd\u53d6. Parameters: indices ( list ) \u2013 \u7d22\u5f15\u7684\u5217\u8868 class torch.utils.data.sampler.WeightedRandomSampler(weights, num_samples, replacement=True) \u4f7f\u7528\u7ed9\u5b9a\u7684\u6982\u7387 (\u6743\u91cd) \u5bf9 [0,..,len(weights)-1] \u8303\u56f4\u7684\u5143\u7d20\u8fdb\u884c\u91c7\u6837. | Parameters: | weights ( list ) \u2013 \u6743\u91cd\u5217\u8868, \u6ca1\u5fc5\u8981\u52a0\u8d77\u6765\u7b49\u4e8e 1 num_samples ( int ) \u2013 \u62bd\u6837\u6570\u91cf replacement ( bool ) \u2013 \u8bbe\u5b9a\u4e3a True , \u4f7f\u7528\u6709\u653e\u56de\u91c7\u6837\u6cd5. \u8bbe\u5b9a\u4e3a False , \u91c7\u7528\u65e0\u653e\u56de\u91c7\u6837\u6cd5, \u8fd9\u610f\u5473\u7740\u5bf9\u4e8e\u4e00\u884c\u6765\u8bf4,\u5f53\u4e00\u4e2a \u6837\u672c\u7d22\u5f15\u88ab\u53d6\u5230\u540e, \u5bf9\u4e8e\u6539\u884c, \u8fd9\u4e2a\u6837\u672c\u7d22\u5f15\u4e0d\u80fd\u518d\u6b21\u88ab\u53d6\u5230. | | --- | --- | class torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=None, rank=None) \u5c06\u6570\u636e\u52a0\u8f7d\u9650\u5236\u4e3a\u6570\u636e\u96c6\u5b50\u96c6\u7684\u91c7\u6837\u5668. \u5f53\u4e0e:class:<cite>torch.nn.parallel.DistributedDataParallel</cite> \u7ec4\u5408\u4f7f\u7528\u65f6, \u7279\u522b\u6709\u7528. \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u53ef\u4ee5\u5c06\u5206\u5e03\u5f0f\u91c7\u6837\u5668\u5b9e\u4f8b\u4f5c\u4e3aData Loader\u91c7\u6837\u5668, \u5e76\u4e14\u52a0\u8f7d\u4e00\u4e2a\u539f\u59cb\u6570\u636e\u96c6\u7684\u5b50\u96c6\u5e76\u72ec\u5360\u8be5\u6570\u636e\u5b50\u96c6. Note \u6570\u636e\u96c6\u88ab\u5047\u5b9a\u4e3a\u4e0d\u53d8\u7684\u5927\u5c0f. | Parameters: | dataset \u2013 \u91c7\u6837\u7684\u6570\u636e\u96c6. num_replicas ( optional ) \u2013 \u53c2\u4e0e\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u8fdb\u7a0b\u6570\u91cf. rank ( optional ) \u2013 \u5728 num_replicas \u4e2d, \u5f53\u524d\u8fdb\u7a0b\u7684\u7b49\u7ea7. | | --- | --- |","title":"torch.utils.data"},{"location":"67/","text":"torch.utils.model_zoo torch.utils.model_zoo.load_url(url, model_dir=None, map_location=None) \u4ece\u7ed9\u5b9a\u7684 URL \u5904\u52a0\u8f7d Torch \u5e8f\u5217\u5316\u5bf9\u8c61. \u5982\u679c\u8be5\u5bf9\u8c61\u5df2\u7ecf\u5b58\u5728\u4e8e <cite>model_dir</cite> \u4e2d, \u5219\u5c06\u88ab\u53cd\u5e8f\u5217\u5316\u5e76\u8fd4\u56de. URL \u7684\u6587\u4ef6\u540d\u90e8\u5206\u5e94\u8be5\u9075\u5faa\u547d\u540d\u7ea6\u5b9a filename-&lt;sha256&gt;.ext \u5176\u4e2d &lt;sha256&gt; \u662f\u6587\u4ef6\u5185\u5bb9\u7684 SHA256 \u54c8\u5e0c\u7684\u524d\u516b\u4f4d\u6216\u66f4\u591a\u4f4d\u6570. \u54c8\u5e0c\u7528\u4e8e\u786e\u4fdd\u552f\u4e00\u7684\u540d\u79f0\u5e76\u9a8c\u8bc1\u6587\u4ef6\u7684\u5185\u5bb9. <cite>model_dir</cite> \u7684\u9ed8\u8ba4\u503c\u4e3a $TORCH_HOME/models \u5176\u4e2d $TORCH_HOME \u9ed8\u8ba4\u503c\u4e3a ~/.torch . \u53ef\u4ee5\u4f7f\u7528 $TORCH_MODEL_ZOO \u73af\u5883\u53d8\u91cf\u6765\u8986\u76d6\u9ed8\u8ba4\u76ee\u5f55. | Parameters: | url ( string ) \u2013 \u9700\u8981\u4e0b\u8f7d\u5bf9\u8c61\u7684 URL model_dir ( string , optional ) \u2013 \u4fdd\u5b58\u5bf9\u8c61\u7684\u76ee\u5f55 map_location ( optional ) \u2013 \u4e00\u4e2a\u51fd\u6570\u6216\u8005\u4e00\u4e2a\u5b57\u5178,\u6307\u5b9a\u5982\u4f55\u91cd\u65b0\u6620\u5c04\u5b58\u50a8\u4f4d\u7f6e (\u8be6\u60c5\u67e5\u9605 torch.load) | | --- | --- | Example >>> state_dict = torch.utils.model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')","title":"torch.utils.model_zoo"},{"location":"67/#torchutilsmodel_zoo","text":"torch.utils.model_zoo.load_url(url, model_dir=None, map_location=None) \u4ece\u7ed9\u5b9a\u7684 URL \u5904\u52a0\u8f7d Torch \u5e8f\u5217\u5316\u5bf9\u8c61. \u5982\u679c\u8be5\u5bf9\u8c61\u5df2\u7ecf\u5b58\u5728\u4e8e <cite>model_dir</cite> \u4e2d, \u5219\u5c06\u88ab\u53cd\u5e8f\u5217\u5316\u5e76\u8fd4\u56de. URL \u7684\u6587\u4ef6\u540d\u90e8\u5206\u5e94\u8be5\u9075\u5faa\u547d\u540d\u7ea6\u5b9a filename-&lt;sha256&gt;.ext \u5176\u4e2d &lt;sha256&gt; \u662f\u6587\u4ef6\u5185\u5bb9\u7684 SHA256 \u54c8\u5e0c\u7684\u524d\u516b\u4f4d\u6216\u66f4\u591a\u4f4d\u6570. \u54c8\u5e0c\u7528\u4e8e\u786e\u4fdd\u552f\u4e00\u7684\u540d\u79f0\u5e76\u9a8c\u8bc1\u6587\u4ef6\u7684\u5185\u5bb9. <cite>model_dir</cite> \u7684\u9ed8\u8ba4\u503c\u4e3a $TORCH_HOME/models \u5176\u4e2d $TORCH_HOME \u9ed8\u8ba4\u503c\u4e3a ~/.torch . \u53ef\u4ee5\u4f7f\u7528 $TORCH_MODEL_ZOO \u73af\u5883\u53d8\u91cf\u6765\u8986\u76d6\u9ed8\u8ba4\u76ee\u5f55. | Parameters: | url ( string ) \u2013 \u9700\u8981\u4e0b\u8f7d\u5bf9\u8c61\u7684 URL model_dir ( string , optional ) \u2013 \u4fdd\u5b58\u5bf9\u8c61\u7684\u76ee\u5f55 map_location ( optional ) \u2013 \u4e00\u4e2a\u51fd\u6570\u6216\u8005\u4e00\u4e2a\u5b57\u5178,\u6307\u5b9a\u5982\u4f55\u91cd\u65b0\u6620\u5c04\u5b58\u50a8\u4f4d\u7f6e (\u8be6\u60c5\u67e5\u9605 torch.load) | | --- | --- | Example >>> state_dict = torch.utils.model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')","title":"torch.utils.model_zoo"},{"location":"68/","text":"torch.onnx torch.onnx \u6a21\u5757\u53ef\u4ee5\u5c06\u6a21\u578b\u5bfc\u51fa\u6210 ONNX IR \u5f62\u5f0f.\u88ab\u5bfc\u51fa\u7684\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 ONNX \u5e93\u88ab\u91cd\u65b0\u5bfc\u5165, \u7136\u540e\u8f6c\u5316\u4e3a\u53ef\u4ee5\u5728\u5176\u5b83\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e0a\u8fd0\u884c\u7684\u6a21\u578b. \u793a\u4f8b:\u4ecePytorch\u5230Caffe2\u7684\u7aef\u5bf9\u7aefAlexNet\u6a21\u578b \u8fd9\u91cc\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u811a\u672c\u7a0b\u5e8f,\u5b83\u5c06\u4e00\u4e2a\u5728 torchvision \u4e2d\u5df2\u7ecf\u5b9a\u4e49\u7684\u9884\u8bad\u7ec3 AlexNet \u6a21\u578b\u5bfc\u51fa\u5230 ONNX \u683c\u5f0f. \u5b83\u4f1a\u8fd0\u884c\u4e00\u6b21,\u7136\u540e\u628a\u6a21\u578b\u4fdd\u5b58\u81f3 alexnet.proto : from torch.autograd import Variable import torch.onnx import torchvision dummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda() model = torchvision.models.alexnet(pretrained=True).cuda() torch.onnx.export(model, dummy_input, \"alexnet.proto\", verbose=True) \u5f97\u5230\u7684 alexnet.proto \u662f\u4e00\u4e2a protobuf \u4e8c\u503c\u6587\u4ef6, \u5b83\u5305\u542b\u6240\u5bfc\u51fa\u6a21\u578b ( \u8fd9\u91cc\u662f AlexNet )\u4e2d\u7f51\u7edc\u67b6\u6784\u548c\u7f51\u7edc\u53c2\u6570. \u5173\u952e\u53c2\u6570 verbose=True \u4f1a\u4f7f\u5bfc\u51fa\u8fc7\u7a0b\u4e2d\u6253\u5370\u51fa\u8be5\u7f51\u7edc\u7684\u53ef\u8bfb\u8868\u793a: # All parameters are encoded explicitly as inputs. By convention, # learned parameters (ala nn.Module.state_dict) are first, and the # actual inputs are last. graph(%1 : Float(64, 3, 11, 11) %2 : Float(64) # The definition sites of all variables are annotated with type # information, specifying the type and size of tensors. # For example, %3 is a 192 x 64 x 5 x 5 tensor of floats. %3 : Float(192, 64, 5, 5) %4 : Float(192) # ---- omitted for brevity ---- %15 : Float(1000, 4096) %16 : Float(1000) %17 : Float(10, 3, 224, 224)) { # the actual input! # Every statement consists of some output tensors (and their types), # the operator to be run (with its attributes, e.g., kernels, strides, # etc.), its input tensors (%17, %1) %19 : UNKNOWN_TYPE = Conv[kernels=[11, 11], strides=[4, 4], pads=[2, 2, 2, 2], dilations=[1, 1], group=1](%17, %1), uses = [[%20.i0]]; # UNKNOWN_TYPE: sometimes type information is not known. We hope to eliminate # all such cases in a later release. %20 : Float(10, 64, 55, 55) = Add[broadcast=1, axis=1](%19, %2), uses = [%21.i0]; %21 : Float(10, 64, 55, 55) = Relu(%20), uses = [%22.i0]; %22 : Float(10, 64, 27, 27) = MaxPool[kernels=[3, 3], pads=[0, 0, 0, 0], dilations=[1, 1], strides=[2, 2]](%21), uses = [%23.i0]; # ... # Finally, a network returns some tensors return (%58); } \u4f60\u53ef\u4ee5\u4f7f\u7528 onnx \u5e93\u9a8c\u8bc1 protobuf, \u5e76\u4e14\u7528 conda \u5b89\u88c5 onnx conda install -c conda-forge onnx \u7136\u540e\u8fd0\u884c: import onnx # Load the ONNX model model = onnx.load(\"alexnet.proto\") # Check that the IR is well formed onnx.checker.check_model(model) # Print a human readable representation of the graph onnx.helper.printable_graph(model.graph) \u4e3a\u4e86\u80fd\u591f\u4f7f\u7528 caffe2 \u8fd0\u884c\u811a\u672c, \u4f60\u9700\u8981\u4e09\u6837\u4e1c\u897f: \u4f60\u9700\u8981\u5b89\u88c5 Caffe2. \u5982\u679c\u4f60\u4e4b\u524d\u6ca1\u6709\u5b89\u88c5,\u8bf7\u53c2\u7167 \u5b89\u88c5\u6307\u5357 . \u4f60\u9700\u8981\u5b89\u88c5 onnx-caffe2 ,\u4e00\u4e2a\u7eaf Python \u7684\u5e93,\u5b83\u4e3a ONNX \u63d0\u4f9b\u4e86 Caffe2 \u7684 \u540e\u7aef.\u4f60\u53ef\u4ee5\u4f7f\u7528 pip \u5b89\u88c5 onnx-caffe2 : ```py pip install onnx-caffe2 ``` \u4e00\u65e6\u8fd9\u4e9b\u5b89\u88c5\u5b8c\u6210,\u4f60\u5c31\u53ef\u4ee5\u4f7f\u7528 Caffe2 \u7684\u540e\u7aef: # ...continuing from above import onnx_caffe2.backend as backend import numpy as np rep = backend.prepare(model, device=\"CUDA:0\") # or \"CPU\" # For the Caffe2 backend: # rep.predict_net is the Caffe2 protobuf for the network # rep.workspace is the Caffe2 workspace for the network # (see the class onnx_caffe2.backend.Workspace) outputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32)) # To run networks with more than one input, pass a tuple # rather than a single numpy ndarray. print(outputs[0]) \u4e4b\u540e,\u6211\u4eec\u8fd8\u4f1a\u63d0\u4f9b\u5176\u5b83\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u540e\u7aef\u652f\u6301. \u5c40\u9650 ONNX \u5bfc\u51fa\u5668\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f68\u8ff9\u7684\u5bfc\u51fa\u5668,\u8fd9\u610f\u5473\u7740\u5b83\u6267\u884c\u65f6\u9700\u8981\u8fd0\u884c\u4e00\u6b21\u6a21\u578b,\u7136\u540e\u5bfc\u51fa\u5b9e\u9645\u53c2\u4e0e\u8fd0\u7b97\u7684\u8fd0\u7b97\u7b26. \u8fd9\u4e5f\u610f\u5473\u7740, \u5982\u679c\u4f60\u7684\u6a21\u578b\u662f\u52a8\u6001\u7684,\u4f8b\u5982,\u6539\u53d8\u4e00\u4e9b\u4f9d\u8d56\u4e8e\u8f93\u5165\u6570\u636e\u7684\u64cd\u4f5c,\u8fd9\u65f6\u7684\u5bfc\u51fa\u7ed3\u679c\u662f\u4e0d\u51c6\u786e\u7684.\u540c\u6837,\u4e00 \u4e2a\u8f68\u8ff9\u53ef\u80fd\u53ea\u5bf9\u4e00\u4e2a\u5177\u4f53\u7684\u8f93\u5165\u5c3a\u5bf8\u6709\u6548 (\u8fd9\u662f\u4e3a\u4ec0\u4e48\u6211\u4eec\u5728\u8f68\u8ff9\u4e2d\u9700\u8981\u6709\u660e\u786e\u7684\u8f93\u5165\u7684\u539f\u56e0\u4e4b\u4e00.) \u6211\u4eec\u5efa\u8bae\u68c0\u67e5 \u6a21\u578b\u7684\u8f68\u8ff9,\u786e\u4fdd\u88ab\u8ffd\u8e2a\u7684\u8fd0\u7b97\u7b26\u662f\u5408\u7406\u7684. Pytorch \u548c Caffe2 \u4e2d\u7684\u4e00\u4e9b\u8fd0\u7b97\u7b26\u7ecf\u5e38\u6709\u7740\u6570\u503c\u4e0a\u7684\u5dee\u5f02.\u6839\u636e\u6a21\u578b\u7684\u7ed3\u6784,\u8fd9\u4e9b\u5dee\u5f02\u53ef\u80fd\u662f\u5fae\u5c0f\u7684,\u4f46\u5b83\u4eec\u4f1a\u5728 \u8868\u73b0\u4e0a\u4ea7\u751f\u5f88\u5927\u7684\u5dee\u522b (\u5c24\u5176\u662f\u5bf9\u4e8e\u672a\u8bad\u7ec3\u7684\u6a21\u578b.) \u4e4b\u540e,\u4e3a\u4e86\u5e2e\u52a9\u4f60\u5728\u51c6\u786e\u5ea6\u8981\u6c42\u5f88\u9ad8\u7684\u60c5\u51b5\u4e2d,\u80fd\u591f\u8f7b\u677e\u5730\u907f\u514d\u8fd9 \u4e9b\u5dee\u5f02\u5e26\u6765\u7684\u5f71\u54cd,\u6211\u4eec\u8ba1\u5212\u8ba9 Caffe2 \u80fd\u591f\u76f4\u63a5\u8c03\u7528 Torch \u7684\u8fd0\u7b97\u7b26. \u652f\u6301\u7684\u8fd0\u7b97\u7b26 \u4ee5\u4e0b\u662f\u5df2\u7ecf\u88ab\u652f\u6301\u7684\u8fd0\u7b97\u7b26: add (nonzero alpha not supported) sub (nonzero alpha not supported) mul div cat mm addmm neg tanh sigmoid mean t expand (only when used before a broadcasting ONNX operator; e.g., add) transpose view split squeeze prelu (single weight shared among input channels not supported) threshold (non-zero threshold/non-zero value not supported) leaky_relu glu softmax avg_pool2d (ceil_mode not supported) log_softmax unfold (experimental support with ATen-Caffe2 integration) elu Conv BatchNorm MaxPool1d (ceil_mode not supported) MaxPool2d (ceil_mode not supported) MaxPool3d (ceil_mode not supported) Embedding (no optional arguments supported) RNN ConstantPadNd Dropout FeatureDropout (training mode not supported) Index (constant integer and tuple indices supported) Negate \u4e0a\u9762\u7684\u8fd0\u7b97\u7b26\u8db3\u591f\u5bfc\u51fa\u4e0b\u9762\u7684\u6a21\u578b: AlexNet DCGAN DenseNet Inception (\u6ce8\u610f:\u8be5\u6a21\u578b\u5bf9\u64cd\u4f5c\u7b26\u5341\u5206\u654f\u611f) ResNet SuperResolution VGG word_language_model \u7528\u4e8e\u6307\u5b9a\u8fd0\u7b97\u7b26\u5b9a\u4e49\u7684\u63a5\u53e3\u662f\u9ad8\u5ea6\u5b9e\u9a8c\u6027\u7684,\u5e76\u4e14\u8fd8\u6ca1\u6709\u8bb0\u5f55.\u559c\u6b22\u63a2\u7d22\u7684\u7528\u6237\u5e94\u8be5\u6ce8\u610f,\u8fd9\u4e9bAPI\u53ef\u80fd\u4f1a\u5728\u4e4b\u540e\u88ab\u4fee\u6539. Functions torch.onnx.export(model, args, f, export_params=True, verbose=False, training=False) \u5c06\u4e00\u4e2a\u6a21\u578b\u5bfc\u51fa\u6210 ONNX \u683c\u5f0f.\u8fd9\u4e2a\u5bfc\u51fa\u5668\u4e3a\u4e86\u5f97\u5230\u6a21\u578b\u8fd0\u884c\u7684\u8f68\u8ff9,\u4f1a\u8fd0\u884c\u4e00\u6b21\u4f60\u7684\u6a21\u578b.\u540c\u65f6,\u5b83\u4e0d\u652f\u6301\u52a8\u6001\u6a21\u578b\uff08\u5982 RNN.\uff09 \u4e5f\u53ef\u53c2\u8003: onnx-export \u53c2\u6570: model (torch.nn.Module): \u5c06\u88ab\u5bfc\u51fa\u6a21\u578b. args (tuple of arguments): \u6a21\u578b\u7684\u8f93\u5165, model(*args) \u5fc5\u987b\u662f\u5bf9\u6a21\u578b\u7684\u6709\u6548\u8c03\u7528.\u4efb\u4f55\u975e\u53d8\u91cf\u53c2\u6570\u5c06\u88ab\u786c\u7f16\u7801\u5230\u5bfc\u51fa\u7684\u6a21\u578b\u4e2d.\u4efb\u4f55\u53d8\u91cf\u53c2\u6570\u90fd\u5c06\u6309\u7167\u5b83\u4eec\u5728\u53c2\u6570\u4e2d\u51fa\u73b0\u7684\u987a\u5e8f,\u6210\u4e3a\u8f93\u51fa\u6a21\u578b\u7684\u8f93\u5165.\u5982\u679c args \u662f\u4e00\u4e2a\u53d8\u91cf,\u76f8\u5f53\u4e8e\u7528\u8be5\u53d8\u91cf\u7684\u4e00\u4e2a\u5143\u7ec4\u6765\u8c03\u7528\u5b83.\uff08\u6ce8\u610f:\u76ee\u524d\u8fd8\u4e0d\u652f\u6301\u5c06\u5173\u952e\u53c2\u6570\u4f20\u9012\u7ed9\u6a21\u578b,\u5982\u679c\u9700\u8981,\u8bf7\u8054\u7cfb\u6211\u4eec.\uff09 f: \u4e00\u4e2a\u7c7b\u6587\u4ef6\u5bf9\u8c61\uff08\u5fc5\u987b\u5b9e\u73b0\u8fd4\u56de\u6587\u4ef6\u63cf\u8ff0\u7684fileno\uff09\u6216\u4e00\u4e2a\u5305\u542b\u6587\u4ef6\u540d\u7684\u5b57\u7b26\u4e32. \u4e00\u4e2a\u4e8c\u8fdb\u5236 Protobuf \u5c06\u88ab\u5199\u5165\u8fd9\u4e2a\u6587\u4ef6. export_params (bool, default True): \u5982\u679c\u6307\u5b9a,\u6240\u6709\u53c2\u6570\u5c06\u88ab\u5bfc\u51fa.\u5982\u679c\u8981\u5bfc\u51fa\u672a\u7ecf\u8bad\u7ec3\u7684\u6a21\u578b,\u8bf7\u5c06\u5176\u8bbe\u7f6e\u4e3a False.\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u5bfc\u51fa\u7684\u6a21\u578b\u5c06\u9996\u5148\u5c06\u5176\u6240\u6709\u53c2\u6570\u4f5c\u4e3a\u53c2\u6570,\u987a\u5e8f\u7531 model.state_dict().values() \u6307\u5b9a. verbose (bool, default False): \u5982\u679c\u6307\u5b9a,\u4f1a\u6253\u5370\u51fa\u6b63\u5728\u5bfc\u51fa\u8f68\u8ff9\u7684\u8c03\u5f0f\u63cf\u8ff0. training (bool, default False): \u5728\u8bad\u7ec3\u6a21\u5f0f\u4e0b\u8f93\u51fa\u6a21\u578b.\u76ee\u524d, ONNX \u53ea\u662f\u4f5c\u4e3a\u5bfc\u51fa\u6a21\u578b\u7684\u63a5\u53e3,\u6240\u4ee5\u4f60\u901a\u5e38\u4e0d\u9700\u8981\u5c06\u5176\u8bbe\u4e3a True.","title":"torch.onnx"},{"location":"68/#torchonnx","text":"torch.onnx \u6a21\u5757\u53ef\u4ee5\u5c06\u6a21\u578b\u5bfc\u51fa\u6210 ONNX IR \u5f62\u5f0f.\u88ab\u5bfc\u51fa\u7684\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 ONNX \u5e93\u88ab\u91cd\u65b0\u5bfc\u5165, \u7136\u540e\u8f6c\u5316\u4e3a\u53ef\u4ee5\u5728\u5176\u5b83\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e0a\u8fd0\u884c\u7684\u6a21\u578b.","title":"torch.onnx"},{"location":"68/#pytorchcaffe2alexnet","text":"\u8fd9\u91cc\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u811a\u672c\u7a0b\u5e8f,\u5b83\u5c06\u4e00\u4e2a\u5728 torchvision \u4e2d\u5df2\u7ecf\u5b9a\u4e49\u7684\u9884\u8bad\u7ec3 AlexNet \u6a21\u578b\u5bfc\u51fa\u5230 ONNX \u683c\u5f0f. \u5b83\u4f1a\u8fd0\u884c\u4e00\u6b21,\u7136\u540e\u628a\u6a21\u578b\u4fdd\u5b58\u81f3 alexnet.proto : from torch.autograd import Variable import torch.onnx import torchvision dummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda() model = torchvision.models.alexnet(pretrained=True).cuda() torch.onnx.export(model, dummy_input, \"alexnet.proto\", verbose=True) \u5f97\u5230\u7684 alexnet.proto \u662f\u4e00\u4e2a protobuf \u4e8c\u503c\u6587\u4ef6, \u5b83\u5305\u542b\u6240\u5bfc\u51fa\u6a21\u578b ( \u8fd9\u91cc\u662f AlexNet )\u4e2d\u7f51\u7edc\u67b6\u6784\u548c\u7f51\u7edc\u53c2\u6570. \u5173\u952e\u53c2\u6570 verbose=True \u4f1a\u4f7f\u5bfc\u51fa\u8fc7\u7a0b\u4e2d\u6253\u5370\u51fa\u8be5\u7f51\u7edc\u7684\u53ef\u8bfb\u8868\u793a: # All parameters are encoded explicitly as inputs. By convention, # learned parameters (ala nn.Module.state_dict) are first, and the # actual inputs are last. graph(%1 : Float(64, 3, 11, 11) %2 : Float(64) # The definition sites of all variables are annotated with type # information, specifying the type and size of tensors. # For example, %3 is a 192 x 64 x 5 x 5 tensor of floats. %3 : Float(192, 64, 5, 5) %4 : Float(192) # ---- omitted for brevity ---- %15 : Float(1000, 4096) %16 : Float(1000) %17 : Float(10, 3, 224, 224)) { # the actual input! # Every statement consists of some output tensors (and their types), # the operator to be run (with its attributes, e.g., kernels, strides, # etc.), its input tensors (%17, %1) %19 : UNKNOWN_TYPE = Conv[kernels=[11, 11], strides=[4, 4], pads=[2, 2, 2, 2], dilations=[1, 1], group=1](%17, %1), uses = [[%20.i0]]; # UNKNOWN_TYPE: sometimes type information is not known. We hope to eliminate # all such cases in a later release. %20 : Float(10, 64, 55, 55) = Add[broadcast=1, axis=1](%19, %2), uses = [%21.i0]; %21 : Float(10, 64, 55, 55) = Relu(%20), uses = [%22.i0]; %22 : Float(10, 64, 27, 27) = MaxPool[kernels=[3, 3], pads=[0, 0, 0, 0], dilations=[1, 1], strides=[2, 2]](%21), uses = [%23.i0]; # ... # Finally, a network returns some tensors return (%58); } \u4f60\u53ef\u4ee5\u4f7f\u7528 onnx \u5e93\u9a8c\u8bc1 protobuf, \u5e76\u4e14\u7528 conda \u5b89\u88c5 onnx conda install -c conda-forge onnx \u7136\u540e\u8fd0\u884c: import onnx # Load the ONNX model model = onnx.load(\"alexnet.proto\") # Check that the IR is well formed onnx.checker.check_model(model) # Print a human readable representation of the graph onnx.helper.printable_graph(model.graph) \u4e3a\u4e86\u80fd\u591f\u4f7f\u7528 caffe2 \u8fd0\u884c\u811a\u672c, \u4f60\u9700\u8981\u4e09\u6837\u4e1c\u897f: \u4f60\u9700\u8981\u5b89\u88c5 Caffe2. \u5982\u679c\u4f60\u4e4b\u524d\u6ca1\u6709\u5b89\u88c5,\u8bf7\u53c2\u7167 \u5b89\u88c5\u6307\u5357 . \u4f60\u9700\u8981\u5b89\u88c5 onnx-caffe2 ,\u4e00\u4e2a\u7eaf Python \u7684\u5e93,\u5b83\u4e3a ONNX \u63d0\u4f9b\u4e86 Caffe2 \u7684 \u540e\u7aef.\u4f60\u53ef\u4ee5\u4f7f\u7528 pip \u5b89\u88c5 onnx-caffe2 : ```py pip install onnx-caffe2 ``` \u4e00\u65e6\u8fd9\u4e9b\u5b89\u88c5\u5b8c\u6210,\u4f60\u5c31\u53ef\u4ee5\u4f7f\u7528 Caffe2 \u7684\u540e\u7aef: # ...continuing from above import onnx_caffe2.backend as backend import numpy as np rep = backend.prepare(model, device=\"CUDA:0\") # or \"CPU\" # For the Caffe2 backend: # rep.predict_net is the Caffe2 protobuf for the network # rep.workspace is the Caffe2 workspace for the network # (see the class onnx_caffe2.backend.Workspace) outputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32)) # To run networks with more than one input, pass a tuple # rather than a single numpy ndarray. print(outputs[0]) \u4e4b\u540e,\u6211\u4eec\u8fd8\u4f1a\u63d0\u4f9b\u5176\u5b83\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u540e\u7aef\u652f\u6301.","title":"\u793a\u4f8b:\u4ecePytorch\u5230Caffe2\u7684\u7aef\u5bf9\u7aefAlexNet\u6a21\u578b"},{"location":"68/#_1","text":"ONNX \u5bfc\u51fa\u5668\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f68\u8ff9\u7684\u5bfc\u51fa\u5668,\u8fd9\u610f\u5473\u7740\u5b83\u6267\u884c\u65f6\u9700\u8981\u8fd0\u884c\u4e00\u6b21\u6a21\u578b,\u7136\u540e\u5bfc\u51fa\u5b9e\u9645\u53c2\u4e0e\u8fd0\u7b97\u7684\u8fd0\u7b97\u7b26. \u8fd9\u4e5f\u610f\u5473\u7740, \u5982\u679c\u4f60\u7684\u6a21\u578b\u662f\u52a8\u6001\u7684,\u4f8b\u5982,\u6539\u53d8\u4e00\u4e9b\u4f9d\u8d56\u4e8e\u8f93\u5165\u6570\u636e\u7684\u64cd\u4f5c,\u8fd9\u65f6\u7684\u5bfc\u51fa\u7ed3\u679c\u662f\u4e0d\u51c6\u786e\u7684.\u540c\u6837,\u4e00 \u4e2a\u8f68\u8ff9\u53ef\u80fd\u53ea\u5bf9\u4e00\u4e2a\u5177\u4f53\u7684\u8f93\u5165\u5c3a\u5bf8\u6709\u6548 (\u8fd9\u662f\u4e3a\u4ec0\u4e48\u6211\u4eec\u5728\u8f68\u8ff9\u4e2d\u9700\u8981\u6709\u660e\u786e\u7684\u8f93\u5165\u7684\u539f\u56e0\u4e4b\u4e00.) \u6211\u4eec\u5efa\u8bae\u68c0\u67e5 \u6a21\u578b\u7684\u8f68\u8ff9,\u786e\u4fdd\u88ab\u8ffd\u8e2a\u7684\u8fd0\u7b97\u7b26\u662f\u5408\u7406\u7684. Pytorch \u548c Caffe2 \u4e2d\u7684\u4e00\u4e9b\u8fd0\u7b97\u7b26\u7ecf\u5e38\u6709\u7740\u6570\u503c\u4e0a\u7684\u5dee\u5f02.\u6839\u636e\u6a21\u578b\u7684\u7ed3\u6784,\u8fd9\u4e9b\u5dee\u5f02\u53ef\u80fd\u662f\u5fae\u5c0f\u7684,\u4f46\u5b83\u4eec\u4f1a\u5728 \u8868\u73b0\u4e0a\u4ea7\u751f\u5f88\u5927\u7684\u5dee\u522b (\u5c24\u5176\u662f\u5bf9\u4e8e\u672a\u8bad\u7ec3\u7684\u6a21\u578b.) \u4e4b\u540e,\u4e3a\u4e86\u5e2e\u52a9\u4f60\u5728\u51c6\u786e\u5ea6\u8981\u6c42\u5f88\u9ad8\u7684\u60c5\u51b5\u4e2d,\u80fd\u591f\u8f7b\u677e\u5730\u907f\u514d\u8fd9 \u4e9b\u5dee\u5f02\u5e26\u6765\u7684\u5f71\u54cd,\u6211\u4eec\u8ba1\u5212\u8ba9 Caffe2 \u80fd\u591f\u76f4\u63a5\u8c03\u7528 Torch \u7684\u8fd0\u7b97\u7b26.","title":"\u5c40\u9650"},{"location":"68/#_2","text":"\u4ee5\u4e0b\u662f\u5df2\u7ecf\u88ab\u652f\u6301\u7684\u8fd0\u7b97\u7b26: add (nonzero alpha not supported) sub (nonzero alpha not supported) mul div cat mm addmm neg tanh sigmoid mean t expand (only when used before a broadcasting ONNX operator; e.g., add) transpose view split squeeze prelu (single weight shared among input channels not supported) threshold (non-zero threshold/non-zero value not supported) leaky_relu glu softmax avg_pool2d (ceil_mode not supported) log_softmax unfold (experimental support with ATen-Caffe2 integration) elu Conv BatchNorm MaxPool1d (ceil_mode not supported) MaxPool2d (ceil_mode not supported) MaxPool3d (ceil_mode not supported) Embedding (no optional arguments supported) RNN ConstantPadNd Dropout FeatureDropout (training mode not supported) Index (constant integer and tuple indices supported) Negate \u4e0a\u9762\u7684\u8fd0\u7b97\u7b26\u8db3\u591f\u5bfc\u51fa\u4e0b\u9762\u7684\u6a21\u578b: AlexNet DCGAN DenseNet Inception (\u6ce8\u610f:\u8be5\u6a21\u578b\u5bf9\u64cd\u4f5c\u7b26\u5341\u5206\u654f\u611f) ResNet SuperResolution VGG word_language_model \u7528\u4e8e\u6307\u5b9a\u8fd0\u7b97\u7b26\u5b9a\u4e49\u7684\u63a5\u53e3\u662f\u9ad8\u5ea6\u5b9e\u9a8c\u6027\u7684,\u5e76\u4e14\u8fd8\u6ca1\u6709\u8bb0\u5f55.\u559c\u6b22\u63a2\u7d22\u7684\u7528\u6237\u5e94\u8be5\u6ce8\u610f,\u8fd9\u4e9bAPI\u53ef\u80fd\u4f1a\u5728\u4e4b\u540e\u88ab\u4fee\u6539.","title":"\u652f\u6301\u7684\u8fd0\u7b97\u7b26"},{"location":"68/#functions","text":"torch.onnx.export(model, args, f, export_params=True, verbose=False, training=False) \u5c06\u4e00\u4e2a\u6a21\u578b\u5bfc\u51fa\u6210 ONNX \u683c\u5f0f.\u8fd9\u4e2a\u5bfc\u51fa\u5668\u4e3a\u4e86\u5f97\u5230\u6a21\u578b\u8fd0\u884c\u7684\u8f68\u8ff9,\u4f1a\u8fd0\u884c\u4e00\u6b21\u4f60\u7684\u6a21\u578b.\u540c\u65f6,\u5b83\u4e0d\u652f\u6301\u52a8\u6001\u6a21\u578b\uff08\u5982 RNN.\uff09 \u4e5f\u53ef\u53c2\u8003: onnx-export \u53c2\u6570: model (torch.nn.Module): \u5c06\u88ab\u5bfc\u51fa\u6a21\u578b. args (tuple of arguments): \u6a21\u578b\u7684\u8f93\u5165, model(*args) \u5fc5\u987b\u662f\u5bf9\u6a21\u578b\u7684\u6709\u6548\u8c03\u7528.\u4efb\u4f55\u975e\u53d8\u91cf\u53c2\u6570\u5c06\u88ab\u786c\u7f16\u7801\u5230\u5bfc\u51fa\u7684\u6a21\u578b\u4e2d.\u4efb\u4f55\u53d8\u91cf\u53c2\u6570\u90fd\u5c06\u6309\u7167\u5b83\u4eec\u5728\u53c2\u6570\u4e2d\u51fa\u73b0\u7684\u987a\u5e8f,\u6210\u4e3a\u8f93\u51fa\u6a21\u578b\u7684\u8f93\u5165.\u5982\u679c args \u662f\u4e00\u4e2a\u53d8\u91cf,\u76f8\u5f53\u4e8e\u7528\u8be5\u53d8\u91cf\u7684\u4e00\u4e2a\u5143\u7ec4\u6765\u8c03\u7528\u5b83.\uff08\u6ce8\u610f:\u76ee\u524d\u8fd8\u4e0d\u652f\u6301\u5c06\u5173\u952e\u53c2\u6570\u4f20\u9012\u7ed9\u6a21\u578b,\u5982\u679c\u9700\u8981,\u8bf7\u8054\u7cfb\u6211\u4eec.\uff09 f: \u4e00\u4e2a\u7c7b\u6587\u4ef6\u5bf9\u8c61\uff08\u5fc5\u987b\u5b9e\u73b0\u8fd4\u56de\u6587\u4ef6\u63cf\u8ff0\u7684fileno\uff09\u6216\u4e00\u4e2a\u5305\u542b\u6587\u4ef6\u540d\u7684\u5b57\u7b26\u4e32. \u4e00\u4e2a\u4e8c\u8fdb\u5236 Protobuf \u5c06\u88ab\u5199\u5165\u8fd9\u4e2a\u6587\u4ef6. export_params (bool, default True): \u5982\u679c\u6307\u5b9a,\u6240\u6709\u53c2\u6570\u5c06\u88ab\u5bfc\u51fa.\u5982\u679c\u8981\u5bfc\u51fa\u672a\u7ecf\u8bad\u7ec3\u7684\u6a21\u578b,\u8bf7\u5c06\u5176\u8bbe\u7f6e\u4e3a False.\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u5bfc\u51fa\u7684\u6a21\u578b\u5c06\u9996\u5148\u5c06\u5176\u6240\u6709\u53c2\u6570\u4f5c\u4e3a\u53c2\u6570,\u987a\u5e8f\u7531 model.state_dict().values() \u6307\u5b9a. verbose (bool, default False): \u5982\u679c\u6307\u5b9a,\u4f1a\u6253\u5370\u51fa\u6b63\u5728\u5bfc\u51fa\u8f68\u8ff9\u7684\u8c03\u5f0f\u63cf\u8ff0. training (bool, default False): \u5728\u8bad\u7ec3\u6a21\u5f0f\u4e0b\u8f93\u51fa\u6a21\u578b.\u76ee\u524d, ONNX \u53ea\u662f\u4f5c\u4e3a\u5bfc\u51fa\u6a21\u578b\u7684\u63a5\u53e3,\u6240\u4ee5\u4f60\u901a\u5e38\u4e0d\u9700\u8981\u5c06\u5176\u8bbe\u4e3a True.","title":"Functions"},{"location":"69/","text":"torchvision \u53c2\u8003","title":"torchvision \u53c2\u8003"},{"location":"69/#torchvision","text":"","title":"torchvision \u53c2\u8003"},{"location":"7/","text":"PyTorch for former Torch users Author : Soumith Chintala \u5728\u672c\u6559\u7a0b\u4e2d, \u4f60\u5c06\u5b66\u4e60\u5230\u4ee5\u4e0b\u5185\u5bb9: \u4f7f\u7528 torch Tensors, \u5b83\u548c (Lua)Torch \u6709\u5f88\u5927\u7684\u4e0d\u540c \u4f7f\u7528 autograd package \u6784\u5efa\u795e\u7ecf\u7f51\u7edc \u6784\u5efa\u4e00\u4e2a ConvNet \u6784\u5efa\u4e00\u4e2a Recurrent Net \u4f7f\u7528\u591a\u4e2a GPUs Tensors Autograd (\u81ea\u52a8\u6c42\u5bfc) nn package Multi-GPU examples","title":"PyTorch for former Torch users"},{"location":"7/#pytorch-for-former-torch-users","text":"Author : Soumith Chintala \u5728\u672c\u6559\u7a0b\u4e2d, \u4f60\u5c06\u5b66\u4e60\u5230\u4ee5\u4e0b\u5185\u5bb9: \u4f7f\u7528 torch Tensors, \u5b83\u548c (Lua)Torch \u6709\u5f88\u5927\u7684\u4e0d\u540c \u4f7f\u7528 autograd package \u6784\u5efa\u795e\u7ecf\u7f51\u7edc \u6784\u5efa\u4e00\u4e2a ConvNet \u6784\u5efa\u4e00\u4e2a Recurrent Net \u4f7f\u7528\u591a\u4e2a GPUs Tensors Autograd (\u81ea\u52a8\u6c42\u5bfc) nn package Multi-GPU examples","title":"PyTorch for former Torch users"},{"location":"70/","text":"torchvision \u6a21\u5757 torchvision \u5e93\u5305\u542b\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u4e9b\u5e38\u7528\u7684\u6570\u636e\u96c6, \u6a21\u578b\u67b6\u6784\u4ee5\u53ca\u56fe\u50cf\u53d8\u6362\u65b9\u6cd5. Package Reference torchvision.datasets MNIST Fashion-MNIST COCO LSUN ImageFolder Imagenet-12 CIFAR STL10 SVHN PhotoTour torchvision.models Alexnet VGG ResNet SqueezeNet DenseNet Inception v3 torchvision.transforms PIL Image \u4e0a\u7684\u53d8\u6362 torch.*Tensor \u4e0a\u7684\u53d8\u6362 \u8f6c\u6362\u7c7b\u578b\u7684\u53d8\u6362 \u901a\u7528\u7684\u53d8\u6362 torchvision.utils torchvision.get_image_backend() \u83b7\u53d6\u7528\u4e8e\u52a0\u8f7d\u56fe\u50cf\u7684\u5305\u7684\u540d\u79f0 torchvision.set_image_backend(backend) \u6307\u5b9a\u7528\u4e8e\u52a0\u8f7d\u56fe\u50cf\u7684\u5305. Parameters: backend ( string ) \u2013 \u56fe\u50cf\u5904\u7406\u540e\u7aef\u7684\u540d\u79f0. {\u2018PIL\u2019, \u2018accimage\u2019} \u4e4b\u4e00. accimage \u4f7f\u7528 Intel IPP library\uff08\u9ad8\u6027\u80fd\u56fe\u50cf\u52a0\u8f7d\u548c\u589e\u5f3a\u7a0b\u5e8f\u6a21\u62df\u7684\u7a0b\u5e8f\uff09.\u901a\u5e38\u6bd4PIL\u5e93\u8981\u5feb, \u4f46\u662f\u4e0d\u652f\u6301\u8bb8\u591a\u64cd\u4f5c.","title":"torchvision"},{"location":"70/#torchvision","text":"\u6a21\u5757 torchvision \u5e93\u5305\u542b\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u4e9b\u5e38\u7528\u7684\u6570\u636e\u96c6, \u6a21\u578b\u67b6\u6784\u4ee5\u53ca\u56fe\u50cf\u53d8\u6362\u65b9\u6cd5. Package Reference torchvision.datasets MNIST Fashion-MNIST COCO LSUN ImageFolder Imagenet-12 CIFAR STL10 SVHN PhotoTour torchvision.models Alexnet VGG ResNet SqueezeNet DenseNet Inception v3 torchvision.transforms PIL Image \u4e0a\u7684\u53d8\u6362 torch.*Tensor \u4e0a\u7684\u53d8\u6362 \u8f6c\u6362\u7c7b\u578b\u7684\u53d8\u6362 \u901a\u7528\u7684\u53d8\u6362 torchvision.utils torchvision.get_image_backend() \u83b7\u53d6\u7528\u4e8e\u52a0\u8f7d\u56fe\u50cf\u7684\u5305\u7684\u540d\u79f0 torchvision.set_image_backend(backend) \u6307\u5b9a\u7528\u4e8e\u52a0\u8f7d\u56fe\u50cf\u7684\u5305. Parameters: backend ( string ) \u2013 \u56fe\u50cf\u5904\u7406\u540e\u7aef\u7684\u540d\u79f0. {\u2018PIL\u2019, \u2018accimage\u2019} \u4e4b\u4e00. accimage \u4f7f\u7528 Intel IPP library\uff08\u9ad8\u6027\u80fd\u56fe\u50cf\u52a0\u8f7d\u548c\u589e\u5f3a\u7a0b\u5e8f\u6a21\u62df\u7684\u7a0b\u5e8f\uff09.\u901a\u5e38\u6bd4PIL\u5e93\u8981\u5feb, \u4f46\u662f\u4e0d\u652f\u6301\u8bb8\u591a\u64cd\u4f5c.","title":"torchvision"},{"location":"71/","text":"torchvision.datasets \u6240\u6709\u7684\u6570\u636e\u96c6\u90fd\u662f torch.utils.data.Dataset \u7c7b\u7684\u5b50\u7c7b, \u4e5f\u5c31\u662f\u8bf4, \u4ed6\u4eec\u5185\u90e8\u90fd\u5b9e\u73b0\u4e86 __getitem__ \u548c __len__ \u8fd9\u4e24\u4e2a\u65b9\u6cd5. \u540c\u65f6, \u4ed6\u4eec\u4e5f\u90fd\u53ef\u4ee5\u4f20\u9012\u7ed9\u7c7b torch.utils.data.Dataset , \u5b83\u53ef\u4ee5\u4f7f\u7528 torch.multiprocessing \u5de5\u4f5c\u5668\u6765\u5e76\u884c\u7684\u52a0\u8f7d\u591a\u4e2a\u6837\u672c. Example: imagenet_data = torchvision.datasets.ImageFolder('path/to/imagenet_root/') data_loader = torch.utils.data.DataLoader(imagenet_data, batch_size=4, shuffle=True, num_workers=args.nThreads) \u53ef\u7528\u7684\u6570\u636e\u96c6\u5982\u4e0b\u6240\u793a: Datasets MNIST Fashion-MNIST COCO Captions Detection LSUN ImageFolder Imagenet-12 CIFAR STL10 SVHN PhotoTour \u6240\u6709\u6570\u636e\u96c6\u90fd\u6709\u51e0\u4e4e\u76f8\u4f3c\u7684 API, \u5b83\u4eec\u6709\u4e24\u4e2a\u666e\u901a\u7684\u53c2\u6570: transform \u548c target_transform \u53ef\u5206\u522b\u7684\u5bf9\u8f93\u5165\u548c\u76ee\u6807\u6570\u636e\u96c6\u8fdb\u884c\u53d8\u6362. - transform : \u8f93\u5165\u539f\u59cb\u56fe\u7247, \u8fd4\u56de\u8f6c\u6362\u540e\u7684\u56fe\u7247. - target_transform : \u8f93\u5165\u4e3a target, \u8fd4\u56de\u8f6c\u6362\u540e\u7684 target. MNIST class torchvision.datasets.MNIST(root, train=True, transform=None, target_transform=None, download=False) MNIST Dataset. | Parameters: | root ( string ) \u2013 processed/training.pt \u548c processed/test.pt \u5b58\u5728\u7684\u4e3b\u76ee\u5f55. train ( bool , optional ) \u2013 \u5982\u679c True, \u6570\u636e\u6765\u81ea\u8bad\u7ec3\u96c6 training.pt , \u5982\u679c False, \u6570\u636e\u6765\u81ea\u6d4b\u8bd5\u96c6 test.pt . download ( bool , optional ) \u2013 \u5982\u679c true, \u5c31\u4ece\u7f51\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4e14\u653e\u5230 root \u76ee\u5f55\u4e0b. \u5982\u679c\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d, \u90a3\u4e48\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. | | --- | --- | Fashion-MNIST class torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=False) Fashion-MNIST Dataset. | Parameters: | root ( string ) \u2013 processed/training.pt \u548c processed/test.pt \u5b58\u5728\u7684\u4e3b\u76ee\u5f55. train ( bool , optional ) \u2013 \u5982\u679c True, \u6570\u636e\u6765\u81ea\u8bad\u7ec3\u96c6 training.pt , \u5982\u679c False, \u6570\u636e\u6765\u81ea\u6d4b\u8bd5\u96c6 test.pt . download ( bool , optional ) \u2013 \u5982\u679c true, \u5c31\u4ece\u7f51\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4e14\u653e\u5230 root \u76ee\u5f55\u4e0b. \u5982\u679c\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d, \u90a3\u4e48\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. | | --- | --- | COCO Note \u9700\u8981\u5b89\u88c5 COCO API Captions class torchvision.datasets.CocoCaptions(root, annFile, transform=None, target_transform=None) MS Coco Captions Dataset. | Parameters: | root ( string ) \u2013 \u6570\u636e\u96c6\u4e0b\u8f7d\u5b58\u653e\u7684\u4e3b\u76ee\u5f55. annFile ( string ) \u2013 json \u6ce8\u91ca\u6587\u4ef6\u5b58\u653e\u7684\u8def\u5f84 transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.ToTensor target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. | | --- | --- | Example import torchvision.datasets as dset import torchvision.transforms as transforms cap = dset.CocoCaptions(root = 'dir where images are', annFile = 'json annotation file', transform=transforms.ToTensor()) print('Number of samples: ', len(cap)) img, target = cap[3] # load 4th sample print(\"Image Size: \", img.size()) print(target) Output: Number of samples: 82783 Image Size: (3L, 427L, 640L) [u'A plane emitting smoke stream flying over a mountain.', u'A plane darts across a bright blue sky behind a mountain covered in snow', u'A plane leaves a contrail above the snowy mountain top.', u'A mountain that has a plane flying overheard in the distance.', u'A mountain view with a plume of smoke in the background'] __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: Tuple (image, target). \u76ee\u6807\u662f\u4e00\u4e2a\u56fe\u50cf\u6807\u6ce8\u7684\u5217\u8868. --- --- Return type: tuple --- --- Detection class torchvision.datasets.CocoDetection(root, annFile, transform=None, target_transform=None) MS Coco Detection Dataset. | Parameters: | root ( string ) \u2013 \u6570\u636e\u96c6\u4e0b\u8f7d\u5b58\u653e\u7684\u4e3b\u76ee\u5f55. annFile ( string ) \u2013 json \u6ce8\u91ca\u6587\u4ef6\u5b58\u653e\u7684\u8def\u5f84 transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.ToTensor target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: Tuple (image, target). \u76ee\u6807\u662f\u7531 coco.loadAnns \u8fd4\u56de\u7684\u5bf9\u8c61. --- --- Return type: tuple --- --- LSUN class torchvision.datasets.LSUN(db_path, classes='train', transform=None, target_transform=None) LSUN dataset. | Parameters: | db_path ( string ) \u2013 \u6570\u636e\u96c6\u6587\u4ef6\u5b58\u653e\u7684\u4e3b\u76ee\u5f55. classes ( string or list ) \u2013 {\u2018train\u2019, \u2018val\u2019, \u2018test\u2019} \u4e2d\u7684\u4e00\u4e2a, \u6216\u8005\u662f\u4e00\u4e2a\u8981\u8f7d\u5165\u79cd\u7c7b\u7684\u5217\u8868. e,g. [\u2018bedroom_train\u2019, \u2018church_train\u2019]. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: Tuple (image, target) \u76ee\u6807\u662f\u76ee\u6807\u7c7b\u522b\u7684\u7d22\u5f15. --- --- Return type: tuple --- --- ImageFolder class torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=<function default_loader at 0x432aa28>) \u4e00\u4e2a\u901a\u7528\u7684\u6570\u636e\u52a0\u8f7d\u5668, \u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u4ee5\u4ee5\u4e0b\u65b9\u5f0f\u7ec4\u7ec7: root/dog/xxx.png root/dog/xxy.png root/dog/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png | Parameters: | root ( string ) \u2013 \u4e3b\u76ee\u5f55. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. loader \u2013 \u4e00\u4e2a\u4ece\u7ed9\u5b9a\u8def\u5f84\u8f7d\u5165\u56fe\u50cf\u7684\u51fd\u6570. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: (image, target) \u76ee\u6807\u662f\u76ee\u6807\u7c7b\u522b\u7684class_index. --- --- Return type: tuple --- --- Imagenet-12 \u8fd9\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e2a ImageFolder \u6570\u636e\u96c6\u8f7b\u6613\u5b9e\u73b0. \u8be5\u6570\u636e\u9884\u5904\u7406\u8fc7\u7a0b\u5982 \u8fd9\u91cc\u63cf\u8ff0\u7684 \u6240\u793a \u8fd9\u91cc\u662f\u4e00\u4e2a\u9884\u5904\u7406\u793a\u4f8b . CIFAR class torchvision.datasets.CIFAR10(root, train=True, transform=None, target_transform=None, download=False) CIFAR10 Dataset. | Parameters: | root ( string ) \u2013 cifar-10-batches-py \u5b58\u5728\u7684\u4e3b\u76ee\u5f55. train ( bool , optional ) \u2013 \u5982\u679c True, \u6570\u636e\u6765\u81ea\u8bad\u7ec3\u96c6, \u5982\u679c False, \u6570\u636e\u6765\u81ea\u6d4b\u8bd5\u96c6. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. download ( bool , optional ) \u2013 \u5982\u679c true, \u5c31\u4ece\u7f51\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4e14\u653e\u5230 root \u76ee\u5f55\u4e0b. \u5982\u679c\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d, \u90a3\u4e48\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: (image, target) \u76ee\u6807\u662f\u76ee\u6807\u5206\u7c7b\u7684\u7d22\u5f15. --- --- Return type: tuple --- --- class torchvision.datasets.CIFAR100(root, train=True, transform=None, target_transform=None, download=False) CIFAR100 Dataset. <cite>CIFAR10</cite> Dataset \u7684\u4e00\u4e2a\u5b50\u7c7b. STL10 class torchvision.datasets.STL10(root, split='train', transform=None, target_transform=None, download=False) STL10 Dataset. | Parameters: | root ( string ) \u2013 [ ](#id5)stl10_binary \u6570\u636e\u96c6\u5b58\u653e\u7684\u4e3b\u76ee\u5f55. split ( string ) \u2013 {\u2018train\u2019, \u2018test\u2019, \u2018unlabeled\u2019, \u2018train+unlabeled\u2019} \u4e2d\u7684\u4e00\u4e2a. \u5b83\u662f\u6839\u636e\u6570\u636e\u96c6\u9009\u62e9\u7684. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. download ( bool , optional ) \u2013 \u5982\u679c true, \u5c31\u4ece\u7f51\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4e14\u653e\u5230 root \u76ee\u5f55\u4e0b. \u5982\u679c\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d, \u90a3\u4e48\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: (image, target) \u76ee\u6807\u662f\u76ee\u6807\u7c7b\u7684\u7d22\u5f15. --- --- Return type: tuple --- --- SVHN class torchvision.datasets.SVHN(root, split='train', transform=None, target_transform=None, download=False) SVHN Dataset. Note: \u539f\u59cb\u7684 SVHN \u6570\u636e\u96c6\u628a\u6807\u7b7e <cite>10</cite> \u5206\u7ed9\u4e86\u6570\u5b57 <cite>0</cite>. \u7136\u800c\u5728\u8fd9\u4e2a\u6570\u636e\u96c6, \u6211\u4eec\u628a\u6807\u7b7e <cite>0</cite> \u5206\u7ed9\u4e86\u6570\u5b57 <cite>0</cite> \u4ee5\u4fbf \u548c PyTorch \u7684\u635f\u5931\u51fd\u6570\u4e0d\u4ea7\u751f\u51b2\u7a81, \u5b83\u671f\u5f85\u7684\u7c7b\u6807\u7b7e\u7684\u8303\u56f4\u662f <cite>[0, C-1]</cite>. | Parameters: | root ( string ) \u2013 [ ](#id8)SVHN \u6570\u636e\u96c6\u5b58\u653e\u7684\u4e3b\u76ee\u5f55. split ( string ) \u2013 {\u2018train\u2019, \u2018test\u2019, \u2018extra\u2019} \u4e2d\u7684\u4e00\u4e2a. \u5b83\u662f\u6839\u636e\u6570\u636e\u96c6\u9009\u62e9\u7684. \u2018extra\u2019 \u662f\u4e00\u4e2a\u989d\u5916\u7684\u8bad\u7ec3\u96c6. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. download ( bool , optional ) \u2013 \u5982\u679c true, \u5c31\u4ece\u7f51\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4e14\u653e\u5230 root \u76ee\u5f55\u4e0b. \u5982\u679c\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d, \u90a3\u4e48\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: (image, target) \u76ee\u6807\u662f\u76ee\u6807\u7c7b\u7684\u7d22\u5f15. --- --- Return type: tuple --- --- PhotoTour class torchvision.datasets.PhotoTour(root, name, train=True, transform=None, download=False) Learning Local Image Descriptors Data Dataset. | Parameters: | root ( string ) \u2013 \u56fe\u50cf\u5b58\u653e\u7684\u4e3b\u76ee\u5f55. name ( string ) \u2013 \u8f7d\u5165\u7684\u6570\u636e\u96c6\u7684\u540d\u5b57. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. download ( bool , optional ) \u2013 \u5982\u679c true, \u5c31\u4ece\u7f51\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4e14\u653e\u5230 root \u76ee\u5f55\u4e0b. \u5982\u679c\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d, \u90a3\u4e48\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: (data1, data2, matches) --- --- Return type: tuple --- ---","title":"torchvision.datasets"},{"location":"71/#torchvisiondatasets","text":"\u6240\u6709\u7684\u6570\u636e\u96c6\u90fd\u662f torch.utils.data.Dataset \u7c7b\u7684\u5b50\u7c7b, \u4e5f\u5c31\u662f\u8bf4, \u4ed6\u4eec\u5185\u90e8\u90fd\u5b9e\u73b0\u4e86 __getitem__ \u548c __len__ \u8fd9\u4e24\u4e2a\u65b9\u6cd5. \u540c\u65f6, \u4ed6\u4eec\u4e5f\u90fd\u53ef\u4ee5\u4f20\u9012\u7ed9\u7c7b torch.utils.data.Dataset , \u5b83\u53ef\u4ee5\u4f7f\u7528 torch.multiprocessing \u5de5\u4f5c\u5668\u6765\u5e76\u884c\u7684\u52a0\u8f7d\u591a\u4e2a\u6837\u672c. Example: imagenet_data = torchvision.datasets.ImageFolder('path/to/imagenet_root/') data_loader = torch.utils.data.DataLoader(imagenet_data, batch_size=4, shuffle=True, num_workers=args.nThreads) \u53ef\u7528\u7684\u6570\u636e\u96c6\u5982\u4e0b\u6240\u793a: Datasets MNIST Fashion-MNIST COCO Captions Detection LSUN ImageFolder Imagenet-12 CIFAR STL10 SVHN PhotoTour \u6240\u6709\u6570\u636e\u96c6\u90fd\u6709\u51e0\u4e4e\u76f8\u4f3c\u7684 API, \u5b83\u4eec\u6709\u4e24\u4e2a\u666e\u901a\u7684\u53c2\u6570: transform \u548c target_transform \u53ef\u5206\u522b\u7684\u5bf9\u8f93\u5165\u548c\u76ee\u6807\u6570\u636e\u96c6\u8fdb\u884c\u53d8\u6362. - transform : \u8f93\u5165\u539f\u59cb\u56fe\u7247, \u8fd4\u56de\u8f6c\u6362\u540e\u7684\u56fe\u7247. - target_transform : \u8f93\u5165\u4e3a target, \u8fd4\u56de\u8f6c\u6362\u540e\u7684 target.","title":"torchvision.datasets"},{"location":"71/#mnist","text":"class torchvision.datasets.MNIST(root, train=True, transform=None, target_transform=None, download=False) MNIST Dataset. | Parameters: | root ( string ) \u2013 processed/training.pt \u548c processed/test.pt \u5b58\u5728\u7684\u4e3b\u76ee\u5f55. train ( bool , optional ) \u2013 \u5982\u679c True, \u6570\u636e\u6765\u81ea\u8bad\u7ec3\u96c6 training.pt , \u5982\u679c False, \u6570\u636e\u6765\u81ea\u6d4b\u8bd5\u96c6 test.pt . download ( bool , optional ) \u2013 \u5982\u679c true, \u5c31\u4ece\u7f51\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4e14\u653e\u5230 root \u76ee\u5f55\u4e0b. \u5982\u679c\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d, \u90a3\u4e48\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. | | --- | --- |","title":"MNIST"},{"location":"71/#fashion-mnist","text":"class torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=False) Fashion-MNIST Dataset. | Parameters: | root ( string ) \u2013 processed/training.pt \u548c processed/test.pt \u5b58\u5728\u7684\u4e3b\u76ee\u5f55. train ( bool , optional ) \u2013 \u5982\u679c True, \u6570\u636e\u6765\u81ea\u8bad\u7ec3\u96c6 training.pt , \u5982\u679c False, \u6570\u636e\u6765\u81ea\u6d4b\u8bd5\u96c6 test.pt . download ( bool , optional ) \u2013 \u5982\u679c true, \u5c31\u4ece\u7f51\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4e14\u653e\u5230 root \u76ee\u5f55\u4e0b. \u5982\u679c\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d, \u90a3\u4e48\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. | | --- | --- |","title":"Fashion-MNIST"},{"location":"71/#coco","text":"Note \u9700\u8981\u5b89\u88c5 COCO API","title":"COCO"},{"location":"71/#captions","text":"class torchvision.datasets.CocoCaptions(root, annFile, transform=None, target_transform=None) MS Coco Captions Dataset. | Parameters: | root ( string ) \u2013 \u6570\u636e\u96c6\u4e0b\u8f7d\u5b58\u653e\u7684\u4e3b\u76ee\u5f55. annFile ( string ) \u2013 json \u6ce8\u91ca\u6587\u4ef6\u5b58\u653e\u7684\u8def\u5f84 transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.ToTensor target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. | | --- | --- | Example import torchvision.datasets as dset import torchvision.transforms as transforms cap = dset.CocoCaptions(root = 'dir where images are', annFile = 'json annotation file', transform=transforms.ToTensor()) print('Number of samples: ', len(cap)) img, target = cap[3] # load 4th sample print(\"Image Size: \", img.size()) print(target) Output: Number of samples: 82783 Image Size: (3L, 427L, 640L) [u'A plane emitting smoke stream flying over a mountain.', u'A plane darts across a bright blue sky behind a mountain covered in snow', u'A plane leaves a contrail above the snowy mountain top.', u'A mountain that has a plane flying overheard in the distance.', u'A mountain view with a plume of smoke in the background'] __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: Tuple (image, target). \u76ee\u6807\u662f\u4e00\u4e2a\u56fe\u50cf\u6807\u6ce8\u7684\u5217\u8868. --- --- Return type: tuple --- ---","title":"Captions"},{"location":"71/#detection","text":"class torchvision.datasets.CocoDetection(root, annFile, transform=None, target_transform=None) MS Coco Detection Dataset. | Parameters: | root ( string ) \u2013 \u6570\u636e\u96c6\u4e0b\u8f7d\u5b58\u653e\u7684\u4e3b\u76ee\u5f55. annFile ( string ) \u2013 json \u6ce8\u91ca\u6587\u4ef6\u5b58\u653e\u7684\u8def\u5f84 transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.ToTensor target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: Tuple (image, target). \u76ee\u6807\u662f\u7531 coco.loadAnns \u8fd4\u56de\u7684\u5bf9\u8c61. --- --- Return type: tuple --- ---","title":"Detection"},{"location":"71/#lsun","text":"class torchvision.datasets.LSUN(db_path, classes='train', transform=None, target_transform=None) LSUN dataset. | Parameters: | db_path ( string ) \u2013 \u6570\u636e\u96c6\u6587\u4ef6\u5b58\u653e\u7684\u4e3b\u76ee\u5f55. classes ( string or list ) \u2013 {\u2018train\u2019, \u2018val\u2019, \u2018test\u2019} \u4e2d\u7684\u4e00\u4e2a, \u6216\u8005\u662f\u4e00\u4e2a\u8981\u8f7d\u5165\u79cd\u7c7b\u7684\u5217\u8868. e,g. [\u2018bedroom_train\u2019, \u2018church_train\u2019]. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: Tuple (image, target) \u76ee\u6807\u662f\u76ee\u6807\u7c7b\u522b\u7684\u7d22\u5f15. --- --- Return type: tuple --- ---","title":"LSUN"},{"location":"71/#imagefolder","text":"class torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=<function default_loader at 0x432aa28>) \u4e00\u4e2a\u901a\u7528\u7684\u6570\u636e\u52a0\u8f7d\u5668, \u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u4ee5\u4ee5\u4e0b\u65b9\u5f0f\u7ec4\u7ec7: root/dog/xxx.png root/dog/xxy.png root/dog/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png | Parameters: | root ( string ) \u2013 \u4e3b\u76ee\u5f55. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. loader \u2013 \u4e00\u4e2a\u4ece\u7ed9\u5b9a\u8def\u5f84\u8f7d\u5165\u56fe\u50cf\u7684\u51fd\u6570. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: (image, target) \u76ee\u6807\u662f\u76ee\u6807\u7c7b\u522b\u7684class_index. --- --- Return type: tuple --- ---","title":"ImageFolder"},{"location":"71/#imagenet-12","text":"\u8fd9\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e2a ImageFolder \u6570\u636e\u96c6\u8f7b\u6613\u5b9e\u73b0. \u8be5\u6570\u636e\u9884\u5904\u7406\u8fc7\u7a0b\u5982 \u8fd9\u91cc\u63cf\u8ff0\u7684 \u6240\u793a \u8fd9\u91cc\u662f\u4e00\u4e2a\u9884\u5904\u7406\u793a\u4f8b .","title":"Imagenet-12"},{"location":"71/#cifar","text":"class torchvision.datasets.CIFAR10(root, train=True, transform=None, target_transform=None, download=False) CIFAR10 Dataset. | Parameters: | root ( string ) \u2013 cifar-10-batches-py \u5b58\u5728\u7684\u4e3b\u76ee\u5f55. train ( bool , optional ) \u2013 \u5982\u679c True, \u6570\u636e\u6765\u81ea\u8bad\u7ec3\u96c6, \u5982\u679c False, \u6570\u636e\u6765\u81ea\u6d4b\u8bd5\u96c6. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. download ( bool , optional ) \u2013 \u5982\u679c true, \u5c31\u4ece\u7f51\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4e14\u653e\u5230 root \u76ee\u5f55\u4e0b. \u5982\u679c\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d, \u90a3\u4e48\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: (image, target) \u76ee\u6807\u662f\u76ee\u6807\u5206\u7c7b\u7684\u7d22\u5f15. --- --- Return type: tuple --- --- class torchvision.datasets.CIFAR100(root, train=True, transform=None, target_transform=None, download=False) CIFAR100 Dataset. <cite>CIFAR10</cite> Dataset \u7684\u4e00\u4e2a\u5b50\u7c7b.","title":"CIFAR"},{"location":"71/#stl10","text":"class torchvision.datasets.STL10(root, split='train', transform=None, target_transform=None, download=False) STL10 Dataset. | Parameters: | root ( string ) \u2013 [ ](#id5)stl10_binary \u6570\u636e\u96c6\u5b58\u653e\u7684\u4e3b\u76ee\u5f55. split ( string ) \u2013 {\u2018train\u2019, \u2018test\u2019, \u2018unlabeled\u2019, \u2018train+unlabeled\u2019} \u4e2d\u7684\u4e00\u4e2a. \u5b83\u662f\u6839\u636e\u6570\u636e\u96c6\u9009\u62e9\u7684. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. download ( bool , optional ) \u2013 \u5982\u679c true, \u5c31\u4ece\u7f51\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4e14\u653e\u5230 root \u76ee\u5f55\u4e0b. \u5982\u679c\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d, \u90a3\u4e48\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: (image, target) \u76ee\u6807\u662f\u76ee\u6807\u7c7b\u7684\u7d22\u5f15. --- --- Return type: tuple --- ---","title":"STL10"},{"location":"71/#svhn","text":"class torchvision.datasets.SVHN(root, split='train', transform=None, target_transform=None, download=False) SVHN Dataset. Note: \u539f\u59cb\u7684 SVHN \u6570\u636e\u96c6\u628a\u6807\u7b7e <cite>10</cite> \u5206\u7ed9\u4e86\u6570\u5b57 <cite>0</cite>. \u7136\u800c\u5728\u8fd9\u4e2a\u6570\u636e\u96c6, \u6211\u4eec\u628a\u6807\u7b7e <cite>0</cite> \u5206\u7ed9\u4e86\u6570\u5b57 <cite>0</cite> \u4ee5\u4fbf \u548c PyTorch \u7684\u635f\u5931\u51fd\u6570\u4e0d\u4ea7\u751f\u51b2\u7a81, \u5b83\u671f\u5f85\u7684\u7c7b\u6807\u7b7e\u7684\u8303\u56f4\u662f <cite>[0, C-1]</cite>. | Parameters: | root ( string ) \u2013 [ ](#id8)SVHN \u6570\u636e\u96c6\u5b58\u653e\u7684\u4e3b\u76ee\u5f55. split ( string ) \u2013 {\u2018train\u2019, \u2018test\u2019, \u2018extra\u2019} \u4e2d\u7684\u4e00\u4e2a. \u5b83\u662f\u6839\u636e\u6570\u636e\u96c6\u9009\u62e9\u7684. \u2018extra\u2019 \u662f\u4e00\u4e2a\u989d\u5916\u7684\u8bad\u7ec3\u96c6. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. E.g, transforms.RandomCrop target_transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u8f93\u5165 target \u5e76\u4e14 \u8f6c\u6362\u5b83. download ( bool , optional ) \u2013 \u5982\u679c true, \u5c31\u4ece\u7f51\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4e14\u653e\u5230 root \u76ee\u5f55\u4e0b. \u5982\u679c\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d, \u90a3\u4e48\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: (image, target) \u76ee\u6807\u662f\u76ee\u6807\u7c7b\u7684\u7d22\u5f15. --- --- Return type: tuple --- ---","title":"SVHN"},{"location":"71/#phototour","text":"class torchvision.datasets.PhotoTour(root, name, train=True, transform=None, download=False) Learning Local Image Descriptors Data Dataset. | Parameters: | root ( string ) \u2013 \u56fe\u50cf\u5b58\u653e\u7684\u4e3b\u76ee\u5f55. name ( string ) \u2013 \u8f7d\u5165\u7684\u6570\u636e\u96c6\u7684\u540d\u5b57. transform ( callable , optional ) \u2013 \u4e00\u4e2a transform \u51fd\u6570, \u5b83\u8f93\u5165 PIL image \u5e76\u4e14\u8fd4\u56de \u8f6c\u6362\u540e\u7684\u7248\u672c. download ( bool , optional ) \u2013 \u5982\u679c true, \u5c31\u4ece\u7f51\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4e14\u653e\u5230 root \u76ee\u5f55\u4e0b. \u5982\u679c\u6570\u636e\u96c6\u5df2\u7ecf\u4e0b\u8f7d, \u90a3\u4e48\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d. | | --- | --- | __getitem__(index) Parameters: index ( int ) \u2013 Index Returns: (data1, data2, matches) --- --- Return type: tuple --- ---","title":"PhotoTour"},{"location":"72/","text":"torchvision.models torchvision.models \u6a21\u5757\u7684\u5b50\u6a21\u5757\u4e2d\u5305\u542b\u4ee5\u4e0b\u6a21\u578b\u7ed3\u6784: AlexNet VGG ResNet SqueezeNet DenseNet Inception v3 \u4f60\u53ef\u4ee5\u4f7f\u7528\u968f\u673a\u521d\u59cb\u5316\u7684\u6743\u91cd\u6765\u521b\u5efa\u8fd9\u4e9b\u6a21\u578b: import torchvision.models as models resnet18 = models.resnet18() alexnet = models.alexnet() vgg16 = models.vgg16() squeezenet = models.squeezenet1_0() densenet = models.densenet161() inception = models.inception_v3() \u6211\u4eec\u63d0\u4f9b\u4f7f\u7528PyTorch torch.utils.model_zoo \u9884\u8bad\u7ec3 (pre-train)\u7684\u6a21\u578b, \u53ef\u4ee5\u901a\u8fc7\u53c2\u6570 pretrained=True \u6765\u6784\u9020\u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b. import torchvision.models as models resnet18 = models.resnet18(pretrained=True) alexnet = models.alexnet(pretrained=True) squeezenet = models.squeezenet1_0(pretrained=True) vgg16 = models.vgg16(pretrained=True) densenet = models.densenet161(pretrained=True) inception = models.inception_v3(pretrained=True) \u6240\u6709\u9884\u8bad\u7ec3 (pre-train) \u6a21\u578b\u8981\u6c42\u8f93\u5165\u56fe\u50cf\u4f7f\u7528\u76f8\u540c\u7684\u6807\u51c6\u5316\u5904\u7406, \u4f8b\u5982: mini-batches \u4e2d RGB \u4e09\u901a\u9053\u56fe\u50cf\u7684 shape (3 x H x W), H \u548c W \u9700\u8981\u81f3\u5c11\u4e3a 224, \u56fe\u50cf\u5fc5\u987b\u88ab\u52a0\u8f7d\u5728 [0, 1] \u7684\u8303\u56f4\u5185 \u7136\u540e\u4f7f\u7528 mean = [0.485, 0.456, 0.406] \u548c std = [0.229, 0.224, 0.225] \u8fdb\u884c\u6807\u51c6\u5316\u5904\u7406. \u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u8f6c\u6362\u8fdb\u884c\u9884\u6807\u51c6\u5316\u9884\u5904\u7406: normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \u4e00\u4e2a\u4f7f\u7528\u8fd9\u79cd\u6807\u51c6\u5316\u5904\u7406\u7684 imagenet \u6837\u4f8b here ImageNet 1-crop error rates (224x224) Network Top-1 error Top-5 error AlexNet 43.45 20.91 VGG-11 30.98 11.37 VGG-13 30.07 10.75 VGG-16 28.41 9.62 VGG-19 27.62 9.12 VGG-11 with batch normalization 29.62 10.19 VGG-13 with batch normalization 28.45 9.63 VGG-16 with batch normalization 26.63 8.50 VGG-19 with batch normalization 25.76 8.15 ResNet-18 30.24 10.92 ResNet-34 26.70 8.58 ResNet-50 23.85 7.13 ResNet-101 22.63 6.44 ResNet-152 21.69 5.94 SqueezeNet 1.0 41.90 19.58 SqueezeNet 1.1 41.81 19.38 Densenet-121 25.35 7.83 Densenet-169 24.00 7.00 Densenet-201 22.80 6.43 Densenet-161 22.35 6.20 Inception v3 22.55 6.44 Alexnet torchvision.models.alexnet(pretrained=False, **kwargs) AlexNet \u6a21\u578b\u7ed3\u6784\u8bba\u6587\u5730\u5740 \u201cOne weird trick\u2026\u201d . Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. VGG torchvision.models.vgg11(pretrained=False, **kwargs) VGG 11\u5c42\u6a21\u578b (configuration \u201cA\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg11_bn(pretrained=False, **kwargs) \u5e26\u6709\u6279\u6807\u51c6\u5316\uff08batch normalization) \u7684VGG 11\u5c42\u6a21\u578b (configuration \u201cA\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg13(pretrained=False, **kwargs) VGG 13\u5c42\u6a21\u578b (configuration \u201cB\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg13_bn(pretrained=False, **kwargs) \u5e26\u6709\u6279\u6807\u51c6\u5316\uff08batch normalization) \u7684 VGG 13\u5c42\u6a21\u578b (configuration \u201cB\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg16(pretrained=False, **kwargs) VGG 16\u5c42\u6a21\u578b (configuration \u201cD\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg16_bn(pretrained=False, **kwargs) \u5e26\u6709\u6279\u6807\u51c6\u5316\uff08batch normalization) \u7684 VGG 16\u5c42\u6a21\u578b (configuration \u201cD\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg19(pretrained=False, **kwargs) VGG 19\u5c42\u6a21\u578b (configuration \u201cE\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg19_bn(pretrained=False, **kwargs) \u5e26\u6709\u6279\u6807\u51c6\u5316\uff08batch normalization) \u7684 VGG 19\u5c42\u6a21\u578b (configuration \u2018E\u2019) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. ResNet torchvision.models.resnet18(pretrained=False, **kwargs) \u6784\u9020\u4e00\u4e2a ResNet-18 \u6a21\u578b. Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.resnet34(pretrained=False, **kwargs) \u6784\u9020\u4e00\u4e2a ResNet-34 \u6a21\u578b. Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.resnet50(pretrained=False, **kwargs) \u6784\u9020\u4e00\u4e2a ResNet-50 \u6a21\u578b. Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.resnet101(pretrained=False, **kwargs) \u6784\u9020\u4e00\u4e2a ResNet-101 \u6a21\u578b. Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.resnet152(pretrained=False, **kwargs) \u6784\u9020\u4e00\u4e2a ResNet-152 \u6a21\u578b. Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. SqueezeNet torchvision.models.squeezenet1_0(pretrained=False, **kwargs) SqueezeNet \u6a21\u578b\u7ed3\u6784\u6e90\u4e8e\u8bba\u6587: \u201cSqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\u201d Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.squeezenet1_1(pretrained=False, **kwargs) SqueezeNet 1.1 \u6a21\u578b\u6e90\u4e8e\u8bba\u6587: official SqueezeNet repo . SqueezeNet 1.1 \u6bd4 SqueezeNet 1.0 \u51cf\u5c11\u4e86 2.4\u500d\u7684\u8fd0\u7b97\u91cf, \u5e76\u5728\u4e0d\u635f\u4f24\u51c6\u786e\u7387\u7684\u57fa\u7840\u4e0a\u51cf\u5c11\u4e86\u5c11\u8bb8\u53c2\u6570. Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. DenseNet torchvision.models.densenet121(pretrained=False, **kwargs) Densenet-121 \u6a21\u578b\u6e90\u81ea\u4e8e: \u201cDensely Connected Convolutional Networks\u201d Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.densenet169(pretrained=False, **kwargs) Densenet-169 \u6a21\u578b\u6e90\u81ea\u4e8e: \u201cDensely Connected Convolutional Networks\u201d Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.densenet161(pretrained=False, **kwargs) Densenet-161 \u6a21\u578b\u6e90\u81ea\u4e8e: \u201cDensely Connected Convolutional Networks\u201d Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.densenet201(pretrained=False, **kwargs) Densenet-201 \u6a21\u578b\u6e90\u81ea\u4e8e: \u201cDensely Connected Convolutional Networks\u201d Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. Inception v3 torchvision.models.inception_v3(pretrained=False, **kwargs) Inception v3 \u6a21\u578b\u7ed3\u6784\u6e90\u81ea\u4e8e \u201cRethinking the Inception Architecture for Computer Vision\u201d . Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b.","title":"torchvision.models"},{"location":"72/#torchvisionmodels","text":"torchvision.models \u6a21\u5757\u7684\u5b50\u6a21\u5757\u4e2d\u5305\u542b\u4ee5\u4e0b\u6a21\u578b\u7ed3\u6784: AlexNet VGG ResNet SqueezeNet DenseNet Inception v3 \u4f60\u53ef\u4ee5\u4f7f\u7528\u968f\u673a\u521d\u59cb\u5316\u7684\u6743\u91cd\u6765\u521b\u5efa\u8fd9\u4e9b\u6a21\u578b: import torchvision.models as models resnet18 = models.resnet18() alexnet = models.alexnet() vgg16 = models.vgg16() squeezenet = models.squeezenet1_0() densenet = models.densenet161() inception = models.inception_v3() \u6211\u4eec\u63d0\u4f9b\u4f7f\u7528PyTorch torch.utils.model_zoo \u9884\u8bad\u7ec3 (pre-train)\u7684\u6a21\u578b, \u53ef\u4ee5\u901a\u8fc7\u53c2\u6570 pretrained=True \u6765\u6784\u9020\u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b. import torchvision.models as models resnet18 = models.resnet18(pretrained=True) alexnet = models.alexnet(pretrained=True) squeezenet = models.squeezenet1_0(pretrained=True) vgg16 = models.vgg16(pretrained=True) densenet = models.densenet161(pretrained=True) inception = models.inception_v3(pretrained=True) \u6240\u6709\u9884\u8bad\u7ec3 (pre-train) \u6a21\u578b\u8981\u6c42\u8f93\u5165\u56fe\u50cf\u4f7f\u7528\u76f8\u540c\u7684\u6807\u51c6\u5316\u5904\u7406, \u4f8b\u5982: mini-batches \u4e2d RGB \u4e09\u901a\u9053\u56fe\u50cf\u7684 shape (3 x H x W), H \u548c W \u9700\u8981\u81f3\u5c11\u4e3a 224, \u56fe\u50cf\u5fc5\u987b\u88ab\u52a0\u8f7d\u5728 [0, 1] \u7684\u8303\u56f4\u5185 \u7136\u540e\u4f7f\u7528 mean = [0.485, 0.456, 0.406] \u548c std = [0.229, 0.224, 0.225] \u8fdb\u884c\u6807\u51c6\u5316\u5904\u7406. \u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u8f6c\u6362\u8fdb\u884c\u9884\u6807\u51c6\u5316\u9884\u5904\u7406: normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \u4e00\u4e2a\u4f7f\u7528\u8fd9\u79cd\u6807\u51c6\u5316\u5904\u7406\u7684 imagenet \u6837\u4f8b here ImageNet 1-crop error rates (224x224) Network Top-1 error Top-5 error AlexNet 43.45 20.91 VGG-11 30.98 11.37 VGG-13 30.07 10.75 VGG-16 28.41 9.62 VGG-19 27.62 9.12 VGG-11 with batch normalization 29.62 10.19 VGG-13 with batch normalization 28.45 9.63 VGG-16 with batch normalization 26.63 8.50 VGG-19 with batch normalization 25.76 8.15 ResNet-18 30.24 10.92 ResNet-34 26.70 8.58 ResNet-50 23.85 7.13 ResNet-101 22.63 6.44 ResNet-152 21.69 5.94 SqueezeNet 1.0 41.90 19.58 SqueezeNet 1.1 41.81 19.38 Densenet-121 25.35 7.83 Densenet-169 24.00 7.00 Densenet-201 22.80 6.43 Densenet-161 22.35 6.20 Inception v3 22.55 6.44","title":"torchvision.models"},{"location":"72/#alexnet","text":"torchvision.models.alexnet(pretrained=False, **kwargs) AlexNet \u6a21\u578b\u7ed3\u6784\u8bba\u6587\u5730\u5740 \u201cOne weird trick\u2026\u201d . Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b.","title":"Alexnet"},{"location":"72/#vgg","text":"torchvision.models.vgg11(pretrained=False, **kwargs) VGG 11\u5c42\u6a21\u578b (configuration \u201cA\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg11_bn(pretrained=False, **kwargs) \u5e26\u6709\u6279\u6807\u51c6\u5316\uff08batch normalization) \u7684VGG 11\u5c42\u6a21\u578b (configuration \u201cA\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg13(pretrained=False, **kwargs) VGG 13\u5c42\u6a21\u578b (configuration \u201cB\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg13_bn(pretrained=False, **kwargs) \u5e26\u6709\u6279\u6807\u51c6\u5316\uff08batch normalization) \u7684 VGG 13\u5c42\u6a21\u578b (configuration \u201cB\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg16(pretrained=False, **kwargs) VGG 16\u5c42\u6a21\u578b (configuration \u201cD\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg16_bn(pretrained=False, **kwargs) \u5e26\u6709\u6279\u6807\u51c6\u5316\uff08batch normalization) \u7684 VGG 16\u5c42\u6a21\u578b (configuration \u201cD\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg19(pretrained=False, **kwargs) VGG 19\u5c42\u6a21\u578b (configuration \u201cE\u201d) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.vgg19_bn(pretrained=False, **kwargs) \u5e26\u6709\u6279\u6807\u51c6\u5316\uff08batch normalization) \u7684 VGG 19\u5c42\u6a21\u578b (configuration \u2018E\u2019) Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b.","title":"VGG"},{"location":"72/#resnet","text":"torchvision.models.resnet18(pretrained=False, **kwargs) \u6784\u9020\u4e00\u4e2a ResNet-18 \u6a21\u578b. Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.resnet34(pretrained=False, **kwargs) \u6784\u9020\u4e00\u4e2a ResNet-34 \u6a21\u578b. Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.resnet50(pretrained=False, **kwargs) \u6784\u9020\u4e00\u4e2a ResNet-50 \u6a21\u578b. Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.resnet101(pretrained=False, **kwargs) \u6784\u9020\u4e00\u4e2a ResNet-101 \u6a21\u578b. Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.resnet152(pretrained=False, **kwargs) \u6784\u9020\u4e00\u4e2a ResNet-152 \u6a21\u578b. Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b.","title":"ResNet"},{"location":"72/#squeezenet","text":"torchvision.models.squeezenet1_0(pretrained=False, **kwargs) SqueezeNet \u6a21\u578b\u7ed3\u6784\u6e90\u4e8e\u8bba\u6587: \u201cSqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\u201d Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.squeezenet1_1(pretrained=False, **kwargs) SqueezeNet 1.1 \u6a21\u578b\u6e90\u4e8e\u8bba\u6587: official SqueezeNet repo . SqueezeNet 1.1 \u6bd4 SqueezeNet 1.0 \u51cf\u5c11\u4e86 2.4\u500d\u7684\u8fd0\u7b97\u91cf, \u5e76\u5728\u4e0d\u635f\u4f24\u51c6\u786e\u7387\u7684\u57fa\u7840\u4e0a\u51cf\u5c11\u4e86\u5c11\u8bb8\u53c2\u6570. Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b.","title":"SqueezeNet"},{"location":"72/#densenet","text":"torchvision.models.densenet121(pretrained=False, **kwargs) Densenet-121 \u6a21\u578b\u6e90\u81ea\u4e8e: \u201cDensely Connected Convolutional Networks\u201d Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.densenet169(pretrained=False, **kwargs) Densenet-169 \u6a21\u578b\u6e90\u81ea\u4e8e: \u201cDensely Connected Convolutional Networks\u201d Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.densenet161(pretrained=False, **kwargs) Densenet-161 \u6a21\u578b\u6e90\u81ea\u4e8e: \u201cDensely Connected Convolutional Networks\u201d Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b. torchvision.models.densenet201(pretrained=False, **kwargs) Densenet-201 \u6a21\u578b\u6e90\u81ea\u4e8e: \u201cDensely Connected Convolutional Networks\u201d Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b.","title":"DenseNet"},{"location":"72/#inception-v3","text":"torchvision.models.inception_v3(pretrained=False, **kwargs) Inception v3 \u6a21\u578b\u7ed3\u6784\u6e90\u81ea\u4e8e \u201cRethinking the Inception Architecture for Computer Vision\u201d . Parameters: pretrained ( bool ) \u2013 True, \u8fd4\u56de\u4e00\u4e2a\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b.","title":"Inception v3"},{"location":"73/","text":"torchvision.transforms Transforms (\u53d8\u6362) \u662f\u5e38\u89c1\u7684 image transforms (\u56fe\u50cf\u53d8\u6362) .\u4ed6\u4eec\u53ef\u4ee5\u4f7f\u7528 Compose \u7c7b\u4ee5\u94fe\u5728\u4e00\u8d77\u6765\u8fdb\u884c\u64cd\u4f5c. class torchvision.transforms.Compose(transforms) \u5c06\u591a\u4e2a\u53d8\u6362\u7ec4\u5408\u5230\u4e00\u8d77. Parameters: transforms (list of Transform objects) \u2013 \u8981\u7ec4\u5408\u7684\u53d8\u6362\u5217\u8868. Example >>> transforms.Compose([ >>> transforms.CenterCrop(10), >>> transforms.ToTensor(), >>> ]) PIL Image \u4e0a\u7684\u53d8\u6362 class torchvision.transforms.Resize(size, interpolation=2) \u8c03\u6574\u8f93\u5165\u7684 PIL Image \u5c3a\u5bf8\u4e3a\u7ed9\u5b9a\u7684 size\uff08\u5c3a\u5bf8\uff09. | Parameters: | size ( sequence or int ) \u2013 \u671f\u671b\u8f93\u51fa\u7684\u5c3a\u5bf8. \u5982\u679c size\uff08\u5c3a\u5bf8\uff09\u662f\u4e00\u4e2a\u50cf (h, w) \u8fd9\u6837\u7684\u5e8f\u5217, \u5219 output size\uff08\u8f93\u51fa\u5c3a\u5bf8\uff09\u5c06\u4e8e\u6b64\u5339\u914d. \u5982\u679c size\uff08\u5c3a\u5bf8\uff09\u662f\u4e00\u4e2a int \u7c7b\u578b\u7684\u6570\u5b57, \u56fe\u50cf\u8f83\u5c0f\u7684\u8fb9\u7f18\u5c06\u88ab\u5339\u914d\u5230\u8be5\u6570\u5b57. \u4f8b\u5982, \u5982\u679c height > width, \u90a3\u4e48\u56fe\u50cf\u5c06\u4f1a\u88ab\u91cd\u65b0\u7f29\u653e\u5230 (size * height / width, size). \u5373\u6309\u7167size/width\u7684\u6bd4\u503c\u7f29\u653e interpolation ( int , optional ) \u2013 \u671f\u671b\u7684\u63d2\u503c. \u9ed8\u8ba4\u662f PIL.Image.BILINEAR | | --- | --- | class torchvision.transforms.Scale(*args, **kwargs) Note: \u4e3a\u4e86\u652f\u6301 Resize, \u8be5\u53d8\u6362\u5df2\u7ecf\u8fc7\u65f6\u4e86. class torchvision.transforms.CenterCrop(size) \u5728\u4e2d\u5fc3\u88c1\u526a\u6307\u5b9a\u7684 PIL Image. Parameters: size ( sequence or int ) \u2013 \u671f\u671b\u88c1\u526a\u7684\u8f93\u51fa\u5c3a\u5bf8. \u5982\u679c size\uff08\u5c3a\u5bf8\uff09\u662f int \u7c7b\u578b\u7684\u6574\u6570, \u800c\u4e0d\u662f\u50cf (h, w) \u8fd9\u6837\u7c7b\u578b\u7684\u5e8f\u5217, \u88c1\u526a\u51fa\u6765\u7684\u56fe\u50cf\u662f (size, size) \u8fd9\u6837\u7684\u6b63\u65b9\u5f62\u7684. class torchvision.transforms.RandomCrop(size, padding=0) \u5728\u4e00\u4e2a\u968f\u673a\u4f4d\u7f6e\u88c1\u526a\u6307\u5b9a\u7684 PIL Image. | Parameters: | size ( sequence or int ) \u2013 \u671f\u671b\u8f93\u51fa\u7684\u88c1\u526a\u5c3a\u5bf8. \u5982\u679c size\uff08\u5c3a\u5bf8\uff09\u662f int \u7c7b\u578b\u7684\u6574\u6570, \u800c\u4e0d\u662f\u50cf (h, w) \u8fd9\u6837\u7c7b\u578b\u7684\u5e8f\u5217, \u88c1\u526a\u51fa\u6765\u7684\u56fe\u50cf\u662f (size, size) \u8fd9\u6837\u7684\u6b63\u65b9\u5f62\u7684. padding ( int or sequence__, optional ) \u2013 \u56fe\u50cf\u7684\u6bcf\u4e2a\u8fb9\u6846\u4e0a\u7684\u53ef\u9009\u586b\u5145. \u7f3a\u7701\u503c\u662f 0, \u5373\u6ca1\u6709\u586b\u5145. \u5982\u679c\u63d0\u4f9b\u957f\u5ea6\u4e3a 4 \u7684\u5e8f\u5217, \u5219\u5206\u522b\u7528\u4e8e\u586b\u5145\u5de6\u4fa7, \u9876\u90e8, \u53f3\u4fa7, \u5e95\u90e8\u8fb9\u754c. | | --- | --- | class torchvision.transforms.RandomHorizontalFlip \u4ee5\u6982\u73870.5\u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\u56fe\u50cf class torchvision.transforms.RandomVerticalFlip \u4ee5\u6982\u73870.5\u968f\u673a\u5782\u76f4\u7ffb\u8f6c\u56fe\u50cf. class torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2) \u5c06\u7ed9\u5b9a\u7684 PIL \u56fe\u50cf\u88c1\u526a\u4e3a\u968f\u673a\u5927\u5c0f\u548c\u7eb5\u6a2a\u6bd4\u4f8b. \u539f\u59cb\u9ad8\u5bbd\u6bd4\u7684\u968f\u673a\u5927\u5c0f\uff08\u9ed8\u8ba4: 0.08 \u5230 1.0\uff09\u548c\u968f\u673a\u5bbd\u9ad8\u6bd4\uff08\u9ed8\u8ba4: 3/4 \u5230 4/3\uff09\u7684\u88c1\u526a. \u8be5\u88c1\u526a\u6700\u7ec8\u4f1a\u88ab\u8c03\u6574\u4e3a\u6307\u5b9a\u7684\u5c3a\u5bf8. \u8be5\u64cd\u4f5c\u666e\u904d\u7528\u4e8e\u8bad\u7ec3 Inception networks. | Parameters: | size \u2013 \u6bcf\u6761\u8fb9\u7684\u671f\u671b\u7684\u8f93\u51fa\u5c3a\u5bf8 scale \u2013 \u539f\u59cb\u526a\u88c1\u5c3a\u5bf8\u5927\u5c0f\u7684\u8303\u56f4 ratio \u2013 \u539f\u59cb\u88c1\u526a\u7eb5\u6a2a\u6bd4\u7684\u8303\u56f4 interpolation \u2013 Default: PIL.Image.BILINEAR | | --- | --- | class torchvision.transforms.RandomSizedCrop(*args, **kwargs) Note: \u4e3a\u4e86\u652f\u6301 RandomResizedCrop, \u8be5\u53d8\u6362\u5df2\u7ecf\u88ab\u5f03\u7528. class torchvision.transforms.Grayscale(num_output_channels=1) \u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\u50cf. Parameters: num_output_channels ( int ) \u2013 (1 or 3) \u8f93\u51fa\u56fe\u50cf\u6240\u671f\u671b\u7684\u901a\u9053\u6570\u91cf Returns: \u7070\u5ea6\u7248\u672c\u7684\u8f93\u5165. - \u5982\u679c num_output_channels == 1 : \u8fd4\u56de\u7684\u56fe\u50cf\u662f 1 \u901a\u9053 - \u5982\u679c num_output_channels == 3 : \u8fd4\u56de\u7684\u56fe\u50cf\u662f 3 \u901a\u9053, \u5e76\u4e14 r == g == b --- --- Return type: PIL Image --- --- class torchvision.transforms.RandomGrayscale(p=0.1) \u968f\u673a\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\u50cf, \u6982\u7387\u4e3a p (default 0.1). Parameters: p ( float ) \u2013 \u56fe\u50cf\u5e94\u8be5\u88ab\u8f6c\u6362\u6210\u7070\u5ea6\u7684\u6982\u7387. Returns: \u7070\u5ea6\u7248\u672c\u7684\u8f93\u5165\u56fe\u50cf\u7684\u6982\u7387\u4e3a p, \u4e0d\u53d8\u7684\u6982\u7387\u4e3a\uff081-p\uff09 - \u5982\u679c\u8f93\u5165\u56fe\u50cf\u4e3a1\u4e2a\u901a\u9053: \u5219\u7070\u5ea6\u7248\u672c\u662f 1 \u901a\u9053 - \u5982\u679c\u8f93\u5165\u56fe\u50cf\u4e3a3\u4e2a\u901a\u9053: \u5219\u7070\u5ea6\u7248\u672c\u662f 3 \u901a\u9053, \u5e76\u4e14 r == g == b --- --- Return type: PIL Image --- --- class torchvision.transforms.FiveCrop(size) \u5c06\u7ed9\u5b9a\u7684 PIL Image \u88c1\u526a\u6210\u56db\u4e2a\u89d2\u843d\u548c\u4e2d\u5fc3\u88c1\u526a Note \u8be5\u53d8\u6362\u8fd4\u56de\u4e00\u4e2a\u56fe\u50cf\u5143\u7ec4, \u5e76\u4e14\u6570\u636e\u96c6\u8fd4\u56de\u7684\u8f93\u5165\u548c\u76ee\u6807\u7684\u6570\u91cf\u53ef\u80fd\u4e0d\u5339\u914d. \u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u4f8b\u5b50\u6765\u5904\u7406\u8fd9\u4e2a\u95ee\u9898. Parameters: size ( sequence or int ) \u2013 \u671f\u671b\u8f93\u51fa\u7684\u88c1\u526a\u5c3a\u5bf8. \u5982\u679c size\uff08\u5c3a\u5bf8\uff09\u662f <cite>int`</cite> \u7c7b\u578b\u7684\u6574\u6570, \u800c\u4e0d\u662f\u50cf (h, w) \u8fd9\u6837\u7c7b\u578b\u7684\u5e8f\u5217, \u88c1\u526a\u51fa\u6765\u7684\u56fe\u50cf\u662f (size, size) \u8fd9\u6837\u7684\u6b63\u65b9\u5f62\u7684.. Example >>> transform = Compose([ >>> FiveCrop(size), # \u4e00\u4e2a PIL Images \u7684\u5217\u8868 >>> Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # \u8fd4\u56de\u4e00\u4e2a4D Tensor >>> ]) >>> #\u5728\u4f60\u7684\u6d4b\u8bd5\u5faa\u73af\u53ef\u4ee5\u5982\u4e0b\u64cd\u4f5c: >>> input, target = batch # \u8f93\u5165\u662f5DTensor,\u8f93\u51fa\u662f2D >>> bs, ncrops, c, h, w = input.size() >>> result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops >>> result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops class torchvision.transforms.TenCrop(size, vertical_flip=False) \u5c06\u7ed9\u5b9a\u7684 PIL Image \u88c1\u526a\u6210\u56db\u4e2a\u89d2, \u4e2d\u5fc3\u88c1\u526a, \u5e76\u52a0\u4e0a\u8fd9\u4e9b\u7684\u7ffb\u8f6c\u7248\u672c\uff08\u9ed8\u8ba4\u4f7f\u7528\u6c34\u5e73\u7ffb\u8f6c\uff09 Note \u8be5\u53d8\u6362\u8fd4\u56de\u4e00\u4e2a\u56fe\u50cf\u5143\u7ec4, \u5e76\u4e14\u6570\u636e\u96c6\u8fd4\u56de\u7684\u8f93\u5165\u548c\u76ee\u6807\u7684\u6570\u91cf\u53ef\u80fd\u4e0d\u5339\u914d. \u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u4f8b\u5b50\u6765\u5904\u7406\u8fd9\u4e2a\u95ee\u9898. | Parameters: | size ( sequence or int ) \u2013 \u671f\u671b\u8f93\u51fa\u7684\u88c1\u526a\u5c3a\u5bf8. \u5982\u679c size\uff08\u5c3a\u5bf8\uff09\u662f <cite>int</cite> \u7c7b\u578b\u7684\u6574\u6570, \u800c\u4e0d\u662f\u50cf (h, w) \u8fd9\u6837\u7c7b\u578b\u7684\u5e8f\u5217, \u88c1\u526a\u51fa\u6765\u7684\u56fe\u50cf\u662f (size, size) \u8fd9\u6837\u7684\u6b63\u65b9\u5f62\u7684. vertical_flip ( bool ) \u2013 \u4f7f\u7528\u5782\u76f4\u7ffb\u8f6c\u800c\u4e0d\u662f\u6c34\u5e73\u7684\u65b9\u5f0f | | --- | --- | Example >>> transform = Compose([ >>> TenCrop(size), # this is a list of PIL Images >>> Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor >>> ]) >>> #In your test loop you can do the following: >>> input, target = batch # input is a 5d tensor, target is 2d >>> bs, ncrops, c, h, w = input.size() >>> result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops >>> result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops class torchvision.transforms.Pad(padding, fill=0) \u7528\u6307\u5b9a\u7684 \u201cpad\u201d \u503c\u586b\u5145\u6307\u5b9a\u7684 PIL image. | Parameters: | padding ( int or tuple ) \u2013 \u586b\u5145\u6bcf\u4e2a\u8fb9\u6846. \u5982\u679c\u63d0\u4f9b\u4e86\u4e00\u4e2a int \u578b\u7684\u6574\u6570, \u5219\u7528\u4e8e\u586b\u5145\u6240\u6709\u8fb9\u754c. \u5982\u679c\u63d0\u4f9b\u957f\u5ea6\u4e3a 2 \u7684\u5143\u7ec4, \u5219\u8fd9\u662f\u5206\u522b\u5728 \u5de6/\u53f3 \u548c \u4e0a/\u4e0b \u7684\u586b\u5145. \u5982\u679c\u63d0\u4f9b\u957f\u5ea6\u4e3a 4 \u7684\u5143\u7ec4, \u5219\u8fd9\u662f\u5206\u522b\u7528\u4e8e \u5de6, \u4e0a, \u53f3 \u548c \u4e0b \u90e8\u8fb9\u754c\u7684\u586b\u5145. fill \u2013 \u50cf\u7d20\u586b\u5145. \u9ed8\u8ba4\u503c\u4e3a 0. \u5982\u679c\u957f\u5ea6\u4e3a 3 \u7684\u5143\u7ec4, \u5206\u522b\u7528\u4e8e\u586b\u5145 R, G, B \u901a\u9053. | | --- | --- | class torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0) \u968f\u673a\u66f4\u6539\u56fe\u50cf\u7684\u4eae\u5ea6, \u5bf9\u6bd4\u5ea6\u548c\u9971\u548c\u5ea6. | Parameters: | brightness ( float ) \u2013 \u4eae\u5ea6\u6539\u53d8\u7684\u8303\u56f4. brightness_factor \u4ece [max(0, 1 - brightness), 1 + brightness]\u7684\u8303\u56f4\u4e2d\u4e00\u81f4\u9009\u62e9. contrast ( float ) \u2013 \u5bf9\u6bd4\u5ea6\u6539\u53d8\u7684\u8303\u56f4. contrast_factor \u4ece [max(0, 1 - contrast), 1 + contrast]\u7684\u8303\u56f4\u4e2d\u4e00\u81f4\u9009\u62e9. saturation ( float ) \u2013 \u9971\u548c\u5ea6\u6539\u53d8\u7684\u8303\u56f4. saturation_factor \u4ece[max(0, 1 - saturation), 1 + saturation]\u7684\u8303\u56f4\u4e2d\u4e00\u81f4\u9009\u62e9. hue ( float ) \u2013 \u8272\u8c03\u6539\u53d8\u7684\u8303\u56f4. hue_factor \u4ece [-hue, hue]\u7684\u8303\u56f4\u4e2d\u4e00\u81f4\u9009\u62e9. \u5e94\u8be5 >=0 \u4e14 <= 0.5. | | --- | --- | torch.*Tensor \u4e0a\u7684\u53d8\u6362 class torchvision.transforms.Normalize(mean, std) \u7528\u5747\u503c\u548c\u6807\u51c6\u504f\u5dee\u5bf9\u5f20\u91cf\u56fe\u50cf\u8fdb\u884c\u5f52\u4e00\u5316. \u7ed9\u5b9a\u5747\u503c: (M1,...,Mn) \u548c\u6807\u51c6\u5dee: (S1,..,Sn) \u7528\u4e8e n \u4e2a\u901a\u9053, \u8be5\u53d8\u6362\u5c06\u6807\u51c6\u5316\u8f93\u5165 torch.*Tensor \u7684\u6bcf\u4e00\u4e2a\u901a\u9053. \u4f8b\u5982: input[channel] = (input[channel] - mean[channel]) / std[channel] | Parameters: | mean ( sequence ) \u2013 \u6bcf\u4e00\u4e2a\u901a\u9053\u7684\u5747\u503c\u5e8f\u5217. std ( sequence ) \u2013 \u6bcf\u4e00\u4e2a\u901a\u9053\u7684\u6807\u51c6\u5dee\u5e8f\u5217. | | --- | --- | __call__(tensor) Parameters: tensor ( Tensor ) \u2013 \u9700\u8981\u88ab\u5f52\u4e00\u5316\u7684\u5927\u5c0f\u4e3a (C, H, W)Tensor image. Returns: \u5f52\u4e00\u5316\u540e\u7684 Tensor image. --- --- Return type: Tensor --- --- \u8f6c\u6362\u7c7b\u578b\u7684\u53d8\u6362 class torchvision.transforms.ToTensor \u8f6c\u6362\u4e00\u4e2a PIL Image \u6216 numpy.ndarray \u4e3a tensor\uff08\u5f20\u91cf\uff09. \u5c06\u8303\u56f4 [0, 255] \u4e2d\u7684 PIL Image \u6216 numpy.ndarray (H x W x C) \u8f6c\u6362\u5f62\u72b6\u4e3a (C x H x W) , \u503c\u8303\u56f4\u4e3a [0.0, 1.0] \u7684 torch.FloatTensor. __call__(pic) Parameters: pic ( PIL Image or numpy.ndarray ) \u2013 \u5c06\u8981\u88ab\u8f6c\u6362\u4e3a tensor \u7684 Image. Returns: \u8f6c\u6362\u540e\u7684 image. --- --- Return type: Tensor --- --- class torchvision.transforms.ToPILImage(mode=None) \u8f6c\u6362\u4e00\u4e2a tensor \u6216 ndarray \u4e3a PIL Image. \u8f6c\u6362\u4e00\u4e2a\u5f62\u72b6\u4e3a(C x H x W) \u7684 torch.*Tensor \u6216\u4e00\u4e2a\u5f62\u72b6\u4e3a(H x W x C )\u7684numpy ndarray \u81f3\u4e00\u4e2a PIL Image ,\u540c\u65f6\u4fdd\u7559\u503c\u8303\u56f4. Parameters: mode ( PIL.Image mode ) \u2013 \u8f93\u5165\u6570\u636e\u7684\u8272\u57df\u548c\u50cf\u7d20\u6df1\u5ea6 (\u53ef\u9009). \u5982\u679c mode \u4e3a None (\u9ed8\u8ba4) ,\u8fd9\u91cc\u5bf9\u8f93\u5165\u6570\u636e\u6709\u4e00\u4e9b\u5047\u8bbe: 1. \u5982\u679c\u8f93\u5165\u67093\u4e2a\u901a\u9053, mode \u5047\u8bbe\u4e3a RGB . 2. \u5982\u679c\u8f93\u5165\u67094\u4e2a\u901a\u9053, mode \u5047\u8bbe\u4e3a RGBA . 3. \u5982\u679c\u8f93\u5165\u67091\u4e2a\u901a\u9053, mode \u6839\u636e\u6570\u636e\u7c7b\u578b\u786e\u5b9a (i,e, int , float , short ). __call__(pic) Parameters: pic ( Tensor or numpy.ndarray ) \u2013 \u8981\u8f6c\u6362\u4e3aPIL Image\u7684\u56fe\u50cf. Returns: \u8f6c\u6362\u4e3aPIL Image\u7684\u56fe\u50cf. --- --- Return type: PIL Image --- --- \u901a\u7528\u7684\u53d8\u6362 class torchvision.transforms.Lambda(lambd) \u5e94\u7528\u4e00\u4e2a\u7528\u6237\u5b9a\u4e49\u7684 Lambda \u4f5c\u4e3a\u53d8\u6362. Parameters: lambd ( function ) \u2013 Lambda/function \u4ee5\u7528\u4e8e transform.","title":"torchvision.transforms"},{"location":"73/#torchvisiontransforms","text":"Transforms (\u53d8\u6362) \u662f\u5e38\u89c1\u7684 image transforms (\u56fe\u50cf\u53d8\u6362) .\u4ed6\u4eec\u53ef\u4ee5\u4f7f\u7528 Compose \u7c7b\u4ee5\u94fe\u5728\u4e00\u8d77\u6765\u8fdb\u884c\u64cd\u4f5c. class torchvision.transforms.Compose(transforms) \u5c06\u591a\u4e2a\u53d8\u6362\u7ec4\u5408\u5230\u4e00\u8d77. Parameters: transforms (list of Transform objects) \u2013 \u8981\u7ec4\u5408\u7684\u53d8\u6362\u5217\u8868. Example >>> transforms.Compose([ >>> transforms.CenterCrop(10), >>> transforms.ToTensor(), >>> ])","title":"torchvision.transforms"},{"location":"73/#pil-image","text":"class torchvision.transforms.Resize(size, interpolation=2) \u8c03\u6574\u8f93\u5165\u7684 PIL Image \u5c3a\u5bf8\u4e3a\u7ed9\u5b9a\u7684 size\uff08\u5c3a\u5bf8\uff09. | Parameters: | size ( sequence or int ) \u2013 \u671f\u671b\u8f93\u51fa\u7684\u5c3a\u5bf8. \u5982\u679c size\uff08\u5c3a\u5bf8\uff09\u662f\u4e00\u4e2a\u50cf (h, w) \u8fd9\u6837\u7684\u5e8f\u5217, \u5219 output size\uff08\u8f93\u51fa\u5c3a\u5bf8\uff09\u5c06\u4e8e\u6b64\u5339\u914d. \u5982\u679c size\uff08\u5c3a\u5bf8\uff09\u662f\u4e00\u4e2a int \u7c7b\u578b\u7684\u6570\u5b57, \u56fe\u50cf\u8f83\u5c0f\u7684\u8fb9\u7f18\u5c06\u88ab\u5339\u914d\u5230\u8be5\u6570\u5b57. \u4f8b\u5982, \u5982\u679c height > width, \u90a3\u4e48\u56fe\u50cf\u5c06\u4f1a\u88ab\u91cd\u65b0\u7f29\u653e\u5230 (size * height / width, size). \u5373\u6309\u7167size/width\u7684\u6bd4\u503c\u7f29\u653e interpolation ( int , optional ) \u2013 \u671f\u671b\u7684\u63d2\u503c. \u9ed8\u8ba4\u662f PIL.Image.BILINEAR | | --- | --- | class torchvision.transforms.Scale(*args, **kwargs) Note: \u4e3a\u4e86\u652f\u6301 Resize, \u8be5\u53d8\u6362\u5df2\u7ecf\u8fc7\u65f6\u4e86. class torchvision.transforms.CenterCrop(size) \u5728\u4e2d\u5fc3\u88c1\u526a\u6307\u5b9a\u7684 PIL Image. Parameters: size ( sequence or int ) \u2013 \u671f\u671b\u88c1\u526a\u7684\u8f93\u51fa\u5c3a\u5bf8. \u5982\u679c size\uff08\u5c3a\u5bf8\uff09\u662f int \u7c7b\u578b\u7684\u6574\u6570, \u800c\u4e0d\u662f\u50cf (h, w) \u8fd9\u6837\u7c7b\u578b\u7684\u5e8f\u5217, \u88c1\u526a\u51fa\u6765\u7684\u56fe\u50cf\u662f (size, size) \u8fd9\u6837\u7684\u6b63\u65b9\u5f62\u7684. class torchvision.transforms.RandomCrop(size, padding=0) \u5728\u4e00\u4e2a\u968f\u673a\u4f4d\u7f6e\u88c1\u526a\u6307\u5b9a\u7684 PIL Image. | Parameters: | size ( sequence or int ) \u2013 \u671f\u671b\u8f93\u51fa\u7684\u88c1\u526a\u5c3a\u5bf8. \u5982\u679c size\uff08\u5c3a\u5bf8\uff09\u662f int \u7c7b\u578b\u7684\u6574\u6570, \u800c\u4e0d\u662f\u50cf (h, w) \u8fd9\u6837\u7c7b\u578b\u7684\u5e8f\u5217, \u88c1\u526a\u51fa\u6765\u7684\u56fe\u50cf\u662f (size, size) \u8fd9\u6837\u7684\u6b63\u65b9\u5f62\u7684. padding ( int or sequence__, optional ) \u2013 \u56fe\u50cf\u7684\u6bcf\u4e2a\u8fb9\u6846\u4e0a\u7684\u53ef\u9009\u586b\u5145. \u7f3a\u7701\u503c\u662f 0, \u5373\u6ca1\u6709\u586b\u5145. \u5982\u679c\u63d0\u4f9b\u957f\u5ea6\u4e3a 4 \u7684\u5e8f\u5217, \u5219\u5206\u522b\u7528\u4e8e\u586b\u5145\u5de6\u4fa7, \u9876\u90e8, \u53f3\u4fa7, \u5e95\u90e8\u8fb9\u754c. | | --- | --- | class torchvision.transforms.RandomHorizontalFlip \u4ee5\u6982\u73870.5\u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\u56fe\u50cf class torchvision.transforms.RandomVerticalFlip \u4ee5\u6982\u73870.5\u968f\u673a\u5782\u76f4\u7ffb\u8f6c\u56fe\u50cf. class torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2) \u5c06\u7ed9\u5b9a\u7684 PIL \u56fe\u50cf\u88c1\u526a\u4e3a\u968f\u673a\u5927\u5c0f\u548c\u7eb5\u6a2a\u6bd4\u4f8b. \u539f\u59cb\u9ad8\u5bbd\u6bd4\u7684\u968f\u673a\u5927\u5c0f\uff08\u9ed8\u8ba4: 0.08 \u5230 1.0\uff09\u548c\u968f\u673a\u5bbd\u9ad8\u6bd4\uff08\u9ed8\u8ba4: 3/4 \u5230 4/3\uff09\u7684\u88c1\u526a. \u8be5\u88c1\u526a\u6700\u7ec8\u4f1a\u88ab\u8c03\u6574\u4e3a\u6307\u5b9a\u7684\u5c3a\u5bf8. \u8be5\u64cd\u4f5c\u666e\u904d\u7528\u4e8e\u8bad\u7ec3 Inception networks. | Parameters: | size \u2013 \u6bcf\u6761\u8fb9\u7684\u671f\u671b\u7684\u8f93\u51fa\u5c3a\u5bf8 scale \u2013 \u539f\u59cb\u526a\u88c1\u5c3a\u5bf8\u5927\u5c0f\u7684\u8303\u56f4 ratio \u2013 \u539f\u59cb\u88c1\u526a\u7eb5\u6a2a\u6bd4\u7684\u8303\u56f4 interpolation \u2013 Default: PIL.Image.BILINEAR | | --- | --- | class torchvision.transforms.RandomSizedCrop(*args, **kwargs) Note: \u4e3a\u4e86\u652f\u6301 RandomResizedCrop, \u8be5\u53d8\u6362\u5df2\u7ecf\u88ab\u5f03\u7528. class torchvision.transforms.Grayscale(num_output_channels=1) \u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\u50cf. Parameters: num_output_channels ( int ) \u2013 (1 or 3) \u8f93\u51fa\u56fe\u50cf\u6240\u671f\u671b\u7684\u901a\u9053\u6570\u91cf Returns: \u7070\u5ea6\u7248\u672c\u7684\u8f93\u5165. - \u5982\u679c num_output_channels == 1 : \u8fd4\u56de\u7684\u56fe\u50cf\u662f 1 \u901a\u9053 - \u5982\u679c num_output_channels == 3 : \u8fd4\u56de\u7684\u56fe\u50cf\u662f 3 \u901a\u9053, \u5e76\u4e14 r == g == b --- --- Return type: PIL Image --- --- class torchvision.transforms.RandomGrayscale(p=0.1) \u968f\u673a\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\u50cf, \u6982\u7387\u4e3a p (default 0.1). Parameters: p ( float ) \u2013 \u56fe\u50cf\u5e94\u8be5\u88ab\u8f6c\u6362\u6210\u7070\u5ea6\u7684\u6982\u7387. Returns: \u7070\u5ea6\u7248\u672c\u7684\u8f93\u5165\u56fe\u50cf\u7684\u6982\u7387\u4e3a p, \u4e0d\u53d8\u7684\u6982\u7387\u4e3a\uff081-p\uff09 - \u5982\u679c\u8f93\u5165\u56fe\u50cf\u4e3a1\u4e2a\u901a\u9053: \u5219\u7070\u5ea6\u7248\u672c\u662f 1 \u901a\u9053 - \u5982\u679c\u8f93\u5165\u56fe\u50cf\u4e3a3\u4e2a\u901a\u9053: \u5219\u7070\u5ea6\u7248\u672c\u662f 3 \u901a\u9053, \u5e76\u4e14 r == g == b --- --- Return type: PIL Image --- --- class torchvision.transforms.FiveCrop(size) \u5c06\u7ed9\u5b9a\u7684 PIL Image \u88c1\u526a\u6210\u56db\u4e2a\u89d2\u843d\u548c\u4e2d\u5fc3\u88c1\u526a Note \u8be5\u53d8\u6362\u8fd4\u56de\u4e00\u4e2a\u56fe\u50cf\u5143\u7ec4, \u5e76\u4e14\u6570\u636e\u96c6\u8fd4\u56de\u7684\u8f93\u5165\u548c\u76ee\u6807\u7684\u6570\u91cf\u53ef\u80fd\u4e0d\u5339\u914d. \u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u4f8b\u5b50\u6765\u5904\u7406\u8fd9\u4e2a\u95ee\u9898. Parameters: size ( sequence or int ) \u2013 \u671f\u671b\u8f93\u51fa\u7684\u88c1\u526a\u5c3a\u5bf8. \u5982\u679c size\uff08\u5c3a\u5bf8\uff09\u662f <cite>int`</cite> \u7c7b\u578b\u7684\u6574\u6570, \u800c\u4e0d\u662f\u50cf (h, w) \u8fd9\u6837\u7c7b\u578b\u7684\u5e8f\u5217, \u88c1\u526a\u51fa\u6765\u7684\u56fe\u50cf\u662f (size, size) \u8fd9\u6837\u7684\u6b63\u65b9\u5f62\u7684.. Example >>> transform = Compose([ >>> FiveCrop(size), # \u4e00\u4e2a PIL Images \u7684\u5217\u8868 >>> Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # \u8fd4\u56de\u4e00\u4e2a4D Tensor >>> ]) >>> #\u5728\u4f60\u7684\u6d4b\u8bd5\u5faa\u73af\u53ef\u4ee5\u5982\u4e0b\u64cd\u4f5c: >>> input, target = batch # \u8f93\u5165\u662f5DTensor,\u8f93\u51fa\u662f2D >>> bs, ncrops, c, h, w = input.size() >>> result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops >>> result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops class torchvision.transforms.TenCrop(size, vertical_flip=False) \u5c06\u7ed9\u5b9a\u7684 PIL Image \u88c1\u526a\u6210\u56db\u4e2a\u89d2, \u4e2d\u5fc3\u88c1\u526a, \u5e76\u52a0\u4e0a\u8fd9\u4e9b\u7684\u7ffb\u8f6c\u7248\u672c\uff08\u9ed8\u8ba4\u4f7f\u7528\u6c34\u5e73\u7ffb\u8f6c\uff09 Note \u8be5\u53d8\u6362\u8fd4\u56de\u4e00\u4e2a\u56fe\u50cf\u5143\u7ec4, \u5e76\u4e14\u6570\u636e\u96c6\u8fd4\u56de\u7684\u8f93\u5165\u548c\u76ee\u6807\u7684\u6570\u91cf\u53ef\u80fd\u4e0d\u5339\u914d. \u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u4f8b\u5b50\u6765\u5904\u7406\u8fd9\u4e2a\u95ee\u9898. | Parameters: | size ( sequence or int ) \u2013 \u671f\u671b\u8f93\u51fa\u7684\u88c1\u526a\u5c3a\u5bf8. \u5982\u679c size\uff08\u5c3a\u5bf8\uff09\u662f <cite>int</cite> \u7c7b\u578b\u7684\u6574\u6570, \u800c\u4e0d\u662f\u50cf (h, w) \u8fd9\u6837\u7c7b\u578b\u7684\u5e8f\u5217, \u88c1\u526a\u51fa\u6765\u7684\u56fe\u50cf\u662f (size, size) \u8fd9\u6837\u7684\u6b63\u65b9\u5f62\u7684. vertical_flip ( bool ) \u2013 \u4f7f\u7528\u5782\u76f4\u7ffb\u8f6c\u800c\u4e0d\u662f\u6c34\u5e73\u7684\u65b9\u5f0f | | --- | --- | Example >>> transform = Compose([ >>> TenCrop(size), # this is a list of PIL Images >>> Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor >>> ]) >>> #In your test loop you can do the following: >>> input, target = batch # input is a 5d tensor, target is 2d >>> bs, ncrops, c, h, w = input.size() >>> result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops >>> result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops class torchvision.transforms.Pad(padding, fill=0) \u7528\u6307\u5b9a\u7684 \u201cpad\u201d \u503c\u586b\u5145\u6307\u5b9a\u7684 PIL image. | Parameters: | padding ( int or tuple ) \u2013 \u586b\u5145\u6bcf\u4e2a\u8fb9\u6846. \u5982\u679c\u63d0\u4f9b\u4e86\u4e00\u4e2a int \u578b\u7684\u6574\u6570, \u5219\u7528\u4e8e\u586b\u5145\u6240\u6709\u8fb9\u754c. \u5982\u679c\u63d0\u4f9b\u957f\u5ea6\u4e3a 2 \u7684\u5143\u7ec4, \u5219\u8fd9\u662f\u5206\u522b\u5728 \u5de6/\u53f3 \u548c \u4e0a/\u4e0b \u7684\u586b\u5145. \u5982\u679c\u63d0\u4f9b\u957f\u5ea6\u4e3a 4 \u7684\u5143\u7ec4, \u5219\u8fd9\u662f\u5206\u522b\u7528\u4e8e \u5de6, \u4e0a, \u53f3 \u548c \u4e0b \u90e8\u8fb9\u754c\u7684\u586b\u5145. fill \u2013 \u50cf\u7d20\u586b\u5145. \u9ed8\u8ba4\u503c\u4e3a 0. \u5982\u679c\u957f\u5ea6\u4e3a 3 \u7684\u5143\u7ec4, \u5206\u522b\u7528\u4e8e\u586b\u5145 R, G, B \u901a\u9053. | | --- | --- | class torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0) \u968f\u673a\u66f4\u6539\u56fe\u50cf\u7684\u4eae\u5ea6, \u5bf9\u6bd4\u5ea6\u548c\u9971\u548c\u5ea6. | Parameters: | brightness ( float ) \u2013 \u4eae\u5ea6\u6539\u53d8\u7684\u8303\u56f4. brightness_factor \u4ece [max(0, 1 - brightness), 1 + brightness]\u7684\u8303\u56f4\u4e2d\u4e00\u81f4\u9009\u62e9. contrast ( float ) \u2013 \u5bf9\u6bd4\u5ea6\u6539\u53d8\u7684\u8303\u56f4. contrast_factor \u4ece [max(0, 1 - contrast), 1 + contrast]\u7684\u8303\u56f4\u4e2d\u4e00\u81f4\u9009\u62e9. saturation ( float ) \u2013 \u9971\u548c\u5ea6\u6539\u53d8\u7684\u8303\u56f4. saturation_factor \u4ece[max(0, 1 - saturation), 1 + saturation]\u7684\u8303\u56f4\u4e2d\u4e00\u81f4\u9009\u62e9. hue ( float ) \u2013 \u8272\u8c03\u6539\u53d8\u7684\u8303\u56f4. hue_factor \u4ece [-hue, hue]\u7684\u8303\u56f4\u4e2d\u4e00\u81f4\u9009\u62e9. \u5e94\u8be5 >=0 \u4e14 <= 0.5. | | --- | --- |","title":"PIL Image \u4e0a\u7684\u53d8\u6362"},{"location":"73/#torchtensor","text":"class torchvision.transforms.Normalize(mean, std) \u7528\u5747\u503c\u548c\u6807\u51c6\u504f\u5dee\u5bf9\u5f20\u91cf\u56fe\u50cf\u8fdb\u884c\u5f52\u4e00\u5316. \u7ed9\u5b9a\u5747\u503c: (M1,...,Mn) \u548c\u6807\u51c6\u5dee: (S1,..,Sn) \u7528\u4e8e n \u4e2a\u901a\u9053, \u8be5\u53d8\u6362\u5c06\u6807\u51c6\u5316\u8f93\u5165 torch.*Tensor \u7684\u6bcf\u4e00\u4e2a\u901a\u9053. \u4f8b\u5982: input[channel] = (input[channel] - mean[channel]) / std[channel] | Parameters: | mean ( sequence ) \u2013 \u6bcf\u4e00\u4e2a\u901a\u9053\u7684\u5747\u503c\u5e8f\u5217. std ( sequence ) \u2013 \u6bcf\u4e00\u4e2a\u901a\u9053\u7684\u6807\u51c6\u5dee\u5e8f\u5217. | | --- | --- | __call__(tensor) Parameters: tensor ( Tensor ) \u2013 \u9700\u8981\u88ab\u5f52\u4e00\u5316\u7684\u5927\u5c0f\u4e3a (C, H, W)Tensor image. Returns: \u5f52\u4e00\u5316\u540e\u7684 Tensor image. --- --- Return type: Tensor --- ---","title":"torch.*Tensor \u4e0a\u7684\u53d8\u6362"},{"location":"73/#_1","text":"class torchvision.transforms.ToTensor \u8f6c\u6362\u4e00\u4e2a PIL Image \u6216 numpy.ndarray \u4e3a tensor\uff08\u5f20\u91cf\uff09. \u5c06\u8303\u56f4 [0, 255] \u4e2d\u7684 PIL Image \u6216 numpy.ndarray (H x W x C) \u8f6c\u6362\u5f62\u72b6\u4e3a (C x H x W) , \u503c\u8303\u56f4\u4e3a [0.0, 1.0] \u7684 torch.FloatTensor. __call__(pic) Parameters: pic ( PIL Image or numpy.ndarray ) \u2013 \u5c06\u8981\u88ab\u8f6c\u6362\u4e3a tensor \u7684 Image. Returns: \u8f6c\u6362\u540e\u7684 image. --- --- Return type: Tensor --- --- class torchvision.transforms.ToPILImage(mode=None) \u8f6c\u6362\u4e00\u4e2a tensor \u6216 ndarray \u4e3a PIL Image. \u8f6c\u6362\u4e00\u4e2a\u5f62\u72b6\u4e3a(C x H x W) \u7684 torch.*Tensor \u6216\u4e00\u4e2a\u5f62\u72b6\u4e3a(H x W x C )\u7684numpy ndarray \u81f3\u4e00\u4e2a PIL Image ,\u540c\u65f6\u4fdd\u7559\u503c\u8303\u56f4. Parameters: mode ( PIL.Image mode ) \u2013 \u8f93\u5165\u6570\u636e\u7684\u8272\u57df\u548c\u50cf\u7d20\u6df1\u5ea6 (\u53ef\u9009). \u5982\u679c mode \u4e3a None (\u9ed8\u8ba4) ,\u8fd9\u91cc\u5bf9\u8f93\u5165\u6570\u636e\u6709\u4e00\u4e9b\u5047\u8bbe: 1. \u5982\u679c\u8f93\u5165\u67093\u4e2a\u901a\u9053, mode \u5047\u8bbe\u4e3a RGB . 2. \u5982\u679c\u8f93\u5165\u67094\u4e2a\u901a\u9053, mode \u5047\u8bbe\u4e3a RGBA . 3. \u5982\u679c\u8f93\u5165\u67091\u4e2a\u901a\u9053, mode \u6839\u636e\u6570\u636e\u7c7b\u578b\u786e\u5b9a (i,e, int , float , short ). __call__(pic) Parameters: pic ( Tensor or numpy.ndarray ) \u2013 \u8981\u8f6c\u6362\u4e3aPIL Image\u7684\u56fe\u50cf. Returns: \u8f6c\u6362\u4e3aPIL Image\u7684\u56fe\u50cf. --- --- Return type: PIL Image --- ---","title":"\u8f6c\u6362\u7c7b\u578b\u7684\u53d8\u6362"},{"location":"73/#_2","text":"class torchvision.transforms.Lambda(lambd) \u5e94\u7528\u4e00\u4e2a\u7528\u6237\u5b9a\u4e49\u7684 Lambda \u4f5c\u4e3a\u53d8\u6362. Parameters: lambd ( function ) \u2013 Lambda/function \u4ee5\u7528\u4e8e transform.","title":"\u901a\u7528\u7684\u53d8\u6362"},{"location":"74/","text":"torchvision.utils torchvision.utils.make_grid(tensor, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0) \u5236\u4f5c\u4e00\u4e2a\u56fe\u5f62\u7f51\u683c. | Parameters: | tensor ( Tensor \u6216\u8005 list ) \u2013 \u7ed9\u5b9a 4D mini-batch Tensor \u5f62\u72b6\u4e3a (B x C x H x W) \u6216\u8005\u4e00\u4e2a\u540c\u6837\u5f62\u72b6\u7684 list of images. nrow ( int , optional ) \u2013 \u7f51\u683c\u6bcf\u4e00\u884c\u663e\u793a\u7684image\u6570\u91cf. \u6700\u540e\u7f51\u683c\u7684\u5f62\u72b6\u662f (B / nrow, nrow). \u9ed8\u8ba4\u662f 8. padding ( int , optional ) \u2013 \u586b\u5145\u7684\u6570\u91cf. \u9ed8\u8ba4\u4e3a 2. normalize ( bool , optional ) \u2013 \u5982\u679c\u503c\u4e3aTrue,\u901a\u8fc7\u51cf\u53bb\u6700\u5c0f\u50cf\u7d20\u503c\u5e76\u9664\u4ee5\u6700\u5927\u7684\u50cf\u7d20\u503c\u7684\u65b9\u6cd5, \u628a\u56fe\u50cf\u7684\u8303\u56f4\u53d8\u4e3a (0, 1),\u6b64\u8fc7\u7a0b\u4e3a\u5f52\u4e00\u5316\u5904\u7406. range ( tuple , optional ) \u2013 tuple (min, max) \u8fd9\u91cc min \u548c max \u90fd\u662f\u6570\u5b57, \u8fd9\u4e9b\u6570\u5b57\u662f\u7528\u6765\u89c4\u8303 image\u7684. \u9ed8\u8ba4\u60c5\u51b5\u4e0b, min \u548c max \u662f\u4ece tensor \u91cc\u8ba1\u7b97\u51fa\u6765\u7684. scale_each ( bool , optional ) \u2013 \u5982\u679c\u503c\u4e3aTrue, \u6bcf\u4e2aimage\u72ec\u7acb\u89c4\u8303\u5316, \u800c\u4e0d\u662f\u6839\u636e\u6240\u6709image\u7684\u50cf\u7d20\u6700\u5927\u6700\u5c0f\u503c\u6765\u5f52\u4e00\u5316. pad_value ( float , optional ) \u2013 \u586b\u5145\u50cf\u7d20\u7684\u503c. | | --- | --- | Example \u8bf7\u53c2\u9605 \u8fd9\u91cc \u7684\u624b\u518c torchvision.utils.save_image(tensor, filename, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0) \u5c06\u4e00\u4e2a\u7ed9\u5b9a\u7684 Tensor \u4fdd\u5b58\u4e3a image\uff08\u56fe\u50cf\uff09\u6587\u4ef6. | Parameters: | tensor ( Tensor or list ) \u2013 \u88ab\u4fdd\u5b58\u7684\u56fe\u7247. \u5982\u679c\u7ed9\u5b9a\u7684\u662f mini-batch tensor, \u901a\u8fc7\u8c03\u7528 make_grid \u5c06 tensor \u4fdd\u5b58\u4e3a\u7f51\u683c\u56fe\u50cf. **kwargs \u2013 \u5176\u5b83\u53c2\u6570\u6587\u6863\u5728 make_grid \u4e2d. | | --- | --- |","title":"torchvision.utils"},{"location":"74/#torchvisionutils","text":"torchvision.utils.make_grid(tensor, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0) \u5236\u4f5c\u4e00\u4e2a\u56fe\u5f62\u7f51\u683c. | Parameters: | tensor ( Tensor \u6216\u8005 list ) \u2013 \u7ed9\u5b9a 4D mini-batch Tensor \u5f62\u72b6\u4e3a (B x C x H x W) \u6216\u8005\u4e00\u4e2a\u540c\u6837\u5f62\u72b6\u7684 list of images. nrow ( int , optional ) \u2013 \u7f51\u683c\u6bcf\u4e00\u884c\u663e\u793a\u7684image\u6570\u91cf. \u6700\u540e\u7f51\u683c\u7684\u5f62\u72b6\u662f (B / nrow, nrow). \u9ed8\u8ba4\u662f 8. padding ( int , optional ) \u2013 \u586b\u5145\u7684\u6570\u91cf. \u9ed8\u8ba4\u4e3a 2. normalize ( bool , optional ) \u2013 \u5982\u679c\u503c\u4e3aTrue,\u901a\u8fc7\u51cf\u53bb\u6700\u5c0f\u50cf\u7d20\u503c\u5e76\u9664\u4ee5\u6700\u5927\u7684\u50cf\u7d20\u503c\u7684\u65b9\u6cd5, \u628a\u56fe\u50cf\u7684\u8303\u56f4\u53d8\u4e3a (0, 1),\u6b64\u8fc7\u7a0b\u4e3a\u5f52\u4e00\u5316\u5904\u7406. range ( tuple , optional ) \u2013 tuple (min, max) \u8fd9\u91cc min \u548c max \u90fd\u662f\u6570\u5b57, \u8fd9\u4e9b\u6570\u5b57\u662f\u7528\u6765\u89c4\u8303 image\u7684. \u9ed8\u8ba4\u60c5\u51b5\u4e0b, min \u548c max \u662f\u4ece tensor \u91cc\u8ba1\u7b97\u51fa\u6765\u7684. scale_each ( bool , optional ) \u2013 \u5982\u679c\u503c\u4e3aTrue, \u6bcf\u4e2aimage\u72ec\u7acb\u89c4\u8303\u5316, \u800c\u4e0d\u662f\u6839\u636e\u6240\u6709image\u7684\u50cf\u7d20\u6700\u5927\u6700\u5c0f\u503c\u6765\u5f52\u4e00\u5316. pad_value ( float , optional ) \u2013 \u586b\u5145\u50cf\u7d20\u7684\u503c. | | --- | --- | Example \u8bf7\u53c2\u9605 \u8fd9\u91cc \u7684\u624b\u518c torchvision.utils.save_image(tensor, filename, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0) \u5c06\u4e00\u4e2a\u7ed9\u5b9a\u7684 Tensor \u4fdd\u5b58\u4e3a image\uff08\u56fe\u50cf\uff09\u6587\u4ef6. | Parameters: | tensor ( Tensor or list ) \u2013 \u88ab\u4fdd\u5b58\u7684\u56fe\u7247. \u5982\u679c\u7ed9\u5b9a\u7684\u662f mini-batch tensor, \u901a\u8fc7\u8c03\u7528 make_grid \u5c06 tensor \u4fdd\u5b58\u4e3a\u7f51\u683c\u56fe\u50cf. **kwargs \u2013 \u5176\u5b83\u53c2\u6570\u6587\u6863\u5728 make_grid \u4e2d. | | --- | --- |","title":"torchvision.utils"},{"location":"75/","text":"\u9879\u76ee\u76f8\u5173","title":"\u9879\u76ee\u76f8\u5173"},{"location":"75/#_1","text":"","title":"\u9879\u76ee\u76f8\u5173"},{"location":"76/","text":"\u9879\u76ee\u8d21\u732e\u8005 \u8be5\u9875\u9762\u662f apachecn/pytorch-doc-zh \u9879\u76ee\u76f8\u5173\u7684\u8d21\u732e\u8005\u4fe1\u606f. \u9879\u76ee\u8d1f\u8d23\u4eba @\u90a3\u4f0a\u62b9\u5fae\u7b11 \u8d21\u732e\u8005 PyTorch 0.3.0 \u4e2d\u6587\u6587\u6863\u8d21\u732e\u8005 @\u90a3\u4f0a\u62b9\u5fae\u7b11 @\u7247\u523b @\u5c0f\u7476 @\u5c0f\u738b\u5b50 @\u4e4b\u8317 @dabney777 @hadxu @\u738b\u4e91\u5cf0 @xfanplus @\u674e\u96e8\u9f99 @LeeGeong @shunshun @\u6797\u5609\u5e94 @\u4e8e\u589e\u6e90 @\u5409\u601d\u96e8 @\u738b\u5e05 @\u8c08\u7b11\u98ce\u751f @\u53f6\u8212\u6cc9 @Sylvester @\u80e1\u4e1c\u7476 @jizg @\u5c0f\u4e54 @BarrettLi @\u8c22\u5bb6\u67ef @buldajs @Chriskuei @\u51af\u6590 @\u591c\u795e\u6708 @MUSK1881 @K @Dongze Lian @Krok Yin PyTorch 0.2.0 \u4e2d\u6587\u6587\u6863\u8d21\u732e\u8005 @awfssv (\u7ec4\u7ec7\u8005) @ycszen @KeithYin @kophy @swordspoet @dyl745001196 @koshinryuu @tfygg @weigp @ZijunDeng @yichuan9527 \u5efa\u8bae\u53cd\u9988 \u8054\u7cfb\u9879\u76ee\u8d1f\u8d23\u4eba @\u90a3\u4f0a\u62b9\u5fae\u7b11 . \u5728\u6211\u4eec\u7684 apachecn/pytorch-doc-zh github \u4e0a\u63d0 issue. \u53d1\u9001\u90ae\u4ef6\u5230 Email: pytorch#apachecn.org (#\u66ff\u6362\u6210@) . \u5728\u6211\u4eec\u7684 \u7ec4\u7ec7\u5b66\u4e60\u4ea4\u6d41\u7fa4 \u4e2d\u8054\u7cfb\u7fa4\u4e3b/\u7ba1\u7406\u5458\u5373\u53ef.","title":"\u9879\u76ee\u8d21\u732e\u8005"},{"location":"76/#_1","text":"\u8be5\u9875\u9762\u662f apachecn/pytorch-doc-zh \u9879\u76ee\u76f8\u5173\u7684\u8d21\u732e\u8005\u4fe1\u606f.","title":"\u9879\u76ee\u8d21\u732e\u8005"},{"location":"76/#_2","text":"@\u90a3\u4f0a\u62b9\u5fae\u7b11","title":"\u9879\u76ee\u8d1f\u8d23\u4eba"},{"location":"76/#_3","text":"","title":"\u8d21\u732e\u8005"},{"location":"76/#pytorch-030","text":"@\u90a3\u4f0a\u62b9\u5fae\u7b11 @\u7247\u523b @\u5c0f\u7476 @\u5c0f\u738b\u5b50 @\u4e4b\u8317 @dabney777 @hadxu @\u738b\u4e91\u5cf0 @xfanplus @\u674e\u96e8\u9f99 @LeeGeong @shunshun @\u6797\u5609\u5e94 @\u4e8e\u589e\u6e90 @\u5409\u601d\u96e8 @\u738b\u5e05 @\u8c08\u7b11\u98ce\u751f @\u53f6\u8212\u6cc9 @Sylvester @\u80e1\u4e1c\u7476 @jizg @\u5c0f\u4e54 @BarrettLi @\u8c22\u5bb6\u67ef @buldajs @Chriskuei @\u51af\u6590 @\u591c\u795e\u6708 @MUSK1881 @K @Dongze Lian @Krok Yin","title":"PyTorch 0.3.0 \u4e2d\u6587\u6587\u6863\u8d21\u732e\u8005"},{"location":"76/#pytorch-020","text":"@awfssv (\u7ec4\u7ec7\u8005) @ycszen @KeithYin @kophy @swordspoet @dyl745001196 @koshinryuu @tfygg @weigp @ZijunDeng @yichuan9527","title":"PyTorch 0.2.0 \u4e2d\u6587\u6587\u6863\u8d21\u732e\u8005"},{"location":"76/#_4","text":"\u8054\u7cfb\u9879\u76ee\u8d1f\u8d23\u4eba @\u90a3\u4f0a\u62b9\u5fae\u7b11 . \u5728\u6211\u4eec\u7684 apachecn/pytorch-doc-zh github \u4e0a\u63d0 issue. \u53d1\u9001\u90ae\u4ef6\u5230 Email: pytorch#apachecn.org (#\u66ff\u6362\u6210@) . \u5728\u6211\u4eec\u7684 \u7ec4\u7ec7\u5b66\u4e60\u4ea4\u6d41\u7fa4 \u4e2d\u8054\u7cfb\u7fa4\u4e3b/\u7ba1\u7406\u5458\u5373\u53ef.","title":"\u5efa\u8bae\u53cd\u9988"},{"location":"77/","text":"\u7ec4\u7ec7\u5b66\u4e60\u4ea4\u6d41\u7fa4 \u673a\u5668\u5b66\u4e60\u4ea4\u6d41\u7fa4: 629470233 (2000\u4eba) \u5927\u6570\u636e\u4ea4\u6d41\u7fa4: 214293307 (2000\u4eba) Kaggle \u7ade\u8d5b\u4ea4\u6d41\u7fa4: 686932392 (2000\u4eba) \u4e86\u89e3\u6211\u4eec: http://www.apachecn.org/organization/209.html \u52a0\u5165\u7ec4\u7ec7: http://www.apachecn.org/organization/209.html \u66f4\u591a\u5b66 (zhuang) \u4e60 (bi) \u4ea4\u6d41\u7fa4\u8bf7\u53c2\u9605: http://www.apachecn.org/organization/348.html","title":"\u7ec4\u7ec7\u5b66\u4e60\u4ea4\u6d41\u7fa4"},{"location":"77/#_1","text":"\u673a\u5668\u5b66\u4e60\u4ea4\u6d41\u7fa4: 629470233 (2000\u4eba) \u5927\u6570\u636e\u4ea4\u6d41\u7fa4: 214293307 (2000\u4eba) Kaggle \u7ade\u8d5b\u4ea4\u6d41\u7fa4: 686932392 (2000\u4eba) \u4e86\u89e3\u6211\u4eec: http://www.apachecn.org/organization/209.html \u52a0\u5165\u7ec4\u7ec7: http://www.apachecn.org/organization/209.html \u66f4\u591a\u5b66 (zhuang) \u4e60 (bi) \u4ea4\u6d41\u7fa4\u8bf7\u53c2\u9605: http://www.apachecn.org/organization/348.html","title":"\u7ec4\u7ec7\u5b66\u4e60\u4ea4\u6d41\u7fa4"},{"location":"78/","text":"","title":"78"},{"location":"8/","text":"Tensors Tensors \u5728 PyTorch \u4e2d\u7684\u64cd\u4f5c\u65b9\u5f0f \u4e0e Torch \u51e0\u4e4e\u5b8c\u5168\u76f8\u540c. \u7528\u672a\u521d\u59cb\u5316\u7684\u5185\u5b58\u521b\u5efa\u4e00\u4e2a\u5927\u5c0f\u4e3a (5 x 7) \u7684 tensor: import torch a = torch.FloatTensor(5, 7) \u7528 mean=0, var=1 \u7684\u6b63\u6001\u5206\u5e03\u968f\u673a\u521d\u59cb\u5316\u4e00\u4e2atensor: a = torch.randn(5, 7) print(a) print(a.size()) Note torch.Size \u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a tuple, \u56e0\u6b64\u5b83\u652f\u6301\u76f8\u540c\u7684\u64cd\u4f5c Inplace / Out-of-place \u7b2c\u4e00\u4e2a\u4e0d\u540c\u70b9\u5728\u4e8e tensor \u4e0a\u7684\u6240\u6709\u64cd\u4f5c, \u5982\u679c\u60f3\u8981\u5728 tensor \u81ea\u8eab\u4e0a\u8fdb\u884c\u7684\u64cd\u4f5c (in-place) \u5c31\u8981\u52a0\u4e0a\u4e00\u4e2a _ \u4f5c\u4e3a\u540e\u7f00. \u4f8b\u5982, add \u662f\u4e00\u4e2a out-of-place \u7684 version ,\u800c add_ \u662f\u4e00\u4e2a in-place \u7684 version . a.fill_(3.5) # a \u7684\u503c\u73b0\u5728\u53d8\u4e3a 3.5 b = a.add(4.0) # a \u7684\u503c\u4ecd\u7136\u662f 3.5 # \u8fd4\u56de\u7684\u503c 3.5 + 4.0 = 7.5 \u5c06\u4f5c\u4e3ab\u7684\u503c. print(a, b) \u8fd8\u6709\u4e00\u4e9b\u50cf narrow \u7684\u64cd\u4f5c\u662f\u6ca1\u6709 in-place version , \u6240\u4ee5\u4e5f\u5c31\u4e0d\u5b58\u5728 .narrow_ . \u540c\u6837\u7684, \u4e5f\u6709\u50cf fill_ \u7684\u4e00\u4e9b\u64cd\u4f5c\u6ca1\u6709 out-of-place version . \u56e0\u6b64, .fill \u4e5f\u540c\u6837\u4e0d\u5b58\u5728. Zero Indexing (\u96f6\u7d22\u5f15) Tensors \u662f zero-indexed (\u7d22\u5f15\u4ece\u96f6\u5f00\u59cb)\u8fd9\u662f\u53e6\u5916\u4e00\u4e2a\u4e0d\u540c\u70b9. (\u5728 lua \u4e2d, tensors \u662f one-indexed (\u7d22\u5f15\u4ece\u4e00\u5f00\u59cb)) b = a[0, 3] # \u4ece a \u4e2d\u9009\u62e9\u7b2c\u4e00\u884c\u7b2c\u56db\u5217\u7684\u503c. Tensors \u4e5f\u53ef\u4ee5\u7528 Python \u7684\u5207\u7247\u7d22\u5f15 b = a[:, 3:5] # \u4ece a \u4e2d\u9009\u62e9\u6240\u6709\u884c\u4e2d\u7b2c\u56db\u5217\u548c\u7b2c\u4e94\u5217\u7684\u503c. No camel casing \u63a5\u4e0b\u6765\u4e00\u4e2a\u5c0f\u7684\u4e0d\u540c\u662f\u6240\u6709\u7684\u51fd\u6570\u90fd\u4e0d\u662f camelCase \u4e86. \u4f8b\u5982 indexAdd \u73b0\u5728\u88ab\u79f0\u4e3a index_add_ x = torch.ones(5, 5) print(x) z = torch.Tensor(5, 2) z[:, 0] = 10 z[:, 1] = 100 print(z) x.index_add_(1, torch.LongTensor([4, 0]), z) print(x) Numpy Bridge \u5c06 torch Tensor \u8f6c\u6362\u4e3a\u4e00\u4e2a numpy array, \u53cd\u4e4b\u4ea6\u7136. Torch Tensor \u548c numpy array \u5c06\u4f1a\u5171\u7528\u5e95\u5c42\u7684\u5185\u5b58, \u6539\u53d8\u5176\u4e2d\u4e00\u4e2a, \u53e6\u5916\u4e00\u4e2a\u4e5f\u4f1a\u968f\u4e4b\u6539\u53d8. \u5c06 torch Tensor \u8f6c\u6362\u4e3a numpy Array a = torch.ones(5) print(a) b = a.numpy() print(b) a.add_(1) print(a) print(b) # \u770b\u4e00\u4e0b numpy array \u503c\u7684\u53d8\u5316 \u5c06 numpy Array \u8f6c\u6362\u4e3a torch Tensor import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out=a) print(a) print(b) # \u770b\u4e00\u4e0b\u901a\u8fc7\u6539\u53d8 np array \u6765\u81ea\u52a8\u7684\u6539\u53d8 torch Tensor \u9664\u4e86 CharTensor \u4e4b\u5916, \u6240\u6709 CPU \u4e0a\u7684 Tensors \u652f\u6301\u8f6c\u53d8\u4e3a NumPy \u5e76\u4e14 \u8f6c\u6362\u56de\u6765. CUDA Tensors CUDA Tensors \u5728 pytorch \u4e2d\u975e\u5e38\u597d\u7528, \u5e76\u4e14\u4e00\u4e2a CUDA tensor \u4ece CPU \u8f6c\u6362\u5230 GPU \u4ecd\u5c06\u4fdd\u6301\u5b83\u5e95\u5c42\u7684\u7c7b\u578b. # \u8ba9\u6211\u4eec\u5728 CUDA \u53ef\u7528\u7684\u65f6\u5019\u8fd0\u884c\u8fd9\u4e2a\u5355\u5143 if torch.cuda.is_available(): # \u521b\u5efa\u4e00\u4e2a LongTensor \u5e76\u4e14\u5c06\u5176\u8f6c\u6362\u4f7f\u7528 GPU # \u7684 torch.cuda.LongTensor \u7c7b\u578b a = torch.LongTensor(10).fill_(3).cuda() print(type(a)) b = a.cpu() # \u5c06\u5b83\u8f6c\u6362\u5230 CPU # \u7c7b\u578b\u53d8\u56de torch.LongTensor Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: tensor_tutorial.py Download Jupyter notebook: tensor_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"Tensors"},{"location":"8/#tensors","text":"Tensors \u5728 PyTorch \u4e2d\u7684\u64cd\u4f5c\u65b9\u5f0f \u4e0e Torch \u51e0\u4e4e\u5b8c\u5168\u76f8\u540c. \u7528\u672a\u521d\u59cb\u5316\u7684\u5185\u5b58\u521b\u5efa\u4e00\u4e2a\u5927\u5c0f\u4e3a (5 x 7) \u7684 tensor: import torch a = torch.FloatTensor(5, 7) \u7528 mean=0, var=1 \u7684\u6b63\u6001\u5206\u5e03\u968f\u673a\u521d\u59cb\u5316\u4e00\u4e2atensor: a = torch.randn(5, 7) print(a) print(a.size()) Note torch.Size \u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a tuple, \u56e0\u6b64\u5b83\u652f\u6301\u76f8\u540c\u7684\u64cd\u4f5c","title":"Tensors"},{"location":"8/#inplace-out-of-place","text":"\u7b2c\u4e00\u4e2a\u4e0d\u540c\u70b9\u5728\u4e8e tensor \u4e0a\u7684\u6240\u6709\u64cd\u4f5c, \u5982\u679c\u60f3\u8981\u5728 tensor \u81ea\u8eab\u4e0a\u8fdb\u884c\u7684\u64cd\u4f5c (in-place) \u5c31\u8981\u52a0\u4e0a\u4e00\u4e2a _ \u4f5c\u4e3a\u540e\u7f00. \u4f8b\u5982, add \u662f\u4e00\u4e2a out-of-place \u7684 version ,\u800c add_ \u662f\u4e00\u4e2a in-place \u7684 version . a.fill_(3.5) # a \u7684\u503c\u73b0\u5728\u53d8\u4e3a 3.5 b = a.add(4.0) # a \u7684\u503c\u4ecd\u7136\u662f 3.5 # \u8fd4\u56de\u7684\u503c 3.5 + 4.0 = 7.5 \u5c06\u4f5c\u4e3ab\u7684\u503c. print(a, b) \u8fd8\u6709\u4e00\u4e9b\u50cf narrow \u7684\u64cd\u4f5c\u662f\u6ca1\u6709 in-place version , \u6240\u4ee5\u4e5f\u5c31\u4e0d\u5b58\u5728 .narrow_ . \u540c\u6837\u7684, \u4e5f\u6709\u50cf fill_ \u7684\u4e00\u4e9b\u64cd\u4f5c\u6ca1\u6709 out-of-place version . \u56e0\u6b64, .fill \u4e5f\u540c\u6837\u4e0d\u5b58\u5728.","title":"Inplace / Out-of-place"},{"location":"8/#zero-indexing","text":"Tensors \u662f zero-indexed (\u7d22\u5f15\u4ece\u96f6\u5f00\u59cb)\u8fd9\u662f\u53e6\u5916\u4e00\u4e2a\u4e0d\u540c\u70b9. (\u5728 lua \u4e2d, tensors \u662f one-indexed (\u7d22\u5f15\u4ece\u4e00\u5f00\u59cb)) b = a[0, 3] # \u4ece a \u4e2d\u9009\u62e9\u7b2c\u4e00\u884c\u7b2c\u56db\u5217\u7684\u503c. Tensors \u4e5f\u53ef\u4ee5\u7528 Python \u7684\u5207\u7247\u7d22\u5f15 b = a[:, 3:5] # \u4ece a \u4e2d\u9009\u62e9\u6240\u6709\u884c\u4e2d\u7b2c\u56db\u5217\u548c\u7b2c\u4e94\u5217\u7684\u503c.","title":"Zero Indexing (\u96f6\u7d22\u5f15)"},{"location":"8/#no-camel-casing","text":"\u63a5\u4e0b\u6765\u4e00\u4e2a\u5c0f\u7684\u4e0d\u540c\u662f\u6240\u6709\u7684\u51fd\u6570\u90fd\u4e0d\u662f camelCase \u4e86. \u4f8b\u5982 indexAdd \u73b0\u5728\u88ab\u79f0\u4e3a index_add_ x = torch.ones(5, 5) print(x) z = torch.Tensor(5, 2) z[:, 0] = 10 z[:, 1] = 100 print(z) x.index_add_(1, torch.LongTensor([4, 0]), z) print(x)","title":"No camel casing"},{"location":"8/#numpy-bridge","text":"\u5c06 torch Tensor \u8f6c\u6362\u4e3a\u4e00\u4e2a numpy array, \u53cd\u4e4b\u4ea6\u7136. Torch Tensor \u548c numpy array \u5c06\u4f1a\u5171\u7528\u5e95\u5c42\u7684\u5185\u5b58, \u6539\u53d8\u5176\u4e2d\u4e00\u4e2a, \u53e6\u5916\u4e00\u4e2a\u4e5f\u4f1a\u968f\u4e4b\u6539\u53d8.","title":"Numpy Bridge"},{"location":"8/#torch-tensor-numpy-array","text":"a = torch.ones(5) print(a) b = a.numpy() print(b) a.add_(1) print(a) print(b) # \u770b\u4e00\u4e0b numpy array \u503c\u7684\u53d8\u5316","title":"\u5c06 torch Tensor \u8f6c\u6362\u4e3a numpy Array"},{"location":"8/#numpy-array-torch-tensor","text":"import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out=a) print(a) print(b) # \u770b\u4e00\u4e0b\u901a\u8fc7\u6539\u53d8 np array \u6765\u81ea\u52a8\u7684\u6539\u53d8 torch Tensor \u9664\u4e86 CharTensor \u4e4b\u5916, \u6240\u6709 CPU \u4e0a\u7684 Tensors \u652f\u6301\u8f6c\u53d8\u4e3a NumPy \u5e76\u4e14 \u8f6c\u6362\u56de\u6765.","title":"\u5c06 numpy Array \u8f6c\u6362\u4e3a torch Tensor"},{"location":"8/#cuda-tensors","text":"CUDA Tensors \u5728 pytorch \u4e2d\u975e\u5e38\u597d\u7528, \u5e76\u4e14\u4e00\u4e2a CUDA tensor \u4ece CPU \u8f6c\u6362\u5230 GPU \u4ecd\u5c06\u4fdd\u6301\u5b83\u5e95\u5c42\u7684\u7c7b\u578b. # \u8ba9\u6211\u4eec\u5728 CUDA \u53ef\u7528\u7684\u65f6\u5019\u8fd0\u884c\u8fd9\u4e2a\u5355\u5143 if torch.cuda.is_available(): # \u521b\u5efa\u4e00\u4e2a LongTensor \u5e76\u4e14\u5c06\u5176\u8f6c\u6362\u4f7f\u7528 GPU # \u7684 torch.cuda.LongTensor \u7c7b\u578b a = torch.LongTensor(10).fill_(3).cuda() print(type(a)) b = a.cpu() # \u5c06\u5b83\u8f6c\u6362\u5230 CPU # \u7c7b\u578b\u53d8\u56de torch.LongTensor Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: tensor_tutorial.py Download Jupyter notebook: tensor_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"CUDA Tensors"},{"location":"9/","text":"Autograd (\u81ea\u52a8\u6c42\u5bfc) Autograd \u73b0\u5728\u662f torch \u81ea\u52a8\u5fae\u5206\u7684\u6838\u5fc3\u5305 . \u5b83\u662f\u4f7f\u7528\u57fa\u4e8e tape \u7684\u7cfb\u7edf\u6765\u8fdb\u884c\u81ea\u52a8\u5fae\u5206\u7684. \u5728\u524d\u5411\u9636\u6bb5, autograd tape \u4f1a\u8bb0\u4f4f\u5b83\u6267\u884c\u7684\u6240\u6709\u64cd\u4f5c, \u5728\u53cd\u5411\u9636\u6bb5, \u5b83\u5c06\u91cd\u653e\u8fd9\u4e9b\u64cd\u4f5c Variable (\u53d8\u91cf) \u5728 autograd \u4e2d, \u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a Variable \u7c7b, \u5b83\u662f\u4e00\u4e2a\u975e\u5e38\u5355\u8584 \u7684 Tensor \u5305\u88c5\u5668. \u4f60\u53ef\u4ee5\u901a\u8fc7 .data \u8bbf\u95ee\u5230\u539f\u59cb tensor, \u5e76\u5728\u8ba1\u7b97\u5b8c\u53cd\u5411\u4e4b\u540e, \u6c42\u51fa\u8fd9\u4e2a\u53d8\u91cf\u7684\u68af\u5ea6, \u5e76\u5c06\u8fd9\u4e2a\u68af\u5ea6\u7d2f\u52a0\u5230 .grad \u5c5e\u6027\u4e2d. Variable \u8fd8\u6709\u4e00\u4e2a\u5bf9\u4e8e autograd \u7684\u4f7f\u7528\u975e\u5e38\u91cd\u8981\u7684\u7c7b - Function \u7c7b. Variable \u548c Function \u662f\u76f8\u4e92\u5173\u8054\u7684, \u5e76\u521b\u5efa\u4e86\u4e00\u5f20\u65e0\u73af\u56fe, \u5b83\u8bb0\u5f55\u4e00\u4e2a\u5b8c\u6574\u7684\u8ba1\u7b97\u5386\u53f2. \u6bcf\u4e2a Variable \u7684 .grad_fn \u5c5e\u6027\u90fd\u5f15\u7528\u4e86\u4e00\u4e2a\u8ba1\u7b97\u51fa\u8fd9\u4e2aVariable\u7684\u51fd\u6570 (\u9664\u4e86\u7528\u6237\u521b\u5efa\u7684\u53d8\u91cf\u5916 - \u8fd9\u4e9b\u53d8\u91cf\u7684 .grad_fn \u4e3a None ). \u5982\u679c\u4f60\u60f3\u8981\u8ba1\u7b97\u5bfc\u6570, \u4f60\u53ef\u4ee5\u5728 Variable \u4e0a\u8c03\u7528 .backward() . \u5982\u679c Variable \u662f\u4e00\u4e2a\u6807\u91cf (i.e. \u5b83\u62e5\u6709\u4e00\u4e2atensor\u5143\u7d20), \u5219\u4e0d\u9700\u8981\u4e3a backward() \u6307\u5b9a\u4efb\u4f55\u53c2\u6570, \u4f46\u662f\u5982\u679c\u5b83\u5305\u542b\u8bb8\u591a\u7684\u5143\u7d20, \u5219\u9700\u8981\u6307\u5b9a\u4e00\u4e2a grad_output \u53c2\u6570, \u6765\u5339\u914d tensor \u7684 shape. import torch from torch.autograd import Variable x = Variable(torch.ones(2, 2), requires_grad=True) print(x) # \u6ce8\u610f \"Variable containing\" \u884c print(x.data) print(x.grad) print(x.grad_fn) # \u6211\u4eec\u81ea\u5df1\u521b\u5efa\u7684 x \u5bf9 x \u505a\u4e00\u4e2a\u64cd\u4f5c: y = x + 2 print(y) y \u662f\u7531\u524d\u9762\u8ba1\u7b97\u8fd4\u56de\u7684\u7ed3\u679c\u521b\u5efa\u7684, \u56e0\u6b64\u5b83\u6709\u4e00\u4e2a grad_fn print(y.grad_fn) \u5bf9 y \u505a\u66f4\u591a\u7684\u64cd\u4f5c: z = y * y * 3 out = z.mean() print(z, out) \u68af\u5ea6 \u73b0\u5728, \u8ba9\u6211\u4eec\u6765\u53cd\u5411\u4f20\u64ad, \u5e76\u6253\u5370\u51fa d(out)/dx \u7684\u68af\u5ea6 out.backward() print(x.grad) \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u68af\u5ea6\u8ba1\u7b97\u4f1a\u5237\u65b0\u8ba1\u7b97\u56fe\u4e2d\u5305\u542b\u7684\u6240\u6709\u5185\u90e8\u7f13\u51b2\u533a, \u6240\u4ee5\u5982\u679c\u60a8\u60f3\u8981\u5728\u56fe\u7684\u67d0\u4e2a\u90e8\u5206\u5411\u540e\u6267\u884c\u4e24\u6b21\u68af\u5ea6\u8ba1\u7b97,\u5219\u9700\u8981\u5728 \u7b2c\u4e00\u6b21\u4f20\u9012\u8fc7\u7a0b\u4e2d\u8bbe\u7f6e\u53c2\u6570\u4e3a retain_variables = True . x = Variable(torch.ones(2, 2), requires_grad=True) y = x + 2 y.backward(torch.ones(2, 2), retain_graph=True) # retain_variables \u6807\u5fd7\u5c06\u963b\u6b62\u5185\u90e8\u7f13\u51b2\u533a\u88ab\u91ca\u653e print(x.grad) z = y * y print(z) \u53ea\u662f\u53cd\u5411\u4f20\u64ad\u968f\u673a\u68af\u5ea6 gradient = torch.randn(2, 2) # \u5982\u679c\u6211\u4eec\u6ca1\u6709\u6307\u5b9a\u6211\u4eec\u60f3\u4fdd\u7559\u53d8\u91cf, \u8fd9\u5c06\u4f1a\u5931\u8d25 y.backward(gradient) print(x.grad) Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: autograd_tutorial.py Download Jupyter notebook: autograd_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"Autograd (\u81ea\u52a8\u6c42\u5bfc)"},{"location":"9/#autograd","text":"Autograd \u73b0\u5728\u662f torch \u81ea\u52a8\u5fae\u5206\u7684\u6838\u5fc3\u5305 . \u5b83\u662f\u4f7f\u7528\u57fa\u4e8e tape \u7684\u7cfb\u7edf\u6765\u8fdb\u884c\u81ea\u52a8\u5fae\u5206\u7684. \u5728\u524d\u5411\u9636\u6bb5, autograd tape \u4f1a\u8bb0\u4f4f\u5b83\u6267\u884c\u7684\u6240\u6709\u64cd\u4f5c, \u5728\u53cd\u5411\u9636\u6bb5, \u5b83\u5c06\u91cd\u653e\u8fd9\u4e9b\u64cd\u4f5c","title":"Autograd (\u81ea\u52a8\u6c42\u5bfc)"},{"location":"9/#variable","text":"\u5728 autograd \u4e2d, \u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a Variable \u7c7b, \u5b83\u662f\u4e00\u4e2a\u975e\u5e38\u5355\u8584 \u7684 Tensor \u5305\u88c5\u5668. \u4f60\u53ef\u4ee5\u901a\u8fc7 .data \u8bbf\u95ee\u5230\u539f\u59cb tensor, \u5e76\u5728\u8ba1\u7b97\u5b8c\u53cd\u5411\u4e4b\u540e, \u6c42\u51fa\u8fd9\u4e2a\u53d8\u91cf\u7684\u68af\u5ea6, \u5e76\u5c06\u8fd9\u4e2a\u68af\u5ea6\u7d2f\u52a0\u5230 .grad \u5c5e\u6027\u4e2d. Variable \u8fd8\u6709\u4e00\u4e2a\u5bf9\u4e8e autograd \u7684\u4f7f\u7528\u975e\u5e38\u91cd\u8981\u7684\u7c7b - Function \u7c7b. Variable \u548c Function \u662f\u76f8\u4e92\u5173\u8054\u7684, \u5e76\u521b\u5efa\u4e86\u4e00\u5f20\u65e0\u73af\u56fe, \u5b83\u8bb0\u5f55\u4e00\u4e2a\u5b8c\u6574\u7684\u8ba1\u7b97\u5386\u53f2. \u6bcf\u4e2a Variable \u7684 .grad_fn \u5c5e\u6027\u90fd\u5f15\u7528\u4e86\u4e00\u4e2a\u8ba1\u7b97\u51fa\u8fd9\u4e2aVariable\u7684\u51fd\u6570 (\u9664\u4e86\u7528\u6237\u521b\u5efa\u7684\u53d8\u91cf\u5916 - \u8fd9\u4e9b\u53d8\u91cf\u7684 .grad_fn \u4e3a None ). \u5982\u679c\u4f60\u60f3\u8981\u8ba1\u7b97\u5bfc\u6570, \u4f60\u53ef\u4ee5\u5728 Variable \u4e0a\u8c03\u7528 .backward() . \u5982\u679c Variable \u662f\u4e00\u4e2a\u6807\u91cf (i.e. \u5b83\u62e5\u6709\u4e00\u4e2atensor\u5143\u7d20), \u5219\u4e0d\u9700\u8981\u4e3a backward() \u6307\u5b9a\u4efb\u4f55\u53c2\u6570, \u4f46\u662f\u5982\u679c\u5b83\u5305\u542b\u8bb8\u591a\u7684\u5143\u7d20, \u5219\u9700\u8981\u6307\u5b9a\u4e00\u4e2a grad_output \u53c2\u6570, \u6765\u5339\u914d tensor \u7684 shape. import torch from torch.autograd import Variable x = Variable(torch.ones(2, 2), requires_grad=True) print(x) # \u6ce8\u610f \"Variable containing\" \u884c print(x.data) print(x.grad) print(x.grad_fn) # \u6211\u4eec\u81ea\u5df1\u521b\u5efa\u7684 x \u5bf9 x \u505a\u4e00\u4e2a\u64cd\u4f5c: y = x + 2 print(y) y \u662f\u7531\u524d\u9762\u8ba1\u7b97\u8fd4\u56de\u7684\u7ed3\u679c\u521b\u5efa\u7684, \u56e0\u6b64\u5b83\u6709\u4e00\u4e2a grad_fn print(y.grad_fn) \u5bf9 y \u505a\u66f4\u591a\u7684\u64cd\u4f5c: z = y * y * 3 out = z.mean() print(z, out)","title":"Variable (\u53d8\u91cf)"},{"location":"9/#_1","text":"\u73b0\u5728, \u8ba9\u6211\u4eec\u6765\u53cd\u5411\u4f20\u64ad, \u5e76\u6253\u5370\u51fa d(out)/dx \u7684\u68af\u5ea6 out.backward() print(x.grad) \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u68af\u5ea6\u8ba1\u7b97\u4f1a\u5237\u65b0\u8ba1\u7b97\u56fe\u4e2d\u5305\u542b\u7684\u6240\u6709\u5185\u90e8\u7f13\u51b2\u533a, \u6240\u4ee5\u5982\u679c\u60a8\u60f3\u8981\u5728\u56fe\u7684\u67d0\u4e2a\u90e8\u5206\u5411\u540e\u6267\u884c\u4e24\u6b21\u68af\u5ea6\u8ba1\u7b97,\u5219\u9700\u8981\u5728 \u7b2c\u4e00\u6b21\u4f20\u9012\u8fc7\u7a0b\u4e2d\u8bbe\u7f6e\u53c2\u6570\u4e3a retain_variables = True . x = Variable(torch.ones(2, 2), requires_grad=True) y = x + 2 y.backward(torch.ones(2, 2), retain_graph=True) # retain_variables \u6807\u5fd7\u5c06\u963b\u6b62\u5185\u90e8\u7f13\u51b2\u533a\u88ab\u91ca\u653e print(x.grad) z = y * y print(z) \u53ea\u662f\u53cd\u5411\u4f20\u64ad\u968f\u673a\u68af\u5ea6 gradient = torch.randn(2, 2) # \u5982\u679c\u6211\u4eec\u6ca1\u6709\u6307\u5b9a\u6211\u4eec\u60f3\u4fdd\u7559\u53d8\u91cf, \u8fd9\u5c06\u4f1a\u5931\u8d25 y.backward(gradient) print(x.grad) Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: autograd_tutorial.py Download Jupyter notebook: autograd_tutorial.ipynb Gallery generated by Sphinx-Gallery","title":"\u68af\u5ea6"}]}