



<!DOCTYPE html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="PyTorch 0.3.0 中文文档">
      
      
        <link rel="canonical" href="http://pytorch.apachecn.org/35/">
      
      
        <meta name="author" content="ApacheCN Team">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.3">
    
    
      
        <title>Writing Distributed Applications with PyTorch - PyTorch 0.3.0 中文文档</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#ff7043">
      
    
    
      <script src="../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    

    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        }
    });
    </script>
    
    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>
    
    <!-- google webmaster -->
    <meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo" />
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="deep-orange" data-md-color-accent="deep-orange">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="../#writing-distributed-applications-with-pytorch" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="http://pytorch.apachecn.org" title="PyTorch 0.3.0 中文文档" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                PyTorch 0.3.0 中文文档
              </span>
              <span class="md-header-nav__topic">
                Writing Distributed Applications with PyTorch
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/apachecn/pytorch-doc-zh/" title="前往 Github 仓库" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      pytorch-doc-zh
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href=".." title="主页" class="md-tabs__link">
          主页
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../0/" title="中文教程" class="md-tabs__link md-tabs__link--active">
          中文教程
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../45/" title="中文文档" class="md-tabs__link">
          中文文档
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="http://pytorch.apachecn.org" title="PyTorch 0.3.0 中文文档" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    PyTorch 0.3.0 中文文档
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/apachecn/pytorch-doc-zh/" title="前往 Github 仓库" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      pytorch-doc-zh
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      主页
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        主页
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href=".." title="主页" class="md-nav__link">
      主页
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      中文教程
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        中文教程
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../0/" title="初学者教程" class="md-nav__link">
      初学者教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../1/" title="PyTorch 深度学习: 60 分钟极速入门教程" class="md-nav__link">
      PyTorch 深度学习: 60 分钟极速入门教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../2/" title="PyTorch 是什么？" class="md-nav__link">
      PyTorch 是什么？
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../3/" title="自动求导: 自动微分" class="md-nav__link">
      自动求导: 自动微分
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../4/" title="神经网络" class="md-nav__link">
      神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../5/" title="训练一个分类器" class="md-nav__link">
      训练一个分类器
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../6/" title="可选: 数据并行" class="md-nav__link">
      可选: 数据并行
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../7/" title="PyTorch for former Torch users" class="md-nav__link">
      PyTorch for former Torch users
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../8/" title="Tensors" class="md-nav__link">
      Tensors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../9/" title="Autograd (自动求导)" class="md-nav__link">
      Autograd (自动求导)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../10/" title="nn package" class="md-nav__link">
      nn package
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../11/" title="Multi-GPU examples" class="md-nav__link">
      Multi-GPU examples
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../12/" title="跟着例子学习 PyTorch" class="md-nav__link">
      跟着例子学习 PyTorch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../13/" title="Warm-up: numpy" class="md-nav__link">
      Warm-up: numpy
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../14/" title="PyTorch: Tensors" class="md-nav__link">
      PyTorch: Tensors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../15/" title="PyTorch: 变量和autograd" class="md-nav__link">
      PyTorch: 变量和autograd
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../16/" title="PyTorch: 定义新的autograd函数" class="md-nav__link">
      PyTorch: 定义新的autograd函数
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../17/" title="TensorFlow: 静态图" class="md-nav__link">
      TensorFlow: 静态图
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../18/" title="PyTorch: nn包" class="md-nav__link">
      PyTorch: nn包
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../19/" title="PyTorch: optim包" class="md-nav__link">
      PyTorch: optim包
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../20/" title="PyTorch: 定制化nn模块" class="md-nav__link">
      PyTorch: 定制化nn模块
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../21/" title="PyTorch: 动态控制流程 + 权重共享" class="md-nav__link">
      PyTorch: 动态控制流程 + 权重共享
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../22/" title="迁移学习教程" class="md-nav__link">
      迁移学习教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../23/" title="数据加载和处理教程" class="md-nav__link">
      数据加载和处理教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../24/" title="针对NLP的Pytorch深度学习" class="md-nav__link">
      针对NLP的Pytorch深度学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../25/" title="PyTorch介绍" class="md-nav__link">
      PyTorch介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../26/" title="PyTorch深度学习" class="md-nav__link">
      PyTorch深度学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../27/" title="词汇嵌入:编码词汇语义" class="md-nav__link">
      词汇嵌入:编码词汇语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../28/" title="序列模型和 LSTM 网络（长短记忆网络）" class="md-nav__link">
      序列模型和 LSTM 网络（长短记忆网络）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../29/" title="高级教程: 作出动态决策和 Bi-LSTM CRF" class="md-nav__link">
      高级教程: 作出动态决策和 Bi-LSTM CRF
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../30/" title="中级教程" class="md-nav__link">
      中级教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../31/" title="用字符级RNN分类名称" class="md-nav__link">
      用字符级RNN分类名称
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../32/" title="基与字符级RNN（Char-RNN）的人名生成" class="md-nav__link">
      基与字符级RNN（Char-RNN）的人名生成
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../33/" title="用基于注意力机制的seq2seq神经网络进行翻译" class="md-nav__link">
      用基于注意力机制的seq2seq神经网络进行翻译
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../34/" title="强化学习（DQN）教程" class="md-nav__link">
      强化学习（DQN）教程
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Writing Distributed Applications with PyTorch
      </label>
    
    <a href="./" title="Writing Distributed Applications with PyTorch" class="md-nav__link md-nav__link--active">
      Writing Distributed Applications with PyTorch
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#setup" title="Setup" class="md-nav__link">
    Setup
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#point-to-point-communication" title="Point-to-Point Communication" class="md-nav__link">
    Point-to-Point Communication
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#collective-communication" title="Collective Communication" class="md-nav__link">
    Collective Communication
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#distributed-training" title="Distributed Training" class="md-nav__link">
    Distributed Training
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#our-own-ring-allreduce" title="Our Own Ring-Allreduce" class="md-nav__link">
    Our Own Ring-Allreduce
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-topics" title="Advanced Topics" class="md-nav__link">
    Advanced Topics
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#communication-backends" title="Communication Backends" class="md-nav__link">
    Communication Backends
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#initialization-methods" title="Initialization Methods" class="md-nav__link">
    Initialization Methods
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../36/" title="空间转换网络 (Spatial Transformer Networks) 教程" class="md-nav__link">
      空间转换网络 (Spatial Transformer Networks) 教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../37/" title="高级教程" class="md-nav__link">
      高级教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../38/" title="用 PyTorch 做 神经转换 (Neural Transfer)" class="md-nav__link">
      用 PyTorch 做 神经转换 (Neural Transfer)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../39/" title="使用 numpy 和 scipy 创建扩展" class="md-nav__link">
      使用 numpy 和 scipy 创建扩展
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../40/" title="使用 ONNX 将模型从 PyTorch 迁移到 Caffe2 和 Mobile" class="md-nav__link">
      使用 ONNX 将模型从 PyTorch 迁移到 Caffe2 和 Mobile
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../41/" title="为 pytorch 自定义 C 扩展" class="md-nav__link">
      为 pytorch 自定义 C 扩展
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../42/" title="项目相关" class="md-nav__link">
      项目相关
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../43/" title="项目贡献者" class="md-nav__link">
      项目贡献者
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../44/" title="组织学习交流群" class="md-nav__link">
      组织学习交流群
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      中文文档
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        中文文档
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../45/" title="介绍" class="md-nav__link">
      介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../46/" title="自动求导机制" class="md-nav__link">
      自动求导机制
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../47/" title="广播语义" class="md-nav__link">
      广播语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../48/" title="CUDA 语义" class="md-nav__link">
      CUDA 语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../49/" title="扩展 PyTorch" class="md-nav__link">
      扩展 PyTorch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../50/" title="多进程的最佳实践" class="md-nav__link">
      多进程的最佳实践
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../51/" title="序列化语义" class="md-nav__link">
      序列化语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../52/" title="Package 参考" class="md-nav__link">
      Package 参考
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../53/" title="torch" class="md-nav__link">
      torch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../54/" title="torch.Tensor" class="md-nav__link">
      torch.Tensor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../55/" title="torch.sparse" class="md-nav__link">
      torch.sparse
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../56/" title="torch.Storage" class="md-nav__link">
      torch.Storage
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../57/" title="torch.nn" class="md-nav__link">
      torch.nn
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../58/" title="torch.optim" class="md-nav__link">
      torch.optim
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../59/" title="Automatic differentiation package - torch.autograd" class="md-nav__link">
      Automatic differentiation package - torch.autograd
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../60/" title="Probability distributions - torch.distributions" class="md-nav__link">
      Probability distributions - torch.distributions
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../61/" title="Multiprocessing package - torch.multiprocessing" class="md-nav__link">
      Multiprocessing package - torch.multiprocessing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../62/" title="Distributed communication package - torch.distributed" class="md-nav__link">
      Distributed communication package - torch.distributed
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../63/" title="Legacy package - torch.legacy" class="md-nav__link">
      Legacy package - torch.legacy
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../64/" title="torch.cuda" class="md-nav__link">
      torch.cuda
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../65/" title="torch.utils.ffi" class="md-nav__link">
      torch.utils.ffi
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../66/" title="torch.utils.data" class="md-nav__link">
      torch.utils.data
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../67/" title="torch.utils.model_zoo" class="md-nav__link">
      torch.utils.model_zoo
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../68/" title="torch.onnx" class="md-nav__link">
      torch.onnx
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../69/" title="torchvision 参考" class="md-nav__link">
      torchvision 参考
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../70/" title="torchvision" class="md-nav__link">
      torchvision
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../71/" title="torchvision.datasets" class="md-nav__link">
      torchvision.datasets
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../72/" title="torchvision.models" class="md-nav__link">
      torchvision.models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../73/" title="torchvision.transforms" class="md-nav__link">
      torchvision.transforms
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../74/" title="torchvision.utils" class="md-nav__link">
      torchvision.utils
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../75/" title="项目相关" class="md-nav__link">
      项目相关
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../76/" title="项目贡献者" class="md-nav__link">
      项目贡献者
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../77/" title="组织学习交流群" class="md-nav__link">
      组织学习交流群
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#setup" title="Setup" class="md-nav__link">
    Setup
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#point-to-point-communication" title="Point-to-Point Communication" class="md-nav__link">
    Point-to-Point Communication
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#collective-communication" title="Collective Communication" class="md-nav__link">
    Collective Communication
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#distributed-training" title="Distributed Training" class="md-nav__link">
    Distributed Training
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#our-own-ring-allreduce" title="Our Own Ring-Allreduce" class="md-nav__link">
    Our Own Ring-Allreduce
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-topics" title="Advanced Topics" class="md-nav__link">
    Advanced Topics
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#communication-backends" title="Communication Backends" class="md-nav__link">
    Communication Backends
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#initialization-methods" title="Initialization Methods" class="md-nav__link">
    Initialization Methods
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/apachecn/pytorch-doc-zh/edit/master/docs/35.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="writing-distributed-applications-with-pytorch">Writing Distributed Applications with PyTorch</h1>
<p><strong>Author</strong>: <a href="http://seba1511.com">Séb Arnold</a></p>
<p>In this short tutorial, we will be going over the distributed package of PyTorch. We’ll see how to set up the distributed setting, use the different communication strategies, and go over some the internals of the package.</p>
<h2 id="setup">Setup</h2>
<p>The distributed package included in PyTorch (i.e., <code>torch.distributed</code>) enables researchers and practitioners to easily parallelize their computations across processes and clusters of machines. To do so, it leverages the messaging passing semantics allowing each process to communicate data to any of the other processes. As opposed to the multiprocessing (<code>torch.multiprocessing</code>) package, processes can use different communication backends and are not restricted to being executed on the same machine.</p>
<p>In order to get started we need the ability to run multiple processes simultaneously. If you have access to compute cluster you should check with your local sysadmin or use your favorite coordination tool. (e.g., <a href="https://linux.die.net/man/1/pdsh">pdsh</a>, <a href="http://cea-hpc.github.io/clustershell/">clustershell</a>, or <a href="https://slurm.schedmd.com/">others</a>) For the purpose of this tutorial, we will use a single machine and fork multiple processes using the following template.</p>
<pre class="codehilite"><code class="language-py">&quot;&quot;&quot;run.py:&quot;&quot;&quot;
#!/usr/bin/env python
import os
import torch
import torch.distributed as dist
from torch.multiprocessing import Process

def run(rank, size):
    &quot;&quot;&quot; Distributed function to be implemented later. &quot;&quot;&quot;
    pass

def init_processes(rank, size, fn, backend='tcp'):
    &quot;&quot;&quot; Initialize the distributed environment. &quot;&quot;&quot;
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '29500'
    dist.init_process_group(backend, rank=rank, world_size=size)
    fn(rank, size)

if __name__ == &quot;__main__&quot;:
    size = 2
    processes = []
    for rank in range(size):
        p = Process(target=init_processes, args=(rank, size, run))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()</code></pre>


<p>The above script spawns two processes who will each setup the distributed environment, initialize the process group (<code>dist.init_process_group</code>), and finally execute the given <code>run</code> function.</p>
<p>Let’s have a look at the <code>init_processes</code> function. It ensures that every process will be able to coordinate through a master, using the same ip address and port. Note that we used the TCP backend, but we could have used <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a> or <a href="http://github.com/facebookincubator/gloo">Gloo</a> instead. (c.f. <a href="#communication-backends">Section 5.1</a>) We will go over the magic happening in <code>dist.init_process_group</code> at the end of this tutorial, but it essentially allows processes to communicate with each other by sharing their locations.</p>
<h2 id="point-to-point-communication">Point-to-Point Communication</h2>
<p><a href="http://pytorch.apachecn.org/cn/tutorials/_images/send_recv.png"><img alt="Send and Recv" src="../img/d272e50143a78046107f9ff8bc6de612.jpg" /></a></p>
<p>Send and Recv</p>
<p>A transfer of data from one process to another is called a point-to-point communication. These are achieved through the <code>send</code> and <code>recv</code> functions or their <em>immediate</em> counter-parts, <code>isend</code> and <code>irecv</code>.</p>
<pre class="codehilite"><code class="language-py">&quot;&quot;&quot;Blocking point-to-point communication.&quot;&quot;&quot;

def run(rank, size):
    tensor = torch.zeros(1)
    if rank == 0:
        tensor += 1
        # Send the tensor to process 1
        dist.send(tensor=tensor, dst=1)
    else:
        # Receive tensor from process 0
        dist.recv(tensor=tensor, src=0)
    print('Rank ', rank, ' has data ', tensor[0])</code></pre>


<p>In the above example, both processes start with a zero tensor, then process 0 increments the tensor and sends it to process 1 so that they both end up with 1.0. Notice that process 1 needs to allocate memory in order to store the data it will receive.</p>
<p>Also notice that <code>send</code>/<code>recv</code> are <strong>blocking</strong>: both processes stop until the communication is completed. On the other hand immediates are <strong>non-blocking</strong>; the script continues its execution and the methods return a <code>DistributedRequest</code> object upon which we can choose to <code>wait()</code>.</p>
<pre class="codehilite"><code class="language-py">&quot;&quot;&quot;Non-blocking point-to-point communication.&quot;&quot;&quot;

def run(rank, size):
    tensor = torch.zeros(1)
    req = None
    if rank == 0:
        tensor += 1
        # Send the tensor to process 1
        req = dist.isend(tensor=tensor, dst=1)
        print('Rank 0 started sending')
    else:
        # Receive tensor from process 0
        req = dist.irecv(tensor=tensor, src=0)
        print('Rank 1 started receiving')
    req.wait()
    print('Rank ', rank, ' has data ', tensor[0])</code></pre>


<p>When using immediates we have to be careful about with our usage of the sent and received tensors. Since we do not know when the data will be communicated to the other process, we should not modify the sent tensor nor access the received tensor before <code>req.wait()</code> has completed. In other words,</p>
<ul>
<li>writing to <code>tensor</code> after <code>dist.isend()</code> will result in undefined behaviour.</li>
<li>reading from <code>tensor</code> after <code>dist.irecv()</code> will result in undefined behaviour.</li>
</ul>
<p>However, after <code>req.wait()</code> has been executed we are guaranteed that the communication took place, and that the value stored in <code>tensor[0]</code> is 1.0.</p>
<p>Point-to-point communication is useful when we want a fine-grained control over the communication of our processes. They can be used to implement fancy algorithms, such as the one used in <a href="https://github.com/baidu-research/baidu-allreduce">Baidu’s DeepSpeech</a> or <a href="https://research.fb.com/publications/imagenet1kin1h/">Facebook’s large-scale experiments</a>.(c.f. <a href="#our-own-ring-allreduce">Section 4.1</a>)</p>
<h2 id="collective-communication">Collective Communication</h2>
<p>| <a href="http://pytorch.apachecn.org/cn/tutorials/_images/scatter.png"><img alt="Scatter" src="../img/9604889913cb38d8a12cc4b86ae3987a.jpg" /></a></p>
<p>Scatter</p>
<p>| <a href="http://pytorch.apachecn.org/cn/tutorials/_images/gather.png"><img alt="Gather" src="../img/1eaf8e073bae07dc68de94f3277b4855.jpg" /></a></p>
<p>Gather</p>
<p>|
| <a href="http://pytorch.apachecn.org/cn/tutorials/_images/reduce.png"><img alt="Reduce" src="../img/bc5c32aa37249bae805767e5f8010ab5.jpg" /></a></p>
<p>Reduce</p>
<p>| <a href="http://pytorch.apachecn.org/cn/tutorials/_images/all_reduce.png"><img alt="All-Reduce" src="../img/2d539c22f0f4a1730aa475b64cb5c216.jpg" /></a></p>
<p>All-Reduce</p>
<p>|
| <a href="http://pytorch.apachecn.org/cn/tutorials/_images/broadcast.png"><img alt="Broadcast" src="../img/46331a7401d267cc68c92b76d3a20b19.jpg" /></a></p>
<p>Broadcast</p>
<p>| <a href="http://pytorch.apachecn.org/cn/tutorials/_images/all_gather.png"><img alt="All-Gather" src="../img/750dd2f358e9c7105326f7c5f1fc6f80.jpg" /></a></p>
<p>All-Gather</p>
<p>|</p>
<p>As opposed to point-to-point communcation, collectives allow for communication patterns across all processes in a <strong>group</strong>. A group is a subset of all our processes. To create a group, we can pass a list of ranks to <code>dist.new_group(group)</code>. By default, collectives are executed on the all processes, also known as the <strong>world</strong>. For example, in order to obtain the sum of all tensors at all processes, we can use the <code>dist.all_reduce(tensor, op, group)</code> collective.</p>
<pre class="codehilite"><code class="language-py">&quot;&quot;&quot; All-Reduce example.&quot;&quot;&quot;
def run(rank, size):
    &quot;&quot;&quot; Simple point-to-point communication. &quot;&quot;&quot;
    group = dist.new_group([0, 1])
    tensor = torch.ones(1)
    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)
    print('Rank ', rank, ' has data ', tensor[0])</code></pre>


<p>Since we want the sum of all tensors in the group, we use <code>dist.reduce_op.SUM</code> as the reduce operator. Generally speaking, any commutative mathematical operation can be used as an operator. Out-of-the-box, PyTorch comes with 4 such operators, all working at the element-wise level:</p>
<ul>
<li><code>dist.reduce_op.SUM</code>,</li>
<li><code>dist.reduce_op.PRODUCT</code>,</li>
<li><code>dist.reduce_op.MAX</code>,</li>
<li><code>dist.reduce_op.MIN</code>.</li>
</ul>
<p>In addition to <code>dist.all_reduce(tensor, op, group)</code>, there are a total of 6 collectives currently implemented in PyTorch.</p>
<ul>
<li><code>dist.broadcast(tensor, src, group)</code>: Copies <code>tensor</code> from <code>src</code> to all other processes.</li>
<li><code>dist.reduce(tensor, dst, op, group)</code>: Applies <code>op</code> to all <code>tensor</code> and stores the result in <code>dst</code>.</li>
<li><code>dist.all_reduce(tensor, op, group)</code>: Same as reduce, but the result is stored in all processes.</li>
<li><code>dist.scatter(tensor, src, scatter_list, group)</code>: Copies the <code>\(i^{\text{th}}\)</code> tensor <code>scatter_list[i]</code> to the <code>\(i^{\text{th}}\)</code> process.</li>
<li><code>dist.gather(tensor, dst, gather_list, group)</code>: Copies <code>tensor</code> from all processes in <code>dst</code>.</li>
<li><code>dist.all_gather(tensor_list, tensor, group)</code>: Copies <code>tensor</code> from all processes to <code>tensor_list</code>, on all processes.</li>
</ul>
<h2 id="distributed-training">Distributed Training</h2>
<p><strong>Note:</strong> You can find the example script of this section in <a href="https://github.com/seba-1511/dist_tuto.pth/">this GitHub repository</a>.</p>
<p>Now that we understand how the distributed module works, let us write something useful with it. Our goal will be to replicate the functionality of <a href="http://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel">DistributedDataParallel</a>. Of course, this will be a didactic example and in a real-world situtation you should use the official, well-tested and well-optimized version linked above.</p>
<p>Quite simply we want to implement a distributed version of stochastic gradient descent. Our script will let all processes compute the gradients of their model on their batch of data and then average their gradients. In order to ensure similar convergence results when changing the number of processes, we will first have to partition our dataset. (You could also use <a href="https://github.com/pytorch/tnt/blob/master/torchnet/dataset/splitdataset.py#L4">tnt.dataset.SplitDataset</a>, instead of the snippet below.)</p>
<pre class="codehilite"><code class="language-py">&quot;&quot;&quot; Dataset partitioning helper &quot;&quot;&quot;
class Partition(object):

    def __init__(self, data, index):
        self.data = data
        self.index = index

    def __len__(self):
        return len(self.index)

    def __getitem__(self, index):
        data_idx = self.index[index]
        return self.data[data_idx]

class DataPartitioner(object):

    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):
        self.data = data
        self.partitions = []
        rng = Random()
        rng.seed(seed)
        data_len = len(data)
        indexes = [x for x in range(0, data_len)]
        rng.shuffle(indexes)

        for frac in sizes:
            part_len = int(frac * data_len)
            self.partitions.append(indexes[0:part_len])
            indexes = indexes[part_len:]

    def use(self, partition):
        return Partition(self.data, self.partitions[partition])</code></pre>


<p>With the above snippet, we can now simply partition any dataset using the following few lines:</p>
<pre class="codehilite"><code class="language-py">&quot;&quot;&quot; Partitioning MNIST &quot;&quot;&quot;
def partition_dataset():
    dataset = datasets.MNIST('./data', train=True, download=True,
                             transform=transforms.Compose([
                                 transforms.ToTensor(),
                                 transforms.Normalize((0.1307,), (0.3081,))
                             ]))
    size = dist.get_world_size()
    bsz = 128 / float(size)
    partition_sizes = [1.0 / size for _ in range(size)]
    partition = DataPartitioner(dataset, partition_sizes)
    partition = partition.use(dist.get_rank())
    train_set = torch.utils.data.DataLoader(partition,
                                         batch_size=bsz,
                                         shuffle=True)
    return train_set, bsz</code></pre>


<p>Assuming we have 2 replicas, then each process will have a <code>train_set</code> of 60000 / 2 = 30000 samples. We also divide the batch size by the number of replicas in order to maintain the <em>overall</em> batch size of 128.</p>
<p>We can now write our usual forward-backward-optimize training code, and add a function call to average the gradients of our models. (The following is largely inspired from the official <a href="https://github.com/pytorch/examples/blob/master/mnist/main.py">PyTorch MNIST example</a>.)</p>
<pre class="codehilite"><code class="language-py">&quot;&quot;&quot; Distributed Synchronous SGD Example &quot;&quot;&quot;
def run(rank, size):
        torch.manual_seed(1234)
        train_set, bsz = partition_dataset()
        model = Net()
        optimizer = optim.SGD(model.parameters(),
                              lr=0.01, momentum=0.5)

        num_batches = ceil(len(train_set.dataset) / float(bsz))
        for epoch in range(10):
            epoch_loss = 0.0
            for data, target in train_set:
                data, target = Variable(data), Variable(target)
                optimizer.zero_grad()
                output = model(data)
                loss = F.nll_loss(output, target)
                epoch_loss += loss.data[0]
                loss.backward()
                average_gradients(model)
                optimizer.step()
            print('Rank ', dist.get_rank(), ', epoch ',
                  epoch, ': ', epoch_loss / num_batches)</code></pre>


<p>It remains to implement the <code>average_gradients(model)</code> function, which simply takes in a model and averages its gradients across the whole world.</p>
<pre class="codehilite"><code class="language-py">&quot;&quot;&quot; Gradient averaging. &quot;&quot;&quot;
def average_gradients(model):
    size = float(dist.get_world_size())
    for param in model.parameters():
        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)
        param.grad.data /= size</code></pre>


<p><em>Et voilà</em>! We successfully implemented distributed synchronous SGD and could train any model on a large computer cluster.</p>
<p><strong>Note:</strong> While the last sentence is <em>technically</em> true, there are <a href="http://seba-1511.github.io/dist_blog">a lot more tricks</a> required to implement a production-level implementation of synchronous SGD. Again, use what <a href="http://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel">has been tested and optimized</a>.</p>
<h3 id="our-own-ring-allreduce">Our Own Ring-Allreduce</h3>
<p>As an additional challenge, imagine that we wanted to implement DeepSpeech’s efficient ring allreduce. This is fairly easily implemented using point-to-point collectives.</p>
<pre class="codehilite"><code class="language-py">&quot;&quot;&quot; Implementation of a ring-reduce with addition. &quot;&quot;&quot;
def allreduce(send, recv):
    rank = dist.get_rank()
    size = dist.get_world_size()
    send_buff = th.zeros(send.size())
    recv_buff = th.zeros(send.size())
    accum = th.zeros(send.size())
    accum[:] = send[:]

    left = ((rank - 1) + size) % size
    right = (rank + 1) % size

    for i in range(size - 1):
        if i % 2 == 0:
            # Send send_buff
            send_req = dist.isend(send_buff, right)
            dist.recv(recv_buff, left)
            accum[:] += recv[:]
        else:
            # Send recv_buff
            send_req = dist.isend(recv_buff, right)
            dist.recv(send_buff, left)
            accum[:] += send[:]
        send_req.wait()
    recv[:] = accum[:]</code></pre>


<p>In the above script, the <code>allreduce(send, recv)</code> function has a slightly different signature than the ones in PyTorch. It takes a <code>recv</code> tensor and will store the sum of all <code>send</code> tensors in it. As an exercise left to the reader, there is still one difference between our version and the one in DeepSpeech: their implementation divide the gradient tensor into <em>chunks</em>, so as to optimially utilize the communication bandwidth. (Hint: <a href="http://pytorch.org/docs/master/torch.html#torch.chunk">toch.chunk</a>)</p>
<h2 id="advanced-topics">Advanced Topics</h2>
<p>We are now ready to discover some of the more advanced functionalities of <code>torch.distributed</code>. Since there is a lot to cover, this section is divided into two subsections:</p>
<ol>
<li>Communication Backends: where we learn how to use MPI and Gloo for GPU-GPU communication.</li>
<li>Initialization Methods: where we understand how to best setup the initial coordination phase in <code>dist.init_process_group()</code>.</li>
</ol>
<h3 id="communication-backends">Communication Backends</h3>
<p>One of the most elegant aspects of <code>torch.distributed</code> is its ability to abstract and build on top of different backends. As mentioned before, there are currently three backends implemented in PyTorch: TCP, MPI, and Gloo. They each have different specifications and tradeoffs, depending on the desired use-case. A comparative table of supported functions can be found <a href="http://pytorch.org/docs/master/distributed.html#module-torch.distributed">here</a>.</p>
<p><strong>TCP Backend</strong></p>
<p>So far we have made extensive usage of the TCP backend. It is quite handy as a development platform, as it is guaranteed to work on most machines and operating systems. It also supports all point-to-point and collective functions on CPU. However, there is no support for GPUs and its communication routines are not as optimized as the MPI one.</p>
<p><strong>Gloo Backend</strong></p>
<p>The <a href="https://github.com/facebookincubator/gloo">Gloo backend</a> provides an optimized implementation of <em>collective</em> communication procedures, both for CPUs and GPUs. It particularly shines on GPUs as it can perform communication without transferring data to the CPU’s memory using <a href="https://developer.nvidia.com/gpudirect">GPUDirect</a>. It is also capable of using <a href="https://github.com/NVIDIA/nccl">NCCL</a> to perform fast intra-node communication and implements its <a href="https://github.com/facebookincubator/gloo/blob/master/docs/algorithms.md">own algorithms</a> for inter-node routines.</p>
<p>Since version 0.2.0, the Gloo backend is automatically included with the pre-compiled binaries of PyTorch. As you have surely noticed, our distributed SGD example does not work if you put <code>model</code> on the GPU. Let’s fix it by first replacing <code>backend='gloo'</code> in <code>init_processes(rank, size, fn, backend='tcp')</code>. At this point, the script will still run on CPU but uses the Gloo backend behind the scenes. In order to use multiple GPUs, let us also do the following modifications:</p>
<ol>
<li><code>init_processes(rank, size, fn, backend='tcp')</code> <code>\(\rightarrow\)</code> <code>init_processes(rank, size, fn, backend='gloo')</code></li>
<li><code>model = Net()</code> <code>\(\rightarrow\)</code> <code>model = Net().cuda(rank)</code></li>
<li><code>data, target = Variable(data), Variable(target)</code> <code>\(\rightarrow\)</code> <code>data, target = Variable(data.cuda(rank)), Variable(target.cuda(rank))</code></li>
</ol>
<p>With the above modifications, our model is now training on two GPUs and you can monitor their utilization with <code>watch nvidia-smi</code>.</p>
<p><strong>MPI Backend</strong></p>
<p>The Message Passing Interface (MPI) is a standardized tool from the field of high-performance computing. It allows to do point-to-point and collective communications and was the main inspiration for the API of <code>torch.distributed</code>. Several implementations of MPI exist (e.g. <a href="https://www.open-mpi.org/">Open-MPI</a>, <a href="http://mvapich.cse.ohio-state.edu/">MVAPICH2</a>, <a href="https://software.intel.com/en-us/intel-mpi-library">Intel MPI</a>) each optimized for different purposes. The advantage of using the MPI backend lies in MPI’s wide availability - and high-level of optimization - on large computer clusters. <a href="https://developer.nvidia.com/mvapich">Some</a> <a href="https://developer.nvidia.com/ibm-spectrum-mpi">recent</a> <a href="http://www.open-mpi.org/">implementations</a> are also able to take advantage of CUDA IPC and GPU Direct technologies in order to avoid memory copies through the CPU.</p>
<p>Unfortunately, PyTorch’s binaries can not include an MPI implementation and we’ll have to recompile it by hand. Fortunately, this process is fairly simple given that upon compilation, PyTorch will look <em>by itself</em> for an available MPI implementation. The following steps install the MPI backend, by installing PyTorch <a href="https://github.com/pytorch/pytorch#from-source">from sources</a>.</p>
<ol>
<li>Create and activate your Anaconda environment, install all the pre-requisites following <a href="https://github.com/pytorch/pytorch#from-source">the guide</a>, but do <strong>not</strong> run <code>python setup.py install</code> yet.</li>
<li>Choose and install your favorite MPI implementation. Note that enabling CUDA-aware MPI might require some additional steps. In our case, we’ll stick to Open-MPI <em>without</em> GPU support: <code>conda install -c conda-forge openmpi</code></li>
<li>Now, go to your cloned PyTorch repo and execute <code>python setup.py install</code>.</li>
</ol>
<p>In order to test our newly installed backend, a few modifications are required.</p>
<ol>
<li>Replace the content under <code>if __name__ == '__main__':</code> with <code>init_processes(0, 0, run, backend='mpi')</code>.</li>
<li>Run <code>mpirun -n 4 python myscript.py</code>.</li>
</ol>
<p>The reason for these changes is that MPI needs to create its own environment before spawning the processes. MPI will also spawn its own processes and perform the handshake described in <a href="#initialization-methods">Initialization Methods</a>, making the <code>rank</code>and <code>size</code> arguments of <code>init_process_group</code> superfluous. This is actually quite powerful as you can pass additional arguments to <code>mpirun</code> in order to tailor computational resources for each process. (Things like number of cores per process, hand-assigning machines to specific ranks, and <a href="https://www.open-mpi.org/faq/?category=running#mpirun-hostfile">some more</a>) Doing so, you should obtain the same familiar output as with the other communication backends.</p>
<h3 id="initialization-methods">Initialization Methods</h3>
<p>To finish this tutorial, let’s talk about the very first function we called: <code>dist.init_process_group(backend, init_method)</code>. In particular, we will go over the different initialization methods which are responsible for the initial coordination step between each process. Those methods allow you to define how this coordination is done. Depending on your hardware setup, one of these methods should be naturally more suitable than the others. In addition to the following sections, you should also have a look at the <a href="http://pytorch.org/docs/master/distributed.html#initialization">official documentation</a>.</p>
<p>Before diving into the initialization methods, let’s have a quick look at what happens behind <code>init_process_group</code> from the C/C++ perspective.</p>
<ol>
<li>First, the arguments are parsed and validated.</li>
<li>The backend is resolved via the <code>name2channel.at()</code> function. A <code>Channel</code> class is returned, and will be used to perform the data transmission.</li>
<li>The GIL is dropped, and <code>THDProcessGroupInit()</code> is called. This instantiates the channel and adds the address of the master node.</li>
<li>The process with rank 0 will execute the <code>master</code> procedure, while all other ranks will be <code>workers</code>.</li>
<li>The master<ol>
<li>Creates sockets for all workers.</li>
<li>Waits for all workers to connect.</li>
<li>Sends them information about the location of the other processes.</li>
</ol>
</li>
<li>Each worker<ol>
<li>Creates a socket to the master.</li>
<li>Sends their own location information.</li>
<li>Receives information about the other workers.</li>
<li>Opens a socket and handshakes with all other workers.</li>
</ol>
</li>
<li>The initialization is done, and everyone is connected to everyone.</li>
</ol>
<p><strong>Environment Variable</strong></p>
<p>We have been using the environment variable initialization method throughout this tutorial. By setting the following four environment variables on all machines, all processes will be able to properly connect to the master, obtain information about the other processes, and finally handshake with them.</p>
<ul>
<li><code>MASTER_PORT</code>: A free port on the machine that will host the process with rank 0.</li>
<li><code>MASTER_ADDR</code>: IP address of the machine that will host the process with rank 0.</li>
<li><code>WORLD_SIZE</code>: The total number of processes, so that the master knows how many workers to wait for.</li>
<li><code>RANK</code>: Rank of each process, so they will know whether it is the master of a worker.</li>
</ul>
<p><strong>Shared File System</strong></p>
<p>The shared filesystem requires all processes to have access to a shared file system, and will coordinate them through a shared file. This means that each process will open the file, write its information, and wait until everybody did so. After what all required information will be readily available to all processes. In order to avoid race conditions, the file system must support locking through <a href="http://man7.org/linux/man-pages/man2/fcntl.2.html">fcntl</a>. Note that you can specify ranks manually or let the processes figure it out by themselves. Be defining a unique <code>groupname</code> per job you can use the same file path for multiple jobs and safely avoid collision.</p>
<pre class="codehilite"><code class="language-py">dist.init_process_group(init_method='file:///mnt/nfs/sharedfile', world_size=4,
                        group_name='mygroup')</code></pre>


<p><strong>TCP Init &amp; Multicast</strong></p>
<p>Initializing via TCP can be achieved in two different ways:</p>
<ol>
<li>By providing the IP address of the process with rank 0 and the world size.</li>
<li>By providing <em>any</em> valid IP <a href="https://en.wikipedia.org/wiki/Multicast_address">multicast address</a> and the world size.</li>
</ol>
<p>In the first case, all workers will be able to connect to the process with rank 0 and follow the procedure described above.</p>
<pre class="codehilite"><code class="language-py">dist.init_process_group(init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4)</code></pre>


<p>In the second case, the multicast address specifies the group of nodes who might potentially be active and the coordination can be handled by allowing each process to have an initial handshake before following the above procedure. In addition TCP multicast initialization also supports a <code>group_name</code> argument (as with the shared file method) allowing multiple jobs to be scheduled on the same cluster.</p>
<pre class="codehilite"><code class="language-py">dist.init_process_group(init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456',
                        world_size=4)</code></pre>


<p><center></p>
<p><strong>Acknowledgements</strong></p>
<p></center></p>
<p>I’d like to thank the PyTorch developers for doing such a good job on their implementation, documentation, and tests. When the code was unclear, I could always count on the <a href="http://pytorch.org/docs/master/distributed.html">docs</a> or the <a href="https://github.com/pytorch/pytorch/blob/master/test/test_distributed.py">tests</a> to find an answer. In particular, I’d like to thank Soumith Chintala, Adam Paszke, and Natalia Gimelshein for providing insightful comments and answering questions on early drafts.</p>
                
                  
                
              
              
                


              

              <hr/>
              <div align="center">
                  <p><a href="http://www.apachecn.org/" target="_blank"><font face="KaiTi" size="6" color="red">我们一直在努力</font></a><p>
                  <p><a href="https://github.com/apachecn/pytorch-doc-zh/" target="_blank">apachecn/pytorch-doc-zh</a></p>
                  <p><iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&repo=pytorch-doc-zh&type=watch&count=true&v=2" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
                  <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&repo=pytorch-doc-zh&type=star&count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
                  <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&repo=pytorch-doc-zh&type=fork&count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
                  <a target="_blank" href="//shang.qq.com/wpa/qunwpa?idkey=bcee938030cc9e1552deb3bd9617bbbf62d3ec1647e4b60d9cd6b6e8f78ddc03"><img border="0" src="//pub.idqqimg.com/wpa/images/group.png" alt="ML | ApacheCN" title="ML | ApacheCN"></a></p>
                  <div style="text-align:center;margin:0 0 10.5px;"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
                        <ins class="adsbygoogle"
                             style="display:inline-block;width:728px;height:90px"
                             data-ad-client="ca-pub-3565452474788507"
                             data-ad-slot="2543897000"></ins>
                        <script>
                        (adsbygoogle = window.adsbygoogle || []).push({});
                        </script></div>
              </div>

              <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
              <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
              <div id="gitalk-container" class="container-fluid"></div>
              <script type="text/javascript">
                  var gitalk = new Gitalk({
                  clientID: 'f27b87eb424ba43df978',
                  clientSecret: '9b3482a495c5257a1d269d8108b9bfd71f048c3c',
                  repo: 'pytorch-doc-zh',
                  owner: 'apachecn',
                  admin: ['jiangzhonglian'],
                  id: md5(location.pathname),
                  distractionFreeMode: false
                  })
                  gitalk.render('gitalk-container')
              </script>
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../34/" title="强化学习（DQN）教程" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                强化学习（DQN）教程
              </span>
            </div>
          </a>
        
        
          <a href="../36/" title="空间转换网络 (Spatial Transformer Networks) 教程" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                空间转换网络 (Spatial Transformer Networks) 教程
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.583bbe55.js"></script>
      
        
        
          
          <script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../assets/javascripts/lunr/lunr.jp.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
    
      
    
  </body>
</html>