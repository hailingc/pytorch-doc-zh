



<!DOCTYPE html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="PyTorch 0.3.0 中文文档">
      
      
        <link rel="canonical" href="http://pytorch.apachecn.org/54/">
      
      
        <meta name="author" content="ApacheCN Team">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.3">
    
    
      
        <title>torch.Tensor - PyTorch 0.3.0 中文文档</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#ff7043">
      
    
    
      <script src="../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    

    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        }
    });
    </script>
    
    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>
    
    <!-- google webmaster -->
    <meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo" />
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="deep-orange" data-md-color-accent="deep-orange">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="../#torchtensor" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="http://pytorch.apachecn.org" title="PyTorch 0.3.0 中文文档" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                PyTorch 0.3.0 中文文档
              </span>
              <span class="md-header-nav__topic">
                torch.Tensor
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/apachecn/pytorch-doc-zh/" title="前往 Github 仓库" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      pytorch-doc-zh
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href=".." title="主页" class="md-tabs__link">
          主页
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../0/" title="中文教程" class="md-tabs__link">
          中文教程
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../45/" title="中文文档" class="md-tabs__link md-tabs__link--active">
          中文文档
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="http://pytorch.apachecn.org" title="PyTorch 0.3.0 中文文档" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    PyTorch 0.3.0 中文文档
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/apachecn/pytorch-doc-zh/" title="前往 Github 仓库" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      pytorch-doc-zh
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      主页
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        主页
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href=".." title="主页" class="md-nav__link">
      主页
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      中文教程
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        中文教程
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../0/" title="初学者教程" class="md-nav__link">
      初学者教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../1/" title="PyTorch 深度学习: 60 分钟极速入门教程" class="md-nav__link">
      PyTorch 深度学习: 60 分钟极速入门教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../2/" title="PyTorch 是什么？" class="md-nav__link">
      PyTorch 是什么？
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../3/" title="自动求导: 自动微分" class="md-nav__link">
      自动求导: 自动微分
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../4/" title="神经网络" class="md-nav__link">
      神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../5/" title="训练一个分类器" class="md-nav__link">
      训练一个分类器
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../6/" title="可选: 数据并行" class="md-nav__link">
      可选: 数据并行
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../7/" title="PyTorch for former Torch users" class="md-nav__link">
      PyTorch for former Torch users
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../8/" title="Tensors" class="md-nav__link">
      Tensors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../9/" title="Autograd (自动求导)" class="md-nav__link">
      Autograd (自动求导)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../10/" title="nn package" class="md-nav__link">
      nn package
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../11/" title="Multi-GPU examples" class="md-nav__link">
      Multi-GPU examples
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../12/" title="跟着例子学习 PyTorch" class="md-nav__link">
      跟着例子学习 PyTorch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../13/" title="Warm-up: numpy" class="md-nav__link">
      Warm-up: numpy
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../14/" title="PyTorch: Tensors" class="md-nav__link">
      PyTorch: Tensors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../15/" title="PyTorch: 变量和autograd" class="md-nav__link">
      PyTorch: 变量和autograd
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../16/" title="PyTorch: 定义新的autograd函数" class="md-nav__link">
      PyTorch: 定义新的autograd函数
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../17/" title="TensorFlow: 静态图" class="md-nav__link">
      TensorFlow: 静态图
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../18/" title="PyTorch: nn包" class="md-nav__link">
      PyTorch: nn包
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../19/" title="PyTorch: optim包" class="md-nav__link">
      PyTorch: optim包
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../20/" title="PyTorch: 定制化nn模块" class="md-nav__link">
      PyTorch: 定制化nn模块
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../21/" title="PyTorch: 动态控制流程 + 权重共享" class="md-nav__link">
      PyTorch: 动态控制流程 + 权重共享
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../22/" title="迁移学习教程" class="md-nav__link">
      迁移学习教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../23/" title="数据加载和处理教程" class="md-nav__link">
      数据加载和处理教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../24/" title="针对NLP的Pytorch深度学习" class="md-nav__link">
      针对NLP的Pytorch深度学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../25/" title="PyTorch介绍" class="md-nav__link">
      PyTorch介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../26/" title="PyTorch深度学习" class="md-nav__link">
      PyTorch深度学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../27/" title="词汇嵌入:编码词汇语义" class="md-nav__link">
      词汇嵌入:编码词汇语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../28/" title="序列模型和 LSTM 网络（长短记忆网络）" class="md-nav__link">
      序列模型和 LSTM 网络（长短记忆网络）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../29/" title="高级教程: 作出动态决策和 Bi-LSTM CRF" class="md-nav__link">
      高级教程: 作出动态决策和 Bi-LSTM CRF
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../30/" title="中级教程" class="md-nav__link">
      中级教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../31/" title="用字符级RNN分类名称" class="md-nav__link">
      用字符级RNN分类名称
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../32/" title="基与字符级RNN（Char-RNN）的人名生成" class="md-nav__link">
      基与字符级RNN（Char-RNN）的人名生成
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../33/" title="用基于注意力机制的seq2seq神经网络进行翻译" class="md-nav__link">
      用基于注意力机制的seq2seq神经网络进行翻译
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../34/" title="强化学习（DQN）教程" class="md-nav__link">
      强化学习（DQN）教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../35/" title="Writing Distributed Applications with PyTorch" class="md-nav__link">
      Writing Distributed Applications with PyTorch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../36/" title="空间转换网络 (Spatial Transformer Networks) 教程" class="md-nav__link">
      空间转换网络 (Spatial Transformer Networks) 教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../37/" title="高级教程" class="md-nav__link">
      高级教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../38/" title="用 PyTorch 做 神经转换 (Neural Transfer)" class="md-nav__link">
      用 PyTorch 做 神经转换 (Neural Transfer)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../39/" title="使用 numpy 和 scipy 创建扩展" class="md-nav__link">
      使用 numpy 和 scipy 创建扩展
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../40/" title="使用 ONNX 将模型从 PyTorch 迁移到 Caffe2 和 Mobile" class="md-nav__link">
      使用 ONNX 将模型从 PyTorch 迁移到 Caffe2 和 Mobile
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../41/" title="为 pytorch 自定义 C 扩展" class="md-nav__link">
      为 pytorch 自定义 C 扩展
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../42/" title="项目相关" class="md-nav__link">
      项目相关
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../43/" title="项目贡献者" class="md-nav__link">
      项目贡献者
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../44/" title="组织学习交流群" class="md-nav__link">
      组织学习交流群
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" checked>
    
    <label class="md-nav__link" for="nav-3">
      中文文档
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        中文文档
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../45/" title="介绍" class="md-nav__link">
      介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../46/" title="自动求导机制" class="md-nav__link">
      自动求导机制
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../47/" title="广播语义" class="md-nav__link">
      广播语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../48/" title="CUDA 语义" class="md-nav__link">
      CUDA 语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../49/" title="扩展 PyTorch" class="md-nav__link">
      扩展 PyTorch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../50/" title="多进程的最佳实践" class="md-nav__link">
      多进程的最佳实践
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../51/" title="序列化语义" class="md-nav__link">
      序列化语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../52/" title="Package 参考" class="md-nav__link">
      Package 参考
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../53/" title="torch" class="md-nav__link">
      torch
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
    <a href="./" title="torch.Tensor" class="md-nav__link md-nav__link--active">
      torch.Tensor
    </a>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../55/" title="torch.sparse" class="md-nav__link">
      torch.sparse
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../56/" title="torch.Storage" class="md-nav__link">
      torch.Storage
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../57/" title="torch.nn" class="md-nav__link">
      torch.nn
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../58/" title="torch.optim" class="md-nav__link">
      torch.optim
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../59/" title="Automatic differentiation package - torch.autograd" class="md-nav__link">
      Automatic differentiation package - torch.autograd
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../60/" title="Probability distributions - torch.distributions" class="md-nav__link">
      Probability distributions - torch.distributions
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../61/" title="Multiprocessing package - torch.multiprocessing" class="md-nav__link">
      Multiprocessing package - torch.multiprocessing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../62/" title="Distributed communication package - torch.distributed" class="md-nav__link">
      Distributed communication package - torch.distributed
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../63/" title="Legacy package - torch.legacy" class="md-nav__link">
      Legacy package - torch.legacy
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../64/" title="torch.cuda" class="md-nav__link">
      torch.cuda
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../65/" title="torch.utils.ffi" class="md-nav__link">
      torch.utils.ffi
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../66/" title="torch.utils.data" class="md-nav__link">
      torch.utils.data
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../67/" title="torch.utils.model_zoo" class="md-nav__link">
      torch.utils.model_zoo
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../68/" title="torch.onnx" class="md-nav__link">
      torch.onnx
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../69/" title="torchvision 参考" class="md-nav__link">
      torchvision 参考
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../70/" title="torchvision" class="md-nav__link">
      torchvision
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../71/" title="torchvision.datasets" class="md-nav__link">
      torchvision.datasets
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../72/" title="torchvision.models" class="md-nav__link">
      torchvision.models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../73/" title="torchvision.transforms" class="md-nav__link">
      torchvision.transforms
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../74/" title="torchvision.utils" class="md-nav__link">
      torchvision.utils
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../75/" title="项目相关" class="md-nav__link">
      项目相关
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../76/" title="项目贡献者" class="md-nav__link">
      项目贡献者
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../77/" title="组织学习交流群" class="md-nav__link">
      组织学习交流群
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/apachecn/pytorch-doc-zh/edit/master/docs/54.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="torchtensor">torch.Tensor</h1>
<blockquote>
<p><a href="#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> 是一种包含单一数据类型元素的多维矩阵.</p>
</blockquote>
<p>Torch 定义了七种 CPU tensor 类型和八种 GPU tensor 类型:</p>
<table>
<thead>
<tr>
<th>Data type</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td>32-bit floating point</td>
<td><code>torch.FloatTensor</code></td>
<td><code>torch.cuda.FloatTensor</code></td>
</tr>
<tr>
<td>64-bit floating point</td>
<td><code>torch.DoubleTensor</code></td>
<td><code>torch.cuda.DoubleTensor</code></td>
</tr>
<tr>
<td>16-bit floating point</td>
<td><code>torch.HalfTensor</code></td>
<td><code>torch.cuda.HalfTensor</code></td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td><a href="#torch.ByteTensor" title="torch.ByteTensor"><code>torch.ByteTensor</code></a></td>
<td><code>torch.cuda.ByteTensor</code></td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td><code>torch.CharTensor</code></td>
<td><code>torch.cuda.CharTensor</code></td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td><code>torch.ShortTensor</code></td>
<td><code>torch.cuda.ShortTensor</code></td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td><code>torch.IntTensor</code></td>
<td><code>torch.cuda.IntTensor</code></td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td><code>torch.LongTensor</code></td>
<td><code>torch.cuda.LongTensor</code></td>
</tr>
</tbody>
</table>
<blockquote>
<p><a href="#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> 是默认的 tensor 类型(<code>torch.FloatTensor</code>)的简称.</p>
</blockquote>
<p>一个 tensor 对象可以从 Python 的 <a href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.6)"><code>list</code></a> 或者序列(sequence)构建:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.FloatTensor([[1, 2, 3], [4, 5, 6]])
1  2  3
4  5  6
[torch.FloatTensor of size 2x3]</code></pre>


<p>一个空的 tensor 对象可以通过所指定的大小来构建:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.IntTensor(2, 4).zero_()
0  0  0  0
0  0  0  0
[torch.IntTensor of size 2x4]</code></pre>


<p>可以通过 Python 的索引和切片方式来获取或修改 tensor 对象的内容:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])
&gt;&gt;&gt; print(x[1][2])
6.0
&gt;&gt;&gt; x[0][1] = 8
&gt;&gt;&gt; print(x)
 1  8  3
 4  5  6
[torch.FloatTensor of size 2x3]</code></pre>


<p>每一个 tensor 对象都有一个相应的 <code>torch.Storage</code> 用来保存数据. tensor 类提供了一个存储的多维的, 有 <a href="https://en.wikipedia.org/wiki/Stride_of_an_array">跨度(strided)</a> 的视图, 并且在视图上定义了数值运算.</p>
<p>Note</p>
<p>会改变 tensor 对象的函数方法名, 其使用了一个下划线后缀作为标识. 比如, <code>torch.FloatTensor.abs_()</code> 会在原地(in-place)计算绝对值并返回改变后的 tensor. 而 <code>torch.FloatTensor.abs()</code> 会在一个新建的 tensor 中计算结果.</p>
<pre class="codehilite"><code class="language-py">class torch.Tensor</code></pre>


<pre class="codehilite"><code class="language-py">class torch.Tensor(*sizes)</code></pre>


<pre class="codehilite"><code class="language-py">class torch.Tensor(size)</code></pre>


<pre class="codehilite"><code class="language-py">class torch.Tensor(sequence)</code></pre>


<pre class="codehilite"><code class="language-py">class torch.Tensor(ndarray)</code></pre>


<pre class="codehilite"><code class="language-py">class torch.Tensor(tensor)</code></pre>


<pre class="codehilite"><code class="language-py">class torch.Tensor(storage)</code></pre>


<p>可以通过提供大小或者数据来创建一个新的 tensor 对象.</p>
<p>如果没有提供参数, 将返回一个空的零维的 tensor. 如果提供了 <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.14)"><code>numpy.ndarray</code></a>, <a href="#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>, 或者 <code>torch.Storage</code> 作为参数, 其将返回一个与参数共享数据的 tensor 对象. 如果提供一个 Python 序列(sequence)作为参数, 将返回从序列的副本中创建的一个新的 tensor 对象.</p>
<pre class="codehilite"><code class="language-py">abs() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.abs" title="torch.abs"><code>torch.abs()</code></a></p>
<pre class="codehilite"><code class="language-py">abs_() → Tensor</code></pre>


<p><a href="#torch.Tensor.abs" title="torch.Tensor.abs"><code>abs()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">acos() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.acos" title="torch.acos"><code>torch.acos()</code></a></p>
<pre class="codehilite"><code class="language-py">acos_() → Tensor</code></pre>


<p><a href="#torch.Tensor.acos" title="torch.Tensor.acos"><code>acos()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">add(value)</code></pre>


<p>请查看 <a href="torch.html#torch.add" title="torch.add"><code>torch.add()</code></a></p>
<pre class="codehilite"><code class="language-py">add_(value)</code></pre>


<p><a href="#torch.Tensor.add" title="torch.Tensor.add"><code>add()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">addbmm(beta=1, mat, alpha=1, batch1, batch2) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.addbmm" title="torch.addbmm"><code>torch.addbmm()</code></a></p>
<pre class="codehilite"><code class="language-py">addbmm_(beta=1, mat, alpha=1, batch1, batch2) → Tensor</code></pre>


<p><a href="#torch.Tensor.addbmm" title="torch.Tensor.addbmm"><code>addbmm()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">addcdiv(value=1, tensor1, tensor2) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.addcdiv" title="torch.addcdiv"><code>torch.addcdiv()</code></a></p>
<pre class="codehilite"><code class="language-py">addcdiv_(value=1, tensor1, tensor2) → Tensor</code></pre>


<p><a href="#torch.Tensor.addcdiv" title="torch.Tensor.addcdiv"><code>addcdiv()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">addcmul(value=1, tensor1, tensor2) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.addcmul" title="torch.addcmul"><code>torch.addcmul()</code></a></p>
<pre class="codehilite"><code class="language-py">addcmul_(value=1, tensor1, tensor2) → Tensor</code></pre>


<p><a href="#torch.Tensor.addcmul" title="torch.Tensor.addcmul"><code>addcmul()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">addmm(beta=1, mat, alpha=1, mat1, mat2) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.addmm" title="torch.addmm"><code>torch.addmm()</code></a></p>
<pre class="codehilite"><code class="language-py">addmm_(beta=1, mat, alpha=1, mat1, mat2) → Tensor</code></pre>


<p><a href="#torch.Tensor.addmm" title="torch.Tensor.addmm"><code>addmm()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">addmv(beta=1, tensor, alpha=1, mat, vec) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.addmv" title="torch.addmv"><code>torch.addmv()</code></a></p>
<pre class="codehilite"><code class="language-py">addmv_(beta=1, tensor, alpha=1, mat, vec) → Tensor</code></pre>


<p><a href="#torch.Tensor.addmv" title="torch.Tensor.addmv"><code>addmv()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">addr(beta=1, alpha=1, vec1, vec2) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.addr" title="torch.addr"><code>torch.addr()</code></a></p>
<pre class="codehilite"><code class="language-py">addr_(beta=1, alpha=1, vec1, vec2) → Tensor</code></pre>


<p><a href="#torch.Tensor.addr" title="torch.Tensor.addr"><code>addr()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">apply_(callable) → Tensor</code></pre>


<p>将函数 <code>callable</code> 作用于 tensor 的每一个元素, 并将每个元素用 <code>callable</code> 的返回值替换.</p>
<p>Note</p>
<p>该函数只能在 CPU tensor 中使用, 并且不应该用在有较高性能的要求的代码块中.</p>
<pre class="codehilite"><code class="language-py">asin() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.asin" title="torch.asin"><code>torch.asin()</code></a></p>
<pre class="codehilite"><code class="language-py">asin_() → Tensor</code></pre>


<p><a href="#torch.Tensor.asin" title="torch.Tensor.asin"><code>asin()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">atan() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.atan" title="torch.atan"><code>torch.atan()</code></a></p>
<pre class="codehilite"><code class="language-py">atan2(other) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.atan2" title="torch.atan2"><code>torch.atan2()</code></a></p>
<pre class="codehilite"><code class="language-py">atan2_(other) → Tensor</code></pre>


<p><a href="#torch.Tensor.atan2" title="torch.Tensor.atan2"><code>atan2()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">atan_() → Tensor</code></pre>


<p><a href="#torch.Tensor.atan" title="torch.Tensor.atan"><code>atan()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">baddbmm(beta=1, alpha=1, batch1, batch2) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.baddbmm" title="torch.baddbmm"><code>torch.baddbmm()</code></a></p>
<pre class="codehilite"><code class="language-py">baddbmm_(beta=1, alpha=1, batch1, batch2) → Tensor</code></pre>


<p><a href="#torch.Tensor.baddbmm" title="torch.Tensor.baddbmm"><code>baddbmm()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">bernoulli() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.bernoulli" title="torch.bernoulli"><code>torch.bernoulli()</code></a></p>
<pre class="codehilite"><code class="language-py">bernoulli_() → Tensor</code></pre>


<p><a href="#torch.Tensor.bernoulli" title="torch.Tensor.bernoulli"><code>bernoulli()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">bmm(batch2) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.bmm" title="torch.bmm"><code>torch.bmm()</code></a></p>
<pre class="codehilite"><code class="language-py">byte()</code></pre>


<p>将这个 tensor 转换为 byte 类型</p>
<pre class="codehilite"><code class="language-py">cauchy_(median=0, sigma=1, *, generator=None) → Tensor</code></pre>


<p>用柯西分布得到的数值来填充 tensor 中的元素:</p>
<pre class="codehilite"><code class="language-py">\[P(x) = \dfrac{1}{\pi} \dfrac{\sigma}{(x - median)^2 + \sigma^2}\]</code></pre>


<pre class="codehilite"><code class="language-py">ceil() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.ceil" title="torch.ceil"><code>torch.ceil()</code></a></p>
<pre class="codehilite"><code class="language-py">ceil_() → Tensor</code></pre>


<p><a href="#torch.Tensor.ceil" title="torch.Tensor.ceil"><code>ceil()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">char()</code></pre>


<p>将这个 tensor 转换为 char 类型</p>
<pre class="codehilite"><code class="language-py">chunk(n_chunks, dim=0)</code></pre>


<p>将 tensor 分解成 tensor 元组.</p>
<p>请查看 <a href="torch.html#torch.chunk" title="torch.chunk"><code>torch.chunk()</code></a>.</p>
<pre class="codehilite"><code class="language-py">clamp(min, max) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.clamp" title="torch.clamp"><code>torch.clamp()</code></a></p>
<pre class="codehilite"><code class="language-py">clamp_(min, max) → Tensor</code></pre>


<p><a href="#torch.Tensor.clamp" title="torch.Tensor.clamp"><code>clamp()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">clone() → Tensor</code></pre>


<p>返回与原 tensor 具有相同大小和数据类型的 tensor.</p>
<pre class="codehilite"><code class="language-py">contiguous() → Tensor</code></pre>


<p>返回一个内存连续的有相同数据的 tensor, 如果原 tensor 内存连续则返回原 tensor.</p>
<pre class="codehilite"><code class="language-py">copy_(src, async=False, broadcast=True) → Tensor</code></pre>


<p>将 <code>src</code> 中的元素复制到这个 tensor 中并返回这个 tensor</p>
<p>如果 <code>broadcast</code> 是 True, 源 tensor 一定和这个 tensor <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>. 另外, 源 tensor 的元素数量应该和这个 tensor 的元素个数一致. 源 tensor 可以是另一种数据类型, 或者在别的的设备上.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>src</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 被复制的源 tensor</li>
<li><strong>async</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 如果值为 <code>True</code> 并且这个复制操作在 CPU 和 GPU 之间进行, 则拷贝的副本与源信息可能会出现异步(asynchronously). 对于其他类型的复制操作, 这个参数不起作用.</li>
<li><strong>broadcast</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 如果值为 <code>True</code>, <code>src</code> 将广播基础的 tensor 的形状.</li>
</ul>
<p>|
| --- | --- |</p>
<pre class="codehilite"><code class="language-py">cos() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.cos" title="torch.cos"><code>torch.cos()</code></a></p>
<pre class="codehilite"><code class="language-py">cos_() → Tensor</code></pre>


<p><a href="#torch.Tensor.cos" title="torch.Tensor.cos"><code>cos()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">cosh() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.cosh" title="torch.cosh"><code>torch.cosh()</code></a></p>
<pre class="codehilite"><code class="language-py">cosh_() → Tensor</code></pre>


<p><a href="#torch.Tensor.cosh" title="torch.Tensor.cosh"><code>cosh()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">cpu()</code></pre>


<p>如果此 tensor 不在CPU上, 则返回此 tensor 的CPU副本</p>
<pre class="codehilite"><code class="language-py">cross(other, dim=-1) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.cross" title="torch.cross"><code>torch.cross()</code></a></p>
<pre class="codehilite"><code class="language-py">cuda(device=None, async=False)</code></pre>


<p>返回此对象在 CUDA 内存中的一个副本 .</p>
<p>如果此对象已经在 CUDA 内存中并且在正确的设备上 , 那么不会执行复制操作 , 直接返回原对象 .</p>
<p>| Parameters: | </p>
<ul>
<li><strong>device</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 目标 GPU 的 id . 默认值是当前设备 .</li>
<li><strong>async</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 如果为 <code>True</code> 并且源位于锁定内存中 , 则副本相对于主机是异步的 . 否则此参数不起效果 .</li>
</ul>
<p>|
| --- | --- |</p>
<pre class="codehilite"><code class="language-py">cumprod(dim) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.cumprod" title="torch.cumprod"><code>torch.cumprod()</code></a></p>
<pre class="codehilite"><code class="language-py">cumsum(dim) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.cumsum" title="torch.cumsum"><code>torch.cumsum()</code></a></p>
<pre class="codehilite"><code class="language-py">data_ptr() → int</code></pre>


<p>返回 tensor 第一个元素的地址.</p>
<pre class="codehilite"><code class="language-py">diag(diagonal=0) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.diag" title="torch.diag"><code>torch.diag()</code></a></p>
<pre class="codehilite"><code class="language-py">dim() → int</code></pre>


<p>返回 tensor 的维数.</p>
<pre class="codehilite"><code class="language-py">dist(other, p=2) → float</code></pre>


<p>请查看 <a href="torch.html#torch.dist" title="torch.dist"><code>torch.dist()</code></a></p>
<pre class="codehilite"><code class="language-py">div(value)</code></pre>


<p>请查看 <a href="torch.html#torch.div" title="torch.div"><code>torch.div()</code></a></p>
<pre class="codehilite"><code class="language-py">div_(value)</code></pre>


<p><a href="#torch.Tensor.div" title="torch.Tensor.div"><code>div()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">dot(tensor2) → float</code></pre>


<p>请查看 <a href="torch.html#torch.dot" title="torch.dot"><code>torch.dot()</code></a></p>
<pre class="codehilite"><code class="language-py">double()</code></pre>


<p>将这个 tensor 转换为 double 类型</p>
<pre class="codehilite"><code class="language-py">eig(eigenvectors=False) -&gt; (Tensor, Tensor)</code></pre>


<p>请查看 <a href="torch.html#torch.eig" title="torch.eig"><code>torch.eig()</code></a></p>
<pre class="codehilite"><code class="language-py">element_size() → int</code></pre>


<p>返回单个元素的字节大小.</p>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.FloatTensor().element_size()
4
&gt;&gt;&gt; torch.ByteTensor().element_size()
1</code></pre>


<pre class="codehilite"><code class="language-py">eq(other) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.eq" title="torch.eq"><code>torch.eq()</code></a></p>
<pre class="codehilite"><code class="language-py">eq_(other) → Tensor</code></pre>


<p><a href="#torch.Tensor.eq" title="torch.Tensor.eq"><code>eq()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">equal(other) → bool</code></pre>


<p>请查看 <a href="torch.html#torch.equal" title="torch.equal"><code>torch.equal()</code></a></p>
<pre class="codehilite"><code class="language-py">erf() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.erf" title="torch.erf"><code>torch.erf()</code></a></p>
<pre class="codehilite"><code class="language-py">erf_()</code></pre>


<pre class="codehilite"><code class="language-py">erfinv() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.erfinv" title="torch.erfinv"><code>torch.erfinv()</code></a></p>
<pre class="codehilite"><code class="language-py">erfinv_()</code></pre>


<pre class="codehilite"><code class="language-py">exp() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.exp" title="torch.exp"><code>torch.exp()</code></a></p>
<pre class="codehilite"><code class="language-py">exp_() → Tensor</code></pre>


<p><a href="#torch.Tensor.exp" title="torch.Tensor.exp"><code>exp()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">expand(*sizes) → Tensor</code></pre>


<p>返回 tensor 单个维度扩展到大的一个新的视图.</p>
<p>传递 -1 作为一个维度的大小, 表示这个维度的大小不做改变.</p>
<p>Tensor 也可以扩展到一个很大的维数, 新添加的维度将放在前面. (对于新的维度, 大小不能设置为 -1 .)</p>
<p>扩展一个 tensor 不是分配一个新的内存, 而只是在这个存在的 tensor 上, 通过设置 <code>stride</code> 为 0, 创建一个新的某个维度从 1 扩展到很大的视图. 任何大小为 1 的维度, 在不用重新分配内存的情况下, 可以扩展到随意任何一个值.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>*sizes</strong> (<em>torch.Size</em> <em>or</em> <em>int...</em>) – 期望扩展的大小</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.Tensor([[1], [2], [3]])
&gt;&gt;&gt; x.size()
torch.Size([3, 1])
&gt;&gt;&gt; x.expand(3, 4)
 1  1  1  1
 2  2  2  2
 3  3  3  3
[torch.FloatTensor of size 3x4]
&gt;&gt;&gt; x.expand(-1, 4)   # -1 means not changing the size of that dimension
 1  1  1  1
 2  2  2  2
 3  3  3  3
[torch.FloatTensor of size 3x4]</code></pre>


<pre class="codehilite"><code class="language-py">expand_as(tensor)</code></pre>


<p>将此 tensor 展开为指定 tensor 的大小.</p>
<p>这相当于:</p>
<pre class="codehilite"><code class="language-py">self.expand(tensor.size())</code></pre>


<pre class="codehilite"><code class="language-py">exponential_(lambd=1, *, generator=None) → Tensor</code></pre>


<p>将该 tensor 用指数分布得到的元素填充:</p>
<pre class="codehilite"><code class="language-py">\[P(x) = \lambda e^{-\lambda x}\]</code></pre>


<pre class="codehilite"><code class="language-py">fill_(value) → Tensor</code></pre>


<p>将该 tensor 用指定的数值填充.</p>
<pre class="codehilite"><code class="language-py">float()</code></pre>


<p>将这个 tensor 转换为 float 类型</p>
<pre class="codehilite"><code class="language-py">floor() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.floor" title="torch.floor"><code>torch.floor()</code></a></p>
<pre class="codehilite"><code class="language-py">floor_() → Tensor</code></pre>


<p><a href="#torch.Tensor.floor" title="torch.Tensor.floor"><code>floor()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">fmod(divisor) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.fmod" title="torch.fmod"><code>torch.fmod()</code></a></p>
<pre class="codehilite"><code class="language-py">fmod_(divisor) → Tensor</code></pre>


<p><a href="#torch.Tensor.fmod" title="torch.Tensor.fmod"><code>fmod()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">frac() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.frac" title="torch.frac"><code>torch.frac()</code></a></p>
<pre class="codehilite"><code class="language-py">frac_() → Tensor</code></pre>


<p><a href="#torch.Tensor.frac" title="torch.Tensor.frac"><code>frac()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">gather(dim, index) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.gather" title="torch.gather"><code>torch.gather()</code></a></p>
<pre class="codehilite"><code class="language-py">ge(other) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.ge" title="torch.ge"><code>torch.ge()</code></a></p>
<pre class="codehilite"><code class="language-py">ge_(other) → Tensor</code></pre>


<p><a href="#torch.Tensor.ge" title="torch.Tensor.ge"><code>ge()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">gels(A) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.gels" title="torch.gels"><code>torch.gels()</code></a></p>
<pre class="codehilite"><code class="language-py">geometric_(p, *, generator=None) → Tensor</code></pre>


<p>将该 tensor 用几何分布得到的元素填充:</p>
<pre class="codehilite"><code class="language-py">\[P(X=k) = (1 - p)^{k - 1} p\]</code></pre>


<pre class="codehilite"><code class="language-py">geqrf() -&gt; (Tensor, Tensor)</code></pre>


<p>请查看 <a href="torch.html#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a></p>
<pre class="codehilite"><code class="language-py">ger(vec2) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.ger" title="torch.ger"><code>torch.ger()</code></a></p>
<pre class="codehilite"><code class="language-py">gesv(A) → Tensor, Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.gesv" title="torch.gesv"><code>torch.gesv()</code></a></p>
<pre class="codehilite"><code class="language-py">gt(other) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.gt" title="torch.gt"><code>torch.gt()</code></a></p>
<pre class="codehilite"><code class="language-py">gt_(other) → Tensor</code></pre>


<p><a href="#torch.Tensor.gt" title="torch.Tensor.gt"><code>gt()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">half()</code></pre>


<p>将这个 tensor 转换为 half-precision float 类型</p>
<pre class="codehilite"><code class="language-py">histc(bins=100, min=0, max=0) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.histc" title="torch.histc"><code>torch.histc()</code></a></p>
<pre class="codehilite"><code class="language-py">index(m) → Tensor</code></pre>


<p>用一个二进制的掩码或沿着一个给定的维度从 tensor 中选取元素. <code>tensor.index(m)</code> 等同于 <code>tensor[m]</code>.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>m</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a> <em>or</em> <a href="#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#slice" title="(in Python v3.6)"><em>slice</em></a>) – 用来选取元素的维度或掩码</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">index_add_(dim, index, tensor) → Tensor</code></pre>


<p>按参数 index 给出的索引序列, 将参数 tensor 中的元素加到原来的 tensor 中. 参数 tensor 的尺寸必须严格地与原 tensor 匹配, 否则会发生错误.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 索引 index 所指向的维度</li>
<li><strong>index</strong> (<em>LongTensor</em>) – 从参数 tensor 中选取数据的索引序列</li>
<li><strong>tensor</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 包含需要相加的元素的 tensor</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.Tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]])
&gt;&gt;&gt; t = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
&gt;&gt;&gt; index = torch.LongTensor([0, 2, 1])
&gt;&gt;&gt; x.index_add_(0, index, t)
&gt;&gt;&gt; x
 2   3   4
 8   9  10
 5   6   7
[torch.FloatTensor of size 3x3]</code></pre>


<pre class="codehilite"><code class="language-py">index_copy_(dim, index, tensor) → Tensor</code></pre>


<p>按参数 index 给出的索引序列, 将参数 tensor 中的元素复制到原来的 tensor 中. 参数 tensor 的尺寸必须严格地与原 tensor 匹配, 否则会发生错误.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 索引 index 所指向的维度</li>
<li><strong>index</strong> (<em>LongTensor</em>) – 从参数 tensor 中选取数据的索引序列</li>
<li><strong>tensor</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 包含需要复制的元素的 tensor</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.Tensor(3, 3)
&gt;&gt;&gt; t = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
&gt;&gt;&gt; index = torch.LongTensor([0, 2, 1])
&gt;&gt;&gt; x.index_copy_(0, index, t)
&gt;&gt;&gt; x
 1  2  3
 7  8  9
 4  5  6
[torch.FloatTensor of size 3x3]</code></pre>


<pre class="codehilite"><code class="language-py">index_fill_(dim, index, val) → Tensor</code></pre>


<p>按参数 index 给出的索引序列, 将原 tensor 中的元素用 <code>val</code> 填充.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 索引 index 所指向的维度</li>
<li><strong>index</strong> (<em>LongTensor</em>) – 从参数 val 中选取数据的索引序列</li>
<li><strong>val</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 用来填充的值</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
&gt;&gt;&gt; index = torch.LongTensor([0, 2])
&gt;&gt;&gt; x.index_fill_(1, index, -1)
&gt;&gt;&gt; x
-1  2 -1
-1  5 -1
-1  8 -1
[torch.FloatTensor of size 3x3]</code></pre>


<pre class="codehilite"><code class="language-py">index_select(dim, index) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.index_select" title="torch.index_select"><code>torch.index_select()</code></a></p>
<pre class="codehilite"><code class="language-py">int()</code></pre>


<p>将这个 tensor 转换为 int 类型</p>
<pre class="codehilite"><code class="language-py">inverse() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.inverse" title="torch.inverse"><code>torch.inverse()</code></a></p>
<pre class="codehilite"><code class="language-py">is_contiguous() → bool</code></pre>


<p>以 C 语言的内存模型为原则, 如果该 tensor 在内如果该 tensor 在内存中连续的, 则返回 True.</p>
<p><code>is_cuda</code></p>
<pre class="codehilite"><code class="language-py">is_pinned()</code></pre>


<p>如果 tensor 驻留在固定内存中, 则返回 true</p>
<pre class="codehilite"><code class="language-py">is_set_to(tensor) → bool</code></pre>


<p>如果此对象从 Torch C API 引用的 <code>THTensor</code> 对象与参数 tensor 引用的对象一致, 则返回True.</p>
<pre class="codehilite"><code class="language-py">is_signed()</code></pre>


<pre class="codehilite"><code class="language-py">kthvalue(k, dim=None, keepdim=False) -&gt; (Tensor, LongTensor)</code></pre>


<p>请查看 <a href="torch.html#torch.kthvalue" title="torch.kthvalue"><code>torch.kthvalue()</code></a></p>
<pre class="codehilite"><code class="language-py">le(other) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.le" title="torch.le"><code>torch.le()</code></a></p>
<pre class="codehilite"><code class="language-py">le_(other) → Tensor</code></pre>


<p><a href="#torch.Tensor.le" title="torch.Tensor.le"><code>le()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">lerp(start, end, weight)</code></pre>


<p>请查看 <a href="torch.html#torch.lerp" title="torch.lerp"><code>torch.lerp()</code></a></p>
<pre class="codehilite"><code class="language-py">lerp_(start, end, weight)</code></pre>


<p><a href="#torch.Tensor.lerp" title="torch.Tensor.lerp"><code>lerp()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">log() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.log" title="torch.log"><code>torch.log()</code></a></p>
<pre class="codehilite"><code class="language-py">log1p() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.log1p" title="torch.log1p"><code>torch.log1p()</code></a></p>
<pre class="codehilite"><code class="language-py">log1p_() → Tensor</code></pre>


<p><a href="#torch.Tensor.log1p" title="torch.Tensor.log1p"><code>log1p()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">log_() → Tensor</code></pre>


<p><a href="#torch.Tensor.log" title="torch.Tensor.log"><code>log()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">log_normal_(mean=1, std=2, *, generator=None)</code></pre>


<p>将该 tensor 用均值为 mean (µ), 标准差为 std (σ) 的对数正态分布得到的元素填充. 要注意 <a href="torch.html#torch.mean" title="torch.mean"><code>mean</code></a> 和 <code>stdv</code> 是基本正态分布的均值和标准差, 不是返回的分布:</p>
<pre class="codehilite"><code class="language-py">\[P(x) = \dfrac{1}{x \sigma \sqrt{2\pi}} e^{-\dfrac{(\ln x - \mu)^2}{2\sigma^2}}\]</code></pre>


<pre class="codehilite"><code class="language-py">long()</code></pre>


<p>将这个 tensor 转换为 long 类型</p>
<pre class="codehilite"><code class="language-py">lt(other) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.lt" title="torch.lt"><code>torch.lt()</code></a></p>
<pre class="codehilite"><code class="language-py">lt_(other) → Tensor</code></pre>


<p><a href="#torch.Tensor.lt" title="torch.Tensor.lt"><code>lt()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">map_(tensor, callable)</code></pre>


<p>将 <code>callable</code> 作用于本 tensor 和参数 tensor 中的每一个元素, 并将结果存放在本 tensor 中. 本 tensor 和参数 tensor 都必须是 <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<blockquote>
<p><code>callable</code> 应该有下列标志:</p>
<p>```py
def callable(a, b) -&gt; number</p>
<p>```</p>
</blockquote>
<pre class="codehilite"><code class="language-py">masked_scatter_(mask, source)</code></pre>


<p>复制 <code>source</code> 的元素到本 tensor 被:attr:<cite>mask</cite> 中值为 1 的元素标记的位置中. <code>mask</code> 的形状和本 tensor 的形状必须是可广播的 ( <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> ). <code>source</code> 中元素的个数最少为 <code>mask</code> 中值为1的元素的个数.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>mask</strong> (<a href="#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) – 二进制掩码</li>
<li><strong>source</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 复制的源 tensor</li>
</ul>
<p>|
| --- | --- |</p>
<p>Note</p>
<p><code>mask</code> 作用于 <code>self</code> 自身的 tensor, 而不是参数 <code>source</code> 的 tensor.</p>
<pre class="codehilite"><code class="language-py">masked_fill_(mask, value)</code></pre>


<p>将本 tensor 被 <code>mask</code> 中值为 1 的元素标记的位置, 用 <code>value</code> 填充. <code>mask</code> 的形状和本 tensor 的形状必须是可广播的 (<a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>).</p>
<p>Fills elements of this tensor with <code>value</code> where <code>mask</code> is one.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>mask</strong> (<a href="#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) – 二进制掩码</li>
<li><strong>value</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 用来填充的值</li>
</ul>
<p>|
| --- | --- |</p>
<pre class="codehilite"><code class="language-py">masked_select(mask) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.masked_select" title="torch.masked_select"><code>torch.masked_select()</code></a></p>
<pre class="codehilite"><code class="language-py">matmul(other)</code></pre>


<p>两个 tensor 的矩阵乘积.</p>
<p>请查看 <a href="torch.html#torch.matmul" title="torch.matmul"><code>torch.matmul()</code></a>.</p>
<pre class="codehilite"><code class="language-py">max(dim=None, keepdim=False) -&gt; float or (Tensor, Tensor)</code></pre>


<p>请查看 <a href="torch.html#torch.max" title="torch.max"><code>torch.max()</code></a></p>
<pre class="codehilite"><code class="language-py">mean(dim=None, keepdim=False) -&gt; float or (Tensor, Tensor)</code></pre>


<p>请查看 <a href="torch.html#torch.mean" title="torch.mean"><code>torch.mean()</code></a></p>
<pre class="codehilite"><code class="language-py">median(dim=None, keepdim=False) -&gt; (Tensor, LongTensor)</code></pre>


<p>请查看 <a href="torch.html#torch.median" title="torch.median"><code>torch.median()</code></a></p>
<pre class="codehilite"><code class="language-py">min(dim=None, keepdim=False) -&gt; float or (Tensor, Tensor)</code></pre>


<p>请查看 <a href="torch.html#torch.min" title="torch.min"><code>torch.min()</code></a></p>
<pre class="codehilite"><code class="language-py">mm(mat2) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.mm" title="torch.mm"><code>torch.mm()</code></a></p>
<pre class="codehilite"><code class="language-py">mode(dim=None, keepdim=False) -&gt; (Tensor, LongTensor)</code></pre>


<p>请查看 <a href="torch.html#torch.mode" title="torch.mode"><code>torch.mode()</code></a></p>
<pre class="codehilite"><code class="language-py">mul(value) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.mul" title="torch.mul"><code>torch.mul()</code></a></p>
<pre class="codehilite"><code class="language-py">mul_(value)</code></pre>


<p><a href="#torch.Tensor.mul" title="torch.Tensor.mul"><code>mul()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">multinomial(num_samples, replacement=False, *, generator=None)</code></pre>


<p>请查看 <a href="torch.html#torch.multinomial" title="torch.multinomial"><code>torch.multinomial()</code></a></p>
<pre class="codehilite"><code class="language-py">mv(vec) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.mv" title="torch.mv"><code>torch.mv()</code></a></p>
<pre class="codehilite"><code class="language-py">narrow(dimension, start, length) → Tensor</code></pre>


<p>返回一个本 tensor 经过缩小后的 tensor. 维度 dim 缩小范围是 <code>start</code> 到 <code>start + length</code>. 原 tensor 与返回的 tensor 共享相同的底层存储.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>dimension</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 需要缩小的维度</li>
<li><strong>start</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 起始维度</li>
<li><strong>length</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) –</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
&gt;&gt;&gt; x.narrow(0, 0, 2)
 1  2  3
 4  5  6
[torch.FloatTensor of size 2x3]
&gt;&gt;&gt; x.narrow(1, 1, 2)
 2  3
 5  6
 8  9
[torch.FloatTensor of size 3x2]</code></pre>


<pre class="codehilite"><code class="language-py">ndimension() → int</code></pre>


<p><a href="#torch.Tensor.dim" title="torch.Tensor.dim"><code>dim()</code></a> 的另一种表示</p>
<pre class="codehilite"><code class="language-py">ne(other) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.ne" title="torch.ne"><code>torch.ne()</code></a></p>
<pre class="codehilite"><code class="language-py">ne_(other) → Tensor</code></pre>


<p><a href="#torch.Tensor.ne" title="torch.Tensor.ne"><code>ne()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">neg() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.neg" title="torch.neg"><code>torch.neg()</code></a></p>
<pre class="codehilite"><code class="language-py">neg_() → Tensor</code></pre>


<p><a href="#torch.Tensor.neg" title="torch.Tensor.neg"><code>neg()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">nelement() → int</code></pre>


<p><a href="#torch.Tensor.numel" title="torch.Tensor.numel"><code>numel()</code></a> 的另一种表示</p>
<pre class="codehilite"><code class="language-py">new(*args, **kwargs)</code></pre>


<p>构造相同数据类型的新 tensor.</p>
<pre class="codehilite"><code class="language-py">nonzero() → LongTensor</code></pre>


<p>请查看 <a href="torch.html#torch.nonzero" title="torch.nonzero"><code>torch.nonzero()</code></a></p>
<pre class="codehilite"><code class="language-py">norm(p=2, dim=None, keepdim=False) → float</code></pre>


<p>请查看 <a href="torch.html#torch.norm" title="torch.norm"><code>torch.norm()</code></a></p>
<pre class="codehilite"><code class="language-py">normal_(mean=0, std=1, *, generator=None)</code></pre>


<p>将 tensor 用均值为 <a href="torch.html#torch.mean" title="torch.mean"><code>mean</code></a> 和标准差为 :attr:[<code>](#id1)std</code>的正态分布填充.</p>
<pre class="codehilite"><code class="language-py">numel() → int</code></pre>


<p>请查看 <a href="torch.html#torch.numel" title="torch.numel"><code>torch.numel()</code></a></p>
<pre class="codehilite"><code class="language-py">numpy() → ndarray</code></pre>


<p>将该 tensor 以 NumPy <code>ndarray</code> 的形式返回. 两者共享相同的底层存储. 原 tensor 的改变会影响到 <code>ndarray</code>, 反之也一样.</p>
<pre class="codehilite"><code class="language-py">orgqr(input2) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.orgqr" title="torch.orgqr"><code>torch.orgqr()</code></a></p>
<pre class="codehilite"><code class="language-py">ormqr(input2, input3, left=True, transpose=False) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.ormqr" title="torch.ormqr"><code>torch.ormqr()</code></a></p>
<pre class="codehilite"><code class="language-py">permute(*dims)</code></pre>


<p>排列该 tensor 的尺寸.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>*dims</strong> (<em>int...</em>) – 按所期望的维数排序</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.randn(2, 3, 5)
&gt;&gt;&gt; x.size()
torch.Size([2, 3, 5])
&gt;&gt;&gt; x.permute(2, 0, 1).size()
torch.Size([5, 2, 3])</code></pre>


<pre class="codehilite"><code class="language-py">pin_memory()</code></pre>


<p>如果 tensor 尚未固定, 则将 tensor 复制到固定内存.</p>
<pre class="codehilite"><code class="language-py">potrf(upper=True) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.potrf" title="torch.potrf"><code>torch.potrf()</code></a></p>
<pre class="codehilite"><code class="language-py">potri(upper=True) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.potri" title="torch.potri"><code>torch.potri()</code></a></p>
<pre class="codehilite"><code class="language-py">potrs(input2, upper=True) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.potrs" title="torch.potrs"><code>torch.potrs()</code></a></p>
<pre class="codehilite"><code class="language-py">pow(exponent)</code></pre>


<p>请查看 <a href="torch.html#torch.pow" title="torch.pow"><code>torch.pow()</code></a></p>
<pre class="codehilite"><code class="language-py">pow_(exponent)</code></pre>


<p><a href="#torch.Tensor.pow" title="torch.Tensor.pow"><code>pow()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">prod(dim=None, keepdim=False) → float</code></pre>


<p>请查看 <a href="torch.html#torch.prod" title="torch.prod"><code>torch.prod()</code></a></p>
<pre class="codehilite"><code class="language-py">pstrf(upper=True, tol=-1) -&gt; (Tensor, IntTensor)</code></pre>


<p>请查看 <a href="torch.html#torch.pstrf" title="torch.pstrf"><code>torch.pstrf()</code></a></p>
<pre class="codehilite"><code class="language-py">put_(indices, tensor, accumulate=False) → Tensor</code></pre>


<p>复制 <code>tensor</code> 内的元素到 indices 指定的位置. 为了达到索引的目的, <code>self</code> tensor 被当做一维 (1D) 的 tensor.</p>
<p>如果 <code>accumulate</code> 是 <code>True</code>, <code>tensor</code> 内的元素累加到 <code>self</code> 中. 如果 <code>accumulate</code> 是 <code>False</code>, 在索引包含重复的值时, 行为未定义.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>indices</strong> (<em>LongTensor</em>) – self 的索引</li>
<li><strong>tensor</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 包含需要复制值的 tensor</li>
<li><strong>accumulate</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 如果是 True, 元素累加到 self</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; src = torch.Tensor([[4, 3, 5],
...                     [6, 7, 8]])
&gt;&gt;&gt; src.put_(torch.LongTensor([1, 3]), torch.Tensor([9, 10]))
 4   9   5
 10   7   8
[torch.FloatTensor of size 2x3]</code></pre>


<pre class="codehilite"><code class="language-py">qr() -&gt; (Tensor, Tensor)</code></pre>


<p>请查看 <a href="torch.html#torch.qr" title="torch.qr"><code>torch.qr()</code></a></p>
<pre class="codehilite"><code class="language-py">random_(from=0, to=None, *, generator=None)</code></pre>


<p>将 tensor 用在 [from, to - 1] 上的离散均匀分布进行填充. 如果没有特别说明, 填入的值由本 tensor 的数据类型限定范围. 但是, 对于浮点类型 (floating point types), 如果没有特别说明, 取值范围是<a href="mantissa,小数部分的长度">0, 2^mantissa</a>, 以确保每个数都是可表示的. 例如, &lt;cite&gt;torch.DoubleTensor(1).random_()&lt;/cite&gt; 将均匀分布在[0, 2^53].</p>
<pre class="codehilite"><code class="language-py">reciprocal() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.reciprocal" title="torch.reciprocal"><code>torch.reciprocal()</code></a></p>
<pre class="codehilite"><code class="language-py">reciprocal_() → Tensor</code></pre>


<p><a href="#torch.Tensor.reciprocal" title="torch.Tensor.reciprocal"><code>reciprocal()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">remainder(divisor) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.remainder" title="torch.remainder"><code>torch.remainder()</code></a></p>
<pre class="codehilite"><code class="language-py">remainder_(divisor) → Tensor</code></pre>


<p><a href="#torch.Tensor.remainder" title="torch.Tensor.remainder"><code>remainder()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">renorm(p, dim, maxnorm) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.renorm" title="torch.renorm"><code>torch.renorm()</code></a></p>
<pre class="codehilite"><code class="language-py">renorm_(p, dim, maxnorm) → Tensor</code></pre>


<p><a href="#torch.Tensor.renorm" title="torch.Tensor.renorm"><code>renorm()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">repeat(*sizes)</code></pre>


<p>沿着指定的尺寸重复 tensor.</p>
<p>和 <a href="#torch.Tensor.expand" title="torch.Tensor.expand"><code>expand()</code></a> 不同, 这个函数复制 tensor 的数据.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>*sizes</strong> (<em>torch.Size</em> <em>or</em> <em>int...</em>) – 沿每个维度重复 tensor 的次数</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.Tensor([1, 2, 3])
&gt;&gt;&gt; x.repeat(4, 2)
 1  2  3  1  2  3
 1  2  3  1  2  3
 1  2  3  1  2  3
 1  2  3  1  2  3
[torch.FloatTensor of size 4x6]
&gt;&gt;&gt; x.repeat(4, 2, 1).size()
torch.Size([4, 2, 3])</code></pre>


<pre class="codehilite"><code class="language-py">resize_(*sizes)</code></pre>


<p>将 tensor 的大小调整为指定的大小. 如果元素个数比当前的内存大小大, 就将底层存储大小调整为与新元素数目一致的大小. 如果元素个数比当前内存小, 则底层存储不会被改变. 原来tensor中被保存下来的元素将保持不变, 但新内存将不会被初始化.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>sizes</strong> (<em>torch.Size</em> <em>or</em> <em>int...</em>) – 期望的大小</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.Tensor([[1, 2], [3, 4], [5, 6]])
&gt;&gt;&gt; x.resize_(2, 2)
&gt;&gt;&gt; x
 1  2
 3  4
[torch.FloatTensor of size 2x2]</code></pre>


<pre class="codehilite"><code class="language-py">resize_as_(tensor)</code></pre>


<p>将本 tensor 的大小调整为参数 tensor 的大小. 等效于:</p>
<pre class="codehilite"><code class="language-py">self.resize_(tensor.size())</code></pre>


<pre class="codehilite"><code class="language-py">round() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.round" title="torch.round"><code>torch.round()</code></a></p>
<pre class="codehilite"><code class="language-py">round_() → Tensor</code></pre>


<p><a href="#torch.Tensor.round" title="torch.Tensor.round"><code>round()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">rsqrt() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.rsqrt" title="torch.rsqrt"><code>torch.rsqrt()</code></a></p>
<pre class="codehilite"><code class="language-py">rsqrt_() → Tensor</code></pre>


<p><a href="#torch.Tensor.rsqrt" title="torch.Tensor.rsqrt"><code>rsqrt()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">scatter_(dim, index, src) → Tensor</code></pre>


<p>将 <code>src</code> 中的所有值按照 <a href="#torch.Tensor.index" title="torch.Tensor.index"><code>index</code></a> 确定的索引顺序写入本 tensor 中. 给定的 dim 声明索引的维度, dim 按照 <a href="#torch.Tensor.gather" title="torch.Tensor.gather"><code>gather()</code></a> 中的描述的规则来确定.</p>
<p>注意, 关于 gather, index 的值必须是 &lt;cite&gt;0&lt;/cite&gt; 到 &lt;cite&gt;(self.size(dim) -1)&lt;/cite&gt; 区间, 而且, 属于同一维度的一行的值必须是唯一的.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 索引的轴向</li>
<li><strong>index</strong> (<em>LongTensor</em>) – 散射元素的索引指数</li>
<li><strong>src</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – 散射的源元素</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.rand(2, 5)
&gt;&gt;&gt; x

 0.4319  0.6500  0.4080  0.8760  0.2355
 0.2609  0.4711  0.8486  0.8573  0.1029
[torch.FloatTensor of size 2x5]

&gt;&gt;&gt; torch.zeros(3, 5).scatter_(0, torch.LongTensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)

 0.4319  0.4711  0.8486  0.8760  0.2355
 0.0000  0.6500  0.0000  0.8573  0.0000
 0.2609  0.0000  0.4080  0.0000  0.1029
[torch.FloatTensor of size 3x5]

&gt;&gt;&gt; z = torch.zeros(2, 4).scatter_(1, torch.LongTensor([[2], [3]]), 1.23)
&gt;&gt;&gt; z

 0.0000  0.0000  1.2300  0.0000
 0.0000  0.0000  0.0000  1.2300
[torch.FloatTensor of size 2x4]</code></pre>


<pre class="codehilite"><code class="language-py">select(dim, index) → Tensor or number</code></pre>


<p>沿着 dim 给定的维度, 按照 index 切片. 如果这个 tensor 是一维的, 返回一个数字. 否则, 返回一个给定维度已经被移除的 tensor.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 切片的维度</li>
<li><strong>index</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 用来选取的索引</li>
</ul>
<p>|
| --- | --- |</p>
<p>Note</p>
<p><code>select</code>等效于切片. 例如, <code>`tensor.select(0, index)</code>()<code>等效于</code>tensor[index]<code>和</code>tensor.select(2, index)<code>等效于</code>tensor[:,:,index]`.</p>
<pre class="codehilite"><code class="language-py">set_(source=None, storage_offset=0, size=None, stride=None)</code></pre>


<p>设置底层存储, 大小, 和步长. 如果 <code>source</code> 是一个 tensor 对象, 本 tensor 和该 tensor 共享底层存储, 并且大小和步长一样. 在其中一个 tensor 中改变元素, 会音响到另一个 tensor.</p>
<p>如果 <code>source</code> 是一个 <code>Storage</code>, 则将设置底层内存, 偏移量, 大小和步长.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>source</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>Storage</em>) – 用到的 tensor 或 storage</li>
<li><strong>storage_offset</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – storage 的偏移量</li>
<li><strong>size</strong> (<em>torch.Size</em>) – 期望的大小. 默认为源 tensor 的大小.</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a>) – 期望的步长. 默认为 C 相邻内存的步长.</li>
</ul>
<p>|
| --- | --- |</p>
<pre class="codehilite"><code class="language-py">share_memory_()</code></pre>


<p>将底层存储移到共享内存.</p>
<p>如果底层存储已经在共享内存和CUDA tensor 中, 则这是无操作. 共享内存中的 tensor 不能调整大小.</p>
<pre class="codehilite"><code class="language-py">short()</code></pre>


<p>将这个 tensor 转换为 short 类型</p>
<pre class="codehilite"><code class="language-py">sigmoid() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.sigmoid" title="torch.sigmoid"><code>torch.sigmoid()</code></a></p>
<pre class="codehilite"><code class="language-py">sigmoid_() → Tensor</code></pre>


<p><a href="#torch.Tensor.sigmoid" title="torch.Tensor.sigmoid"><code>sigmoid()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">sign() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.sign" title="torch.sign"><code>torch.sign()</code></a></p>
<pre class="codehilite"><code class="language-py">sign_() → Tensor</code></pre>


<p><a href="#torch.Tensor.sign" title="torch.Tensor.sign"><code>sign()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">sin() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.sin" title="torch.sin"><code>torch.sin()</code></a></p>
<pre class="codehilite"><code class="language-py">sin_() → Tensor</code></pre>


<p><a href="#torch.Tensor.sin" title="torch.Tensor.sin"><code>sin()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">sinh() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.sinh" title="torch.sinh"><code>torch.sinh()</code></a></p>
<pre class="codehilite"><code class="language-py">sinh_() → Tensor</code></pre>


<p><a href="#torch.Tensor.sinh" title="torch.Tensor.sinh"><code>sinh()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">size() → torch.Size</code></pre>


<p>返回 tensor 的大小. 返回的值是 <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><code>tuple</code></a> 的子类.</p>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; torch.Tensor(3, 4, 5).size()
torch.Size([3, 4, 5])</code></pre>


<pre class="codehilite"><code class="language-py">sort(dim=None, descending=False) -&gt; (Tensor, LongTensor)</code></pre>


<p>请查看 <a href="torch.html#torch.sort" title="torch.sort"><code>torch.sort()</code></a></p>
<pre class="codehilite"><code class="language-py">split(split_size, dim=0)</code></pre>


<p>将 tensor 分解成 tensor 元组.</p>
<p>See <a href="torch.html#torch.split" title="torch.split"><code>torch.split()</code></a>.</p>
<pre class="codehilite"><code class="language-py">sqrt() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.sqrt" title="torch.sqrt"><code>torch.sqrt()</code></a></p>
<pre class="codehilite"><code class="language-py">sqrt_() → Tensor</code></pre>


<p><a href="#torch.Tensor.sqrt" title="torch.Tensor.sqrt"><code>sqrt()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">squeeze(dim=None)</code></pre>


<p>请查看 <a href="torch.html#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a></p>
<pre class="codehilite"><code class="language-py">squeeze_(dim=None)</code></pre>


<p><a href="#torch.Tensor.squeeze" title="torch.Tensor.squeeze"><code>squeeze()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">std(dim=None, unbiased=True, keepdim=False) → float</code></pre>


<p>请查看 <a href="torch.html#torch.std" title="torch.std"><code>torch.std()</code></a></p>
<pre class="codehilite"><code class="language-py">storage() → torch.Storage</code></pre>


<p>返回底层存储</p>
<pre class="codehilite"><code class="language-py">storage_offset() → int</code></pre>


<p>按照储存元素个数的偏移返回 tensor 在底层存储中的偏移量(不是按照字节计算).</p>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.Tensor([1, 2, 3, 4, 5])
&gt;&gt;&gt; x.storage_offset()
0
&gt;&gt;&gt; x[3:].storage_offset()
3</code></pre>


<p><em>classmethod</em> <code>storage_type</code>()</p>
<pre class="codehilite"><code class="language-py">stride(dim) → tuple or int</code></pre>


<p>返回 tesnor 的步长. 步长是指按照 dim 指定的维度, 从一个元素到下一个元素需要跳跃的距离. 当没有指定维度, 会计算所有维度的步长, 并返回一个 tuple. 当给定维度时, 返回这个维度的步长.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 期望的需要计算步长的维度.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])
&gt;&gt;&gt; x.stride()
(5, 1)
&gt;&gt;&gt;x.stride(0)
5
&gt;&gt;&gt; x.stride(-1)
1</code></pre>


<pre class="codehilite"><code class="language-py">sub(value, other) → Tensor</code></pre>


<p>从 tensor 中抽取一个标量或张量. 如果 <code>value</code> 和 <code>other</code> 都是给定的, 则在使用之前 <code>other</code>的每一个元素都会被 :attr:<code>value</code> 缩放.</p>
<p>如果 <code>other</code> 是一个tensor, <code>other</code> 的形状必须于基础 tensor 的形状是可广播的 ( <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> ).</p>
<pre class="codehilite"><code class="language-py">sub_(x) → Tensor</code></pre>


<p><a href="#torch.Tensor.sub" title="torch.Tensor.sub"><code>sub()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">sum(dim=None, keepdim=False) → float</code></pre>


<p>请查看 <a href="torch.html#torch.sum" title="torch.sum"><code>torch.sum()</code></a></p>
<pre class="codehilite"><code class="language-py">svd(some=True) -&gt; (Tensor, Tensor, Tensor)</code></pre>


<p>请查看 <a href="torch.html#torch.svd" title="torch.svd"><code>torch.svd()</code></a></p>
<pre class="codehilite"><code class="language-py">symeig(eigenvectors=False, upper=True) -&gt; (Tensor, Tensor)</code></pre>


<p>请查看 <a href="torch.html#torch.symeig" title="torch.symeig"><code>torch.symeig()</code></a></p>
<pre class="codehilite"><code class="language-py">t() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.t" title="torch.t"><code>torch.t()</code></a></p>
<pre class="codehilite"><code class="language-py">t_() → Tensor</code></pre>


<p><a href="#torch.Tensor.t" title="torch.Tensor.t"><code>t()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">take(indices) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.take" title="torch.take"><code>torch.take()</code></a></p>
<pre class="codehilite"><code class="language-py">tan()</code></pre>


<pre class="codehilite"><code class="language-py">tan_() → Tensor</code></pre>


<p><a href="#torch.Tensor.tan" title="torch.Tensor.tan"><code>tan()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">tanh() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.tanh" title="torch.tanh"><code>torch.tanh()</code></a></p>
<pre class="codehilite"><code class="language-py">tanh_() → Tensor</code></pre>


<p><a href="#torch.Tensor.tanh" title="torch.Tensor.tanh"><code>tanh()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">tolist()</code></pre>


<p>返回此 tensor 的嵌套列表表示.</p>
<pre class="codehilite"><code class="language-py">topk(k, dim=None, largest=True, sorted=True) -&gt; (Tensor, LongTensor)</code></pre>


<p>请查看 <a href="torch.html#torch.topk" title="torch.topk"><code>torch.topk()</code></a></p>
<pre class="codehilite"><code class="language-py">trace() → float</code></pre>


<p>请查看 <a href="torch.html#torch.trace" title="torch.trace"><code>torch.trace()</code></a></p>
<pre class="codehilite"><code class="language-py">transpose(dim0, dim1) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.transpose" title="torch.transpose"><code>torch.transpose()</code></a></p>
<pre class="codehilite"><code class="language-py">transpose_(dim0, dim1) → Tensor</code></pre>


<p><a href="#torch.Tensor.transpose" title="torch.Tensor.transpose"><code>transpose()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">tril(k=0) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.tril" title="torch.tril"><code>torch.tril()</code></a></p>
<pre class="codehilite"><code class="language-py">tril_(k=0) → Tensor</code></pre>


<p><a href="#torch.Tensor.tril" title="torch.Tensor.tril"><code>tril()</code></a></p>
<pre class="codehilite"><code class="language-py">triu(k=0) → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.triu" title="torch.triu"><code>torch.triu()</code></a></p>
<pre class="codehilite"><code class="language-py">triu_(k=0) → Tensor</code></pre>


<p><a href="#torch.Tensor.triu" title="torch.Tensor.triu"><code>triu()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">trtrs(A, upper=True, transpose=False, unitriangular=False) -&gt; (Tensor, Tensor)</code></pre>


<p>请查看 <a href="torch.html#torch.trtrs" title="torch.trtrs"><code>torch.trtrs()</code></a></p>
<pre class="codehilite"><code class="language-py">trunc() → Tensor</code></pre>


<p>请查看 <a href="torch.html#torch.trunc" title="torch.trunc"><code>torch.trunc()</code></a></p>
<pre class="codehilite"><code class="language-py">trunc_() → Tensor</code></pre>


<p><a href="#torch.Tensor.trunc" title="torch.Tensor.trunc"><code>trunc()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">type(new_type=None, async=False)</code></pre>


<p>如果没有指定 &lt;cite&gt;new_type&lt;/cite&gt; 则返回该类型 , 否则将此对象转换为指定类型 .</p>
<p>如果已经是正确的类型 , 则不执行复制并直接返回原对象 .</p>
<p>| Parameters: | </p>
<ul>
<li><strong>new_type</strong> (<a href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.6)"><em>type</em></a> <em>or</em> <a href="https://docs.python.org/3/library/string.html#module-string" title="(in Python v3.6)"><em>string</em></a>) – 期望的类型</li>
<li><strong>async</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – 如果为 <code>True</code> , 并且源在锁定内存中而目标在GPU中 , 则副本将与主机异步执行 , 反之亦然 . 否则此参数不起效果 .</li>
</ul>
<p>|
| --- | --- |</p>
<pre class="codehilite"><code class="language-py">type_as(tensor)</code></pre>


<p>将此张量转换为给定 tensor 的类型.</p>
<p>如果 tensor 已经是正确的类型, 这是一个无用的操作. 这相当于:</p>
<pre class="codehilite"><code class="language-py">self.type(tensor.type())</code></pre>


<p>Params: tensor (Tensor): tensor 具有所需的类型</p>
<pre class="codehilite"><code class="language-py">unfold(dim, size, step) → Tensor</code></pre>


<p>返回一个在 <a href="#torch.Tensor.dim" title="torch.Tensor.dim"><code>dim</code></a> 维度上包含所有 <a href="#torch.Tensor.size" title="torch.Tensor.size"><code>size</code></a> 大小切片的 tensor.</p>
<blockquote>
<p><code>step</code> 说明两个切片之间的步长.</p>
</blockquote>
<p>如果 &lt;cite&gt;sizedim&lt;/cite&gt; 是原tensor在 dim 维度原来的大小, 则返回的 tensor 在 &lt;cite&gt;dim&lt;/cite&gt; 维度的大小是 &lt;cite&gt;(sizedim - size) / step + 1&lt;/cite&gt;</p>
<p>一个额外的切片大小的维度已经添加在返回的 tensor 中.</p>
<p>| Parameters: | </p>
<ul>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 需要展开的维度</li>
<li><strong>size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 每一个分片需要展开的大小</li>
<li><strong>step</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – 相邻分片之间的步长</li>
</ul>
<p>|
| --- | --- |</p>
<p>Example:</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.arange(1, 8)
&gt;&gt;&gt; x

 1
 2
 3
 4
 5
 6
 7
[torch.FloatTensor of size 7]

&gt;&gt;&gt; x.unfold(0, 2, 1)

 1  2
 2  3
 3  4
 4  5
 5  6
 6  7
[torch.FloatTensor of size 6x2]

&gt;&gt;&gt; x.unfold(0, 2, 2)

 1  2
 3  4
 5  6
[torch.FloatTensor of size 3x2]</code></pre>


<pre class="codehilite"><code class="language-py">uniform_(from=0, to=1) → Tensor</code></pre>


<p>将 tensor 用从均匀分布中抽样得到的值填充:</p>
<pre class="codehilite"><code class="language-py">unsqueeze(dim)</code></pre>


<p>请查看 <a href="torch.html#torch.unsqueeze" title="torch.unsqueeze"><code>torch.unsqueeze()</code></a></p>
<pre class="codehilite"><code class="language-py">unsqueeze_(dim)</code></pre>


<p><a href="#torch.Tensor.unsqueeze" title="torch.Tensor.unsqueeze"><code>unsqueeze()</code></a> 的 in-place 运算形式</p>
<pre class="codehilite"><code class="language-py">var(dim=None, unbiased=True, keepdim=False) → float</code></pre>


<p>请查看 <a href="torch.html#torch.var" title="torch.var"><code>torch.var()</code></a></p>
<pre class="codehilite"><code class="language-py">view(*args) → Tensor</code></pre>


<p>返回一个有相同数据但大小不同的新的 tensor.</p>
<p>返回的 tensor 与原 tensor 共享相同的数据, 一定有相同数目的元素, 但大小不同. 一个 tensor 必须是连续的 ( <a href="#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code>contiguous()</code></a> ) 才能被查看.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>args</strong> (<em>torch.Size</em> <em>or</em> <em>int...</em>) – 期望的大小</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; x = torch.randn(4, 4)
&gt;&gt;&gt; x.size()
torch.Size([4, 4])
&gt;&gt;&gt; y = x.view(16)
&gt;&gt;&gt; y.size()
torch.Size([16])
&gt;&gt;&gt; z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
&gt;&gt;&gt; z.size()
torch.Size([2, 8])</code></pre>


<pre class="codehilite"><code class="language-py">view_as(tensor)</code></pre>


<p>将该 tensor 作为指定的 tensor 返回查看.</p>
<p>这相当于:</p>
<pre class="codehilite"><code class="language-py">self.view(tensor.size())</code></pre>


<pre class="codehilite"><code class="language-py">zero_()</code></pre>


<p>用0填充该 tensor.</p>
<pre class="codehilite"><code class="language-py">class torch.ByteTensor</code></pre>


<p>下面这些函数方法只存在于 <a href="#torch.ByteTensor" title="torch.ByteTensor"><code>torch.ByteTensor</code></a>.</p>
<pre class="codehilite"><code class="language-py">all() → bool</code></pre>


<p>如果 tensor 里的所有元素都是非零的, 则返回 True, 否在返回 False.</p>
<pre class="codehilite"><code class="language-py">any() → bool</code></pre>


<p>如果 tensor 里的存在元素是非零的, 则返回 True, 否在返回 False.</p>
                
                  
                
              
              
                


              

              <hr/>
              <div align="center">
                  <p><a href="http://www.apachecn.org/" target="_blank"><font face="KaiTi" size="6" color="red">我们一直在努力</font></a><p>
                  <p><a href="https://github.com/apachecn/pytorch-doc-zh/" target="_blank">apachecn/pytorch-doc-zh</a></p>
                  <p><iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&repo=pytorch-doc-zh&type=watch&count=true&v=2" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
                  <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&repo=pytorch-doc-zh&type=star&count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
                  <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&repo=pytorch-doc-zh&type=fork&count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
                  <a target="_blank" href="//shang.qq.com/wpa/qunwpa?idkey=bcee938030cc9e1552deb3bd9617bbbf62d3ec1647e4b60d9cd6b6e8f78ddc03"><img border="0" src="//pub.idqqimg.com/wpa/images/group.png" alt="ML | ApacheCN" title="ML | ApacheCN"></a></p>
                  <div style="text-align:center;margin:0 0 10.5px;"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
                        <ins class="adsbygoogle"
                             style="display:inline-block;width:728px;height:90px"
                             data-ad-client="ca-pub-3565452474788507"
                             data-ad-slot="2543897000"></ins>
                        <script>
                        (adsbygoogle = window.adsbygoogle || []).push({});
                        </script></div>
              </div>

              <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
              <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
              <div id="gitalk-container" class="container-fluid"></div>
              <script type="text/javascript">
                  var gitalk = new Gitalk({
                  clientID: 'f27b87eb424ba43df978',
                  clientSecret: '9b3482a495c5257a1d269d8108b9bfd71f048c3c',
                  repo: 'pytorch-doc-zh',
                  owner: 'apachecn',
                  admin: ['jiangzhonglian'],
                  id: md5(location.pathname),
                  distractionFreeMode: false
                  })
                  gitalk.render('gitalk-container')
              </script>
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../53/" title="torch" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                torch
              </span>
            </div>
          </a>
        
        
          <a href="../55/" title="torch.sparse" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                torch.sparse
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.583bbe55.js"></script>
      
        
        
          
          <script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../assets/javascripts/lunr/lunr.jp.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
    
      
    
  </body>
</html>