



<!DOCTYPE html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="PyTorch 0.3.0 中文文档">
      
      
        <link rel="canonical" href="http://pytorch.apachecn.org/58/">
      
      
        <meta name="author" content="ApacheCN Team">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.3">
    
    
      
        <title>torch.optim - PyTorch 0.3.0 中文文档</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#ff7043">
      
    
    
      <script src="../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    

    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        }
    });
    </script>
    
    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>
    
    <!-- google webmaster -->
    <meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo" />
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="deep-orange" data-md-color-accent="deep-orange">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="../#torchoptim" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="http://pytorch.apachecn.org" title="PyTorch 0.3.0 中文文档" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                PyTorch 0.3.0 中文文档
              </span>
              <span class="md-header-nav__topic">
                torch.optim
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/apachecn/pytorch-doc-zh/" title="前往 Github 仓库" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      pytorch-doc-zh
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href=".." title="主页" class="md-tabs__link">
          主页
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../0/" title="中文教程" class="md-tabs__link">
          中文教程
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../45/" title="中文文档" class="md-tabs__link md-tabs__link--active">
          中文文档
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="http://pytorch.apachecn.org" title="PyTorch 0.3.0 中文文档" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    PyTorch 0.3.0 中文文档
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/apachecn/pytorch-doc-zh/" title="前往 Github 仓库" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      pytorch-doc-zh
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      主页
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        主页
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href=".." title="主页" class="md-nav__link">
      主页
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      中文教程
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        中文教程
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../0/" title="初学者教程" class="md-nav__link">
      初学者教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../1/" title="PyTorch 深度学习: 60 分钟极速入门教程" class="md-nav__link">
      PyTorch 深度学习: 60 分钟极速入门教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../2/" title="PyTorch 是什么？" class="md-nav__link">
      PyTorch 是什么？
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../3/" title="自动求导: 自动微分" class="md-nav__link">
      自动求导: 自动微分
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../4/" title="神经网络" class="md-nav__link">
      神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../5/" title="训练一个分类器" class="md-nav__link">
      训练一个分类器
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../6/" title="可选: 数据并行" class="md-nav__link">
      可选: 数据并行
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../7/" title="PyTorch for former Torch users" class="md-nav__link">
      PyTorch for former Torch users
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../8/" title="Tensors" class="md-nav__link">
      Tensors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../9/" title="Autograd (自动求导)" class="md-nav__link">
      Autograd (自动求导)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../10/" title="nn package" class="md-nav__link">
      nn package
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../11/" title="Multi-GPU examples" class="md-nav__link">
      Multi-GPU examples
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../12/" title="跟着例子学习 PyTorch" class="md-nav__link">
      跟着例子学习 PyTorch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../13/" title="Warm-up: numpy" class="md-nav__link">
      Warm-up: numpy
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../14/" title="PyTorch: Tensors" class="md-nav__link">
      PyTorch: Tensors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../15/" title="PyTorch: 变量和autograd" class="md-nav__link">
      PyTorch: 变量和autograd
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../16/" title="PyTorch: 定义新的autograd函数" class="md-nav__link">
      PyTorch: 定义新的autograd函数
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../17/" title="TensorFlow: 静态图" class="md-nav__link">
      TensorFlow: 静态图
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../18/" title="PyTorch: nn包" class="md-nav__link">
      PyTorch: nn包
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../19/" title="PyTorch: optim包" class="md-nav__link">
      PyTorch: optim包
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../20/" title="PyTorch: 定制化nn模块" class="md-nav__link">
      PyTorch: 定制化nn模块
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../21/" title="PyTorch: 动态控制流程 + 权重共享" class="md-nav__link">
      PyTorch: 动态控制流程 + 权重共享
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../22/" title="迁移学习教程" class="md-nav__link">
      迁移学习教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../23/" title="数据加载和处理教程" class="md-nav__link">
      数据加载和处理教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../24/" title="针对NLP的Pytorch深度学习" class="md-nav__link">
      针对NLP的Pytorch深度学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../25/" title="PyTorch介绍" class="md-nav__link">
      PyTorch介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../26/" title="PyTorch深度学习" class="md-nav__link">
      PyTorch深度学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../27/" title="词汇嵌入:编码词汇语义" class="md-nav__link">
      词汇嵌入:编码词汇语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../28/" title="序列模型和 LSTM 网络（长短记忆网络）" class="md-nav__link">
      序列模型和 LSTM 网络（长短记忆网络）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../29/" title="高级教程: 作出动态决策和 Bi-LSTM CRF" class="md-nav__link">
      高级教程: 作出动态决策和 Bi-LSTM CRF
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../30/" title="中级教程" class="md-nav__link">
      中级教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../31/" title="用字符级RNN分类名称" class="md-nav__link">
      用字符级RNN分类名称
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../32/" title="基与字符级RNN（Char-RNN）的人名生成" class="md-nav__link">
      基与字符级RNN（Char-RNN）的人名生成
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../33/" title="用基于注意力机制的seq2seq神经网络进行翻译" class="md-nav__link">
      用基于注意力机制的seq2seq神经网络进行翻译
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../34/" title="强化学习（DQN）教程" class="md-nav__link">
      强化学习（DQN）教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../35/" title="Writing Distributed Applications with PyTorch" class="md-nav__link">
      Writing Distributed Applications with PyTorch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../36/" title="空间转换网络 (Spatial Transformer Networks) 教程" class="md-nav__link">
      空间转换网络 (Spatial Transformer Networks) 教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../37/" title="高级教程" class="md-nav__link">
      高级教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../38/" title="用 PyTorch 做 神经转换 (Neural Transfer)" class="md-nav__link">
      用 PyTorch 做 神经转换 (Neural Transfer)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../39/" title="使用 numpy 和 scipy 创建扩展" class="md-nav__link">
      使用 numpy 和 scipy 创建扩展
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../40/" title="使用 ONNX 将模型从 PyTorch 迁移到 Caffe2 和 Mobile" class="md-nav__link">
      使用 ONNX 将模型从 PyTorch 迁移到 Caffe2 和 Mobile
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../41/" title="为 pytorch 自定义 C 扩展" class="md-nav__link">
      为 pytorch 自定义 C 扩展
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../42/" title="项目相关" class="md-nav__link">
      项目相关
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../43/" title="项目贡献者" class="md-nav__link">
      项目贡献者
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../44/" title="组织学习交流群" class="md-nav__link">
      组织学习交流群
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" checked>
    
    <label class="md-nav__link" for="nav-3">
      中文文档
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        中文文档
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../45/" title="介绍" class="md-nav__link">
      介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../46/" title="自动求导机制" class="md-nav__link">
      自动求导机制
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../47/" title="广播语义" class="md-nav__link">
      广播语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../48/" title="CUDA 语义" class="md-nav__link">
      CUDA 语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../49/" title="扩展 PyTorch" class="md-nav__link">
      扩展 PyTorch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../50/" title="多进程的最佳实践" class="md-nav__link">
      多进程的最佳实践
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../51/" title="序列化语义" class="md-nav__link">
      序列化语义
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../52/" title="Package 参考" class="md-nav__link">
      Package 参考
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../53/" title="torch" class="md-nav__link">
      torch
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../54/" title="torch.Tensor" class="md-nav__link">
      torch.Tensor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../55/" title="torch.sparse" class="md-nav__link">
      torch.sparse
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../56/" title="torch.Storage" class="md-nav__link">
      torch.Storage
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../57/" title="torch.nn" class="md-nav__link">
      torch.nn
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        torch.optim
      </label>
    
    <a href="./" title="torch.optim" class="md-nav__link md-nav__link--active">
      torch.optim
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#optimizer" title="如何使用 optimizer (优化器)" class="md-nav__link">
    如何使用 optimizer (优化器)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" title="构建" class="md-nav__link">
    构建
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" title="为每个参数单独设置选项" class="md-nav__link">
    为每个参数单独设置选项
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" title="进行单步优化" class="md-nav__link">
    进行单步优化
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#optimizerstep" title="optimizer.step()" class="md-nav__link">
    optimizer.step()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizerstepclosure" title="optimizer.step(closure)" class="md-nav__link">
    optimizer.step(closure)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" title="算法" class="md-nav__link">
    算法
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" title="如何调整学习率" class="md-nav__link">
    如何调整学习率
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../59/" title="Automatic differentiation package - torch.autograd" class="md-nav__link">
      Automatic differentiation package - torch.autograd
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../60/" title="Probability distributions - torch.distributions" class="md-nav__link">
      Probability distributions - torch.distributions
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../61/" title="Multiprocessing package - torch.multiprocessing" class="md-nav__link">
      Multiprocessing package - torch.multiprocessing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../62/" title="Distributed communication package - torch.distributed" class="md-nav__link">
      Distributed communication package - torch.distributed
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../63/" title="Legacy package - torch.legacy" class="md-nav__link">
      Legacy package - torch.legacy
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../64/" title="torch.cuda" class="md-nav__link">
      torch.cuda
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../65/" title="torch.utils.ffi" class="md-nav__link">
      torch.utils.ffi
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../66/" title="torch.utils.data" class="md-nav__link">
      torch.utils.data
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../67/" title="torch.utils.model_zoo" class="md-nav__link">
      torch.utils.model_zoo
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../68/" title="torch.onnx" class="md-nav__link">
      torch.onnx
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../69/" title="torchvision 参考" class="md-nav__link">
      torchvision 参考
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../70/" title="torchvision" class="md-nav__link">
      torchvision
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../71/" title="torchvision.datasets" class="md-nav__link">
      torchvision.datasets
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../72/" title="torchvision.models" class="md-nav__link">
      torchvision.models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../73/" title="torchvision.transforms" class="md-nav__link">
      torchvision.transforms
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../74/" title="torchvision.utils" class="md-nav__link">
      torchvision.utils
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../75/" title="项目相关" class="md-nav__link">
      项目相关
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../76/" title="项目贡献者" class="md-nav__link">
      项目贡献者
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../77/" title="组织学习交流群" class="md-nav__link">
      组织学习交流群
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#optimizer" title="如何使用 optimizer (优化器)" class="md-nav__link">
    如何使用 optimizer (优化器)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" title="构建" class="md-nav__link">
    构建
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" title="为每个参数单独设置选项" class="md-nav__link">
    为每个参数单独设置选项
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" title="进行单步优化" class="md-nav__link">
    进行单步优化
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#optimizerstep" title="optimizer.step()" class="md-nav__link">
    optimizer.step()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizerstepclosure" title="optimizer.step(closure)" class="md-nav__link">
    optimizer.step(closure)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" title="算法" class="md-nav__link">
    算法
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" title="如何调整学习率" class="md-nav__link">
    如何调整学习率
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/apachecn/pytorch-doc-zh/edit/master/docs/58.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="torchoptim">torch.optim</h1>
<p><a href="#module-torch.optim" title="torch.optim"><code>torch.optim</code></a> is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.</p>
<h2 id="optimizer">如何使用 optimizer (优化器)</h2>
<p>为了使用 <a href="#module-torch.optim" title="torch.optim"><code>torch.optim</code></a> 你需要创建一个 optimizer 对象, 这个对象能够保持当前的状态以及依靠梯度计算 来完成参数更新.</p>
<h3 id="_1">构建</h3>
<p>要构建一个 <a href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code>Optimizer</code></a> 你需要一个可迭代的参数 (全部都应该是 <a href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><code>Variable</code></a>) 进行优化. 然后, 你能够设置优化器的参数选项, 例如学习率, 权重衰减等.</p>
<p>Note</p>
<p>如果你需要通过 &lt;cite&gt;.cuda()&lt;/cite&gt; 将模型移动到 GPU 上, 请在构建优化器之前来移动. 模型的参数在进行 &lt;cite&gt;.cuda()&lt;/cite&gt; 之后将变成不同的对象,该对象与之前调用的参数不同.</p>
<p>通常来说, 在对优化器进行构建和调用的时候, 你应该要确保优化参数位于相同的 地点.</p>
<p>例子</p>
<pre class="codehilite"><code class="language-py">optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr = 0.0001)</code></pre>


<h3 id="_2">为每个参数单独设置选项</h3>
<p><a href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code>Optimizer</code></a> 也支持为每个参数单独设置选项. 若要这么做, 不要直接使用 :class: &lt;cite&gt;~torch.autograd.Variable&lt;/cite&gt; 的迭代, 而是使用 <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.6)"><code>dict</code></a> 的迭代. 每一个 dict 都分别定义了一组参数, 并且应该要包含 <code>params</code> 键,这个键对应列表的参数. 其他的键应该与 optimizer 所接受的其他参数的关键字相匹配, 并且会被用于对这组参数的优化.</p>
<p>Note</p>
<p>你仍然能够传递选项作为关键字参数.在未重写这些选项的组中, 它们会被用作默认值. 这非常适用于当你只想改动一个参数组的选项, 但其他参数组的选项不变的情况.</p>
<p>例如, 当我们想指定每一层的学习率时, 这是非常有用的:</p>
<pre class="codehilite"><code class="language-py">optim.SGD([
                {'params': model.base.parameters()},
                {'params': model.classifier.parameters(), 'lr': 1e-3}
            ], lr=1e-2, momentum=0.9)</code></pre>


<p>这意味着 <code>model.base</code> 的参数将会使用 <code>1e-2</code> 的学习率,<code>model.classifier</code> 的参数将会使用 <code>1e-3</code> 的学习率, 并且 <code>0.9</code> 的 momentum 将应用于所有参数.</p>
<h3 id="_3">进行单步优化</h3>
<p>所有的优化器都实现了 <a href="#torch.optim.Optimizer.step" title="torch.optim.Optimizer.step"><code>step()</code></a> 方法, 且更新到所有的参数. 它可以通过以下两种方式来使用:</p>
<h4 id="optimizerstep"><code>optimizer.step()</code></h4>
<p>这是大多数 optimizer 所支持的简化版本. 一旦使用 <a href="autograd.html#torch.autograd.Variable.backward" title="torch.autograd.Variable.backward"><code>backward()</code></a> 之类的函数计算出来梯度之后我们就可以调用这个函数了.</p>
<p>例子</p>
<pre class="codehilite"><code class="language-py">for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()</code></pre>


<h4 id="optimizerstepclosure"><code>optimizer.step(closure)</code></h4>
<p>一些优化算法例如 Conjugate Gradient 和 LBFGS 需要重复多次计算函数, 因此你需要传入一个闭包去允许它们重新计算你的模型. 这个闭包应当清空梯度, 计算损失, 然后返回.</p>
<p>例子</p>
<pre class="codehilite"><code class="language-py">for input, target in dataset:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss
    optimizer.step(closure)</code></pre>


<h2 id="_4">算法</h2>
<pre class="codehilite"><code class="language-py">class torch.optim.Optimizer(params, defaults)</code></pre>


<p>优化器的基类.</p>
<p>Args: * params (iterable): :class:<a href="#id7"><code>](#id5)Variable 或 :class:[</code></a>dict 的迭代, 指定了应该优化哪些参数. * defaults: (dict): 包含了优化选项默认值的字典(一个参数组没有指定的参数选项将会使用默认值).</p>
<pre class="codehilite"><code class="language-py">add_param_group(param_group)</code></pre>


<p>增加一组参数到 <a href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code>Optimizer</code></a> 的 &lt;cite&gt;param_groups&lt;/cite&gt; 里面.</p>
<p>当微调一个预训练好的网络作为冻结层时是有用的, 它能够使用可训练的和可增加的参数到 <a href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code>Optimizer</code></a> 作为一个训练预处理.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>param_group</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – 指定这一组中具有特殊优化选项的那些 Variables 能够被优化.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">load_state_dict(state_dict)</code></pre>


<p>加载优化器状态.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>state_dict</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – 优化器状态. 是调用 <a href="#torch.optim.Optimizer.state_dict" title="torch.optim.Optimizer.state_dict"><code>state_dict()</code></a> 时所返回的对象.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">state_dict()</code></pre>


<p>以 <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.6)"><code>dict</code></a> 的形式返回优化器的状态.</p>
<p>它包含两部分内容:</p>
<ul>
<li>state - 一个包含当前优化状态的字典（dict）, 字典里的内容因优化器的不同而变换.</li>
<li>param_groups - 一个包含所有参数组的字典（dict）.</li>
</ul>
<pre class="codehilite"><code class="language-py">step(closure)</code></pre>


<p>进行单次优化(参数更新).</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>closure</strong> (<a href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) – 一个重新评价模型并返回 loss 的闭包大多数优化器可选择.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">zero_grad()</code></pre>


<p>Clears the gradients of all optimized <code>Variable</code> s.</p>
<pre class="codehilite"><code class="language-py">class torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)</code></pre>


<p>实施 Adadelta 算法.</p>
<p>它在 <a href="https://arxiv.org/abs/1212.5701">ADADELTA: 一种可调节学习率的方法</a> 中提出</p>
<p>Args: * params (iterable): 通过参数迭代去优化或者字典的形式定义参数组. * rho (float, optional): 用来计算平均平方梯度的系数(默认值: 0.9) * eps (float, optional): 增加分母来确保数值稳定性(默认值: 1e-6) * lr (float, optional): 在将 delta 应用于参数之前对其进行系数的缩放(默认值: 1.0) * weight_decay (float, optional): 权重衰减 (L2正则化) (默认值: 0)</p>
<pre class="codehilite"><code class="language-py">step(closure=None)</code></pre>


<p>实行单步优化.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>closure</strong> (<a href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a><em>,</em> <em>optional</em>) – 重新评估模型并返回误差损失的闭包.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">class torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0)</code></pre>


<p>实现 Adagrad 算法.</p>
<p>它在 <a href="http://jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a> 中被提出.</p>
<p>Args: * params (iterable): 迭代的优化参数或者以字典的形式定义参数组 * lr (float, optional): 学习率 (默认值: 1e-2) * lr_decay (float, optional): 学习率衰减 (默认值: 0) * weight_decay (float, optional): 权重衰减 (L2正则化) (默认值: 0)</p>
<pre class="codehilite"><code class="language-py">step(closure=None)</code></pre>


<p>进行单步优化.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>closure</strong> (<a href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a><em>,</em> <em>optional</em>) – 一个重新评价模型并返回误差的闭包.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)</code></pre>


<p>实现 Adam 算法.</p>
<p>它在 <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a> 中被提出.</p>
<p>Args: * params (iterable): 迭代的优化参数或者以字典的形式定义参数组. * lr (float, optional): 学习率 (默认值: 1e-3) * betas (Tuple[float, float], optional): 用来计算梯度和平方梯度的系数 (默认值: (0.9, 0.999)) * eps (float, optional): 增加分母来确保数值稳定性 (默认值: 1e-8) * weight_decay (float, optional): 权重衰减 (L2 正则化) (默认值: 0)</p>
<pre class="codehilite"><code class="language-py">step(closure=None)</code></pre>


<p>进行单步优化.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>closure</strong> (<a href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a><em>,</em> <em>optional</em>) – 一个重新评价模型并返回误差的闭包.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">class torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)</code></pre>


<p>实现上一版本 Adam 算法来适用于 sparse tensors.</p>
<p>在这个变化下,只将显示出来的梯度进行更新存储并且只将这部分梯度应用到参数中.</p>
<p>Args: * params (iterable): 待优化的迭代参数或者是定义了参数组的 dict * lr (float, optional): 学习率 (default: 1e-3) * betas (Tuple[float, float], optional): 用来计算梯度和平方梯度的系数 (默认值: (0.9, 0.999)) * eps (float, optional): 增加分母来确保数值稳定性 (默认值: 1e-8)</p>
<pre class="codehilite"><code class="language-py">step(closure=None)</code></pre>


<p>进行单步优化.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>closure</strong> (<a href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a><em>,</em> <em>optional</em>) – 一个重新评价模型并返回 loss 的闭包, 对于大多数参数来说是可选的.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">class torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)</code></pre>


<p>实现 Adamax 算法 ( Adam 的一种基于无穷范数的变种).</p>
<p>它在 <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a> 中被提出.</p>
<p>Args: * params (iterable): 迭代的优化参数或者以字典的形式定义参数组. * lr (float, optional): 学习率 (默认值: 2e-3) * betas (Tuple[float, float], optional): 用来计算梯度和平方梯度的系数 * eps (float, optional): 增加分母来确保数值稳定性 (默认值: 1e-8) * weight_decay (float, optional): 权重衰减 (L2 正则化) (默认值: 0)</p>
<pre class="codehilite"><code class="language-py">step(closure=None)</code></pre>


<p>进行单步优化.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>closure</strong> (<a href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a><em>,</em> <em>optional</em>) – 一个重新评价模型并返回误差的闭包.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">class torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)</code></pre>


<p>实现平均随机梯度下降算法.</p>
<p>它在 <a href="http://dl.acm.org/citation.cfm?id=131098">Acceleration of stochastic approximation by averaging</a> 中被提出</p>
<p>Args: * params (iterable): 迭代的优化参数或者以字典的形式定义参数组 * lr (float, optional): 学习率 (默认值: 1e-2) * lambd (float, optional): 衰减期 (默认值: 1e-4) * alpha (float, optional): eta 更新的权重 (默认值: 0.75) * t0 (float, optional): 指明在哪一次开始平均化 (默认值: 1e6) * weight_decay (float, optional): 权重衰减 (L2 正则化) (默认值: 0)</p>
<pre class="codehilite"><code class="language-py">step(closure=None)</code></pre>


<p>进行单步优化.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>closure</strong> (<a href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a><em>,</em> <em>optional</em>) – 一个重新评价模型并返回误差的闭包.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">class torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None)</code></pre>


<p>实现 L-BFGS 算法.</p>
<p>Warning</p>
<p>这个 optimizer 不支持为每个参数单独设置选项以及不支持参数组（只能有一个）.</p>
<p>Warning</p>
<p>目前所有的参数不得不都在同一设备上. 这在将来会得到改进.</p>
<p>Note</p>
<p>这是一个内存高度密集的 optimizer (它要求额外的 <code>param_bytes * (history_size + 1)</code> 个字节). 如果它不适应内存, 尝试减小历史规格, 或者使用不同的算法.</p>
<p>Args: * lr (float): 学习率 (默认值: 1) * max_iter (int): 每一步优化的最大迭代次数 (默认值: 20) * max_eval (int): 每一步优化的最大函数评估次数 (默认值: max_iter * 1.25). * tolerance_grad (float): 一阶最优的终止容忍度 (默认值: 1e-5). * tolerance_change (float): 在函数值/参数变化量上的终止容忍度 (默认值: 1e-9). * history_size (int): 更新历史尺寸 (默认值: 100).</p>
<pre class="codehilite"><code class="language-py">step(closure)</code></pre>


<p>进行单步优化.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>closure</strong> (<a href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) – 一个重新评价模型并返回 loss 的闭包, 对于大多数参数来说是可选的.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)</code></pre>


<p>实现 RMSprop 算法.</p>
<p>由 G. Hinton 在此提出 <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">course</a>.</p>
<p>中心版本首次出现在 <a href="https://arxiv.org/pdf/1308.0850v5.pdf">Generating Sequences With Recurrent Neural Networks</a>.</p>
<p>算法: * params (iterable): 待优化的迭代参数或者是定义了参数组的 dict * lr (float, optional): 学习率 (默认值: 1e-2) * momentum (float, optional): 动量因子 (默认值: 0) * alpha (float, optional): 平滑常量 (default: 0.99) * eps (float, optional): 为了增加数值计算的稳定性而加到分母里的项 (默认值: 1e-8) * centered (bool, optional) : 如果为 <code>True</code>, 计算 RMSProp 的中值, 并且用它的方差预测值对梯度进行归一化 * weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</p>
<pre class="codehilite"><code class="language-py">step(closure=None)</code></pre>


<p>Performs a single optimization step.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>closure</strong> (<a href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a><em>,</em> <em>optional</em>) – A closure that reevaluates the model and returns the loss.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">class torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))</code></pre>


<p>实现弹性反向传播算法.</p>
<p>Args: * params (iterable): 待优化的迭代参数或者是定义了参数组的 dict * lr (float, optional): 学习率 (默认值: 1e-2) * etas (Tuple[float, float], optional): 一对 (etaminus, etaplis), t它们分别是乘法</p>
<blockquote>
<p>的增加和减小的因子 (默认值: (0.5, 1.2))</p>
</blockquote>
<ul>
<li>step_sizes (Tuple[float, float], optional): 允许的一对最小和最大的步长 (默认值: (1e-6, 50))</li>
</ul>
<pre class="codehilite"><code class="language-py">step(closure=None)</code></pre>


<p>进行单步优化.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>closure</strong> (<a href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a><em>,</em> <em>optional</em>) – 一个重新评价模型并返回 loss 的闭包, 对于大多数参数来说是可选的.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">class torch.optim.SGD(params, lr=&lt;object object&gt;, momentum=0, dampening=0, weight_decay=0, nesterov=False)</code></pre>


<p>实现随机梯度下降算法（ momentum 可选）.</p>
<p>Nesterov 动量基于 <a href="http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf">On the importance of initialization and momentum in deep learning</a> 中的公式.</p>
<p>Args: * params (iterable): 待优化的迭代参数或者是定义了参数组的 dict * lr (float): 学习率 * momentum (float, optional): 动量因子 (默认值: 0) * weight_decay (float, optional): 权重衰减 (L2 正则化) (默认值: 0) * dampening (float, optional): 动量的抑制因子 (默认值: 0) * nesterov (bool, optional): 使用 Nesterov 动量 (默认值: False)</p>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
&gt;&gt;&gt; optimizer.zero_grad()
&gt;&gt;&gt; loss_fn(model(input), target).backward()
&gt;&gt;&gt; optimizer.step()</code></pre>


<p>Note</p>
<p>带有动量 /Nesterov 的 SGD 的实现稍微不同于 Sutskever 等人以及其他框架中的实现. 考虑动量的具体情况, 更新可以写成</p>
<pre class="codehilite"><code class="language-py">\[\begin{split}v = \rho * v + g \\ p = p - lr * v\end{split}\]</code></pre>


<p>其中 p, g, v 和 <code>\(\rho\)</code> 分别是参数、梯度、速度和动量.</p>
<p>这跟 Sutskever 等人以及其他框架的实现是相反的, 它们采用这样的更新.</p>
<pre class="codehilite"><code class="language-py">\[\begin{split}v = \rho * v + lr * g \\ p = p - v\end{split}\]</code></pre>


<p>Nesterov 的版本也相应的被修改了.</p>
<pre class="codehilite"><code class="language-py">step(closure=None)</code></pre>


<p>进行单步优化.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>closure</strong> (<a href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a><em>,</em> <em>optional</em>) – 一个重新评价模型并返回 loss 的闭包, 对于大多数参数来说是可选的.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="_5">如何调整学习率</h2>
<table>
<thead>
<tr>
<th>mod:</th>
<th>&lt;cite&gt;torch.optim.lr_scheduler&lt;/cite&gt; 基于循环的次数提供了一些方法来调节学习率.</th>
</tr>
</thead>
<tbody>
<tr>
<td>class:</td>
<td>&lt;cite&gt;torch.optim.lr_scheduler.ReduceLROnPlateau&lt;/cite&gt; 基于验证测量结果来设置不同的学习率.</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-py">class torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)</code></pre>


<p>将每个参数组的学习速率设置为给定函数的初始LR. 当 last_epoch=-1, 设置出事的 lr 作为 lr.</p>
<p>Args: * optimizer (Optimizer): 封装好的优化器. * lr_lambda (function or list): 计算给定整数参数历元的乘法因子的函数, 或者一系列的此类函数, 每组的一个都在 optimizer.param_groups 中. * last_epoch (int): 最后一个 epoch 的索引. 默认值: -1.</p>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; # Assuming optimizer has two groups.
&gt;&gt;&gt; lambda1 = lambda epoch: epoch // 30
&gt;&gt;&gt; lambda2 = lambda epoch: 0.95 ** epoch
&gt;&gt;&gt; scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])
&gt;&gt;&gt; for epoch in range(100):
&gt;&gt;&gt;     scheduler.step()
&gt;&gt;&gt;     train(...)
&gt;&gt;&gt;     validate(...)</code></pre>


<pre class="codehilite"><code class="language-py">class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)</code></pre>


<p>通过 gamma 在每一个 epoch 里面的 step_size 设置每个参数组的初始学习率衰减变量. 当 last_epoch=-1, 设置初始 lr 为 lr.</p>
<p>Args: * optimizer (Optimizer): 封装好的优化器. * step_size (int): 学习率衰减周期. * gamma (float): 学习率衰减的乘法因子. 默认值: 0.1. * last_epoch (int): 最后一个 epoch 的索引. 默认值: -1.</p>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; # Assuming optimizer uses lr = 0.5 for all groups
&gt;&gt;&gt; # lr = 0.05     if epoch &lt; 30
&gt;&gt;&gt; # lr = 0.005    if 30 &lt;= epoch &lt; 60
&gt;&gt;&gt; # lr = 0.0005   if 60 &lt;= epoch &lt; 90
&gt;&gt;&gt; # ...
&gt;&gt;&gt; scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
&gt;&gt;&gt; for epoch in range(100):
&gt;&gt;&gt;     scheduler.step()
&gt;&gt;&gt;     train(...)
&gt;&gt;&gt;     validate(...)</code></pre>


<pre class="codehilite"><code class="language-py">class torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)</code></pre>


<p>一旦 epoch 的数量达到了一个临界点通过 gamma 在每一个 epoch 里面的 step_size 设置每个参数 组的初始学习率衰减变量.当 last_epoch=-1, 设置初始 lr 作为 lr.</p>
<p>Args: * optimizer (Optimizer): 封装好的优化器. * milestones (list): epoch 索引列表. 必须为递增的. * gamma (float): 学习率衰减的乘法因子.</p>
<blockquote>
<p>默认值: 0.1.</p>
</blockquote>
<ul>
<li>last_epoch (int): 最后一个 epoch 的索引. 默认值: -1.</li>
</ul>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; # Assuming optimizer uses lr = 0.5 for all groups
&gt;&gt;&gt; # lr = 0.05     if epoch &lt; 30
&gt;&gt;&gt; # lr = 0.005    if 30 &lt;= epoch &lt; 80
&gt;&gt;&gt; # lr = 0.0005   if epoch &gt;= 80
&gt;&gt;&gt; scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)
&gt;&gt;&gt; for epoch in range(100):
&gt;&gt;&gt;     scheduler.step()
&gt;&gt;&gt;     train(...)
&gt;&gt;&gt;     validate(...)</code></pre>


<pre class="codehilite"><code class="language-py">class torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1)</code></pre>


<p>通过 gamma 在每一个 epoch 里面的 step_size 设置每个参数组的初始学习率衰减变量 . 当 last_epoch=-1, 设置初始 lr 作为 lr. :param optimizer: 封装好的优化器. :type optimizer: Optimizer :param gamma: 学习率衰减的乘法因子. :type gamma: float :param last_epoch: 最后一个 epoch 的索引. 默认值: -1. :type last_epoch: int</p>
<pre class="codehilite"><code class="language-py">class torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)</code></pre>


<p>当一个指标已经停止提升时减少学习率.模型通常受益于通过一次2-10的学习停止因素减少学习率 这个调度程序读取一个指标质量 以及看到 ‘patience’ 的数量在一个 epoch 里面如果没有提升, 这时学习率已经减小.</p>
<p>Args: * optimizer (Optimizer): 封装好的优化器.</p>
<blockquote>
<p>mode (str): &lt;cite&gt;min&lt;/cite&gt;, &lt;cite&gt;max&lt;/cite&gt; 其中一个. 在 &lt;cite&gt;min&lt;/cite&gt; 模块下,当质量监测已经 停止下降时 lr 将被减少; 在 &lt;cite&gt;max&lt;/cite&gt; 模块下 当质量监测已经停止上升时 lr 将 被减少. 默认值: ‘min’.</p>
</blockquote>
<ul>
<li>factor (float): 哪个学习率将会被减少的影响因子 .</li>
<li>new_lr = lr * factor. 默认值: 0.1.</li>
<li>patience (int): epoch 中没有改善的次数, 学习率将会降低. . 默认值: 10.</li>
<li>verbose (bool): 若为 <code>True</code>, 每次更新打印信息到控制台输出. 默认值: <code>False</code>.</li>
<li>threshold (float): 测量新的最佳阈值, 只关注有重大意义的改变. 默认值: 1e-4.</li>
<li>threshold_mode (str): &lt;cite&gt;rel&lt;/cite&gt;, &lt;cite&gt;abs&lt;/cite&gt; 中的一个. 在 &lt;cite&gt;rel&lt;/cite&gt; 模式下, dynamic_threshold = best * ( 1 + threshold ) 在 ‘max’ 模式下或者在 &lt;cite&gt;min&lt;/cite&gt; 模式下 best * ( 1 - threshold ) . 在 &lt;cite&gt;abs&lt;/cite&gt; 模式下, dynamic_threshold = best + threshold 在 &lt;cite&gt;max&lt;/cite&gt; 模式下或者在 &lt;cite&gt;min&lt;/cite&gt; 模式下 best - threshold . 默认值: ‘rel’.</li>
<li>cooldown (int): lr 已经减少之后去等待最佳的正常操作之前的 epoch 数目. 默认值: 0.</li>
<li>min_lr (float or list): 一个列表的标量.所有参数组或每个组的学习率下限. 默认值: 0.</li>
<li>eps (float): lr 最小的衰减值适应于. 如果新 lr 和旧 lr 之间的差异小于 eps,更新可以忽略. 默认值: 1e-8.</li>
</ul>
<p>Example</p>
<pre class="codehilite"><code class="language-py">&gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
&gt;&gt;&gt; scheduler = ReduceLROnPlateau(optimizer, 'min')
&gt;&gt;&gt; for epoch in range(10):
&gt;&gt;&gt;     train(...)
&gt;&gt;&gt;     val_loss = validate(...)
&gt;&gt;&gt;     # Note that step should be called after validate()
&gt;&gt;&gt;     scheduler.step(val_loss)</code></pre>
                
                  
                
              
              
                


              

              <hr/>
              <div align="center">
                  <p><a href="http://www.apachecn.org/" target="_blank"><font face="KaiTi" size="6" color="red">我们一直在努力</font></a><p>
                  <p><a href="https://github.com/apachecn/pytorch-doc-zh/" target="_blank">apachecn/pytorch-doc-zh</a></p>
                  <p><iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&repo=pytorch-doc-zh&type=watch&count=true&v=2" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
                  <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&repo=pytorch-doc-zh&type=star&count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
                  <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&repo=pytorch-doc-zh&type=fork&count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
                  <a target="_blank" href="//shang.qq.com/wpa/qunwpa?idkey=bcee938030cc9e1552deb3bd9617bbbf62d3ec1647e4b60d9cd6b6e8f78ddc03"><img border="0" src="//pub.idqqimg.com/wpa/images/group.png" alt="ML | ApacheCN" title="ML | ApacheCN"></a></p>
                  <div style="text-align:center;margin:0 0 10.5px;"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
                        <ins class="adsbygoogle"
                             style="display:inline-block;width:728px;height:90px"
                             data-ad-client="ca-pub-3565452474788507"
                             data-ad-slot="2543897000"></ins>
                        <script>
                        (adsbygoogle = window.adsbygoogle || []).push({});
                        </script></div>
              </div>

              <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
              <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
              <div id="gitalk-container" class="container-fluid"></div>
              <script type="text/javascript">
                  var gitalk = new Gitalk({
                  clientID: 'f27b87eb424ba43df978',
                  clientSecret: '9b3482a495c5257a1d269d8108b9bfd71f048c3c',
                  repo: 'pytorch-doc-zh',
                  owner: 'apachecn',
                  admin: ['jiangzhonglian'],
                  id: md5(location.pathname),
                  distractionFreeMode: false
                  })
                  gitalk.render('gitalk-container')
              </script>
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../57/" title="torch.nn" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                torch.nn
              </span>
            </div>
          </a>
        
        
          <a href="../59/" title="Automatic differentiation package - torch.autograd" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                Automatic differentiation package - torch.autograd
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.583bbe55.js"></script>
      
        
        
          
          <script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../assets/javascripts/lunr/lunr.jp.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
    
      
    
  </body>
</html>